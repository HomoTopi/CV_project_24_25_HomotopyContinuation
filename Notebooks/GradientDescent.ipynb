{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7bf8c720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import HomoTopiContinuation.Plotter.Plotter as Plotter\n",
    "import HomoTopiContinuation.SceneGenerator.scene_generator as sg\n",
    "import HomoTopiContinuation.Rectifier.standard_rectifier as sr\n",
    "import HomoTopiContinuation.Rectifier.homotopyc_rectifier as hr\n",
    "import HomoTopiContinuation.Rectifier.numeric_rectifier as nr\n",
    "from HomoTopiContinuation.DataStructures.datastructures import Circle, ConicJax, ConicsJax, Homography\n",
    "from HomoTopiContinuation.ConicWarper.ConicWarper import ConicWarper\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "064f42ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circle 1:\n",
      "[[     1      0   -310]\n",
      " [     0      1   -100]\n",
      " [  -310   -100 105200]]\n",
      "[1.0, 0.0, 1.0, -620.0, -200.0, 105200.0]\n",
      "Circle 2:\n",
      "[[     1      0   -330]\n",
      " [     0      1   -150]\n",
      " [  -330   -150 131375]]\n",
      "[1.0, 0.0, 1.0, -660.0, -300.0, 131375.0]\n",
      "Circle 3:\n",
      "[[     1      0   -350]\n",
      " [     0      1   -200]\n",
      " [  -350   -200 162475]]\n",
      "[1.0, 0.0, 1.0, -700.0, -400.0, 162475.0]\n",
      "[Scene Described]\n",
      "[Scene Generated]\n"
     ]
    }
   ],
   "source": [
    "def sceneDefinition() -> sg.SceneDescription:\n",
    "    # Parameters\n",
    "    f = 100\n",
    "    theta = 60\n",
    "\n",
    "    # Define the circles\n",
    "    c1 = Circle(\n",
    "        np.array([10+300, 100]), 30)\n",
    "    c2 = Circle(\n",
    "        np.array([30+300, 150]), 5)\n",
    "    c3 = Circle(\n",
    "        np.array([50+300, 200]), 5)\n",
    "\n",
    "    print(\"Circle 1:\")\n",
    "    print(c1.to_conic().M)\n",
    "    print([float(p) for p in c1.to_conic().to_algebraic_form()])\n",
    "    print(\"Circle 2:\")\n",
    "    print(c2.to_conic().M)\n",
    "    print([float(p) for p in c2.to_conic().to_algebraic_form()])\n",
    "    print(\"Circle 3:\")\n",
    "    print(c3.to_conic().M)\n",
    "    print([float(p) for p in c3.to_conic().to_algebraic_form()])\n",
    "\n",
    "    offset = np.array([0, 0, 100])\n",
    "    noiseScale = 0.00000000\n",
    "\n",
    "    return sg.SceneDescription(f, theta, offset, c1, c2, c3, noiseScale)\n",
    "\n",
    "sceneDescription = sceneDefinition()\n",
    "print(\"[Scene Described]\")\n",
    "\n",
    "img = sg.SceneGenerator().generate_scene(sceneDescription)\n",
    "print(\"[Scene Generated]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b96516aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\polimi\\master\\2sem\\cv_homotopy_project\\repo\\src\\HomoTopiContinuation\\Plotter\\Plotter.py:80: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  self.ax.legend()\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the results\n",
    "plotter = Plotter.Plotter(2, 2, title=\"Base Scene with Circles\")\n",
    "\n",
    "plotter.plotScene(\n",
    "    sceneDescription=sceneDescription,\n",
    "    img=img,\n",
    ")\n",
    "\n",
    "plotter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "03d0d0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "warpedConics = ConicsJax(img.C_img_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a9b1ce3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9.9999997e-05  5.3693575e-04 -9.4693571e-02]\n",
      " [ 5.3693575e-04  4.2488202e-03 -6.8078732e-01]\n",
      " [-9.4693571e-02 -6.8078732e-01  1.1138927e+02]]\n",
      "[[ 9.9999997e-05  5.7157676e-04 -1.0015768e-01]\n",
      " [ 5.7157676e-04  5.3804806e-03 -8.5275388e-01]\n",
      " [-1.0015768e-01 -8.5275388e-01  1.3748347e+02]]\n",
      "[[ 9.9999997e-05  6.0621777e-04 -1.0562178e-01]\n",
      " [ 6.0621777e-04  6.6598905e-03 -1.0480258e+00]\n",
      " [-1.0562178e-01 -1.0480258e+00  1.6725375e+02]]\n"
     ]
    }
   ],
   "source": [
    "for c in warpedConics:\n",
    "    print(c._M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4c5560a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(2.4793816, dtype=float32)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss(H_inv: jnp.array, conic: ConicJax) -> float:\n",
    "    warpedConic = conic.applyHomographyFromInv(H_inv)\n",
    "    axes = warpedConic.computeSemiAxes()\n",
    "    return jnp.abs(jnp.log(axes[0]/axes[1]))\n",
    "\n",
    "loss = jax.jit(loss, static_argnames=['conic'])\n",
    "\n",
    "def lossConics(H_inv:jnp.array, conics: ConicsJax) -> float:\n",
    "    return jnp.mean(\n",
    "        jnp.array([\n",
    "            loss(H_inv, conics.C1),\n",
    "            loss(H_inv, conics.C2),\n",
    "            loss(H_inv, conics.C3)\n",
    "        ])\n",
    "    )\n",
    "\n",
    "lossConics = jax.jit(lossConics, static_argnames=['conics'])\n",
    "\n",
    "H_inv = jnp.eye(3)\n",
    "lossConics(H_inv, warpedConics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "80878c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[  -0.9762672 ,    0.21463752,    0.        ],\n",
       "       [   0.21463624,    0.97626716,    0.        ],\n",
       "       [ 212.90979   , -182.20139   ,    0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient = jax.grad(lossConics, argnums=0)\n",
    "gradient = jax.jit(gradient, static_argnames=['conics'])\n",
    "\n",
    "H_inv = jnp.eye(3)\n",
    "gradient(H_inv, warpedConics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0a2bc8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 2.4785964488983154\n",
      "Iteration 1, Loss: 2.477811574935913\n",
      "Iteration 2, Loss: 2.477027416229248\n",
      "Iteration 3, Loss: 2.476243734359741\n",
      "Iteration 4, Loss: 2.4754600524902344\n",
      "Iteration 5, Loss: 2.474677085876465\n",
      "Iteration 6, Loss: 2.4738948345184326\n",
      "Iteration 7, Loss: 2.4731128215789795\n",
      "Iteration 8, Loss: 2.4723310470581055\n",
      "Iteration 9, Loss: 2.4715499877929688\n",
      "Iteration 10, Loss: 2.4707694053649902\n",
      "Iteration 11, Loss: 2.46998929977417\n",
      "Iteration 12, Loss: 2.4692091941833496\n",
      "Iteration 13, Loss: 2.4684300422668457\n",
      "Iteration 14, Loss: 2.467651128768921\n",
      "Iteration 15, Loss: 2.4668726921081543\n",
      "Iteration 16, Loss: 2.466094732284546\n",
      "Iteration 17, Loss: 2.4653167724609375\n",
      "Iteration 18, Loss: 2.4645395278930664\n",
      "Iteration 19, Loss: 2.4637629985809326\n",
      "Iteration 20, Loss: 2.462986707687378\n",
      "Iteration 21, Loss: 2.4622106552124023\n",
      "Iteration 22, Loss: 2.461435317993164\n",
      "Iteration 23, Loss: 2.460660457611084\n",
      "Iteration 24, Loss: 2.459885597229004\n",
      "Iteration 25, Loss: 2.459111213684082\n",
      "Iteration 26, Loss: 2.4583375453948975\n",
      "Iteration 27, Loss: 2.457564353942871\n",
      "Iteration 28, Loss: 2.4567911624908447\n",
      "Iteration 29, Loss: 2.4560189247131348\n",
      "Iteration 30, Loss: 2.4552464485168457\n",
      "Iteration 31, Loss: 2.454474925994873\n",
      "Iteration 32, Loss: 2.4537036418914795\n",
      "Iteration 33, Loss: 2.452932834625244\n",
      "Iteration 34, Loss: 2.452162265777588\n",
      "Iteration 35, Loss: 2.45139217376709\n",
      "Iteration 36, Loss: 2.45062255859375\n",
      "Iteration 37, Loss: 2.4498534202575684\n",
      "Iteration 38, Loss: 2.449084520339966\n",
      "Iteration 39, Loss: 2.4483160972595215\n",
      "Iteration 40, Loss: 2.4475479125976562\n",
      "Iteration 41, Loss: 2.446780204772949\n",
      "Iteration 42, Loss: 2.4460132122039795\n",
      "Iteration 43, Loss: 2.445246458053589\n",
      "Iteration 44, Loss: 2.4444799423217773\n",
      "Iteration 45, Loss: 2.443713903427124\n",
      "Iteration 46, Loss: 2.442948341369629\n",
      "Iteration 47, Loss: 2.442183017730713\n",
      "Iteration 48, Loss: 2.441418409347534\n",
      "Iteration 49, Loss: 2.4406538009643555\n",
      "Iteration 50, Loss: 2.439889907836914\n",
      "Iteration 51, Loss: 2.4391260147094727\n",
      "Iteration 52, Loss: 2.4383628368377686\n",
      "Iteration 53, Loss: 2.4375996589660645\n",
      "Iteration 54, Loss: 2.436837673187256\n",
      "Iteration 55, Loss: 2.436075210571289\n",
      "Iteration 56, Loss: 2.4353137016296387\n",
      "Iteration 57, Loss: 2.4345521926879883\n",
      "Iteration 58, Loss: 2.433791160583496\n",
      "Iteration 59, Loss: 2.433030605316162\n",
      "Iteration 60, Loss: 2.432270050048828\n",
      "Iteration 61, Loss: 2.4315104484558105\n",
      "Iteration 62, Loss: 2.430751323699951\n",
      "Iteration 63, Loss: 2.4299919605255127\n",
      "Iteration 64, Loss: 2.4292333126068115\n",
      "Iteration 65, Loss: 2.4284749031066895\n",
      "Iteration 66, Loss: 2.4277169704437256\n",
      "Iteration 67, Loss: 2.42695951461792\n",
      "Iteration 68, Loss: 2.4262022972106934\n",
      "Iteration 69, Loss: 2.425445318222046\n",
      "Iteration 70, Loss: 2.4246888160705566\n",
      "Iteration 71, Loss: 2.4239325523376465\n",
      "Iteration 72, Loss: 2.4231770038604736\n",
      "Iteration 73, Loss: 2.422421455383301\n",
      "Iteration 74, Loss: 2.4216666221618652\n",
      "Iteration 75, Loss: 2.4209117889404297\n",
      "Iteration 76, Loss: 2.4201574325561523\n",
      "Iteration 77, Loss: 2.4194037914276123\n",
      "Iteration 78, Loss: 2.418649911880493\n",
      "Iteration 79, Loss: 2.4178967475891113\n",
      "Iteration 80, Loss: 2.4171438217163086\n",
      "Iteration 81, Loss: 2.416391372680664\n",
      "Iteration 82, Loss: 2.4156394004821777\n",
      "Iteration 83, Loss: 2.4148874282836914\n",
      "Iteration 84, Loss: 2.4141359329223633\n",
      "Iteration 85, Loss: 2.4133849143981934\n",
      "Iteration 86, Loss: 2.4126343727111816\n",
      "Iteration 87, Loss: 2.411883592605591\n",
      "Iteration 88, Loss: 2.4111335277557373\n",
      "Iteration 89, Loss: 2.410383701324463\n",
      "Iteration 90, Loss: 2.409634590148926\n",
      "Iteration 91, Loss: 2.4088854789733887\n",
      "Iteration 92, Loss: 2.4081368446350098\n",
      "Iteration 93, Loss: 2.407388687133789\n",
      "Iteration 94, Loss: 2.4066405296325684\n",
      "Iteration 95, Loss: 2.4058926105499268\n",
      "Iteration 96, Loss: 2.4051456451416016\n",
      "Iteration 97, Loss: 2.4043986797332764\n",
      "Iteration 98, Loss: 2.4036519527435303\n",
      "Iteration 99, Loss: 2.4029054641723633\n",
      "Iteration 100, Loss: 2.4021596908569336\n",
      "Iteration 101, Loss: 2.401413917541504\n",
      "Iteration 102, Loss: 2.4006686210632324\n",
      "Iteration 103, Loss: 2.399923801422119\n",
      "Iteration 104, Loss: 2.3991787433624268\n",
      "Iteration 105, Loss: 2.398434638977051\n",
      "Iteration 106, Loss: 2.3976902961730957\n",
      "Iteration 107, Loss: 2.396946907043457\n",
      "Iteration 108, Loss: 2.3962035179138184\n",
      "Iteration 109, Loss: 2.395460367202759\n",
      "Iteration 110, Loss: 2.3947176933288574\n",
      "Iteration 111, Loss: 2.3939757347106934\n",
      "Iteration 112, Loss: 2.393233299255371\n",
      "Iteration 113, Loss: 2.392491579055786\n",
      "Iteration 114, Loss: 2.3917500972747803\n",
      "Iteration 115, Loss: 2.3910090923309326\n",
      "Iteration 116, Loss: 2.390268087387085\n",
      "Iteration 117, Loss: 2.3895277976989746\n",
      "Iteration 118, Loss: 2.3887877464294434\n",
      "Iteration 119, Loss: 2.388047933578491\n",
      "Iteration 120, Loss: 2.3873085975646973\n",
      "Iteration 121, Loss: 2.386569023132324\n",
      "Iteration 122, Loss: 2.3858304023742676\n",
      "Iteration 123, Loss: 2.385091781616211\n",
      "Iteration 124, Loss: 2.3843531608581543\n",
      "Iteration 125, Loss: 2.383615255355835\n",
      "Iteration 126, Loss: 2.382877826690674\n",
      "Iteration 127, Loss: 2.3821401596069336\n",
      "Iteration 128, Loss: 2.3814034461975098\n",
      "Iteration 129, Loss: 2.380666732788086\n",
      "Iteration 130, Loss: 2.379929780960083\n",
      "Iteration 131, Loss: 2.3791940212249756\n",
      "Iteration 132, Loss: 2.37845778465271\n",
      "Iteration 133, Loss: 2.3777225017547607\n",
      "Iteration 134, Loss: 2.3769872188568115\n",
      "Iteration 135, Loss: 2.3762521743774414\n",
      "Iteration 136, Loss: 2.3755176067352295\n",
      "Iteration 137, Loss: 2.374783515930176\n",
      "Iteration 138, Loss: 2.374049186706543\n",
      "Iteration 139, Loss: 2.3733155727386475\n",
      "Iteration 140, Loss: 2.37258243560791\n",
      "Iteration 141, Loss: 2.3718490600585938\n",
      "Iteration 142, Loss: 2.3711159229278564\n",
      "Iteration 143, Loss: 2.3703837394714355\n",
      "Iteration 144, Loss: 2.3696513175964355\n",
      "Iteration 145, Loss: 2.3689193725585938\n",
      "Iteration 146, Loss: 2.368187665939331\n",
      "Iteration 147, Loss: 2.3674559593200684\n",
      "Iteration 148, Loss: 2.366724967956543\n",
      "Iteration 149, Loss: 2.3659939765930176\n",
      "Iteration 150, Loss: 2.3652634620666504\n",
      "Iteration 151, Loss: 2.3645331859588623\n",
      "Iteration 152, Loss: 2.3638033866882324\n",
      "Iteration 153, Loss: 2.3630733489990234\n",
      "Iteration 154, Loss: 2.3623437881469727\n",
      "Iteration 155, Loss: 2.36161470413208\n",
      "Iteration 156, Loss: 2.3608860969543457\n",
      "Iteration 157, Loss: 2.360157012939453\n",
      "Iteration 158, Loss: 2.359428644180298\n",
      "Iteration 159, Loss: 2.358700752258301\n",
      "Iteration 160, Loss: 2.357973098754883\n",
      "Iteration 161, Loss: 2.357245445251465\n",
      "Iteration 162, Loss: 2.356518268585205\n",
      "Iteration 163, Loss: 2.3557913303375244\n",
      "Iteration 164, Loss: 2.3550643920898438\n",
      "Iteration 165, Loss: 2.3543381690979004\n",
      "Iteration 166, Loss: 2.353611707687378\n",
      "Iteration 167, Loss: 2.3528859615325928\n",
      "Iteration 168, Loss: 2.3521604537963867\n",
      "Iteration 169, Loss: 2.3514351844787598\n",
      "Iteration 170, Loss: 2.350710391998291\n",
      "Iteration 171, Loss: 2.349985122680664\n",
      "Iteration 172, Loss: 2.3492608070373535\n",
      "Iteration 173, Loss: 2.348536252975464\n",
      "Iteration 174, Loss: 2.3478119373321533\n",
      "Iteration 175, Loss: 2.34708833694458\n",
      "Iteration 176, Loss: 2.3463644981384277\n",
      "Iteration 177, Loss: 2.345641613006592\n",
      "Iteration 178, Loss: 2.344918727874756\n",
      "Iteration 179, Loss: 2.34419584274292\n",
      "Iteration 180, Loss: 2.343473196029663\n",
      "Iteration 181, Loss: 2.3427510261535645\n",
      "Iteration 182, Loss: 2.342029094696045\n",
      "Iteration 183, Loss: 2.3413071632385254\n",
      "Iteration 184, Loss: 2.340585947036743\n",
      "Iteration 185, Loss: 2.339864730834961\n",
      "Iteration 186, Loss: 2.3391435146331787\n",
      "Iteration 187, Loss: 2.3384227752685547\n",
      "Iteration 188, Loss: 2.3377022743225098\n",
      "Iteration 189, Loss: 2.336982250213623\n",
      "Iteration 190, Loss: 2.3362622261047363\n",
      "Iteration 191, Loss: 2.3355424404144287\n",
      "Iteration 192, Loss: 2.3348228931427\n",
      "Iteration 193, Loss: 2.334103584289551\n",
      "Iteration 194, Loss: 2.3333845138549805\n",
      "Iteration 195, Loss: 2.3326659202575684\n",
      "Iteration 196, Loss: 2.3319473266601562\n",
      "Iteration 197, Loss: 2.3312289714813232\n",
      "Iteration 198, Loss: 2.3305106163024902\n",
      "Iteration 199, Loss: 2.3297929763793945\n",
      "Iteration 200, Loss: 2.329075574874878\n",
      "Iteration 201, Loss: 2.3283581733703613\n",
      "Iteration 202, Loss: 2.3276407718658447\n",
      "Iteration 203, Loss: 2.3269243240356445\n",
      "Iteration 204, Loss: 2.3262076377868652\n",
      "Iteration 205, Loss: 2.325490951538086\n",
      "Iteration 206, Loss: 2.324775218963623\n",
      "Iteration 207, Loss: 2.324059247970581\n",
      "Iteration 208, Loss: 2.323343276977539\n",
      "Iteration 209, Loss: 2.3226280212402344\n",
      "Iteration 210, Loss: 2.3219125270843506\n",
      "Iteration 211, Loss: 2.321197509765625\n",
      "Iteration 212, Loss: 2.3204827308654785\n",
      "Iteration 213, Loss: 2.319768190383911\n",
      "Iteration 214, Loss: 2.3190536499023438\n",
      "Iteration 215, Loss: 2.3183398246765137\n",
      "Iteration 216, Loss: 2.3176259994506836\n",
      "Iteration 217, Loss: 2.3169124126434326\n",
      "Iteration 218, Loss: 2.3161988258361816\n",
      "Iteration 219, Loss: 2.3154854774475098\n",
      "Iteration 220, Loss: 2.314772367477417\n",
      "Iteration 221, Loss: 2.3140599727630615\n",
      "Iteration 222, Loss: 2.313347578048706\n",
      "Iteration 223, Loss: 2.3126349449157715\n",
      "Iteration 224, Loss: 2.311923027038574\n",
      "Iteration 225, Loss: 2.311210870742798\n",
      "Iteration 226, Loss: 2.3104991912841797\n",
      "Iteration 227, Loss: 2.3097875118255615\n",
      "Iteration 228, Loss: 2.3090765476226807\n",
      "Iteration 229, Loss: 2.3083655834198\n",
      "Iteration 230, Loss: 2.307654619216919\n",
      "Iteration 231, Loss: 2.306943893432617\n",
      "Iteration 232, Loss: 2.3062336444854736\n",
      "Iteration 233, Loss: 2.30552339553833\n",
      "Iteration 234, Loss: 2.3048136234283447\n",
      "Iteration 235, Loss: 2.3041036128997803\n",
      "Iteration 236, Loss: 2.303394079208374\n",
      "Iteration 237, Loss: 2.3026845455169678\n",
      "Iteration 238, Loss: 2.301975727081299\n",
      "Iteration 239, Loss: 2.301266670227051\n",
      "Iteration 240, Loss: 2.300557851791382\n",
      "Iteration 241, Loss: 2.299849033355713\n",
      "Iteration 242, Loss: 2.299140691757202\n",
      "Iteration 243, Loss: 2.2984328269958496\n",
      "Iteration 244, Loss: 2.297724723815918\n",
      "Iteration 245, Loss: 2.2970170974731445\n",
      "Iteration 246, Loss: 2.296309471130371\n",
      "Iteration 247, Loss: 2.2956020832061768\n",
      "Iteration 248, Loss: 2.2948946952819824\n",
      "Iteration 249, Loss: 2.2941880226135254\n",
      "Iteration 250, Loss: 2.2934815883636475\n",
      "Iteration 251, Loss: 2.2927746772766113\n",
      "Iteration 252, Loss: 2.2920684814453125\n",
      "Iteration 253, Loss: 2.2913618087768555\n",
      "Iteration 254, Loss: 2.290656089782715\n",
      "Iteration 255, Loss: 2.289950370788574\n",
      "Iteration 256, Loss: 2.2892446517944336\n",
      "Iteration 257, Loss: 2.288539409637451\n",
      "Iteration 258, Loss: 2.2878341674804688\n",
      "Iteration 259, Loss: 2.2871291637420654\n",
      "Iteration 260, Loss: 2.286424398422241\n",
      "Iteration 261, Loss: 2.285719871520996\n",
      "Iteration 262, Loss: 2.285015344619751\n",
      "Iteration 263, Loss: 2.284311056137085\n",
      "Iteration 264, Loss: 2.283607006072998\n",
      "Iteration 265, Loss: 2.2829031944274902\n",
      "Iteration 266, Loss: 2.2821993827819824\n",
      "Iteration 267, Loss: 2.2814958095550537\n",
      "Iteration 268, Loss: 2.280792236328125\n",
      "Iteration 269, Loss: 2.2800891399383545\n",
      "Iteration 270, Loss: 2.279386520385742\n",
      "Iteration 271, Loss: 2.2786834239959717\n",
      "Iteration 272, Loss: 2.2779808044433594\n",
      "Iteration 273, Loss: 2.2772786617279053\n",
      "Iteration 274, Loss: 2.276576042175293\n",
      "Iteration 275, Loss: 2.275874137878418\n",
      "Iteration 276, Loss: 2.275172233581543\n",
      "Iteration 277, Loss: 2.274470567703247\n",
      "Iteration 278, Loss: 2.2737691402435303\n",
      "Iteration 279, Loss: 2.2730672359466553\n",
      "Iteration 280, Loss: 2.2723660469055176\n",
      "Iteration 281, Loss: 2.271665096282959\n",
      "Iteration 282, Loss: 2.2709641456604004\n",
      "Iteration 283, Loss: 2.270263671875\n",
      "Iteration 284, Loss: 2.2695631980895996\n",
      "Iteration 285, Loss: 2.268862724304199\n",
      "Iteration 286, Loss: 2.268162727355957\n",
      "Iteration 287, Loss: 2.2674622535705566\n",
      "Iteration 288, Loss: 2.2667624950408936\n",
      "Iteration 289, Loss: 2.2660629749298096\n",
      "Iteration 290, Loss: 2.2653634548187256\n",
      "Iteration 291, Loss: 2.2646641731262207\n",
      "Iteration 292, Loss: 2.2639646530151367\n",
      "Iteration 293, Loss: 2.263265609741211\n",
      "Iteration 294, Loss: 2.2625668048858643\n",
      "Iteration 295, Loss: 2.2618682384490967\n",
      "Iteration 296, Loss: 2.26116943359375\n",
      "Iteration 297, Loss: 2.2604711055755615\n",
      "Iteration 298, Loss: 2.259772777557373\n",
      "Iteration 299, Loss: 2.2590746879577637\n",
      "Iteration 300, Loss: 2.2583770751953125\n",
      "Iteration 301, Loss: 2.257678985595703\n",
      "Iteration 302, Loss: 2.256981611251831\n",
      "Iteration 303, Loss: 2.256284236907959\n",
      "Iteration 304, Loss: 2.255587100982666\n",
      "Iteration 305, Loss: 2.254889965057373\n",
      "Iteration 306, Loss: 2.254192590713501\n",
      "Iteration 307, Loss: 2.253495931625366\n",
      "Iteration 308, Loss: 2.2527995109558105\n",
      "Iteration 309, Loss: 2.2521026134490967\n",
      "Iteration 310, Loss: 2.25140643119812\n",
      "Iteration 311, Loss: 2.2507100105285645\n",
      "Iteration 312, Loss: 2.250014066696167\n",
      "Iteration 313, Loss: 2.2493181228637695\n",
      "Iteration 314, Loss: 2.2486226558685303\n",
      "Iteration 315, Loss: 2.247926712036133\n",
      "Iteration 316, Loss: 2.2472314834594727\n",
      "Iteration 317, Loss: 2.2465357780456543\n",
      "Iteration 318, Loss: 2.2458407878875732\n",
      "Iteration 319, Loss: 2.245145797729492\n",
      "Iteration 320, Loss: 2.2444510459899902\n",
      "Iteration 321, Loss: 2.243756055831909\n",
      "Iteration 322, Loss: 2.2430613040924072\n",
      "Iteration 323, Loss: 2.2423667907714844\n",
      "Iteration 324, Loss: 2.2416725158691406\n",
      "Iteration 325, Loss: 2.240978240966797\n",
      "Iteration 326, Loss: 2.2402842044830322\n",
      "Iteration 327, Loss: 2.2395901679992676\n",
      "Iteration 328, Loss: 2.2388968467712402\n",
      "Iteration 329, Loss: 2.2382030487060547\n",
      "Iteration 330, Loss: 2.237509250640869\n",
      "Iteration 331, Loss: 2.236815929412842\n",
      "Iteration 332, Loss: 2.2361226081848145\n",
      "Iteration 333, Loss: 2.235429525375366\n",
      "Iteration 334, Loss: 2.234736919403076\n",
      "Iteration 335, Loss: 2.234043836593628\n",
      "Iteration 336, Loss: 2.2333507537841797\n",
      "Iteration 337, Loss: 2.2326583862304688\n",
      "Iteration 338, Loss: 2.2319657802581787\n",
      "Iteration 339, Loss: 2.231273651123047\n",
      "Iteration 340, Loss: 2.230581283569336\n",
      "Iteration 341, Loss: 2.229888916015625\n",
      "Iteration 342, Loss: 2.2291972637176514\n",
      "Iteration 343, Loss: 2.2285056114196777\n",
      "Iteration 344, Loss: 2.227813720703125\n",
      "Iteration 345, Loss: 2.2271223068237305\n",
      "Iteration 346, Loss: 2.226430654525757\n",
      "Iteration 347, Loss: 2.2257397174835205\n",
      "Iteration 348, Loss: 2.225048542022705\n",
      "Iteration 349, Loss: 2.2243576049804688\n",
      "Iteration 350, Loss: 2.223666191101074\n",
      "Iteration 351, Loss: 2.222975730895996\n",
      "Iteration 352, Loss: 2.222285032272339\n",
      "Iteration 353, Loss: 2.2215943336486816\n",
      "Iteration 354, Loss: 2.2209038734436035\n",
      "Iteration 355, Loss: 2.2202134132385254\n",
      "Iteration 356, Loss: 2.2195236682891846\n",
      "Iteration 357, Loss: 2.2188334465026855\n",
      "Iteration 358, Loss: 2.2181432247161865\n",
      "Iteration 359, Loss: 2.2174530029296875\n",
      "Iteration 360, Loss: 2.216763496398926\n",
      "Iteration 361, Loss: 2.216073513031006\n",
      "Iteration 362, Loss: 2.2153842449188232\n",
      "Iteration 363, Loss: 2.2146949768066406\n",
      "Iteration 364, Loss: 2.214005470275879\n",
      "Iteration 365, Loss: 2.2133164405822754\n",
      "Iteration 366, Loss: 2.212627410888672\n",
      "Iteration 367, Loss: 2.21193790435791\n",
      "Iteration 368, Loss: 2.211249351501465\n",
      "Iteration 369, Loss: 2.2105607986450195\n",
      "Iteration 370, Loss: 2.209872245788574\n",
      "Iteration 371, Loss: 2.20918345451355\n",
      "Iteration 372, Loss: 2.208495616912842\n",
      "Iteration 373, Loss: 2.2078065872192383\n",
      "Iteration 374, Loss: 2.207118511199951\n",
      "Iteration 375, Loss: 2.206430435180664\n",
      "Iteration 376, Loss: 2.205742120742798\n",
      "Iteration 377, Loss: 2.205054521560669\n",
      "Iteration 378, Loss: 2.204366683959961\n",
      "Iteration 379, Loss: 2.203678846359253\n",
      "Iteration 380, Loss: 2.202991485595703\n",
      "Iteration 381, Loss: 2.2023043632507324\n",
      "Iteration 382, Loss: 2.2016165256500244\n",
      "Iteration 383, Loss: 2.2009291648864746\n",
      "Iteration 384, Loss: 2.200241804122925\n",
      "Iteration 385, Loss: 2.199554920196533\n",
      "Iteration 386, Loss: 2.1988677978515625\n",
      "Iteration 387, Loss: 2.198180675506592\n",
      "Iteration 388, Loss: 2.1974945068359375\n",
      "Iteration 389, Loss: 2.196807384490967\n",
      "Iteration 390, Loss: 2.1961207389831543\n",
      "Iteration 391, Loss: 2.1954336166381836\n",
      "Iteration 392, Loss: 2.1947476863861084\n",
      "Iteration 393, Loss: 2.194061040878296\n",
      "Iteration 394, Loss: 2.1933746337890625\n",
      "Iteration 395, Loss: 2.1926889419555664\n",
      "Iteration 396, Loss: 2.192002773284912\n",
      "Iteration 397, Loss: 2.191316604614258\n",
      "Iteration 398, Loss: 2.1906309127807617\n",
      "Iteration 399, Loss: 2.1899452209472656\n",
      "Iteration 400, Loss: 2.1892595291137695\n",
      "Iteration 401, Loss: 2.1885735988616943\n",
      "Iteration 402, Loss: 2.1878881454467773\n",
      "Iteration 403, Loss: 2.1872029304504395\n",
      "Iteration 404, Loss: 2.1865172386169434\n",
      "Iteration 405, Loss: 2.1858317852020264\n",
      "Iteration 406, Loss: 2.1851468086242676\n",
      "Iteration 407, Loss: 2.1844615936279297\n",
      "Iteration 408, Loss: 2.18377685546875\n",
      "Iteration 409, Loss: 2.183091640472412\n",
      "Iteration 410, Loss: 2.1824071407318115\n",
      "Iteration 411, Loss: 2.1817221641540527\n",
      "Iteration 412, Loss: 2.181037425994873\n",
      "Iteration 413, Loss: 2.1803526878356934\n",
      "Iteration 414, Loss: 2.1796681880950928\n",
      "Iteration 415, Loss: 2.178983688354492\n",
      "Iteration 416, Loss: 2.1782994270324707\n",
      "Iteration 417, Loss: 2.177615165710449\n",
      "Iteration 418, Loss: 2.1769309043884277\n",
      "Iteration 419, Loss: 2.176246404647827\n",
      "Iteration 420, Loss: 2.1755623817443848\n",
      "Iteration 421, Loss: 2.1748785972595215\n",
      "Iteration 422, Loss: 2.1741950511932373\n",
      "Iteration 423, Loss: 2.1735105514526367\n",
      "Iteration 424, Loss: 2.1728267669677734\n",
      "Iteration 425, Loss: 2.1721434593200684\n",
      "Iteration 426, Loss: 2.171459674835205\n",
      "Iteration 427, Loss: 2.170775890350342\n",
      "Iteration 428, Loss: 2.1700925827026367\n",
      "Iteration 429, Loss: 2.1694090366363525\n",
      "Iteration 430, Loss: 2.1687259674072266\n",
      "Iteration 431, Loss: 2.1680421829223633\n",
      "Iteration 432, Loss: 2.1673591136932373\n",
      "Iteration 433, Loss: 2.1666760444641113\n",
      "Iteration 434, Loss: 2.1659927368164062\n",
      "Iteration 435, Loss: 2.1653096675872803\n",
      "Iteration 436, Loss: 2.164626359939575\n",
      "Iteration 437, Loss: 2.1639435291290283\n",
      "Iteration 438, Loss: 2.1632604598999023\n",
      "Iteration 439, Loss: 2.1625776290893555\n",
      "Iteration 440, Loss: 2.161895275115967\n",
      "Iteration 441, Loss: 2.16121244430542\n",
      "Iteration 442, Loss: 2.1605300903320312\n",
      "Iteration 443, Loss: 2.1598474979400635\n",
      "Iteration 444, Loss: 2.1591649055480957\n",
      "Iteration 445, Loss: 2.158482551574707\n",
      "Iteration 446, Loss: 2.1578001976013184\n",
      "Iteration 447, Loss: 2.1571176052093506\n",
      "Iteration 448, Loss: 2.15643572807312\n",
      "Iteration 449, Loss: 2.1557533740997314\n",
      "Iteration 450, Loss: 2.155071258544922\n",
      "Iteration 451, Loss: 2.1543893814086914\n",
      "Iteration 452, Loss: 2.1537070274353027\n",
      "Iteration 453, Loss: 2.1530251502990723\n",
      "Iteration 454, Loss: 2.152343273162842\n",
      "Iteration 455, Loss: 2.151660680770874\n",
      "Iteration 456, Loss: 2.1509790420532227\n",
      "Iteration 457, Loss: 2.1502976417541504\n",
      "Iteration 458, Loss: 2.149615526199341\n",
      "Iteration 459, Loss: 2.1489341259002686\n",
      "Iteration 460, Loss: 2.148252487182617\n",
      "Iteration 461, Loss: 2.1475706100463867\n",
      "Iteration 462, Loss: 2.1468896865844727\n",
      "Iteration 463, Loss: 2.146207809448242\n",
      "Iteration 464, Loss: 2.14552640914917\n",
      "Iteration 465, Loss: 2.144845485687256\n",
      "Iteration 466, Loss: 2.1441636085510254\n",
      "Iteration 467, Loss: 2.143482208251953\n",
      "Iteration 468, Loss: 2.142801284790039\n",
      "Iteration 469, Loss: 2.142119884490967\n",
      "Iteration 470, Loss: 2.1414384841918945\n",
      "Iteration 471, Loss: 2.1407575607299805\n",
      "Iteration 472, Loss: 2.1400763988494873\n",
      "Iteration 473, Loss: 2.139395236968994\n",
      "Iteration 474, Loss: 2.138714075088501\n",
      "Iteration 475, Loss: 2.138032913208008\n",
      "Iteration 476, Loss: 2.137352466583252\n",
      "Iteration 477, Loss: 2.136671543121338\n",
      "Iteration 478, Loss: 2.135990858078003\n",
      "Iteration 479, Loss: 2.1353096961975098\n",
      "Iteration 480, Loss: 2.1346287727355957\n",
      "Iteration 481, Loss: 2.1339478492736816\n",
      "Iteration 482, Loss: 2.133267402648926\n",
      "Iteration 483, Loss: 2.1325864791870117\n",
      "Iteration 484, Loss: 2.131906032562256\n",
      "Iteration 485, Loss: 2.131225347518921\n",
      "Iteration 486, Loss: 2.130544662475586\n",
      "Iteration 487, Loss: 2.12986421585083\n",
      "Iteration 488, Loss: 2.129183292388916\n",
      "Iteration 489, Loss: 2.1285030841827393\n",
      "Iteration 490, Loss: 2.1278226375579834\n",
      "Iteration 491, Loss: 2.1271419525146484\n",
      "Iteration 492, Loss: 2.1264615058898926\n",
      "Iteration 493, Loss: 2.125781536102295\n",
      "Iteration 494, Loss: 2.12510085105896\n",
      "Iteration 495, Loss: 2.124420642852783\n",
      "Iteration 496, Loss: 2.1237406730651855\n",
      "Iteration 497, Loss: 2.1230602264404297\n",
      "Iteration 498, Loss: 2.122380018234253\n",
      "Iteration 499, Loss: 2.121699810028076\n",
      "Iteration 500, Loss: 2.1210193634033203\n",
      "Iteration 501, Loss: 2.1203389167785645\n",
      "Iteration 502, Loss: 2.119658946990967\n",
      "Iteration 503, Loss: 2.118978977203369\n",
      "Iteration 504, Loss: 2.1182985305786133\n",
      "Iteration 505, Loss: 2.1176183223724365\n",
      "Iteration 506, Loss: 2.1169381141662598\n",
      "Iteration 507, Loss: 2.116258144378662\n",
      "Iteration 508, Loss: 2.1155786514282227\n",
      "Iteration 509, Loss: 2.1148977279663086\n",
      "Iteration 510, Loss: 2.114218235015869\n",
      "Iteration 511, Loss: 2.1135380268096924\n",
      "Iteration 512, Loss: 2.1128575801849365\n",
      "Iteration 513, Loss: 2.112177848815918\n",
      "Iteration 514, Loss: 2.1114978790283203\n",
      "Iteration 515, Loss: 2.1108176708221436\n",
      "Iteration 516, Loss: 2.110137939453125\n",
      "Iteration 517, Loss: 2.1094579696655273\n",
      "Iteration 518, Loss: 2.1087779998779297\n",
      "Iteration 519, Loss: 2.108098030090332\n",
      "Iteration 520, Loss: 2.1074185371398926\n",
      "Iteration 521, Loss: 2.106738805770874\n",
      "Iteration 522, Loss: 2.106058359146118\n",
      "Iteration 523, Loss: 2.1053786277770996\n",
      "Iteration 524, Loss: 2.104698896408081\n",
      "Iteration 525, Loss: 2.1040189266204834\n",
      "Iteration 526, Loss: 2.1033389568328857\n",
      "Iteration 527, Loss: 2.102659225463867\n",
      "Iteration 528, Loss: 2.1019792556762695\n",
      "Iteration 529, Loss: 2.10129976272583\n",
      "Iteration 530, Loss: 2.1006200313568115\n",
      "Iteration 531, Loss: 2.0999395847320557\n",
      "Iteration 532, Loss: 2.099260091781616\n",
      "Iteration 533, Loss: 2.0985803604125977\n",
      "Iteration 534, Loss: 2.097900390625\n",
      "Iteration 535, Loss: 2.0972208976745605\n",
      "Iteration 536, Loss: 2.096540927886963\n",
      "Iteration 537, Loss: 2.0958609580993652\n",
      "Iteration 538, Loss: 2.0951809883117676\n",
      "Iteration 539, Loss: 2.094501495361328\n",
      "Iteration 540, Loss: 2.0938212871551514\n",
      "Iteration 541, Loss: 2.093141555786133\n",
      "Iteration 542, Loss: 2.092461585998535\n",
      "Iteration 543, Loss: 2.0917820930480957\n",
      "Iteration 544, Loss: 2.091101884841919\n",
      "Iteration 545, Loss: 2.090421676635742\n",
      "Iteration 546, Loss: 2.089742660522461\n",
      "Iteration 547, Loss: 2.089062452316284\n",
      "Iteration 548, Loss: 2.0883822441101074\n",
      "Iteration 549, Loss: 2.087702512741089\n",
      "Iteration 550, Loss: 2.0870227813720703\n",
      "Iteration 551, Loss: 2.0863428115844727\n",
      "Iteration 552, Loss: 2.085662603378296\n",
      "Iteration 553, Loss: 2.0849828720092773\n",
      "Iteration 554, Loss: 2.0843029022216797\n",
      "Iteration 555, Loss: 2.083622694015503\n",
      "Iteration 556, Loss: 2.0829429626464844\n",
      "Iteration 557, Loss: 2.0822629928588867\n",
      "Iteration 558, Loss: 2.081583261489868\n",
      "Iteration 559, Loss: 2.0809028148651123\n",
      "Iteration 560, Loss: 2.0802230834960938\n",
      "Iteration 561, Loss: 2.079543113708496\n",
      "Iteration 562, Loss: 2.0788629055023193\n",
      "Iteration 563, Loss: 2.0781829357147217\n",
      "Iteration 564, Loss: 2.077502965927124\n",
      "Iteration 565, Loss: 2.0768227577209473\n",
      "Iteration 566, Loss: 2.0761430263519287\n",
      "Iteration 567, Loss: 2.0754623413085938\n",
      "Iteration 568, Loss: 2.074782609939575\n",
      "Iteration 569, Loss: 2.0741024017333984\n",
      "Iteration 570, Loss: 2.0734219551086426\n",
      "Iteration 571, Loss: 2.072742223739624\n",
      "Iteration 572, Loss: 2.07206130027771\n",
      "Iteration 573, Loss: 2.0713815689086914\n",
      "Iteration 574, Loss: 2.0707011222839355\n",
      "Iteration 575, Loss: 2.0700206756591797\n",
      "Iteration 576, Loss: 2.069340229034424\n",
      "Iteration 577, Loss: 2.068660259246826\n",
      "Iteration 578, Loss: 2.0679798126220703\n",
      "Iteration 579, Loss: 2.0672993659973145\n",
      "Iteration 580, Loss: 2.0666184425354004\n",
      "Iteration 581, Loss: 2.0659384727478027\n",
      "Iteration 582, Loss: 2.0652570724487305\n",
      "Iteration 583, Loss: 2.064577102661133\n",
      "Iteration 584, Loss: 2.0638961791992188\n",
      "Iteration 585, Loss: 2.063216209411621\n",
      "Iteration 586, Loss: 2.062535285949707\n",
      "Iteration 587, Loss: 2.061854362487793\n",
      "Iteration 588, Loss: 2.061173915863037\n",
      "Iteration 589, Loss: 2.060493230819702\n",
      "Iteration 590, Loss: 2.059812545776367\n",
      "Iteration 591, Loss: 2.059131622314453\n",
      "Iteration 592, Loss: 2.0584511756896973\n",
      "Iteration 593, Loss: 2.057769775390625\n",
      "Iteration 594, Loss: 2.057089328765869\n",
      "Iteration 595, Loss: 2.056407928466797\n",
      "Iteration 596, Loss: 2.055727005004883\n",
      "Iteration 597, Loss: 2.0550460815429688\n",
      "Iteration 598, Loss: 2.0543651580810547\n",
      "Iteration 599, Loss: 2.0536839962005615\n",
      "Iteration 600, Loss: 2.0530028343200684\n",
      "Iteration 601, Loss: 2.0523219108581543\n",
      "Iteration 602, Loss: 2.051640510559082\n",
      "Iteration 603, Loss: 2.0509591102600098\n",
      "Iteration 604, Loss: 2.0502777099609375\n",
      "Iteration 605, Loss: 2.0495972633361816\n",
      "Iteration 606, Loss: 2.048915386199951\n",
      "Iteration 607, Loss: 2.0482335090637207\n",
      "Iteration 608, Loss: 2.0475521087646484\n",
      "Iteration 609, Loss: 2.046870470046997\n",
      "Iteration 610, Loss: 2.046189069747925\n",
      "Iteration 611, Loss: 2.0455076694488525\n",
      "Iteration 612, Loss: 2.044825792312622\n",
      "Iteration 613, Loss: 2.04414439201355\n",
      "Iteration 614, Loss: 2.0434625148773193\n",
      "Iteration 615, Loss: 2.042780637741089\n",
      "Iteration 616, Loss: 2.0420985221862793\n",
      "Iteration 617, Loss: 2.041417121887207\n",
      "Iteration 618, Loss: 2.0407352447509766\n",
      "Iteration 619, Loss: 2.040052890777588\n",
      "Iteration 620, Loss: 2.0393707752227783\n",
      "Iteration 621, Loss: 2.0386886596679688\n",
      "Iteration 622, Loss: 2.038006544113159\n",
      "Iteration 623, Loss: 2.0373241901397705\n",
      "Iteration 624, Loss: 2.036642074584961\n",
      "Iteration 625, Loss: 2.0359597206115723\n",
      "Iteration 626, Loss: 2.0352771282196045\n",
      "Iteration 627, Loss: 2.0345945358276367\n",
      "Iteration 628, Loss: 2.033912181854248\n",
      "Iteration 629, Loss: 2.0332298278808594\n",
      "Iteration 630, Loss: 2.0325465202331543\n",
      "Iteration 631, Loss: 2.0318641662597656\n",
      "Iteration 632, Loss: 2.0311813354492188\n",
      "Iteration 633, Loss: 2.03049898147583\n",
      "Iteration 634, Loss: 2.029815673828125\n",
      "Iteration 635, Loss: 2.029132604598999\n",
      "Iteration 636, Loss: 2.028449773788452\n",
      "Iteration 637, Loss: 2.027766466140747\n",
      "Iteration 638, Loss: 2.027083396911621\n",
      "Iteration 639, Loss: 2.026400327682495\n",
      "Iteration 640, Loss: 2.025716781616211\n",
      "Iteration 641, Loss: 2.025033950805664\n",
      "Iteration 642, Loss: 2.0243499279022217\n",
      "Iteration 643, Loss: 2.0236668586730957\n",
      "Iteration 644, Loss: 2.0229833126068115\n",
      "Iteration 645, Loss: 2.0222995281219482\n",
      "Iteration 646, Loss: 2.021616220474243\n",
      "Iteration 647, Loss: 2.020932674407959\n",
      "Iteration 648, Loss: 2.0202484130859375\n",
      "Iteration 649, Loss: 2.019564390182495\n",
      "Iteration 650, Loss: 2.0188803672790527\n",
      "Iteration 651, Loss: 2.0181965827941895\n",
      "Iteration 652, Loss: 2.017512321472168\n",
      "Iteration 653, Loss: 2.0168280601501465\n",
      "Iteration 654, Loss: 2.016143798828125\n",
      "Iteration 655, Loss: 2.0154597759246826\n",
      "Iteration 656, Loss: 2.014774799346924\n",
      "Iteration 657, Loss: 2.0140905380249023\n",
      "Iteration 658, Loss: 2.0134057998657227\n",
      "Iteration 659, Loss: 2.012721061706543\n",
      "Iteration 660, Loss: 2.0120368003845215\n",
      "Iteration 661, Loss: 2.0113515853881836\n",
      "Iteration 662, Loss: 2.010666847229004\n",
      "Iteration 663, Loss: 2.009981632232666\n",
      "Iteration 664, Loss: 2.009296417236328\n",
      "Iteration 665, Loss: 2.0086114406585693\n",
      "Iteration 666, Loss: 2.0079259872436523\n",
      "Iteration 667, Loss: 2.0072407722473145\n",
      "Iteration 668, Loss: 2.0065553188323975\n",
      "Iteration 669, Loss: 2.0058698654174805\n",
      "Iteration 670, Loss: 2.0051839351654053\n",
      "Iteration 671, Loss: 2.0044989585876465\n",
      "Iteration 672, Loss: 2.0038132667541504\n",
      "Iteration 673, Loss: 2.003127098083496\n",
      "Iteration 674, Loss: 2.00244140625\n",
      "Iteration 675, Loss: 2.0017547607421875\n",
      "Iteration 676, Loss: 2.0010690689086914\n",
      "Iteration 677, Loss: 2.000382900238037\n",
      "Iteration 678, Loss: 1.999696135520935\n",
      "Iteration 679, Loss: 1.9990100860595703\n",
      "Iteration 680, Loss: 1.9983234405517578\n",
      "Iteration 681, Loss: 1.997637152671814\n",
      "Iteration 682, Loss: 1.996950387954712\n",
      "Iteration 683, Loss: 1.9962635040283203\n",
      "Iteration 684, Loss: 1.9955768585205078\n",
      "Iteration 685, Loss: 1.9948896169662476\n",
      "Iteration 686, Loss: 1.994202971458435\n",
      "Iteration 687, Loss: 1.9935157299041748\n",
      "Iteration 688, Loss: 1.9928282499313354\n",
      "Iteration 689, Loss: 1.9921410083770752\n",
      "Iteration 690, Loss: 1.9914534091949463\n",
      "Iteration 691, Loss: 1.9907662868499756\n",
      "Iteration 692, Loss: 1.9900784492492676\n",
      "Iteration 693, Loss: 1.9893907308578491\n",
      "Iteration 694, Loss: 1.9887025356292725\n",
      "Iteration 695, Loss: 1.988014578819275\n",
      "Iteration 696, Loss: 1.9873266220092773\n",
      "Iteration 697, Loss: 1.9866384267807007\n",
      "Iteration 698, Loss: 1.985950231552124\n",
      "Iteration 699, Loss: 1.9852619171142578\n",
      "Iteration 700, Loss: 1.9845733642578125\n",
      "Iteration 701, Loss: 1.9838852882385254\n",
      "Iteration 702, Loss: 1.983196496963501\n",
      "Iteration 703, Loss: 1.9825074672698975\n",
      "Iteration 704, Loss: 1.9818181991577148\n",
      "Iteration 705, Loss: 1.9811294078826904\n",
      "Iteration 706, Loss: 1.9804408550262451\n",
      "Iteration 707, Loss: 1.979751467704773\n",
      "Iteration 708, Loss: 1.9790619611740112\n",
      "Iteration 709, Loss: 1.978372573852539\n",
      "Iteration 710, Loss: 1.9776828289031982\n",
      "Iteration 711, Loss: 1.9769935607910156\n",
      "Iteration 712, Loss: 1.9763034582138062\n",
      "Iteration 713, Loss: 1.9756135940551758\n",
      "Iteration 714, Loss: 1.9749234914779663\n",
      "Iteration 715, Loss: 1.9742335081100464\n",
      "Iteration 716, Loss: 1.9735430479049683\n",
      "Iteration 717, Loss: 1.9728527069091797\n",
      "Iteration 718, Loss: 1.9721622467041016\n",
      "Iteration 719, Loss: 1.9714715480804443\n",
      "Iteration 720, Loss: 1.9707807302474976\n",
      "Iteration 721, Loss: 1.9700899124145508\n",
      "Iteration 722, Loss: 1.969398856163025\n",
      "Iteration 723, Loss: 1.9687080383300781\n",
      "Iteration 724, Loss: 1.9680169820785522\n",
      "Iteration 725, Loss: 1.9673254489898682\n",
      "Iteration 726, Loss: 1.9666341543197632\n",
      "Iteration 727, Loss: 1.9659422636032104\n",
      "Iteration 728, Loss: 1.9652509689331055\n",
      "Iteration 729, Loss: 1.9645588397979736\n",
      "Iteration 730, Loss: 1.963866949081421\n",
      "Iteration 731, Loss: 1.9631747007369995\n",
      "Iteration 732, Loss: 1.9624824523925781\n",
      "Iteration 733, Loss: 1.9617900848388672\n",
      "Iteration 734, Loss: 1.9610975980758667\n",
      "Iteration 735, Loss: 1.9604051113128662\n",
      "Iteration 736, Loss: 1.959712028503418\n",
      "Iteration 737, Loss: 1.9590190649032593\n",
      "Iteration 738, Loss: 1.9583261013031006\n",
      "Iteration 739, Loss: 1.9576330184936523\n",
      "Iteration 740, Loss: 1.9569400548934937\n",
      "Iteration 741, Loss: 1.956246256828308\n",
      "Iteration 742, Loss: 1.9555528163909912\n",
      "Iteration 743, Loss: 1.9548590183258057\n",
      "Iteration 744, Loss: 1.9541653394699097\n",
      "Iteration 745, Loss: 1.9534711837768555\n",
      "Iteration 746, Loss: 1.9527771472930908\n",
      "Iteration 747, Loss: 1.9520833492279053\n",
      "Iteration 748, Loss: 1.9513890743255615\n",
      "Iteration 749, Loss: 1.950693964958191\n",
      "Iteration 750, Loss: 1.949999213218689\n",
      "Iteration 751, Loss: 1.9493045806884766\n",
      "Iteration 752, Loss: 1.948609709739685\n",
      "Iteration 753, Loss: 1.9479148387908936\n",
      "Iteration 754, Loss: 1.9472192525863647\n",
      "Iteration 755, Loss: 1.9465240240097046\n",
      "Iteration 756, Loss: 1.9458286762237549\n",
      "Iteration 757, Loss: 1.9451327323913574\n",
      "Iteration 758, Loss: 1.944437026977539\n",
      "Iteration 759, Loss: 1.9437408447265625\n",
      "Iteration 760, Loss: 1.9430450201034546\n",
      "Iteration 761, Loss: 1.9423487186431885\n",
      "Iteration 762, Loss: 1.9416522979736328\n",
      "Iteration 763, Loss: 1.940955400466919\n",
      "Iteration 764, Loss: 1.9402589797973633\n",
      "Iteration 765, Loss: 1.9395617246627808\n",
      "Iteration 766, Loss: 1.938865065574646\n",
      "Iteration 767, Loss: 1.938167929649353\n",
      "Iteration 768, Loss: 1.9374704360961914\n",
      "Iteration 769, Loss: 1.9367733001708984\n",
      "Iteration 770, Loss: 1.9360754489898682\n",
      "Iteration 771, Loss: 1.9353774785995483\n",
      "Iteration 772, Loss: 1.9346798658370972\n",
      "Iteration 773, Loss: 1.9339816570281982\n",
      "Iteration 774, Loss: 1.9332833290100098\n",
      "Iteration 775, Loss: 1.9325851202011108\n",
      "Iteration 776, Loss: 1.9318864345550537\n",
      "Iteration 777, Loss: 1.931187391281128\n",
      "Iteration 778, Loss: 1.93048894405365\n",
      "Iteration 779, Loss: 1.9297895431518555\n",
      "Iteration 780, Loss: 1.9290902614593506\n",
      "Iteration 781, Loss: 1.9283909797668457\n",
      "Iteration 782, Loss: 1.9276912212371826\n",
      "Iteration 783, Loss: 1.9269918203353882\n",
      "Iteration 784, Loss: 1.9262917041778564\n",
      "Iteration 785, Loss: 1.9255918264389038\n",
      "Iteration 786, Loss: 1.924891471862793\n",
      "Iteration 787, Loss: 1.924190878868103\n",
      "Iteration 788, Loss: 1.9234905242919922\n",
      "Iteration 789, Loss: 1.9227899312973022\n",
      "Iteration 790, Loss: 1.922088623046875\n",
      "Iteration 791, Loss: 1.9213879108428955\n",
      "Iteration 792, Loss: 1.9206864833831787\n",
      "Iteration 793, Loss: 1.9199851751327515\n",
      "Iteration 794, Loss: 1.9192829132080078\n",
      "Iteration 795, Loss: 1.9185817241668701\n",
      "Iteration 796, Loss: 1.917879581451416\n",
      "Iteration 797, Loss: 1.917177438735962\n",
      "Iteration 798, Loss: 1.9164752960205078\n",
      "Iteration 799, Loss: 1.9157724380493164\n",
      "Iteration 800, Loss: 1.9150699377059937\n",
      "Iteration 801, Loss: 1.914367437362671\n",
      "Iteration 802, Loss: 1.9136643409729004\n",
      "Iteration 803, Loss: 1.9129610061645508\n",
      "Iteration 804, Loss: 1.9122576713562012\n",
      "Iteration 805, Loss: 1.9115540981292725\n",
      "Iteration 806, Loss: 1.9108505249023438\n",
      "Iteration 807, Loss: 1.9101464748382568\n",
      "Iteration 808, Loss: 1.9094423055648804\n",
      "Iteration 809, Loss: 1.9087378978729248\n",
      "Iteration 810, Loss: 1.9080337285995483\n",
      "Iteration 811, Loss: 1.9073289632797241\n",
      "Iteration 812, Loss: 1.9066238403320312\n",
      "Iteration 813, Loss: 1.905918836593628\n",
      "Iteration 814, Loss: 1.905213713645935\n",
      "Iteration 815, Loss: 1.904508352279663\n",
      "Iteration 816, Loss: 1.9038026332855225\n",
      "Iteration 817, Loss: 1.9030966758728027\n",
      "Iteration 818, Loss: 1.9023908376693726\n",
      "Iteration 819, Loss: 1.9016854763031006\n",
      "Iteration 820, Loss: 1.9009780883789062\n",
      "Iteration 821, Loss: 1.9002716541290283\n",
      "Iteration 822, Loss: 1.8995651006698608\n",
      "Iteration 823, Loss: 1.8988579511642456\n",
      "Iteration 824, Loss: 1.8981504440307617\n",
      "Iteration 825, Loss: 1.8974436521530151\n",
      "Iteration 826, Loss: 1.8967359066009521\n",
      "Iteration 827, Loss: 1.8960285186767578\n",
      "Iteration 828, Loss: 1.8953202962875366\n",
      "Iteration 829, Loss: 1.894612193107605\n",
      "Iteration 830, Loss: 1.8939037322998047\n",
      "Iteration 831, Loss: 1.8931950330734253\n",
      "Iteration 832, Loss: 1.892486572265625\n",
      "Iteration 833, Loss: 1.891777515411377\n",
      "Iteration 834, Loss: 1.891068458557129\n",
      "Iteration 835, Loss: 1.8903584480285645\n",
      "Iteration 836, Loss: 1.8896491527557373\n",
      "Iteration 837, Loss: 1.8889397382736206\n",
      "Iteration 838, Loss: 1.8882297277450562\n",
      "Iteration 839, Loss: 1.887519121170044\n",
      "Iteration 840, Loss: 1.8868088722229004\n",
      "Iteration 841, Loss: 1.8860986232757568\n",
      "Iteration 842, Loss: 1.8853874206542969\n",
      "Iteration 843, Loss: 1.8846763372421265\n",
      "Iteration 844, Loss: 1.883965015411377\n",
      "Iteration 845, Loss: 1.883253574371338\n",
      "Iteration 846, Loss: 1.8825421333312988\n",
      "Iteration 847, Loss: 1.8818302154541016\n",
      "Iteration 848, Loss: 1.8811180591583252\n",
      "Iteration 849, Loss: 1.880405306816101\n",
      "Iteration 850, Loss: 1.8796929121017456\n",
      "Iteration 851, Loss: 1.8789803981781006\n",
      "Iteration 852, Loss: 1.878267765045166\n",
      "Iteration 853, Loss: 1.877554178237915\n",
      "Iteration 854, Loss: 1.8768408298492432\n",
      "Iteration 855, Loss: 1.876127004623413\n",
      "Iteration 856, Loss: 1.8754136562347412\n",
      "Iteration 857, Loss: 1.874699354171753\n",
      "Iteration 858, Loss: 1.8739843368530273\n",
      "Iteration 859, Loss: 1.8732703924179077\n",
      "Iteration 860, Loss: 1.8725559711456299\n",
      "Iteration 861, Loss: 1.871840476989746\n",
      "Iteration 862, Loss: 1.8711254596710205\n",
      "Iteration 863, Loss: 1.8704097270965576\n",
      "Iteration 864, Loss: 1.8696941137313843\n",
      "Iteration 865, Loss: 1.8689779043197632\n",
      "Iteration 866, Loss: 1.8682620525360107\n",
      "Iteration 867, Loss: 1.8675458431243896\n",
      "Iteration 868, Loss: 1.8668291568756104\n",
      "Iteration 869, Loss: 1.8661119937896729\n",
      "Iteration 870, Loss: 1.8653945922851562\n",
      "Iteration 871, Loss: 1.8646776676177979\n",
      "Iteration 872, Loss: 1.8639600276947021\n",
      "Iteration 873, Loss: 1.8632420301437378\n",
      "Iteration 874, Loss: 1.8625237941741943\n",
      "Iteration 875, Loss: 1.8618059158325195\n",
      "Iteration 876, Loss: 1.8610873222351074\n",
      "Iteration 877, Loss: 1.8603686094284058\n",
      "Iteration 878, Loss: 1.8596491813659668\n",
      "Iteration 879, Loss: 1.8589309453964233\n",
      "Iteration 880, Loss: 1.8582110404968262\n",
      "Iteration 881, Loss: 1.8574910163879395\n",
      "Iteration 882, Loss: 1.8567713499069214\n",
      "Iteration 883, Loss: 1.8560508489608765\n",
      "Iteration 884, Loss: 1.855330467224121\n",
      "Iteration 885, Loss: 1.854609489440918\n",
      "Iteration 886, Loss: 1.8538888692855835\n",
      "Iteration 887, Loss: 1.8531675338745117\n",
      "Iteration 888, Loss: 1.852446436882019\n",
      "Iteration 889, Loss: 1.85172438621521\n",
      "Iteration 890, Loss: 1.85100257396698\n",
      "Iteration 891, Loss: 1.8502798080444336\n",
      "Iteration 892, Loss: 1.849557638168335\n",
      "Iteration 893, Loss: 1.848834753036499\n",
      "Iteration 894, Loss: 1.8481115102767944\n",
      "Iteration 895, Loss: 1.8473892211914062\n",
      "Iteration 896, Loss: 1.846665382385254\n",
      "Iteration 897, Loss: 1.8459410667419434\n",
      "Iteration 898, Loss: 1.8452174663543701\n",
      "Iteration 899, Loss: 1.8444931507110596\n",
      "Iteration 900, Loss: 1.8437689542770386\n",
      "Iteration 901, Loss: 1.8430436849594116\n",
      "Iteration 902, Loss: 1.8423190116882324\n",
      "Iteration 903, Loss: 1.841593623161316\n",
      "Iteration 904, Loss: 1.8408682346343994\n",
      "Iteration 905, Loss: 1.8401421308517456\n",
      "Iteration 906, Loss: 1.8394159078598022\n",
      "Iteration 907, Loss: 1.8386895656585693\n",
      "Iteration 908, Loss: 1.8379631042480469\n",
      "Iteration 909, Loss: 1.8372361660003662\n",
      "Iteration 910, Loss: 1.8365087509155273\n",
      "Iteration 911, Loss: 1.8357815742492676\n",
      "Iteration 912, Loss: 1.8350534439086914\n",
      "Iteration 913, Loss: 1.8343257904052734\n",
      "Iteration 914, Loss: 1.8335976600646973\n",
      "Iteration 915, Loss: 1.8328689336776733\n",
      "Iteration 916, Loss: 1.8321402072906494\n",
      "Iteration 917, Loss: 1.8314108848571777\n",
      "Iteration 918, Loss: 1.8306816816329956\n",
      "Iteration 919, Loss: 1.8299516439437866\n",
      "Iteration 920, Loss: 1.8292220830917358\n",
      "Iteration 921, Loss: 1.8284916877746582\n",
      "Iteration 922, Loss: 1.8277616500854492\n",
      "Iteration 923, Loss: 1.827030897140503\n",
      "Iteration 924, Loss: 1.8262996673583984\n",
      "Iteration 925, Loss: 1.8255680799484253\n",
      "Iteration 926, Loss: 1.8248367309570312\n",
      "Iteration 927, Loss: 1.8241046667099\n",
      "Iteration 928, Loss: 1.8233726024627686\n",
      "Iteration 929, Loss: 1.8226398229599\n",
      "Iteration 930, Loss: 1.8219068050384521\n",
      "Iteration 931, Loss: 1.8211740255355835\n",
      "Iteration 932, Loss: 1.8204405307769775\n",
      "Iteration 933, Loss: 1.8197072744369507\n",
      "Iteration 934, Loss: 1.8189730644226074\n",
      "Iteration 935, Loss: 1.8182390928268433\n",
      "Iteration 936, Loss: 1.8175041675567627\n",
      "Iteration 937, Loss: 1.8167693614959717\n",
      "Iteration 938, Loss: 1.8160340785980225\n",
      "Iteration 939, Loss: 1.8152984380722046\n",
      "Iteration 940, Loss: 1.8145630359649658\n",
      "Iteration 941, Loss: 1.813827395439148\n",
      "Iteration 942, Loss: 1.8130906820297241\n",
      "Iteration 943, Loss: 1.812354326248169\n",
      "Iteration 944, Loss: 1.8116172552108765\n",
      "Iteration 945, Loss: 1.810880184173584\n",
      "Iteration 946, Loss: 1.8101423978805542\n",
      "Iteration 947, Loss: 1.809404730796814\n",
      "Iteration 948, Loss: 1.8086665868759155\n",
      "Iteration 949, Loss: 1.807928442955017\n",
      "Iteration 950, Loss: 1.8071893453598022\n",
      "Iteration 951, Loss: 1.8064502477645874\n",
      "Iteration 952, Loss: 1.805710792541504\n",
      "Iteration 953, Loss: 1.8049710988998413\n",
      "Iteration 954, Loss: 1.8042311668395996\n",
      "Iteration 955, Loss: 1.8034909963607788\n",
      "Iteration 956, Loss: 1.802750587463379\n",
      "Iteration 957, Loss: 1.8020094633102417\n",
      "Iteration 958, Loss: 1.8012679815292358\n",
      "Iteration 959, Loss: 1.8005261421203613\n",
      "Iteration 960, Loss: 1.7997844219207764\n",
      "Iteration 961, Loss: 1.7990421056747437\n",
      "Iteration 962, Loss: 1.7982993125915527\n",
      "Iteration 963, Loss: 1.7975566387176514\n",
      "Iteration 964, Loss: 1.796813726425171\n",
      "Iteration 965, Loss: 1.796069860458374\n",
      "Iteration 966, Loss: 1.7953259944915771\n",
      "Iteration 967, Loss: 1.7945817708969116\n",
      "Iteration 968, Loss: 1.793837547302246\n",
      "Iteration 969, Loss: 1.7930922508239746\n",
      "Iteration 970, Loss: 1.7923479080200195\n",
      "Iteration 971, Loss: 1.7916018962860107\n",
      "Iteration 972, Loss: 1.7908557653427124\n",
      "Iteration 973, Loss: 1.7901099920272827\n",
      "Iteration 974, Loss: 1.7893632650375366\n",
      "Iteration 975, Loss: 1.7886165380477905\n",
      "Iteration 976, Loss: 1.7878693342208862\n",
      "Iteration 977, Loss: 1.7871217727661133\n",
      "Iteration 978, Loss: 1.7863744497299194\n",
      "Iteration 979, Loss: 1.78562593460083\n",
      "Iteration 980, Loss: 1.7848773002624512\n",
      "Iteration 981, Loss: 1.7841291427612305\n",
      "Iteration 982, Loss: 1.7833797931671143\n",
      "Iteration 983, Loss: 1.7826298475265503\n",
      "Iteration 984, Loss: 1.7818801403045654\n",
      "Iteration 985, Loss: 1.7811295986175537\n",
      "Iteration 986, Loss: 1.780379056930542\n",
      "Iteration 987, Loss: 1.7796287536621094\n",
      "Iteration 988, Loss: 1.7788772583007812\n",
      "Iteration 989, Loss: 1.7781256437301636\n",
      "Iteration 990, Loss: 1.7773735523223877\n",
      "Iteration 991, Loss: 1.7766212224960327\n",
      "Iteration 992, Loss: 1.7758684158325195\n",
      "Iteration 993, Loss: 1.7751154899597168\n",
      "Iteration 994, Loss: 1.7743620872497559\n",
      "Iteration 995, Loss: 1.7736085653305054\n",
      "Iteration 996, Loss: 1.7728543281555176\n",
      "Iteration 997, Loss: 1.7720999717712402\n",
      "Iteration 998, Loss: 1.7713453769683838\n",
      "Iteration 999, Loss: 1.7705901861190796\n",
      "Iteration 1000, Loss: 1.7698345184326172\n",
      "Iteration 1001, Loss: 1.7690786123275757\n",
      "Iteration 1002, Loss: 1.768322467803955\n",
      "Iteration 1003, Loss: 1.7675659656524658\n",
      "Iteration 1004, Loss: 1.7668088674545288\n",
      "Iteration 1005, Loss: 1.7660516500473022\n",
      "Iteration 1006, Loss: 1.7652935981750488\n",
      "Iteration 1007, Loss: 1.764535665512085\n",
      "Iteration 1008, Loss: 1.763777494430542\n",
      "Iteration 1009, Loss: 1.763018012046814\n",
      "Iteration 1010, Loss: 1.7622588872909546\n",
      "Iteration 1011, Loss: 1.7614996433258057\n",
      "Iteration 1012, Loss: 1.7607396841049194\n",
      "Iteration 1013, Loss: 1.7599797248840332\n",
      "Iteration 1014, Loss: 1.7592179775238037\n",
      "Iteration 1015, Loss: 1.7584574222564697\n",
      "Iteration 1016, Loss: 1.7576956748962402\n",
      "Iteration 1017, Loss: 1.7569345235824585\n",
      "Iteration 1018, Loss: 1.756171703338623\n",
      "Iteration 1019, Loss: 1.7554091215133667\n",
      "Iteration 1020, Loss: 1.7546463012695312\n",
      "Iteration 1021, Loss: 1.7538831233978271\n",
      "Iteration 1022, Loss: 1.7531189918518066\n",
      "Iteration 1023, Loss: 1.7523550987243652\n",
      "Iteration 1024, Loss: 1.7515907287597656\n",
      "Iteration 1025, Loss: 1.7508256435394287\n",
      "Iteration 1026, Loss: 1.7500600814819336\n",
      "Iteration 1027, Loss: 1.7492942810058594\n",
      "Iteration 1028, Loss: 1.7485284805297852\n",
      "Iteration 1029, Loss: 1.747761607170105\n",
      "Iteration 1030, Loss: 1.746995210647583\n",
      "Iteration 1031, Loss: 1.746227741241455\n",
      "Iteration 1032, Loss: 1.7454599142074585\n",
      "Iteration 1033, Loss: 1.7446918487548828\n",
      "Iteration 1034, Loss: 1.7439236640930176\n",
      "Iteration 1035, Loss: 1.7431544065475464\n",
      "Iteration 1036, Loss: 1.7423856258392334\n",
      "Iteration 1037, Loss: 1.741615653038025\n",
      "Iteration 1038, Loss: 1.7408454418182373\n",
      "Iteration 1039, Loss: 1.7400751113891602\n",
      "Iteration 1040, Loss: 1.7393039464950562\n",
      "Iteration 1041, Loss: 1.738532543182373\n",
      "Iteration 1042, Loss: 1.7377610206604004\n",
      "Iteration 1043, Loss: 1.73698890209198\n",
      "Iteration 1044, Loss: 1.7362160682678223\n",
      "Iteration 1045, Loss: 1.735442876815796\n",
      "Iteration 1046, Loss: 1.7346699237823486\n",
      "Iteration 1047, Loss: 1.733896255493164\n",
      "Iteration 1048, Loss: 1.733121633529663\n",
      "Iteration 1049, Loss: 1.732347011566162\n",
      "Iteration 1050, Loss: 1.7315725088119507\n",
      "Iteration 1051, Loss: 1.7307965755462646\n",
      "Iteration 1052, Loss: 1.730020523071289\n",
      "Iteration 1053, Loss: 1.7292439937591553\n",
      "Iteration 1054, Loss: 1.7284672260284424\n",
      "Iteration 1055, Loss: 1.7276901006698608\n",
      "Iteration 1056, Loss: 1.726912498474121\n",
      "Iteration 1057, Loss: 1.7261346578598022\n",
      "Iteration 1058, Loss: 1.725356101989746\n",
      "Iteration 1059, Loss: 1.7245771884918213\n",
      "Iteration 1060, Loss: 1.72379732131958\n",
      "Iteration 1061, Loss: 1.7230174541473389\n",
      "Iteration 1062, Loss: 1.7222379446029663\n",
      "Iteration 1063, Loss: 1.7214572429656982\n",
      "Iteration 1064, Loss: 1.7206761837005615\n",
      "Iteration 1065, Loss: 1.7198946475982666\n",
      "Iteration 1066, Loss: 1.7191128730773926\n",
      "Iteration 1067, Loss: 1.718329906463623\n",
      "Iteration 1068, Loss: 1.7175474166870117\n",
      "Iteration 1069, Loss: 1.7167638540267944\n",
      "Iteration 1070, Loss: 1.715980052947998\n",
      "Iteration 1071, Loss: 1.7151960134506226\n",
      "Iteration 1072, Loss: 1.7144111394882202\n",
      "Iteration 1073, Loss: 1.7136259078979492\n",
      "Iteration 1074, Loss: 1.7128404378890991\n",
      "Iteration 1075, Loss: 1.7120544910430908\n",
      "Iteration 1076, Loss: 1.7112679481506348\n",
      "Iteration 1077, Loss: 1.71048104763031\n",
      "Iteration 1078, Loss: 1.709693431854248\n",
      "Iteration 1079, Loss: 1.708905577659607\n",
      "Iteration 1080, Loss: 1.7081173658370972\n",
      "Iteration 1081, Loss: 1.7073287963867188\n",
      "Iteration 1082, Loss: 1.7065391540527344\n",
      "Iteration 1083, Loss: 1.70574951171875\n",
      "Iteration 1084, Loss: 1.7049592733383179\n",
      "Iteration 1085, Loss: 1.7041685581207275\n",
      "Iteration 1086, Loss: 1.7033777236938477\n",
      "Iteration 1087, Loss: 1.7025859355926514\n",
      "Iteration 1088, Loss: 1.7017940282821655\n",
      "Iteration 1089, Loss: 1.7010014057159424\n",
      "Iteration 1090, Loss: 1.7002077102661133\n",
      "Iteration 1091, Loss: 1.6994147300720215\n",
      "Iteration 1092, Loss: 1.6986205577850342\n",
      "Iteration 1093, Loss: 1.6978263854980469\n",
      "Iteration 1094, Loss: 1.6970312595367432\n",
      "Iteration 1095, Loss: 1.6962361335754395\n",
      "Iteration 1096, Loss: 1.6954402923583984\n",
      "Iteration 1097, Loss: 1.6946437358856201\n",
      "Iteration 1098, Loss: 1.6938467025756836\n",
      "Iteration 1099, Loss: 1.6930488348007202\n",
      "Iteration 1100, Loss: 1.692251205444336\n",
      "Iteration 1101, Loss: 1.691452980041504\n",
      "Iteration 1102, Loss: 1.6906542778015137\n",
      "Iteration 1103, Loss: 1.6898549795150757\n",
      "Iteration 1104, Loss: 1.6890552043914795\n",
      "Iteration 1105, Loss: 1.6882550716400146\n",
      "Iteration 1106, Loss: 1.6874539852142334\n",
      "Iteration 1107, Loss: 1.686652421951294\n",
      "Iteration 1108, Loss: 1.6858506202697754\n",
      "Iteration 1109, Loss: 1.6850484609603882\n",
      "Iteration 1110, Loss: 1.6842454671859741\n",
      "Iteration 1111, Loss: 1.6834419965744019\n",
      "Iteration 1112, Loss: 1.6826379299163818\n",
      "Iteration 1113, Loss: 1.6818339824676514\n",
      "Iteration 1114, Loss: 1.6810288429260254\n",
      "Iteration 1115, Loss: 1.6802237033843994\n",
      "Iteration 1116, Loss: 1.6794180870056152\n",
      "Iteration 1117, Loss: 1.6786112785339355\n",
      "Iteration 1118, Loss: 1.6778043508529663\n",
      "Iteration 1119, Loss: 1.6769970655441284\n",
      "Iteration 1120, Loss: 1.6761893033981323\n",
      "Iteration 1121, Loss: 1.6753809452056885\n",
      "Iteration 1122, Loss: 1.6745717525482178\n",
      "Iteration 1123, Loss: 1.673762321472168\n",
      "Iteration 1124, Loss: 1.6729519367218018\n",
      "Iteration 1125, Loss: 1.6721413135528564\n",
      "Iteration 1126, Loss: 1.6713298559188843\n",
      "Iteration 1127, Loss: 1.6705182790756226\n",
      "Iteration 1128, Loss: 1.6697063446044922\n",
      "Iteration 1129, Loss: 1.6688932180404663\n",
      "Iteration 1130, Loss: 1.6680803298950195\n",
      "Iteration 1131, Loss: 1.667266607284546\n",
      "Iteration 1132, Loss: 1.6664522886276245\n",
      "Iteration 1133, Loss: 1.665637493133545\n",
      "Iteration 1134, Loss: 1.6648218631744385\n",
      "Iteration 1135, Loss: 1.664006233215332\n",
      "Iteration 1136, Loss: 1.6631892919540405\n",
      "Iteration 1137, Loss: 1.662372350692749\n",
      "Iteration 1138, Loss: 1.6615545749664307\n",
      "Iteration 1139, Loss: 1.6607369184494019\n",
      "Iteration 1140, Loss: 1.659918189048767\n",
      "Iteration 1141, Loss: 1.6590988636016846\n",
      "Iteration 1142, Loss: 1.6582791805267334\n",
      "Iteration 1143, Loss: 1.6574586629867554\n",
      "Iteration 1144, Loss: 1.656637191772461\n",
      "Iteration 1145, Loss: 1.6558160781860352\n",
      "Iteration 1146, Loss: 1.6549947261810303\n",
      "Iteration 1147, Loss: 1.654171347618103\n",
      "Iteration 1148, Loss: 1.6533482074737549\n",
      "Iteration 1149, Loss: 1.6525248289108276\n",
      "Iteration 1150, Loss: 1.651700735092163\n",
      "Iteration 1151, Loss: 1.650875449180603\n",
      "Iteration 1152, Loss: 1.6500499248504639\n",
      "Iteration 1153, Loss: 1.6492242813110352\n",
      "Iteration 1154, Loss: 1.648397445678711\n",
      "Iteration 1155, Loss: 1.6475703716278076\n",
      "Iteration 1156, Loss: 1.6467422246932983\n",
      "Iteration 1157, Loss: 1.645914077758789\n",
      "Iteration 1158, Loss: 1.645085096359253\n",
      "Iteration 1159, Loss: 1.6442556381225586\n",
      "Iteration 1160, Loss: 1.6434253454208374\n",
      "Iteration 1161, Loss: 1.642594814300537\n",
      "Iteration 1162, Loss: 1.641763687133789\n",
      "Iteration 1163, Loss: 1.640931487083435\n",
      "Iteration 1164, Loss: 1.6400989294052124\n",
      "Iteration 1165, Loss: 1.6392663717269897\n",
      "Iteration 1166, Loss: 1.6384320259094238\n",
      "Iteration 1167, Loss: 1.6375977993011475\n",
      "Iteration 1168, Loss: 1.6367629766464233\n",
      "Iteration 1169, Loss: 1.6359279155731201\n",
      "Iteration 1170, Loss: 1.6350921392440796\n",
      "Iteration 1171, Loss: 1.6342551708221436\n",
      "Iteration 1172, Loss: 1.6334178447723389\n",
      "Iteration 1173, Loss: 1.6325801610946655\n",
      "Iteration 1174, Loss: 1.6317415237426758\n",
      "Iteration 1175, Loss: 1.6309025287628174\n",
      "Iteration 1176, Loss: 1.6300625801086426\n",
      "Iteration 1177, Loss: 1.6292228698730469\n",
      "Iteration 1178, Loss: 1.6283814907073975\n",
      "Iteration 1179, Loss: 1.6275399923324585\n",
      "Iteration 1180, Loss: 1.6266977787017822\n",
      "Iteration 1181, Loss: 1.6258549690246582\n",
      "Iteration 1182, Loss: 1.6250114440917969\n",
      "Iteration 1183, Loss: 1.6241676807403564\n",
      "Iteration 1184, Loss: 1.62332284450531\n",
      "Iteration 1185, Loss: 1.622477412223816\n",
      "Iteration 1186, Loss: 1.621631383895874\n",
      "Iteration 1187, Loss: 1.620785117149353\n",
      "Iteration 1188, Loss: 1.6199374198913574\n",
      "Iteration 1189, Loss: 1.6190898418426514\n",
      "Iteration 1190, Loss: 1.618241310119629\n",
      "Iteration 1191, Loss: 1.6173923015594482\n",
      "Iteration 1192, Loss: 1.6165422201156616\n",
      "Iteration 1193, Loss: 1.6156915426254272\n",
      "Iteration 1194, Loss: 1.6148402690887451\n",
      "Iteration 1195, Loss: 1.6139891147613525\n",
      "Iteration 1196, Loss: 1.6131365299224854\n",
      "Iteration 1197, Loss: 1.6122831106185913\n",
      "Iteration 1198, Loss: 1.6114296913146973\n",
      "Iteration 1199, Loss: 1.6105754375457764\n",
      "Iteration 1200, Loss: 1.6097201108932495\n",
      "Iteration 1201, Loss: 1.6088645458221436\n",
      "Iteration 1202, Loss: 1.608008623123169\n",
      "Iteration 1203, Loss: 1.6071513891220093\n",
      "Iteration 1204, Loss: 1.6062934398651123\n",
      "Iteration 1205, Loss: 1.6054346561431885\n",
      "Iteration 1206, Loss: 1.6045758724212646\n",
      "Iteration 1207, Loss: 1.6037158966064453\n",
      "Iteration 1208, Loss: 1.6028556823730469\n",
      "Iteration 1209, Loss: 1.601994276046753\n",
      "Iteration 1210, Loss: 1.6011326313018799\n",
      "Iteration 1211, Loss: 1.6002702713012695\n",
      "Iteration 1212, Loss: 1.5994069576263428\n",
      "Iteration 1213, Loss: 1.5985430479049683\n",
      "Iteration 1214, Loss: 1.5976780652999878\n",
      "Iteration 1215, Loss: 1.5968129634857178\n",
      "Iteration 1216, Loss: 1.595947265625\n",
      "Iteration 1217, Loss: 1.5950807332992554\n",
      "Iteration 1218, Loss: 1.5942132472991943\n",
      "Iteration 1219, Loss: 1.5933451652526855\n",
      "Iteration 1220, Loss: 1.59247624874115\n",
      "Iteration 1221, Loss: 1.5916070938110352\n",
      "Iteration 1222, Loss: 1.5907371044158936\n",
      "Iteration 1223, Loss: 1.5898663997650146\n",
      "Iteration 1224, Loss: 1.5889943838119507\n",
      "Iteration 1225, Loss: 1.5881223678588867\n",
      "Iteration 1226, Loss: 1.587249517440796\n",
      "Iteration 1227, Loss: 1.5863755941390991\n",
      "Iteration 1228, Loss: 1.585500717163086\n",
      "Iteration 1229, Loss: 1.5846259593963623\n",
      "Iteration 1230, Loss: 1.5837501287460327\n",
      "Iteration 1231, Loss: 1.5828735828399658\n",
      "Iteration 1232, Loss: 1.5819963216781616\n",
      "Iteration 1233, Loss: 1.581118106842041\n",
      "Iteration 1234, Loss: 1.5802388191223145\n",
      "Iteration 1235, Loss: 1.579359531402588\n",
      "Iteration 1236, Loss: 1.578479528427124\n",
      "Iteration 1237, Loss: 1.5775985717773438\n",
      "Iteration 1238, Loss: 1.576716423034668\n",
      "Iteration 1239, Loss: 1.575834035873413\n",
      "Iteration 1240, Loss: 1.574951171875\n",
      "Iteration 1241, Loss: 1.5740668773651123\n",
      "Iteration 1242, Loss: 1.5731821060180664\n",
      "Iteration 1243, Loss: 1.5722968578338623\n",
      "Iteration 1244, Loss: 1.5714104175567627\n",
      "Iteration 1245, Loss: 1.5705235004425049\n",
      "Iteration 1246, Loss: 1.5696353912353516\n",
      "Iteration 1247, Loss: 1.5687470436096191\n",
      "Iteration 1248, Loss: 1.5678577423095703\n",
      "Iteration 1249, Loss: 1.5669677257537842\n",
      "Iteration 1250, Loss: 1.5660765171051025\n",
      "Iteration 1251, Loss: 1.5651848316192627\n",
      "Iteration 1252, Loss: 1.5642926692962646\n",
      "Iteration 1253, Loss: 1.5633995532989502\n",
      "Iteration 1254, Loss: 1.5625056028366089\n",
      "Iteration 1255, Loss: 1.5616105794906616\n",
      "Iteration 1256, Loss: 1.5607154369354248\n",
      "Iteration 1257, Loss: 1.5598186254501343\n",
      "Iteration 1258, Loss: 1.5589215755462646\n",
      "Iteration 1259, Loss: 1.558023452758789\n",
      "Iteration 1260, Loss: 1.5571248531341553\n",
      "Iteration 1261, Loss: 1.556225299835205\n",
      "Iteration 1262, Loss: 1.5553252696990967\n",
      "Iteration 1263, Loss: 1.5544241666793823\n",
      "Iteration 1264, Loss: 1.553521990776062\n",
      "Iteration 1265, Loss: 1.5526193380355835\n",
      "Iteration 1266, Loss: 1.551715612411499\n",
      "Iteration 1267, Loss: 1.550811529159546\n",
      "Iteration 1268, Loss: 1.5499062538146973\n",
      "Iteration 1269, Loss: 1.5490005016326904\n",
      "Iteration 1270, Loss: 1.548093318939209\n",
      "Iteration 1271, Loss: 1.5471858978271484\n",
      "Iteration 1272, Loss: 1.5462770462036133\n",
      "Iteration 1273, Loss: 1.5453680753707886\n",
      "Iteration 1274, Loss: 1.5444577932357788\n",
      "Iteration 1275, Loss: 1.5435470342636108\n",
      "Iteration 1276, Loss: 1.5426349639892578\n",
      "Iteration 1277, Loss: 1.541722297668457\n",
      "Iteration 1278, Loss: 1.5408085584640503\n",
      "Iteration 1279, Loss: 1.5398945808410645\n",
      "Iteration 1280, Loss: 1.5389792919158936\n",
      "Iteration 1281, Loss: 1.5380630493164062\n",
      "Iteration 1282, Loss: 1.5371465682983398\n",
      "Iteration 1283, Loss: 1.5362284183502197\n",
      "Iteration 1284, Loss: 1.5353095531463623\n",
      "Iteration 1285, Loss: 1.5343904495239258\n",
      "Iteration 1286, Loss: 1.5334699153900146\n",
      "Iteration 1287, Loss: 1.5325486660003662\n",
      "Iteration 1288, Loss: 1.5316261053085327\n",
      "Iteration 1289, Loss: 1.5307033061981201\n",
      "Iteration 1290, Loss: 1.5297796726226807\n",
      "Iteration 1291, Loss: 1.5288550853729248\n",
      "Iteration 1292, Loss: 1.5279293060302734\n",
      "Iteration 1293, Loss: 1.5270025730133057\n",
      "Iteration 1294, Loss: 1.5260753631591797\n",
      "Iteration 1295, Loss: 1.5251468420028687\n",
      "Iteration 1296, Loss: 1.5242180824279785\n",
      "Iteration 1297, Loss: 1.5232881307601929\n",
      "Iteration 1298, Loss: 1.5223569869995117\n",
      "Iteration 1299, Loss: 1.5214252471923828\n",
      "Iteration 1300, Loss: 1.5204927921295166\n",
      "Iteration 1301, Loss: 1.5195589065551758\n",
      "Iteration 1302, Loss: 1.5186240673065186\n",
      "Iteration 1303, Loss: 1.5176886320114136\n",
      "Iteration 1304, Loss: 1.5167526006698608\n",
      "Iteration 1305, Loss: 1.5158151388168335\n",
      "Iteration 1306, Loss: 1.5148768424987793\n",
      "Iteration 1307, Loss: 1.5139377117156982\n",
      "Iteration 1308, Loss: 1.5129978656768799\n",
      "Iteration 1309, Loss: 1.5120563507080078\n",
      "Iteration 1310, Loss: 1.5111145973205566\n",
      "Iteration 1311, Loss: 1.5101712942123413\n",
      "Iteration 1312, Loss: 1.5092277526855469\n",
      "Iteration 1313, Loss: 1.508283019065857\n",
      "Iteration 1314, Loss: 1.5073373317718506\n",
      "Iteration 1315, Loss: 1.5063903331756592\n",
      "Iteration 1316, Loss: 1.5054428577423096\n",
      "Iteration 1317, Loss: 1.504494071006775\n",
      "Iteration 1318, Loss: 1.5035446882247925\n",
      "Iteration 1319, Loss: 1.502593994140625\n",
      "Iteration 1320, Loss: 1.5016424655914307\n",
      "Iteration 1321, Loss: 1.50068998336792\n",
      "Iteration 1322, Loss: 1.4997366666793823\n",
      "Iteration 1323, Loss: 1.4987821578979492\n",
      "Iteration 1324, Loss: 1.4978269338607788\n",
      "Iteration 1325, Loss: 1.4968700408935547\n",
      "Iteration 1326, Loss: 1.495913028717041\n",
      "Iteration 1327, Loss: 1.4949544668197632\n",
      "Iteration 1328, Loss: 1.493995189666748\n",
      "Iteration 1329, Loss: 1.4930347204208374\n",
      "Iteration 1330, Loss: 1.4920734167099\n",
      "Iteration 1331, Loss: 1.491111159324646\n",
      "Iteration 1332, Loss: 1.490147590637207\n",
      "Iteration 1333, Loss: 1.4891833066940308\n",
      "Iteration 1334, Loss: 1.488218069076538\n",
      "Iteration 1335, Loss: 1.48725163936615\n",
      "Iteration 1336, Loss: 1.4862844944000244\n",
      "Iteration 1337, Loss: 1.4853156805038452\n",
      "Iteration 1338, Loss: 1.4843463897705078\n",
      "Iteration 1339, Loss: 1.4833757877349854\n",
      "Iteration 1340, Loss: 1.4824039936065674\n",
      "Iteration 1341, Loss: 1.4814318418502808\n",
      "Iteration 1342, Loss: 1.4804584980010986\n",
      "Iteration 1343, Loss: 1.4794836044311523\n",
      "Iteration 1344, Loss: 1.4785079956054688\n",
      "Iteration 1345, Loss: 1.4775313138961792\n",
      "Iteration 1346, Loss: 1.4765533208847046\n",
      "Iteration 1347, Loss: 1.4755744934082031\n",
      "Iteration 1348, Loss: 1.4745948314666748\n",
      "Iteration 1349, Loss: 1.4736136198043823\n",
      "Iteration 1350, Loss: 1.4726314544677734\n",
      "Iteration 1351, Loss: 1.4716484546661377\n",
      "Iteration 1352, Loss: 1.4706645011901855\n",
      "Iteration 1353, Loss: 1.4696788787841797\n",
      "Iteration 1354, Loss: 1.4686921834945679\n",
      "Iteration 1355, Loss: 1.4677047729492188\n",
      "Iteration 1356, Loss: 1.4667165279388428\n",
      "Iteration 1357, Loss: 1.465726613998413\n",
      "Iteration 1358, Loss: 1.464735984802246\n",
      "Iteration 1359, Loss: 1.4637439250946045\n",
      "Iteration 1360, Loss: 1.4627511501312256\n",
      "Iteration 1361, Loss: 1.4617570638656616\n",
      "Iteration 1362, Loss: 1.460761547088623\n",
      "Iteration 1363, Loss: 1.459766149520874\n",
      "Iteration 1364, Loss: 1.4587678909301758\n",
      "Iteration 1365, Loss: 1.457769751548767\n",
      "Iteration 1366, Loss: 1.4567698240280151\n",
      "Iteration 1367, Loss: 1.4557690620422363\n",
      "Iteration 1368, Loss: 1.4547672271728516\n",
      "Iteration 1369, Loss: 1.4537642002105713\n",
      "Iteration 1370, Loss: 1.4527597427368164\n",
      "Iteration 1371, Loss: 1.4517545700073242\n",
      "Iteration 1372, Loss: 1.4507479667663574\n",
      "Iteration 1373, Loss: 1.4497404098510742\n",
      "Iteration 1374, Loss: 1.448731780052185\n",
      "Iteration 1375, Loss: 1.4477214813232422\n",
      "Iteration 1376, Loss: 1.4467105865478516\n",
      "Iteration 1377, Loss: 1.4456981420516968\n",
      "Iteration 1378, Loss: 1.444684386253357\n",
      "Iteration 1379, Loss: 1.4436700344085693\n",
      "Iteration 1380, Loss: 1.442654013633728\n",
      "Iteration 1381, Loss: 1.4416368007659912\n",
      "Iteration 1382, Loss: 1.4406185150146484\n",
      "Iteration 1383, Loss: 1.4395993947982788\n",
      "Iteration 1384, Loss: 1.4385783672332764\n",
      "Iteration 1385, Loss: 1.4375566244125366\n",
      "Iteration 1386, Loss: 1.4365336894989014\n",
      "Iteration 1387, Loss: 1.4355090856552124\n",
      "Iteration 1388, Loss: 1.434483528137207\n",
      "Iteration 1389, Loss: 1.4334572553634644\n",
      "Iteration 1390, Loss: 1.4324288368225098\n",
      "Iteration 1391, Loss: 1.4313998222351074\n",
      "Iteration 1392, Loss: 1.4303696155548096\n",
      "Iteration 1393, Loss: 1.4293378591537476\n",
      "Iteration 1394, Loss: 1.4283050298690796\n",
      "Iteration 1395, Loss: 1.4272708892822266\n",
      "Iteration 1396, Loss: 1.4262354373931885\n",
      "Iteration 1397, Loss: 1.4251984357833862\n",
      "Iteration 1398, Loss: 1.4241604804992676\n",
      "Iteration 1399, Loss: 1.423121452331543\n",
      "Iteration 1400, Loss: 1.4220808744430542\n",
      "Iteration 1401, Loss: 1.4210389852523804\n",
      "Iteration 1402, Loss: 1.4199962615966797\n",
      "Iteration 1403, Loss: 1.4189517498016357\n",
      "Iteration 1404, Loss: 1.4179058074951172\n",
      "Iteration 1405, Loss: 1.4168593883514404\n",
      "Iteration 1406, Loss: 1.4158109426498413\n",
      "Iteration 1407, Loss: 1.4147614240646362\n",
      "Iteration 1408, Loss: 1.4137104749679565\n",
      "Iteration 1409, Loss: 1.412658452987671\n",
      "Iteration 1410, Loss: 1.4116047620773315\n",
      "Iteration 1411, Loss: 1.410549521446228\n",
      "Iteration 1412, Loss: 1.409493327140808\n",
      "Iteration 1413, Loss: 1.4084362983703613\n",
      "Iteration 1414, Loss: 1.4073771238327026\n",
      "Iteration 1415, Loss: 1.406317114830017\n",
      "Iteration 1416, Loss: 1.4052555561065674\n",
      "Iteration 1417, Loss: 1.4041929244995117\n",
      "Iteration 1418, Loss: 1.4031285047531128\n",
      "Iteration 1419, Loss: 1.4020628929138184\n",
      "Iteration 1420, Loss: 1.4009959697723389\n",
      "Iteration 1421, Loss: 1.3999276161193848\n",
      "Iteration 1422, Loss: 1.3988574743270874\n",
      "Iteration 1423, Loss: 1.3977864980697632\n",
      "Iteration 1424, Loss: 1.3967139720916748\n",
      "Iteration 1425, Loss: 1.395640254020691\n",
      "Iteration 1426, Loss: 1.3945646286010742\n",
      "Iteration 1427, Loss: 1.3934882879257202\n",
      "Iteration 1428, Loss: 1.392410159111023\n",
      "Iteration 1429, Loss: 1.391330599784851\n",
      "Iteration 1430, Loss: 1.3902499675750732\n",
      "Iteration 1431, Loss: 1.3891671895980835\n",
      "Iteration 1432, Loss: 1.3880834579467773\n",
      "Iteration 1433, Loss: 1.3869984149932861\n",
      "Iteration 1434, Loss: 1.3859117031097412\n",
      "Iteration 1435, Loss: 1.3848233222961426\n",
      "Iteration 1436, Loss: 1.383734107017517\n",
      "Iteration 1437, Loss: 1.382643461227417\n",
      "Iteration 1438, Loss: 1.381550669670105\n",
      "Iteration 1439, Loss: 1.3804569244384766\n",
      "Iteration 1440, Loss: 1.3793611526489258\n",
      "Iteration 1441, Loss: 1.3782641887664795\n",
      "Iteration 1442, Loss: 1.3771655559539795\n",
      "Iteration 1443, Loss: 1.376065969467163\n",
      "Iteration 1444, Loss: 1.3749642372131348\n",
      "Iteration 1445, Loss: 1.37386155128479\n",
      "Iteration 1446, Loss: 1.3727569580078125\n",
      "Iteration 1447, Loss: 1.37165105342865\n",
      "Iteration 1448, Loss: 1.3705438375473022\n",
      "Iteration 1449, Loss: 1.3694347143173218\n",
      "Iteration 1450, Loss: 1.3683241605758667\n",
      "Iteration 1451, Loss: 1.3672120571136475\n",
      "Iteration 1452, Loss: 1.3660987615585327\n",
      "Iteration 1453, Loss: 1.3649834394454956\n",
      "Iteration 1454, Loss: 1.3638668060302734\n",
      "Iteration 1455, Loss: 1.3627485036849976\n",
      "Iteration 1456, Loss: 1.361628770828247\n",
      "Iteration 1457, Loss: 1.3605074882507324\n",
      "Iteration 1458, Loss: 1.3593847751617432\n",
      "Iteration 1459, Loss: 1.3582595586776733\n",
      "Iteration 1460, Loss: 1.3571337461471558\n",
      "Iteration 1461, Loss: 1.3560056686401367\n",
      "Iteration 1462, Loss: 1.3548762798309326\n",
      "Iteration 1463, Loss: 1.353745460510254\n",
      "Iteration 1464, Loss: 1.3526133298873901\n",
      "Iteration 1465, Loss: 1.3514790534973145\n",
      "Iteration 1466, Loss: 1.3503432273864746\n",
      "Iteration 1467, Loss: 1.3492053747177124\n",
      "Iteration 1468, Loss: 1.3480662107467651\n",
      "Iteration 1469, Loss: 1.346925973892212\n",
      "Iteration 1470, Loss: 1.3457831144332886\n",
      "Iteration 1471, Loss: 1.3446388244628906\n",
      "Iteration 1472, Loss: 1.3434933423995972\n",
      "Iteration 1473, Loss: 1.3423460721969604\n",
      "Iteration 1474, Loss: 1.3411970138549805\n",
      "Iteration 1475, Loss: 1.3400458097457886\n",
      "Iteration 1476, Loss: 1.3388934135437012\n",
      "Iteration 1477, Loss: 1.3377392292022705\n",
      "Iteration 1478, Loss: 1.3365833759307861\n",
      "Iteration 1479, Loss: 1.3354253768920898\n",
      "Iteration 1480, Loss: 1.334266185760498\n",
      "Iteration 1481, Loss: 1.3331050872802734\n",
      "Iteration 1482, Loss: 1.3319424390792847\n",
      "Iteration 1483, Loss: 1.330777645111084\n",
      "Iteration 1484, Loss: 1.3296115398406982\n",
      "Iteration 1485, Loss: 1.3284430503845215\n",
      "Iteration 1486, Loss: 1.3272736072540283\n",
      "Iteration 1487, Loss: 1.3261017799377441\n",
      "Iteration 1488, Loss: 1.3249285221099854\n",
      "Iteration 1489, Loss: 1.3237532377243042\n",
      "Iteration 1490, Loss: 1.3225758075714111\n",
      "Iteration 1491, Loss: 1.321397304534912\n",
      "Iteration 1492, Loss: 1.3202168941497803\n",
      "Iteration 1493, Loss: 1.319034457206726\n",
      "Iteration 1494, Loss: 1.3178499937057495\n",
      "Iteration 1495, Loss: 1.3166639804840088\n",
      "Iteration 1496, Loss: 1.3154757022857666\n",
      "Iteration 1497, Loss: 1.3142859935760498\n",
      "Iteration 1498, Loss: 1.3130943775177002\n",
      "Iteration 1499, Loss: 1.3119009733200073\n",
      "Iteration 1500, Loss: 1.310705542564392\n",
      "Iteration 1501, Loss: 1.3095080852508545\n",
      "Iteration 1502, Loss: 1.3083089590072632\n",
      "Iteration 1503, Loss: 1.30710768699646\n",
      "Iteration 1504, Loss: 1.3059051036834717\n",
      "Iteration 1505, Loss: 1.304700255393982\n",
      "Iteration 1506, Loss: 1.3034934997558594\n",
      "Iteration 1507, Loss: 1.3022844791412354\n",
      "Iteration 1508, Loss: 1.3010742664337158\n",
      "Iteration 1509, Loss: 1.2998613119125366\n",
      "Iteration 1510, Loss: 1.2986468076705933\n",
      "Iteration 1511, Loss: 1.2974305152893066\n",
      "Iteration 1512, Loss: 1.2962117195129395\n",
      "Iteration 1513, Loss: 1.2949912548065186\n",
      "Iteration 1514, Loss: 1.293769121170044\n",
      "Iteration 1515, Loss: 1.2925446033477783\n",
      "Iteration 1516, Loss: 1.2913182973861694\n",
      "Iteration 1517, Loss: 1.290089726448059\n",
      "Iteration 1518, Loss: 1.2888593673706055\n",
      "Iteration 1519, Loss: 1.2876269817352295\n",
      "Iteration 1520, Loss: 1.2863922119140625\n",
      "Iteration 1521, Loss: 1.2851555347442627\n",
      "Iteration 1522, Loss: 1.2839165925979614\n",
      "Iteration 1523, Loss: 1.282676100730896\n",
      "Iteration 1524, Loss: 1.28143310546875\n",
      "Iteration 1525, Loss: 1.2801883220672607\n",
      "Iteration 1526, Loss: 1.2789413928985596\n",
      "Iteration 1527, Loss: 1.2776920795440674\n",
      "Iteration 1528, Loss: 1.276440978050232\n",
      "Iteration 1529, Loss: 1.275187611579895\n",
      "Iteration 1530, Loss: 1.2739320993423462\n",
      "Iteration 1531, Loss: 1.2726746797561646\n",
      "Iteration 1532, Loss: 1.271415114402771\n",
      "Iteration 1533, Loss: 1.270153284072876\n",
      "Iteration 1534, Loss: 1.2688889503479004\n",
      "Iteration 1535, Loss: 1.2676228284835815\n",
      "Iteration 1536, Loss: 1.2663543224334717\n",
      "Iteration 1537, Loss: 1.2650840282440186\n",
      "Iteration 1538, Loss: 1.2638108730316162\n",
      "Iteration 1539, Loss: 1.2625360488891602\n",
      "Iteration 1540, Loss: 1.2612589597702026\n",
      "Iteration 1541, Loss: 1.2599796056747437\n",
      "Iteration 1542, Loss: 1.2586973905563354\n",
      "Iteration 1543, Loss: 1.2574137449264526\n",
      "Iteration 1544, Loss: 1.2561275959014893\n",
      "Iteration 1545, Loss: 1.2548388242721558\n",
      "Iteration 1546, Loss: 1.2535480260849\n",
      "Iteration 1547, Loss: 1.2522552013397217\n",
      "Iteration 1548, Loss: 1.2509596347808838\n",
      "Iteration 1549, Loss: 1.249662160873413\n",
      "Iteration 1550, Loss: 1.2483619451522827\n",
      "Iteration 1551, Loss: 1.24705970287323\n",
      "Iteration 1552, Loss: 1.2457547187805176\n",
      "Iteration 1553, Loss: 1.2444475889205933\n",
      "Iteration 1554, Loss: 1.2431384325027466\n",
      "Iteration 1555, Loss: 1.2418266534805298\n",
      "Iteration 1556, Loss: 1.2405121326446533\n",
      "Iteration 1557, Loss: 1.239195704460144\n",
      "Iteration 1558, Loss: 1.2378766536712646\n",
      "Iteration 1559, Loss: 1.2365554571151733\n",
      "Iteration 1560, Loss: 1.2352315187454224\n",
      "Iteration 1561, Loss: 1.2339049577713013\n",
      "Iteration 1562, Loss: 1.2325760126113892\n",
      "Iteration 1563, Loss: 1.2312445640563965\n",
      "Iteration 1564, Loss: 1.229910969734192\n",
      "Iteration 1565, Loss: 1.228574514389038\n",
      "Iteration 1566, Loss: 1.2272357940673828\n",
      "Iteration 1567, Loss: 1.2258943319320679\n",
      "Iteration 1568, Loss: 1.2245502471923828\n",
      "Iteration 1569, Loss: 1.2232038974761963\n",
      "Iteration 1570, Loss: 1.2218546867370605\n",
      "Iteration 1571, Loss: 1.2205032110214233\n",
      "Iteration 1572, Loss: 1.2191489934921265\n",
      "Iteration 1573, Loss: 1.2177923917770386\n",
      "Iteration 1574, Loss: 1.216432809829712\n",
      "Iteration 1575, Loss: 1.2150713205337524\n",
      "Iteration 1576, Loss: 1.213706374168396\n",
      "Iteration 1577, Loss: 1.212339162826538\n",
      "Iteration 1578, Loss: 1.2109694480895996\n",
      "Iteration 1579, Loss: 1.2095969915390015\n",
      "Iteration 1580, Loss: 1.2082219123840332\n",
      "Iteration 1581, Loss: 1.2068438529968262\n",
      "Iteration 1582, Loss: 1.2054634094238281\n",
      "Iteration 1583, Loss: 1.2040798664093018\n",
      "Iteration 1584, Loss: 1.2026939392089844\n",
      "Iteration 1585, Loss: 1.201305627822876\n",
      "Iteration 1586, Loss: 1.199913501739502\n",
      "Iteration 1587, Loss: 1.198519229888916\n",
      "Iteration 1588, Loss: 1.1971224546432495\n",
      "Iteration 1589, Loss: 1.1957224607467651\n",
      "Iteration 1590, Loss: 1.194319725036621\n",
      "Iteration 1591, Loss: 1.1929141283035278\n",
      "Iteration 1592, Loss: 1.1915059089660645\n",
      "Iteration 1593, Loss: 1.1900944709777832\n",
      "Iteration 1594, Loss: 1.1886807680130005\n",
      "Iteration 1595, Loss: 1.1872638463974\n",
      "Iteration 1596, Loss: 1.185843825340271\n",
      "Iteration 1597, Loss: 1.1844210624694824\n",
      "Iteration 1598, Loss: 1.182995319366455\n",
      "Iteration 1599, Loss: 1.1815664768218994\n",
      "Iteration 1600, Loss: 1.180134892463684\n",
      "Iteration 1601, Loss: 1.1787002086639404\n",
      "Iteration 1602, Loss: 1.1772626638412476\n",
      "Iteration 1603, Loss: 1.1758222579956055\n",
      "Iteration 1604, Loss: 1.1743783950805664\n",
      "Iteration 1605, Loss: 1.1729316711425781\n",
      "Iteration 1606, Loss: 1.171481966972351\n",
      "Iteration 1607, Loss: 1.1700295209884644\n",
      "Iteration 1608, Loss: 1.1685733795166016\n",
      "Iteration 1609, Loss: 1.167114496231079\n",
      "Iteration 1610, Loss: 1.1656522750854492\n",
      "Iteration 1611, Loss: 1.164186954498291\n",
      "Iteration 1612, Loss: 1.1627187728881836\n",
      "Iteration 1613, Loss: 1.1612472534179688\n",
      "Iteration 1614, Loss: 1.1597727537155151\n",
      "Iteration 1615, Loss: 1.158294677734375\n",
      "Iteration 1616, Loss: 1.156813621520996\n",
      "Iteration 1617, Loss: 1.1553294658660889\n",
      "Iteration 1618, Loss: 1.1538413763046265\n",
      "Iteration 1619, Loss: 1.152350664138794\n",
      "Iteration 1620, Loss: 1.1508567333221436\n",
      "Iteration 1621, Loss: 1.1493597030639648\n",
      "Iteration 1622, Loss: 1.147858738899231\n",
      "Iteration 1623, Loss: 1.1463549137115479\n",
      "Iteration 1624, Loss: 1.1448473930358887\n",
      "Iteration 1625, Loss: 1.1433368921279907\n",
      "Iteration 1626, Loss: 1.1418226957321167\n",
      "Iteration 1627, Loss: 1.1403050422668457\n",
      "Iteration 1628, Loss: 1.1387836933135986\n",
      "Iteration 1629, Loss: 1.137259602546692\n",
      "Iteration 1630, Loss: 1.1357312202453613\n",
      "Iteration 1631, Loss: 1.1341997385025024\n",
      "Iteration 1632, Loss: 1.1326651573181152\n",
      "Iteration 1633, Loss: 1.1311266422271729\n",
      "Iteration 1634, Loss: 1.1295846700668335\n",
      "Iteration 1635, Loss: 1.128039002418518\n",
      "Iteration 1636, Loss: 1.1264901161193848\n",
      "Iteration 1637, Loss: 1.1249372959136963\n",
      "Iteration 1638, Loss: 1.1233808994293213\n",
      "Iteration 1639, Loss: 1.1218204498291016\n",
      "Iteration 1640, Loss: 1.1202571392059326\n",
      "Iteration 1641, Loss: 1.118689775466919\n",
      "Iteration 1642, Loss: 1.11711847782135\n",
      "Iteration 1643, Loss: 1.1155437231063843\n",
      "Iteration 1644, Loss: 1.1139655113220215\n",
      "Iteration 1645, Loss: 1.1123828887939453\n",
      "Iteration 1646, Loss: 1.1107971668243408\n",
      "Iteration 1647, Loss: 1.1092071533203125\n",
      "Iteration 1648, Loss: 1.10761296749115\n",
      "Iteration 1649, Loss: 1.1060161590576172\n",
      "Iteration 1650, Loss: 1.104414463043213\n",
      "Iteration 1651, Loss: 1.1028094291687012\n",
      "Iteration 1652, Loss: 1.1012003421783447\n",
      "Iteration 1653, Loss: 1.0995867252349854\n",
      "Iteration 1654, Loss: 1.0979695320129395\n",
      "Iteration 1655, Loss: 1.0963490009307861\n",
      "Iteration 1656, Loss: 1.0947234630584717\n",
      "Iteration 1657, Loss: 1.0930944681167603\n",
      "Iteration 1658, Loss: 1.091461181640625\n",
      "Iteration 1659, Loss: 1.0898239612579346\n",
      "Iteration 1660, Loss: 1.0881824493408203\n",
      "Iteration 1661, Loss: 1.0865371227264404\n",
      "Iteration 1662, Loss: 1.0848875045776367\n",
      "Iteration 1663, Loss: 1.0832337141036987\n",
      "Iteration 1664, Loss: 1.0815753936767578\n",
      "Iteration 1665, Loss: 1.0799130201339722\n",
      "Iteration 1666, Loss: 1.078246831893921\n",
      "Iteration 1667, Loss: 1.0765761137008667\n",
      "Iteration 1668, Loss: 1.0749009847640991\n",
      "Iteration 1669, Loss: 1.0732215642929077\n",
      "Iteration 1670, Loss: 1.071537733078003\n",
      "Iteration 1671, Loss: 1.0698496103286743\n",
      "Iteration 1672, Loss: 1.0681570768356323\n",
      "Iteration 1673, Loss: 1.0664600133895874\n",
      "Iteration 1674, Loss: 1.064758539199829\n",
      "Iteration 1675, Loss: 1.0630526542663574\n",
      "Iteration 1676, Loss: 1.0613422393798828\n",
      "Iteration 1677, Loss: 1.0596271753311157\n",
      "Iteration 1678, Loss: 1.0579077005386353\n",
      "Iteration 1679, Loss: 1.0561833381652832\n",
      "Iteration 1680, Loss: 1.0544543266296387\n",
      "Iteration 1681, Loss: 1.0527210235595703\n",
      "Iteration 1682, Loss: 1.0509824752807617\n",
      "Iteration 1683, Loss: 1.0492397546768188\n",
      "Iteration 1684, Loss: 1.0474920272827148\n",
      "Iteration 1685, Loss: 1.0457394123077393\n",
      "Iteration 1686, Loss: 1.0439820289611816\n",
      "Iteration 1687, Loss: 1.042220115661621\n",
      "Iteration 1688, Loss: 1.0404529571533203\n",
      "Iteration 1689, Loss: 1.0386812686920166\n",
      "Iteration 1690, Loss: 1.0369045734405518\n",
      "Iteration 1691, Loss: 1.0351228713989258\n",
      "Iteration 1692, Loss: 1.0333364009857178\n",
      "Iteration 1693, Loss: 1.03154456615448\n",
      "Iteration 1694, Loss: 1.0297478437423706\n",
      "Iteration 1695, Loss: 1.0279459953308105\n",
      "Iteration 1696, Loss: 1.0261386632919312\n",
      "Iteration 1697, Loss: 1.0243268013000488\n",
      "Iteration 1698, Loss: 1.0225090980529785\n",
      "Iteration 1699, Loss: 1.020686388015747\n",
      "Iteration 1700, Loss: 1.0188586711883545\n",
      "Iteration 1701, Loss: 1.0170254707336426\n",
      "Iteration 1702, Loss: 1.0151866674423218\n",
      "Iteration 1703, Loss: 1.013343095779419\n",
      "Iteration 1704, Loss: 1.0114939212799072\n",
      "Iteration 1705, Loss: 1.009639024734497\n",
      "Iteration 1706, Loss: 1.0077787637710571\n",
      "Iteration 1707, Loss: 1.0059130191802979\n",
      "Iteration 1708, Loss: 1.0040415525436401\n",
      "Iteration 1709, Loss: 1.0021648406982422\n",
      "Iteration 1710, Loss: 1.0002821683883667\n",
      "Iteration 1711, Loss: 0.9983940124511719\n",
      "Iteration 1712, Loss: 0.9964999556541443\n",
      "Iteration 1713, Loss: 0.9946002960205078\n",
      "Iteration 1714, Loss: 0.9926950335502625\n",
      "Iteration 1715, Loss: 0.9907835721969604\n",
      "Iteration 1716, Loss: 0.9888663291931152\n",
      "Iteration 1717, Loss: 0.9869431853294373\n",
      "Iteration 1718, Loss: 0.9850141406059265\n",
      "Iteration 1719, Loss: 0.9830791354179382\n",
      "Iteration 1720, Loss: 0.9811379909515381\n",
      "Iteration 1721, Loss: 0.9791908264160156\n",
      "Iteration 1722, Loss: 0.9772377014160156\n",
      "Iteration 1723, Loss: 0.9752780795097351\n",
      "Iteration 1724, Loss: 0.9733126163482666\n",
      "Iteration 1725, Loss: 0.9713410139083862\n",
      "Iteration 1726, Loss: 0.9693622589111328\n",
      "Iteration 1727, Loss: 0.9673780798912048\n",
      "Iteration 1728, Loss: 0.9653870463371277\n",
      "Iteration 1729, Loss: 0.9633895754814148\n",
      "Iteration 1730, Loss: 0.9613858461380005\n",
      "Iteration 1731, Loss: 0.9593753218650818\n",
      "Iteration 1732, Loss: 0.9573582410812378\n",
      "Iteration 1733, Loss: 0.9553346633911133\n",
      "Iteration 1734, Loss: 0.9533044099807739\n",
      "Iteration 1735, Loss: 0.951267659664154\n",
      "Iteration 1736, Loss: 0.9492237567901611\n",
      "Iteration 1737, Loss: 0.9471733570098877\n",
      "Iteration 1738, Loss: 0.9451159834861755\n",
      "Iteration 1739, Loss: 0.9430514574050903\n",
      "Iteration 1740, Loss: 0.9409803152084351\n",
      "Iteration 1741, Loss: 0.9389017820358276\n",
      "Iteration 1742, Loss: 0.9368163347244263\n",
      "Iteration 1743, Loss: 0.9347231388092041\n",
      "Iteration 1744, Loss: 0.9326233863830566\n",
      "Iteration 1745, Loss: 0.9305164217948914\n",
      "Iteration 1746, Loss: 0.9284021854400635\n",
      "Iteration 1747, Loss: 0.9262802004814148\n",
      "Iteration 1748, Loss: 0.9241511225700378\n",
      "Iteration 1749, Loss: 0.9220142364501953\n",
      "Iteration 1750, Loss: 0.9198698997497559\n",
      "Iteration 1751, Loss: 0.9177183508872986\n",
      "Iteration 1752, Loss: 0.9155586957931519\n",
      "Iteration 1753, Loss: 0.9133917093276978\n",
      "Iteration 1754, Loss: 0.9112168550491333\n",
      "Iteration 1755, Loss: 0.909034252166748\n",
      "Iteration 1756, Loss: 0.906843364238739\n",
      "Iteration 1757, Loss: 0.9046446681022644\n",
      "Iteration 1758, Loss: 0.9024381041526794\n",
      "Iteration 1759, Loss: 0.9002229571342468\n",
      "Iteration 1760, Loss: 0.8980001211166382\n",
      "Iteration 1761, Loss: 0.8957688808441162\n",
      "Iteration 1762, Loss: 0.8935295343399048\n",
      "Iteration 1763, Loss: 0.8912814855575562\n",
      "Iteration 1764, Loss: 0.8890255689620972\n",
      "Iteration 1765, Loss: 0.8867606520652771\n",
      "Iteration 1766, Loss: 0.8844870924949646\n",
      "Iteration 1767, Loss: 0.8822051882743835\n",
      "Iteration 1768, Loss: 0.8799146413803101\n",
      "Iteration 1769, Loss: 0.8776149153709412\n",
      "Iteration 1770, Loss: 0.8753063082695007\n",
      "Iteration 1771, Loss: 0.8729890584945679\n",
      "Iteration 1772, Loss: 0.8706626296043396\n",
      "Iteration 1773, Loss: 0.8683270812034607\n",
      "Iteration 1774, Loss: 0.865982711315155\n",
      "Iteration 1775, Loss: 0.8636282682418823\n",
      "Iteration 1776, Loss: 0.8612650632858276\n",
      "Iteration 1777, Loss: 0.8588924407958984\n",
      "Iteration 1778, Loss: 0.85651034116745\n",
      "Iteration 1779, Loss: 0.8541181087493896\n",
      "Iteration 1780, Loss: 0.8517169952392578\n",
      "Iteration 1781, Loss: 0.8493058681488037\n",
      "Iteration 1782, Loss: 0.8468847274780273\n",
      "Iteration 1783, Loss: 0.8444535732269287\n",
      "Iteration 1784, Loss: 0.8420127630233765\n",
      "Iteration 1785, Loss: 0.8395617008209229\n",
      "Iteration 1786, Loss: 0.8371002674102783\n",
      "Iteration 1787, Loss: 0.8346285223960876\n",
      "Iteration 1788, Loss: 0.8321466445922852\n",
      "Iteration 1789, Loss: 0.8296539187431335\n",
      "Iteration 1790, Loss: 0.827150821685791\n",
      "Iteration 1791, Loss: 0.8246371150016785\n",
      "Iteration 1792, Loss: 0.8221127390861511\n",
      "Iteration 1793, Loss: 0.8195770978927612\n",
      "Iteration 1794, Loss: 0.8170307874679565\n",
      "Iteration 1795, Loss: 0.8144730925559998\n",
      "Iteration 1796, Loss: 0.8119044303894043\n",
      "Iteration 1797, Loss: 0.8093244433403015\n",
      "Iteration 1798, Loss: 0.8067330718040466\n",
      "Iteration 1799, Loss: 0.8041300177574158\n",
      "Iteration 1800, Loss: 0.8015153408050537\n",
      "Iteration 1801, Loss: 0.7988886833190918\n",
      "Iteration 1802, Loss: 0.7962508201599121\n",
      "Iteration 1803, Loss: 0.7936005592346191\n",
      "Iteration 1804, Loss: 0.7909382581710815\n",
      "Iteration 1805, Loss: 0.788263738155365\n",
      "Iteration 1806, Loss: 0.7855771780014038\n",
      "Iteration 1807, Loss: 0.7828776836395264\n",
      "Iteration 1808, Loss: 0.7801657915115356\n",
      "Iteration 1809, Loss: 0.777441680431366\n",
      "Iteration 1810, Loss: 0.7747039794921875\n",
      "Iteration 1811, Loss: 0.7719535827636719\n",
      "Iteration 1812, Loss: 0.7691901922225952\n",
      "Iteration 1813, Loss: 0.7664132118225098\n",
      "Iteration 1814, Loss: 0.7636231184005737\n",
      "Iteration 1815, Loss: 0.760819673538208\n",
      "Iteration 1816, Loss: 0.7580022811889648\n",
      "Iteration 1817, Loss: 0.7551710605621338\n",
      "Iteration 1818, Loss: 0.7523261308670044\n",
      "Iteration 1819, Loss: 0.7494670748710632\n",
      "Iteration 1820, Loss: 0.7465936541557312\n",
      "Iteration 1821, Loss: 0.743706226348877\n",
      "Iteration 1822, Loss: 0.7408040761947632\n",
      "Iteration 1823, Loss: 0.7378870844841003\n",
      "Iteration 1824, Loss: 0.7349551916122437\n",
      "Iteration 1825, Loss: 0.7320083379745483\n",
      "Iteration 1826, Loss: 0.7290463447570801\n",
      "Iteration 1827, Loss: 0.7260688543319702\n",
      "Iteration 1828, Loss: 0.7230759859085083\n",
      "Iteration 1829, Loss: 0.7200671434402466\n",
      "Iteration 1830, Loss: 0.7170425653457642\n",
      "Iteration 1831, Loss: 0.7140018939971924\n",
      "Iteration 1832, Loss: 0.7109452486038208\n",
      "Iteration 1833, Loss: 0.7078721523284912\n",
      "Iteration 1834, Loss: 0.7047823667526245\n",
      "Iteration 1835, Loss: 0.7016756534576416\n",
      "Iteration 1836, Loss: 0.6985520124435425\n",
      "Iteration 1837, Loss: 0.6954111456871033\n",
      "Iteration 1838, Loss: 0.6922531127929688\n",
      "Iteration 1839, Loss: 0.6890770792961121\n",
      "Iteration 1840, Loss: 0.6858832240104675\n",
      "Iteration 1841, Loss: 0.6826716661453247\n",
      "Iteration 1842, Loss: 0.6794416308403015\n",
      "Iteration 1843, Loss: 0.6761934757232666\n",
      "Iteration 1844, Loss: 0.6729263663291931\n",
      "Iteration 1845, Loss: 0.6696407198905945\n",
      "Iteration 1846, Loss: 0.6663358211517334\n",
      "Iteration 1847, Loss: 0.6630113124847412\n",
      "Iteration 1848, Loss: 0.6596674919128418\n",
      "Iteration 1849, Loss: 0.6563038229942322\n",
      "Iteration 1850, Loss: 0.6529198884963989\n",
      "Iteration 1851, Loss: 0.6495153903961182\n",
      "Iteration 1852, Loss: 0.6460905075073242\n",
      "Iteration 1853, Loss: 0.6426448822021484\n",
      "Iteration 1854, Loss: 0.6391780376434326\n",
      "Iteration 1855, Loss: 0.6356897354125977\n",
      "Iteration 1856, Loss: 0.6321799755096436\n",
      "Iteration 1857, Loss: 0.6286478042602539\n",
      "Iteration 1858, Loss: 0.625093400478363\n",
      "Iteration 1859, Loss: 0.6215164065361023\n",
      "Iteration 1860, Loss: 0.6179168224334717\n",
      "Iteration 1861, Loss: 0.6142944097518921\n",
      "Iteration 1862, Loss: 0.6106480360031128\n",
      "Iteration 1863, Loss: 0.6069779992103577\n",
      "Iteration 1864, Loss: 0.6032835841178894\n",
      "Iteration 1865, Loss: 0.5995649099349976\n",
      "Iteration 1866, Loss: 0.5958213210105896\n",
      "Iteration 1867, Loss: 0.5920529365539551\n",
      "Iteration 1868, Loss: 0.588259220123291\n",
      "Iteration 1869, Loss: 0.5844395160675049\n",
      "Iteration 1870, Loss: 0.580593466758728\n",
      "Iteration 1871, Loss: 0.57672119140625\n",
      "Iteration 1872, Loss: 0.5728214979171753\n",
      "Iteration 1873, Loss: 0.5688945055007935\n",
      "Iteration 1874, Loss: 0.5649399161338806\n",
      "Iteration 1875, Loss: 0.5609573125839233\n",
      "Iteration 1876, Loss: 0.5569462776184082\n",
      "Iteration 1877, Loss: 0.5529060363769531\n",
      "Iteration 1878, Loss: 0.5488365292549133\n",
      "Iteration 1879, Loss: 0.5447369813919067\n",
      "Iteration 1880, Loss: 0.5406073927879333\n",
      "Iteration 1881, Loss: 0.5364471673965454\n",
      "Iteration 1882, Loss: 0.5322553515434265\n",
      "Iteration 1883, Loss: 0.5280324220657349\n",
      "Iteration 1884, Loss: 0.5237770080566406\n",
      "Iteration 1885, Loss: 0.5194889903068542\n",
      "Iteration 1886, Loss: 0.5151678323745728\n",
      "Iteration 1887, Loss: 0.5108128786087036\n",
      "Iteration 1888, Loss: 0.5064235925674438\n",
      "Iteration 1889, Loss: 0.5020000338554382\n",
      "Iteration 1890, Loss: 0.4975407123565674\n",
      "Iteration 1891, Loss: 0.49304577708244324\n",
      "Iteration 1892, Loss: 0.4885144829750061\n",
      "Iteration 1893, Loss: 0.48394569754600525\n",
      "Iteration 1894, Loss: 0.4793400466442108\n",
      "Iteration 1895, Loss: 0.47469544410705566\n",
      "Iteration 1896, Loss: 0.4700124263763428\n",
      "Iteration 1897, Loss: 0.4652896821498871\n",
      "Iteration 1898, Loss: 0.46052655577659607\n",
      "Iteration 1899, Loss: 0.4557228982448578\n",
      "Iteration 1900, Loss: 0.45087742805480957\n",
      "Iteration 1901, Loss: 0.44598984718322754\n",
      "Iteration 1902, Loss: 0.44105929136276245\n",
      "Iteration 1903, Loss: 0.4360853135585785\n",
      "Iteration 1904, Loss: 0.4310663938522339\n",
      "Iteration 1905, Loss: 0.42600297927856445\n",
      "Iteration 1906, Loss: 0.420893132686615\n",
      "Iteration 1907, Loss: 0.4157366454601288\n",
      "Iteration 1908, Loss: 0.4105329215526581\n",
      "Iteration 1909, Loss: 0.4052806794643402\n",
      "Iteration 1910, Loss: 0.3999793231487274\n",
      "Iteration 1911, Loss: 0.39462780952453613\n",
      "Iteration 1912, Loss: 0.3892258107662201\n",
      "Iteration 1913, Loss: 0.38377195596694946\n",
      "Iteration 1914, Loss: 0.3782653212547302\n",
      "Iteration 1915, Loss: 0.37270545959472656\n",
      "Iteration 1916, Loss: 0.36709094047546387\n",
      "Iteration 1917, Loss: 0.3614211678504944\n",
      "Iteration 1918, Loss: 0.3556954562664032\n",
      "Iteration 1919, Loss: 0.34991252422332764\n",
      "Iteration 1920, Loss: 0.34407150745391846\n",
      "Iteration 1921, Loss: 0.33817124366760254\n",
      "Iteration 1922, Loss: 0.332211434841156\n",
      "Iteration 1923, Loss: 0.3261910080909729\n",
      "Iteration 1924, Loss: 0.3201087713241577\n",
      "Iteration 1925, Loss: 0.3139643371105194\n",
      "Iteration 1926, Loss: 0.30775654315948486\n",
      "Iteration 1927, Loss: 0.3014846742153168\n",
      "Iteration 1928, Loss: 0.29514822363853455\n",
      "Iteration 1929, Loss: 0.2887458801269531\n",
      "Iteration 1930, Loss: 0.2822781205177307\n",
      "Iteration 1931, Loss: 0.27574363350868225\n",
      "Iteration 1932, Loss: 0.2691422700881958\n",
      "Iteration 1933, Loss: 0.2624738812446594\n",
      "Iteration 1934, Loss: 0.2557385563850403\n",
      "Iteration 1935, Loss: 0.24893668293952942\n",
      "Iteration 1936, Loss: 0.2420678585767746\n",
      "Iteration 1937, Loss: 0.23513418436050415\n",
      "Iteration 1938, Loss: 0.22813498973846436\n",
      "Iteration 1939, Loss: 0.22107402980327606\n",
      "Iteration 1940, Loss: 0.21395182609558105\n",
      "Iteration 1941, Loss: 0.20677241683006287\n",
      "Iteration 1942, Loss: 0.1995382308959961\n",
      "Iteration 1943, Loss: 0.19225531816482544\n",
      "Iteration 1944, Loss: 0.18492889404296875\n",
      "Iteration 1945, Loss: 0.17756544053554535\n",
      "Iteration 1946, Loss: 0.170175239443779\n",
      "Iteration 1947, Loss: 0.16276812553405762\n",
      "Iteration 1948, Loss: 0.15535877645015717\n",
      "Iteration 1949, Loss: 0.14796245098114014\n",
      "Iteration 1950, Loss: 0.1406007707118988\n",
      "Iteration 1951, Loss: 0.13329604268074036\n",
      "Iteration 1952, Loss: 0.1260780692100525\n",
      "Iteration 1953, Loss: 0.11898128688335419\n",
      "Iteration 1954, Loss: 0.11204412579536438\n",
      "Iteration 1955, Loss: 0.10531207174062729\n",
      "Iteration 1956, Loss: 0.09883499145507812\n",
      "Iteration 1957, Loss: 0.09266635030508041\n",
      "Iteration 1958, Loss: 0.08685898035764694\n",
      "Iteration 1959, Loss: 0.08146412670612335\n",
      "Iteration 1960, Loss: 0.07652166485786438\n",
      "Iteration 1961, Loss: 0.07205850630998611\n",
      "Iteration 1962, Loss: 0.06808318942785263\n",
      "Iteration 1963, Loss: 0.06458418071269989\n",
      "Iteration 1964, Loss: 0.06153231859207153\n",
      "Iteration 1965, Loss: 0.05888363718986511\n",
      "Iteration 1966, Loss: 0.05658585950732231\n",
      "Iteration 1967, Loss: 0.05458451434969902\n",
      "Iteration 1968, Loss: 0.05386132746934891\n",
      "Iteration 1969, Loss: 0.05414098501205444\n",
      "Iteration 1970, Loss: 0.054180972278118134\n",
      "Iteration 1971, Loss: 0.0537857711315155\n",
      "Iteration 1972, Loss: 0.05444929003715515\n",
      "Iteration 1973, Loss: 0.053611546754837036\n",
      "Iteration 1974, Loss: 0.05454271659255028\n",
      "Iteration 1975, Loss: 0.05367954820394516\n",
      "Iteration 1976, Loss: 0.05441763252019882\n",
      "Iteration 1977, Loss: 0.05382373183965683\n",
      "Iteration 1978, Loss: 0.0542362742125988\n",
      "Iteration 1979, Loss: 0.05398353934288025\n",
      "Iteration 1980, Loss: 0.0540611557662487\n",
      "Iteration 1981, Loss: 0.05412868782877922\n",
      "Iteration 1982, Loss: 0.053941331803798676\n",
      "Iteration 1983, Loss: 0.05422261357307434\n",
      "Iteration 1984, Loss: 0.05391956493258476\n",
      "Iteration 1985, Loss: 0.05422380194067955\n",
      "Iteration 1986, Loss: 0.053989049047231674\n",
      "Iteration 1987, Loss: 0.05413752421736717\n",
      "Iteration 1988, Loss: 0.05409596115350723\n",
      "Iteration 1989, Loss: 0.054024647921323776\n",
      "Iteration 1990, Loss: 0.05419537425041199\n",
      "Iteration 1991, Loss: 0.05393455550074577\n",
      "Iteration 1992, Loss: 0.05426079034805298\n",
      "Iteration 1993, Loss: 0.05388907715678215\n",
      "Iteration 1994, Loss: 0.054281096905469894\n",
      "Iteration 1995, Loss: 0.05388779565691948\n",
      "Iteration 1996, Loss: 0.054262012243270874\n",
      "Iteration 1997, Loss: 0.05391722917556763\n",
      "Iteration 1998, Loss: 0.054219938814640045\n",
      "Iteration 1999, Loss: 0.05396057292819023\n",
      "Iteration 2000, Loss: 0.05417311191558838\n",
      "Iteration 2001, Loss: 0.05400277301669121\n",
      "Iteration 2002, Loss: 0.05413631349802017\n",
      "Iteration 2003, Loss: 0.05403223633766174\n",
      "Iteration 2004, Loss: 0.054118718951940536\n",
      "Iteration 2005, Loss: 0.05404273793101311\n",
      "Iteration 2006, Loss: 0.05412144958972931\n",
      "Iteration 2007, Loss: 0.054034553468227386\n",
      "Iteration 2008, Loss: 0.054138652980327606\n",
      "Iteration 2009, Loss: 0.054014772176742554\n",
      "Iteration 2010, Loss: 0.054161619395017624\n",
      "Iteration 2011, Loss: 0.05399229750037193\n",
      "Iteration 2012, Loss: 0.05418188497424126\n",
      "Iteration 2013, Loss: 0.05397512763738632\n",
      "Iteration 2014, Loss: 0.054193802177906036\n",
      "Iteration 2015, Loss: 0.053966980427503586\n",
      "Iteration 2016, Loss: 0.05419626086950302\n",
      "Iteration 2017, Loss: 0.05396793782711029\n",
      "Iteration 2018, Loss: 0.054190874099731445\n",
      "Iteration 2019, Loss: 0.053975146263837814\n",
      "Iteration 2020, Loss: 0.05418100953102112\n",
      "Iteration 2021, Loss: 0.05398504436016083\n",
      "Iteration 2022, Loss: 0.05417078360915184\n",
      "Iteration 2023, Loss: 0.05399443954229355\n",
      "Iteration 2024, Loss: 0.05416324734687805\n",
      "Iteration 2025, Loss: 0.05400001257658005\n",
      "Iteration 2026, Loss: 0.054160457104444504\n",
      "Iteration 2027, Loss: 0.054001323878765106\n",
      "Iteration 2028, Loss: 0.054161570966243744\n",
      "Iteration 2029, Loss: 0.05399898812174797\n",
      "Iteration 2030, Loss: 0.05416563153266907\n",
      "Iteration 2031, Loss: 0.05399467796087265\n",
      "Iteration 2032, Loss: 0.05417054891586304\n",
      "Iteration 2033, Loss: 0.05398987978696823\n",
      "Iteration 2034, Loss: 0.05417455732822418\n",
      "Iteration 2035, Loss: 0.053986430168151855\n",
      "Iteration 2036, Loss: 0.054176997393369675\n",
      "Iteration 2037, Loss: 0.053984783589839935\n",
      "Iteration 2038, Loss: 0.054177455604076385\n",
      "Iteration 2039, Loss: 0.05398518964648247\n",
      "Iteration 2040, Loss: 0.0541759729385376\n",
      "Iteration 2041, Loss: 0.05398699641227722\n",
      "Iteration 2042, Loss: 0.054173704236745834\n",
      "Iteration 2043, Loss: 0.05398936569690704\n",
      "Iteration 2044, Loss: 0.05417140573263168\n",
      "Iteration 2045, Loss: 0.053991373628377914\n",
      "Iteration 2046, Loss: 0.054169826209545135\n",
      "Iteration 2047, Loss: 0.05399243161082268\n",
      "Iteration 2048, Loss: 0.054169438779354095\n",
      "Iteration 2049, Loss: 0.05399268865585327\n",
      "Iteration 2050, Loss: 0.05416988581418991\n",
      "Iteration 2051, Loss: 0.05399186536669731\n",
      "Iteration 2052, Loss: 0.05417092889547348\n",
      "Iteration 2053, Loss: 0.053990721702575684\n",
      "Iteration 2054, Loss: 0.05417199060320854\n",
      "Iteration 2055, Loss: 0.05398958921432495\n",
      "Iteration 2056, Loss: 0.0541728213429451\n",
      "Iteration 2057, Loss: 0.053989045321941376\n",
      "Iteration 2058, Loss: 0.05417313426733017\n",
      "Iteration 2059, Loss: 0.05398900434374809\n",
      "Iteration 2060, Loss: 0.054173003882169724\n",
      "Iteration 2061, Loss: 0.05398939177393913\n",
      "Iteration 2062, Loss: 0.05417240783572197\n",
      "Iteration 2063, Loss: 0.05398990958929062\n",
      "Iteration 2064, Loss: 0.054171930998563766\n",
      "Iteration 2065, Loss: 0.053990378975868225\n",
      "Iteration 2066, Loss: 0.05417146533727646\n",
      "Iteration 2067, Loss: 0.05399131774902344\n",
      "Iteration 2068, Loss: 0.05417138338088989\n",
      "Iteration 2069, Loss: 0.05399123951792717\n",
      "Iteration 2070, Loss: 0.05417166277766228\n",
      "Iteration 2071, Loss: 0.053990885615348816\n",
      "Iteration 2072, Loss: 0.054172467440366745\n",
      "Iteration 2073, Loss: 0.053990304470062256\n",
      "Iteration 2074, Loss: 0.054173003882169724\n",
      "Iteration 2075, Loss: 0.053990066051483154\n",
      "Iteration 2076, Loss: 0.054173171520233154\n",
      "Iteration 2077, Loss: 0.05398997664451599\n",
      "Iteration 2078, Loss: 0.0541730634868145\n",
      "Iteration 2079, Loss: 0.053989868611097336\n",
      "Iteration 2080, Loss: 0.054172903299331665\n",
      "Iteration 2081, Loss: 0.05399010330438614\n",
      "Iteration 2082, Loss: 0.05417273938655853\n",
      "Iteration 2083, Loss: 0.053990334272384644\n",
      "Iteration 2084, Loss: 0.05417265743017197\n",
      "Iteration 2085, Loss: 0.05399038642644882\n",
      "Iteration 2086, Loss: 0.0541725754737854\n",
      "Iteration 2087, Loss: 0.05399038642644882\n",
      "Iteration 2088, Loss: 0.054172731935977936\n",
      "Iteration 2089, Loss: 0.05399042367935181\n",
      "Iteration 2090, Loss: 0.05417276546359062\n",
      "Iteration 2091, Loss: 0.05399037525057793\n",
      "Iteration 2092, Loss: 0.05417288467288017\n",
      "Iteration 2093, Loss: 0.05399030074477196\n",
      "Iteration 2094, Loss: 0.05417289584875107\n",
      "Iteration 2095, Loss: 0.053990330547094345\n",
      "Iteration 2096, Loss: 0.054173003882169724\n",
      "Iteration 2097, Loss: 0.05399010702967644\n",
      "Iteration 2098, Loss: 0.054173003882169724\n",
      "Iteration 2099, Loss: 0.053990256041288376\n",
      "Iteration 2100, Loss: 0.054172735661268234\n",
      "Iteration 2101, Loss: 0.053990304470062256\n",
      "Iteration 2102, Loss: 0.05417269095778465\n",
      "Iteration 2103, Loss: 0.05399038642644882\n",
      "Iteration 2104, Loss: 0.054172586649656296\n",
      "Iteration 2105, Loss: 0.05399053543806076\n",
      "Iteration 2106, Loss: 0.05417277663946152\n",
      "Iteration 2107, Loss: 0.05399014800786972\n",
      "Iteration 2108, Loss: 0.05417293310165405\n",
      "Iteration 2109, Loss: 0.05399029701948166\n",
      "Iteration 2110, Loss: 0.05417308211326599\n",
      "Iteration 2111, Loss: 0.053990211337804794\n",
      "Iteration 2112, Loss: 0.054173052310943604\n",
      "Iteration 2113, Loss: 0.053990140557289124\n",
      "Iteration 2114, Loss: 0.05417312681674957\n",
      "Iteration 2115, Loss: 0.05398991331458092\n",
      "Iteration 2116, Loss: 0.05417294055223465\n",
      "Iteration 2117, Loss: 0.05399010702967644\n",
      "Iteration 2118, Loss: 0.05417288839817047\n",
      "Iteration 2119, Loss: 0.05399026721715927\n",
      "Iteration 2120, Loss: 0.054172661155462265\n",
      "Iteration 2121, Loss: 0.053990304470062256\n",
      "Iteration 2122, Loss: 0.0541725754737854\n",
      "Iteration 2123, Loss: 0.05399050563573837\n",
      "Iteration 2124, Loss: 0.0541725754737854\n",
      "Iteration 2125, Loss: 0.05398976057767868\n",
      "Iteration 2126, Loss: 0.054172735661268234\n",
      "Iteration 2127, Loss: 0.05398974567651749\n",
      "Iteration 2128, Loss: 0.05417361110448837\n",
      "Iteration 2129, Loss: 0.053989313542842865\n",
      "Iteration 2130, Loss: 0.05417368561029434\n",
      "Iteration 2131, Loss: 0.05398954078555107\n",
      "Iteration 2132, Loss: 0.05417349189519882\n",
      "Iteration 2133, Loss: 0.05398958548903465\n",
      "Iteration 2134, Loss: 0.05417332798242569\n",
      "Iteration 2135, Loss: 0.05398979038000107\n",
      "Iteration 2136, Loss: 0.0541728213429451\n",
      "Iteration 2137, Loss: 0.053990185260772705\n",
      "Iteration 2138, Loss: 0.05417250096797943\n",
      "Iteration 2139, Loss: 0.05399049445986748\n",
      "Iteration 2140, Loss: 0.05417230725288391\n",
      "Iteration 2141, Loss: 0.05399053543806076\n",
      "Iteration 2142, Loss: 0.05417249724268913\n",
      "Iteration 2143, Loss: 0.053990572690963745\n",
      "Iteration 2144, Loss: 0.054172538220882416\n",
      "Iteration 2145, Loss: 0.05399042367935181\n",
      "Iteration 2146, Loss: 0.0541725754737854\n",
      "Iteration 2147, Loss: 0.053990453481674194\n",
      "Iteration 2148, Loss: 0.0541725754737854\n",
      "Iteration 2149, Loss: 0.05399037525057793\n",
      "Iteration 2150, Loss: 0.05417277663946152\n",
      "Iteration 2151, Loss: 0.053990256041288376\n",
      "Iteration 2152, Loss: 0.05417289584875107\n",
      "Iteration 2153, Loss: 0.05399010330438614\n",
      "Iteration 2154, Loss: 0.05417309328913689\n",
      "Iteration 2155, Loss: 0.05399005860090256\n",
      "Iteration 2156, Loss: 0.054173052310943604\n",
      "Iteration 2157, Loss: 0.05398993939161301\n",
      "Iteration 2158, Loss: 0.054173171520233154\n",
      "Iteration 2159, Loss: 0.05398993939161301\n",
      "Iteration 2160, Loss: 0.05417320132255554\n",
      "Iteration 2161, Loss: 0.05398982763290405\n",
      "Iteration 2162, Loss: 0.05417286604642868\n",
      "Iteration 2163, Loss: 0.053989291191101074\n",
      "Iteration 2164, Loss: 0.054172810167074203\n",
      "Iteration 2165, Loss: 0.05398957058787346\n",
      "Iteration 2166, Loss: 0.05417250096797943\n",
      "Iteration 2167, Loss: 0.05398976057767868\n",
      "Iteration 2168, Loss: 0.0541718415915966\n",
      "Iteration 2169, Loss: 0.053989849984645844\n",
      "Iteration 2170, Loss: 0.0541716143488884\n",
      "Iteration 2171, Loss: 0.05399007722735405\n",
      "Iteration 2172, Loss: 0.054171688854694366\n",
      "Iteration 2173, Loss: 0.05398983880877495\n",
      "Iteration 2174, Loss: 0.054171957075595856\n",
      "Iteration 2175, Loss: 0.053989797830581665\n",
      "Iteration 2176, Loss: 0.05417211353778839\n",
      "Iteration 2177, Loss: 0.053989484906196594\n",
      "Iteration 2178, Loss: 0.05417223274707794\n",
      "Iteration 2179, Loss: 0.05398936569690704\n",
      "Iteration 2180, Loss: 0.054172348231077194\n",
      "Iteration 2181, Loss: 0.05398925393819809\n",
      "Iteration 2182, Loss: 0.05417242646217346\n",
      "Iteration 2183, Loss: 0.05398944020271301\n",
      "Iteration 2184, Loss: 0.05417222902178764\n",
      "Iteration 2185, Loss: 0.053989481180906296\n",
      "Iteration 2186, Loss: 0.054172150790691376\n",
      "Iteration 2187, Loss: 0.05398964136838913\n",
      "Iteration 2188, Loss: 0.054171882569789886\n",
      "Iteration 2189, Loss: 0.05398973077535629\n",
      "Iteration 2190, Loss: 0.05417172238230705\n",
      "Iteration 2191, Loss: 0.05398999899625778\n",
      "Iteration 2192, Loss: 0.05417168140411377\n",
      "Iteration 2193, Loss: 0.05399002879858017\n",
      "Iteration 2194, Loss: 0.0541718415915966\n",
      "Iteration 2195, Loss: 0.053989872336387634\n",
      "Iteration 2196, Loss: 0.054171960800886154\n",
      "Iteration 2197, Loss: 0.0539897195994854\n",
      "Iteration 2198, Loss: 0.054172154515981674\n",
      "Iteration 2199, Loss: 0.05398952215909958\n",
      "Iteration 2200, Loss: 0.05417212098836899\n",
      "Iteration 2201, Loss: 0.05398952215909958\n",
      "Iteration 2202, Loss: 0.05417218804359436\n",
      "Iteration 2203, Loss: 0.05398945137858391\n",
      "Iteration 2204, Loss: 0.05417212098836899\n",
      "Iteration 2205, Loss: 0.05398937314748764\n",
      "Iteration 2206, Loss: 0.054172154515981674\n",
      "Iteration 2207, Loss: 0.05398960039019585\n",
      "Iteration 2208, Loss: 0.05417212098836899\n",
      "Iteration 2209, Loss: 0.053989529609680176\n",
      "Iteration 2210, Loss: 0.054172154515981674\n",
      "Iteration 2211, Loss: 0.05398960039019585\n",
      "Iteration 2212, Loss: 0.05417212098836899\n",
      "Iteration 2213, Loss: 0.053989529609680176\n",
      "Iteration 2214, Loss: 0.05417203903198242\n",
      "Iteration 2215, Loss: 0.05398960039019585\n",
      "Iteration 2216, Loss: 0.05417196452617645\n",
      "Iteration 2217, Loss: 0.053989529609680176\n",
      "Iteration 2218, Loss: 0.05417203530669212\n",
      "Iteration 2219, Loss: 0.053989723324775696\n",
      "Iteration 2220, Loss: 0.0541718527674675\n",
      "Iteration 2221, Loss: 0.05398976057767868\n",
      "Iteration 2222, Loss: 0.054171882569789886\n",
      "Iteration 2223, Loss: 0.05398976057767868\n",
      "Iteration 2224, Loss: 0.054171882569789886\n",
      "Iteration 2225, Loss: 0.05398957058787346\n",
      "Iteration 2226, Loss: 0.05417196452617645\n",
      "Iteration 2227, Loss: 0.053989604115486145\n",
      "Iteration 2228, Loss: 0.05417212098836899\n",
      "Iteration 2229, Loss: 0.05398957058787346\n",
      "Iteration 2230, Loss: 0.05417212098836899\n",
      "Iteration 2231, Loss: 0.05398949235677719\n",
      "Iteration 2232, Loss: 0.05417222902178764\n",
      "Iteration 2233, Loss: 0.053989529609680176\n",
      "Iteration 2234, Loss: 0.05417226254940033\n",
      "Iteration 2235, Loss: 0.053989559412002563\n",
      "Iteration 2236, Loss: 0.054172150790691376\n",
      "Iteration 2237, Loss: 0.053989678621292114\n",
      "Iteration 2238, Loss: 0.054172150790691376\n",
      "Iteration 2239, Loss: 0.05398964136838913\n",
      "Iteration 2240, Loss: 0.05417218804359436\n",
      "Iteration 2241, Loss: 0.053989678621292114\n",
      "Iteration 2242, Loss: 0.05417218804359436\n",
      "Iteration 2243, Loss: 0.0539897195994854\n",
      "Iteration 2244, Loss: 0.05417212098836899\n",
      "Iteration 2245, Loss: 0.05398960039019585\n",
      "Iteration 2246, Loss: 0.05417212098836899\n",
      "Iteration 2247, Loss: 0.05398960039019585\n",
      "Iteration 2248, Loss: 0.05417212098836899\n",
      "Iteration 2249, Loss: 0.05398956686258316\n",
      "Iteration 2250, Loss: 0.05417218804359436\n",
      "Iteration 2251, Loss: 0.05398945137858391\n",
      "Iteration 2252, Loss: 0.05417203903198242\n",
      "Iteration 2253, Loss: 0.053989529609680176\n",
      "Iteration 2254, Loss: 0.0541718453168869\n",
      "Iteration 2255, Loss: 0.05398957058787346\n",
      "Iteration 2256, Loss: 0.0541718415915966\n",
      "Iteration 2257, Loss: 0.05398968979716301\n",
      "Iteration 2258, Loss: 0.054171882569789886\n",
      "Iteration 2259, Loss: 0.053989797830581665\n",
      "Iteration 2260, Loss: 0.054171960800886154\n",
      "Iteration 2261, Loss: 0.05398973077535629\n",
      "Iteration 2262, Loss: 0.054172031581401825\n",
      "Iteration 2263, Loss: 0.0539897195994854\n",
      "Iteration 2264, Loss: 0.05417211353778839\n",
      "Iteration 2265, Loss: 0.05398937687277794\n",
      "Iteration 2266, Loss: 0.054172318428754807\n",
      "Iteration 2267, Loss: 0.05398928374052048\n",
      "Iteration 2268, Loss: 0.05417235940694809\n",
      "Iteration 2269, Loss: 0.053989287465810776\n",
      "Iteration 2270, Loss: 0.054172467440366745\n",
      "Iteration 2271, Loss: 0.05398932844400406\n",
      "Iteration 2272, Loss: 0.05417215824127197\n",
      "Iteration 2273, Loss: 0.05398952588438988\n",
      "Iteration 2274, Loss: 0.054172031581401825\n",
      "Iteration 2275, Loss: 0.05398968979716301\n",
      "Iteration 2276, Loss: 0.054171692579984665\n",
      "Iteration 2277, Loss: 0.05398968979716301\n",
      "Iteration 2278, Loss: 0.05417165160179138\n",
      "Iteration 2279, Loss: 0.05398987978696823\n",
      "Iteration 2280, Loss: 0.05417180061340332\n",
      "Iteration 2281, Loss: 0.05398976057767868\n",
      "Iteration 2282, Loss: 0.05417199432849884\n",
      "Iteration 2283, Loss: 0.053989604115486145\n",
      "Iteration 2284, Loss: 0.05417222902178764\n",
      "Iteration 2285, Loss: 0.05398952215909958\n",
      "Iteration 2286, Loss: 0.05417238920927048\n",
      "Iteration 2287, Loss: 0.053989291191101074\n",
      "Iteration 2288, Loss: 0.05417250841856003\n",
      "Iteration 2289, Loss: 0.053989287465810776\n",
      "Iteration 2290, Loss: 0.054172318428754807\n",
      "Iteration 2291, Loss: 0.05398925766348839\n",
      "Iteration 2292, Loss: 0.05417212098836899\n",
      "Iteration 2293, Loss: 0.05398945137858391\n",
      "Iteration 2294, Loss: 0.054171960800886154\n",
      "Iteration 2295, Loss: 0.05398957058787346\n",
      "Iteration 2296, Loss: 0.05417200177907944\n",
      "Iteration 2297, Loss: 0.0539897195994854\n",
      "Iteration 2298, Loss: 0.05417203530669212\n",
      "Iteration 2299, Loss: 0.0539897195994854\n",
      "Iteration 2300, Loss: 0.054172080010175705\n",
      "Iteration 2301, Loss: 0.05398960039019585\n",
      "Iteration 2302, Loss: 0.05417215824127197\n",
      "Iteration 2303, Loss: 0.05398936569690704\n",
      "Iteration 2304, Loss: 0.05417219549417496\n",
      "Iteration 2305, Loss: 0.05398945137858391\n",
      "Iteration 2306, Loss: 0.05417218804359436\n",
      "Iteration 2307, Loss: 0.05398960039019585\n",
      "Iteration 2308, Loss: 0.05417215824127197\n",
      "Iteration 2309, Loss: 0.05398960039019585\n",
      "Iteration 2310, Loss: 0.05417218804359436\n",
      "Iteration 2311, Loss: 0.05398945137858391\n",
      "Iteration 2312, Loss: 0.054172080010175705\n",
      "Iteration 2313, Loss: 0.05398949235677719\n",
      "Iteration 2314, Loss: 0.05417218804359436\n",
      "Iteration 2315, Loss: 0.0539897084236145\n",
      "Iteration 2316, Loss: 0.05417218804359436\n",
      "Iteration 2317, Loss: 0.053989481180906296\n",
      "Iteration 2318, Loss: 0.05417238920927048\n",
      "Iteration 2319, Loss: 0.053989287465810776\n",
      "Iteration 2320, Loss: 0.054172318428754807\n",
      "Iteration 2321, Loss: 0.05398925393819809\n",
      "Iteration 2322, Loss: 0.05417224019765854\n",
      "Iteration 2323, Loss: 0.05398937314748764\n",
      "Iteration 2324, Loss: 0.054172150790691376\n",
      "Iteration 2325, Loss: 0.053989410400390625\n",
      "Iteration 2326, Loss: 0.054171960800886154\n",
      "Iteration 2327, Loss: 0.053989604115486145\n",
      "Iteration 2328, Loss: 0.0541718415915966\n",
      "Iteration 2329, Loss: 0.05398968979716301\n",
      "Iteration 2330, Loss: 0.05417191982269287\n",
      "Iteration 2331, Loss: 0.053989723324775696\n",
      "Iteration 2332, Loss: 0.054171957075595856\n",
      "Iteration 2333, Loss: 0.0539897195994854\n",
      "Iteration 2334, Loss: 0.05417200177907944\n",
      "Iteration 2335, Loss: 0.05398961156606674\n",
      "Iteration 2336, Loss: 0.05417200177907944\n",
      "Iteration 2337, Loss: 0.05398957058787346\n",
      "Iteration 2338, Loss: 0.054171960800886154\n",
      "Iteration 2339, Loss: 0.053989529609680176\n",
      "Iteration 2340, Loss: 0.054171882569789886\n",
      "Iteration 2341, Loss: 0.05398964881896973\n",
      "Iteration 2342, Loss: 0.054171960800886154\n",
      "Iteration 2343, Loss: 0.05398968979716301\n",
      "Iteration 2344, Loss: 0.0541718527674675\n",
      "Iteration 2345, Loss: 0.053989678621292114\n",
      "Iteration 2346, Loss: 0.05417200177907944\n",
      "Iteration 2347, Loss: 0.05398964509367943\n",
      "Iteration 2348, Loss: 0.05417203530669212\n",
      "Iteration 2349, Loss: 0.0539897195994854\n",
      "Iteration 2350, Loss: 0.05417211353778839\n",
      "Iteration 2351, Loss: 0.05398964136838913\n",
      "Iteration 2352, Loss: 0.054172269999980927\n",
      "Iteration 2353, Loss: 0.05398933216929436\n",
      "Iteration 2354, Loss: 0.054172318428754807\n",
      "Iteration 2355, Loss: 0.053989212960004807\n",
      "Iteration 2356, Loss: 0.05417255684733391\n",
      "Iteration 2357, Loss: 0.05398909002542496\n",
      "Iteration 2358, Loss: 0.05417255312204361\n",
      "Iteration 2359, Loss: 0.05398913472890854\n",
      "Iteration 2360, Loss: 0.054172322154045105\n",
      "Iteration 2361, Loss: 0.053989291191101074\n",
      "Iteration 2362, Loss: 0.05417215824127197\n",
      "Iteration 2363, Loss: 0.05398945137858391\n",
      "Iteration 2364, Loss: 0.054171882569789886\n",
      "Iteration 2365, Loss: 0.05398954078555107\n",
      "Iteration 2366, Loss: 0.05417199432849884\n",
      "Iteration 2367, Loss: 0.0539897195994854\n",
      "Iteration 2368, Loss: 0.054172031581401825\n",
      "Iteration 2369, Loss: 0.05398961156606674\n",
      "Iteration 2370, Loss: 0.054172080010175705\n",
      "Iteration 2371, Loss: 0.05398952588438988\n",
      "Iteration 2372, Loss: 0.054172199219465256\n",
      "Iteration 2373, Loss: 0.05398952215909958\n",
      "Iteration 2374, Loss: 0.054172269999980927\n",
      "Iteration 2375, Loss: 0.05398937314748764\n",
      "Iteration 2376, Loss: 0.05417218804359436\n",
      "Iteration 2377, Loss: 0.05398945137858391\n",
      "Iteration 2378, Loss: 0.054172299802303314\n",
      "Iteration 2379, Loss: 0.05398952588438988\n",
      "Iteration 2380, Loss: 0.05417210981249809\n",
      "Iteration 2381, Loss: 0.05398961156606674\n",
      "Iteration 2382, Loss: 0.054171882569789886\n",
      "Iteration 2383, Loss: 0.05398976057767868\n",
      "Iteration 2384, Loss: 0.054171960800886154\n",
      "Iteration 2385, Loss: 0.053989797830581665\n",
      "Iteration 2386, Loss: 0.054171886295080185\n",
      "Iteration 2387, Loss: 0.0539897195994854\n",
      "Iteration 2388, Loss: 0.054172080010175705\n",
      "Iteration 2389, Loss: 0.053989678621292114\n",
      "Iteration 2390, Loss: 0.054172080010175705\n",
      "Iteration 2391, Loss: 0.053989481180906296\n",
      "Iteration 2392, Loss: 0.05417215824127197\n",
      "Iteration 2393, Loss: 0.05398940667510033\n",
      "Iteration 2394, Loss: 0.05417224019765854\n",
      "Iteration 2395, Loss: 0.05398944765329361\n",
      "Iteration 2396, Loss: 0.05417215824127197\n",
      "Iteration 2397, Loss: 0.05398957058787346\n",
      "Iteration 2398, Loss: 0.05417203530669212\n",
      "Iteration 2399, Loss: 0.05398968607187271\n",
      "Iteration 2400, Loss: 0.05417200177907944\n",
      "Iteration 2401, Loss: 0.05398964509367943\n",
      "Iteration 2402, Loss: 0.054171960800886154\n",
      "Iteration 2403, Loss: 0.05398964881896973\n",
      "Iteration 2404, Loss: 0.054171960800886154\n",
      "Iteration 2405, Loss: 0.05398983880877495\n",
      "Iteration 2406, Loss: 0.05417199432849884\n",
      "Iteration 2407, Loss: 0.05398964509367943\n",
      "Iteration 2408, Loss: 0.054172199219465256\n",
      "Iteration 2409, Loss: 0.05398933216929436\n",
      "Iteration 2410, Loss: 0.05417227745056152\n",
      "Iteration 2411, Loss: 0.053989212960004807\n",
      "Iteration 2412, Loss: 0.054172396659851074\n",
      "Iteration 2413, Loss: 0.05398932099342346\n",
      "Iteration 2414, Loss: 0.054172128438949585\n",
      "Iteration 2415, Loss: 0.05398944020271301\n",
      "Iteration 2416, Loss: 0.05417203903198242\n",
      "Iteration 2417, Loss: 0.053989604115486145\n",
      "Iteration 2418, Loss: 0.05417199432849884\n",
      "Iteration 2419, Loss: 0.05398961156606674\n",
      "Iteration 2420, Loss: 0.05417199432849884\n",
      "Iteration 2421, Loss: 0.05398968979716301\n",
      "Iteration 2422, Loss: 0.054171882569789886\n",
      "Iteration 2423, Loss: 0.05398961156606674\n",
      "Iteration 2424, Loss: 0.054171960800886154\n",
      "Iteration 2425, Loss: 0.05398976057767868\n",
      "Iteration 2426, Loss: 0.05417206883430481\n",
      "Iteration 2427, Loss: 0.05398964136838913\n",
      "Iteration 2428, Loss: 0.05417203903198242\n",
      "Iteration 2429, Loss: 0.053989674896001816\n",
      "Iteration 2430, Loss: 0.05417210981249809\n",
      "Iteration 2431, Loss: 0.05398957058787346\n",
      "Iteration 2432, Loss: 0.05417203903198242\n",
      "Iteration 2433, Loss: 0.05398949235677719\n",
      "Iteration 2434, Loss: 0.054171960800886154\n",
      "Iteration 2435, Loss: 0.05398964881896973\n",
      "Iteration 2436, Loss: 0.05417191982269287\n",
      "Iteration 2437, Loss: 0.05398983508348465\n",
      "Iteration 2438, Loss: 0.05417200177907944\n",
      "Iteration 2439, Loss: 0.05398976057767868\n",
      "Iteration 2440, Loss: 0.054172199219465256\n",
      "Iteration 2441, Loss: 0.05398944765329361\n",
      "Iteration 2442, Loss: 0.05417243391275406\n",
      "Iteration 2443, Loss: 0.05398917198181152\n",
      "Iteration 2444, Loss: 0.05417262762784958\n",
      "Iteration 2445, Loss: 0.05398912355303764\n",
      "Iteration 2446, Loss: 0.05417262762784958\n",
      "Iteration 2447, Loss: 0.0539889857172966\n",
      "Iteration 2448, Loss: 0.05417235940694809\n",
      "Iteration 2449, Loss: 0.053989291191101074\n",
      "Iteration 2450, Loss: 0.054172199219465256\n",
      "Iteration 2451, Loss: 0.05398952215909958\n",
      "Iteration 2452, Loss: 0.05417191982269287\n",
      "Iteration 2453, Loss: 0.053989723324775696\n",
      "Iteration 2454, Loss: 0.054171960800886154\n",
      "Iteration 2455, Loss: 0.053989678621292114\n",
      "Iteration 2456, Loss: 0.05417203903198242\n",
      "Iteration 2457, Loss: 0.0539897158741951\n",
      "Iteration 2458, Loss: 0.05417218804359436\n",
      "Iteration 2459, Loss: 0.05398960039019585\n",
      "Iteration 2460, Loss: 0.054172269999980927\n",
      "Iteration 2461, Loss: 0.053989481180906296\n",
      "Iteration 2462, Loss: 0.054172273725271225\n",
      "Iteration 2463, Loss: 0.05398955196142197\n",
      "Iteration 2464, Loss: 0.05417238920927048\n",
      "Iteration 2465, Loss: 0.05398937314748764\n",
      "Iteration 2466, Loss: 0.05417216941714287\n",
      "Iteration 2467, Loss: 0.05398944020271301\n",
      "Iteration 2468, Loss: 0.05417223274707794\n",
      "Iteration 2469, Loss: 0.05398940667510033\n",
      "Iteration 2470, Loss: 0.05417212098836899\n",
      "Iteration 2471, Loss: 0.053989559412002563\n",
      "Iteration 2472, Loss: 0.05417196452617645\n",
      "Iteration 2473, Loss: 0.05398956686258316\n",
      "Iteration 2474, Loss: 0.05417191982269287\n",
      "Iteration 2475, Loss: 0.05398961156606674\n",
      "Iteration 2476, Loss: 0.05417191982269287\n",
      "Iteration 2477, Loss: 0.05398976057767868\n",
      "Iteration 2478, Loss: 0.05417191982269287\n",
      "Iteration 2479, Loss: 0.05398964136838913\n",
      "Iteration 2480, Loss: 0.05417200177907944\n",
      "Iteration 2481, Loss: 0.05398961156606674\n",
      "Iteration 2482, Loss: 0.054171960800886154\n",
      "Iteration 2483, Loss: 0.05398976057767868\n",
      "Iteration 2484, Loss: 0.054171957075595856\n",
      "Iteration 2485, Loss: 0.053989753127098083\n",
      "Iteration 2486, Loss: 0.05417203903198242\n",
      "Iteration 2487, Loss: 0.053989604115486145\n",
      "Iteration 2488, Loss: 0.05417199432849884\n",
      "Iteration 2489, Loss: 0.053989723324775696\n",
      "Iteration 2490, Loss: 0.05417199060320854\n",
      "Iteration 2491, Loss: 0.0539897195994854\n",
      "Iteration 2492, Loss: 0.05417210981249809\n",
      "Iteration 2493, Loss: 0.05398964136838913\n",
      "Iteration 2494, Loss: 0.05417203903198242\n",
      "Iteration 2495, Loss: 0.05398957058787346\n",
      "Iteration 2496, Loss: 0.05417203903198242\n",
      "Iteration 2497, Loss: 0.053989559412002563\n",
      "Iteration 2498, Loss: 0.05417211353778839\n",
      "Iteration 2499, Loss: 0.05398949235677719\n",
      "Iteration 2500, Loss: 0.0541718527674675\n",
      "Iteration 2501, Loss: 0.05398957058787346\n",
      "Iteration 2502, Loss: 0.05417191982269287\n",
      "Iteration 2503, Loss: 0.053989723324775696\n",
      "Iteration 2504, Loss: 0.054171960800886154\n",
      "Iteration 2505, Loss: 0.05398987978696823\n",
      "Iteration 2506, Loss: 0.05417191609740257\n",
      "Iteration 2507, Loss: 0.053989723324775696\n",
      "Iteration 2508, Loss: 0.05417191982269287\n",
      "Iteration 2509, Loss: 0.053989678621292114\n",
      "Iteration 2510, Loss: 0.05417200177907944\n",
      "Iteration 2511, Loss: 0.05398964509367943\n",
      "Iteration 2512, Loss: 0.054172199219465256\n",
      "Iteration 2513, Loss: 0.05398933216929436\n",
      "Iteration 2514, Loss: 0.05417238920927048\n",
      "Iteration 2515, Loss: 0.05398928374052048\n",
      "Iteration 2516, Loss: 0.05417243763804436\n",
      "Iteration 2517, Loss: 0.053989287465810776\n",
      "Iteration 2518, Loss: 0.05417238920927048\n",
      "Iteration 2519, Loss: 0.053989287465810776\n",
      "Iteration 2520, Loss: 0.054172269999980927\n",
      "Iteration 2521, Loss: 0.053989410400390625\n",
      "Iteration 2522, Loss: 0.05417203903198242\n",
      "Iteration 2523, Loss: 0.05398949235677719\n",
      "Iteration 2524, Loss: 0.054171886295080185\n",
      "Iteration 2525, Loss: 0.053989678621292114\n",
      "Iteration 2526, Loss: 0.05417203903198242\n",
      "Iteration 2527, Loss: 0.0539897158741951\n",
      "Iteration 2528, Loss: 0.054172080010175705\n",
      "Iteration 2529, Loss: 0.053989678621292114\n",
      "Iteration 2530, Loss: 0.05417218804359436\n",
      "Iteration 2531, Loss: 0.053989484906196594\n",
      "Iteration 2532, Loss: 0.05417242646217346\n",
      "Iteration 2533, Loss: 0.053989361971616745\n",
      "Iteration 2534, Loss: 0.054172396659851074\n",
      "Iteration 2535, Loss: 0.05398932844400406\n",
      "Iteration 2536, Loss: 0.054172467440366745\n",
      "Iteration 2537, Loss: 0.053989291191101074\n",
      "Iteration 2538, Loss: 0.05417224019765854\n",
      "Iteration 2539, Loss: 0.053989481180906296\n",
      "Iteration 2540, Loss: 0.05417200177907944\n",
      "Iteration 2541, Loss: 0.0539897195994854\n",
      "Iteration 2542, Loss: 0.05417187511920929\n",
      "Iteration 2543, Loss: 0.05398968979716301\n",
      "Iteration 2544, Loss: 0.0541718415915966\n",
      "Iteration 2545, Loss: 0.053989797830581665\n",
      "Iteration 2546, Loss: 0.0541718453168869\n",
      "Iteration 2547, Loss: 0.05398976057767868\n",
      "Iteration 2548, Loss: 0.05417203903198242\n",
      "Iteration 2549, Loss: 0.05398949235677719\n",
      "Iteration 2550, Loss: 0.05417230725288391\n",
      "Iteration 2551, Loss: 0.053989361971616745\n",
      "Iteration 2552, Loss: 0.054172396659851074\n",
      "Iteration 2553, Loss: 0.05398932844400406\n",
      "Iteration 2554, Loss: 0.05417227745056152\n",
      "Iteration 2555, Loss: 0.053989361971616745\n",
      "Iteration 2556, Loss: 0.054172199219465256\n",
      "Iteration 2557, Loss: 0.05398933216929436\n",
      "Iteration 2558, Loss: 0.05417205020785332\n",
      "Iteration 2559, Loss: 0.05398952215909958\n",
      "Iteration 2560, Loss: 0.05417200177907944\n",
      "Iteration 2561, Loss: 0.05398968607187271\n",
      "Iteration 2562, Loss: 0.05417203530669212\n",
      "Iteration 2563, Loss: 0.053989678621292114\n",
      "Iteration 2564, Loss: 0.054172076284885406\n",
      "Iteration 2565, Loss: 0.05398961156606674\n",
      "Iteration 2566, Loss: 0.05417196452617645\n",
      "Iteration 2567, Loss: 0.053989529609680176\n",
      "Iteration 2568, Loss: 0.05417203903198242\n",
      "Iteration 2569, Loss: 0.05398956686258316\n",
      "Iteration 2570, Loss: 0.05417215824127197\n",
      "Iteration 2571, Loss: 0.05398960039019585\n",
      "Iteration 2572, Loss: 0.05417212098836899\n",
      "Iteration 2573, Loss: 0.05398952215909958\n",
      "Iteration 2574, Loss: 0.05417210981249809\n",
      "Iteration 2575, Loss: 0.05398964136838913\n",
      "Iteration 2576, Loss: 0.0541718453168869\n",
      "Iteration 2577, Loss: 0.0539897195994854\n",
      "Iteration 2578, Loss: 0.05417199432849884\n",
      "Iteration 2579, Loss: 0.05398983508348465\n",
      "Iteration 2580, Loss: 0.05417199060320854\n",
      "Iteration 2581, Loss: 0.053989678621292114\n",
      "Iteration 2582, Loss: 0.05417203530669212\n",
      "Iteration 2583, Loss: 0.05398964136838913\n",
      "Iteration 2584, Loss: 0.05417212098836899\n",
      "Iteration 2585, Loss: 0.053989291191101074\n",
      "Iteration 2586, Loss: 0.054172318428754807\n",
      "Iteration 2587, Loss: 0.05398940667510033\n",
      "Iteration 2588, Loss: 0.05417235195636749\n",
      "Iteration 2589, Loss: 0.053989291191101074\n",
      "Iteration 2590, Loss: 0.05417215824127197\n",
      "Iteration 2591, Loss: 0.05398945137858391\n",
      "Iteration 2592, Loss: 0.05417218804359436\n",
      "Iteration 2593, Loss: 0.05398956686258316\n",
      "Iteration 2594, Loss: 0.054172199219465256\n",
      "Iteration 2595, Loss: 0.053989559412002563\n",
      "Iteration 2596, Loss: 0.05417215824127197\n",
      "Iteration 2597, Loss: 0.053989484906196594\n",
      "Iteration 2598, Loss: 0.05417224019765854\n",
      "Iteration 2599, Loss: 0.053989481180906296\n",
      "Iteration 2600, Loss: 0.054172348231077194\n",
      "Iteration 2601, Loss: 0.05398924648761749\n",
      "Iteration 2602, Loss: 0.05417254567146301\n",
      "Iteration 2603, Loss: 0.053989242762327194\n",
      "Iteration 2604, Loss: 0.05417227745056152\n",
      "Iteration 2605, Loss: 0.05398925393819809\n",
      "Iteration 2606, Loss: 0.05417219549417496\n",
      "Iteration 2607, Loss: 0.053989529609680176\n",
      "Iteration 2608, Loss: 0.05417191982269287\n",
      "Iteration 2609, Loss: 0.05398980900645256\n",
      "Iteration 2610, Loss: 0.05417156219482422\n",
      "Iteration 2611, Loss: 0.05399003624916077\n",
      "Iteration 2612, Loss: 0.05417148396372795\n",
      "Iteration 2613, Loss: 0.05399011820554733\n",
      "Iteration 2614, Loss: 0.054171644151210785\n",
      "Iteration 2615, Loss: 0.0539899580180645\n",
      "Iteration 2616, Loss: 0.05417180806398392\n",
      "Iteration 2617, Loss: 0.05398961156606674\n",
      "Iteration 2618, Loss: 0.054172318428754807\n",
      "Iteration 2619, Loss: 0.053989361971616745\n",
      "Iteration 2620, Loss: 0.054172590374946594\n",
      "Iteration 2621, Loss: 0.05398901551961899\n",
      "Iteration 2622, Loss: 0.054172590374946594\n",
      "Iteration 2623, Loss: 0.0539889857172966\n",
      "Iteration 2624, Loss: 0.054172396659851074\n",
      "Iteration 2625, Loss: 0.053989287465810776\n",
      "Iteration 2626, Loss: 0.054172269999980927\n",
      "Iteration 2627, Loss: 0.05398949235677719\n",
      "Iteration 2628, Loss: 0.054171960800886154\n",
      "Iteration 2629, Loss: 0.05398968607187271\n",
      "Iteration 2630, Loss: 0.054172031581401825\n",
      "Iteration 2631, Loss: 0.05398976057767868\n",
      "Iteration 2632, Loss: 0.054171960800886154\n",
      "Iteration 2633, Loss: 0.05398964136838913\n",
      "Iteration 2634, Loss: 0.054172080010175705\n",
      "Iteration 2635, Loss: 0.0539897195994854\n",
      "Iteration 2636, Loss: 0.05417212098836899\n",
      "Iteration 2637, Loss: 0.05398952215909958\n",
      "Iteration 2638, Loss: 0.05417216569185257\n",
      "Iteration 2639, Loss: 0.0539894700050354\n",
      "Iteration 2640, Loss: 0.05417235940694809\n",
      "Iteration 2641, Loss: 0.053989291191101074\n",
      "Iteration 2642, Loss: 0.05417235195636749\n",
      "Iteration 2643, Loss: 0.053989361971616745\n",
      "Iteration 2644, Loss: 0.05417238175868988\n",
      "Iteration 2645, Loss: 0.053989410400390625\n",
      "Iteration 2646, Loss: 0.05417203903198242\n",
      "Iteration 2647, Loss: 0.05398961156606674\n",
      "Iteration 2648, Loss: 0.05417180061340332\n",
      "Iteration 2649, Loss: 0.05398973077535629\n",
      "Iteration 2650, Loss: 0.05417172238230705\n",
      "Iteration 2651, Loss: 0.05398976802825928\n",
      "Iteration 2652, Loss: 0.05417172238230705\n",
      "Iteration 2653, Loss: 0.05398976802825928\n",
      "Iteration 2654, Loss: 0.05417179688811302\n",
      "Iteration 2655, Loss: 0.05398973077535629\n",
      "Iteration 2656, Loss: 0.05417172238230705\n",
      "Iteration 2657, Loss: 0.05398976802825928\n",
      "Iteration 2658, Loss: 0.0541718415915966\n",
      "Iteration 2659, Loss: 0.05398990958929062\n",
      "Iteration 2660, Loss: 0.05417210981249809\n",
      "Iteration 2661, Loss: 0.053989678621292114\n",
      "Iteration 2662, Loss: 0.05417218804359436\n",
      "Iteration 2663, Loss: 0.053989410400390625\n",
      "Iteration 2664, Loss: 0.054172199219465256\n",
      "Iteration 2665, Loss: 0.05398944020271301\n",
      "Iteration 2666, Loss: 0.054172150790691376\n",
      "Iteration 2667, Loss: 0.05398952215909958\n",
      "Iteration 2668, Loss: 0.05417206883430481\n",
      "Iteration 2669, Loss: 0.05398961156606674\n",
      "Iteration 2670, Loss: 0.05417200177907944\n",
      "Iteration 2671, Loss: 0.05398964509367943\n",
      "Iteration 2672, Loss: 0.05417215824127197\n",
      "Iteration 2673, Loss: 0.05398952215909958\n",
      "Iteration 2674, Loss: 0.05417215824127197\n",
      "Iteration 2675, Loss: 0.05398937314748764\n",
      "Iteration 2676, Loss: 0.054172080010175705\n",
      "Iteration 2677, Loss: 0.05398949235677719\n",
      "Iteration 2678, Loss: 0.05417206883430481\n",
      "Iteration 2679, Loss: 0.05398957058787346\n",
      "Iteration 2680, Loss: 0.05417200177907944\n",
      "Iteration 2681, Loss: 0.05398964509367943\n",
      "Iteration 2682, Loss: 0.054171882569789886\n",
      "Iteration 2683, Loss: 0.05398964509367943\n",
      "Iteration 2684, Loss: 0.05417191982269287\n",
      "Iteration 2685, Loss: 0.05398976430296898\n",
      "Iteration 2686, Loss: 0.05417199432849884\n",
      "Iteration 2687, Loss: 0.0539897084236145\n",
      "Iteration 2688, Loss: 0.05417218804359436\n",
      "Iteration 2689, Loss: 0.05398901551961899\n",
      "Iteration 2690, Loss: 0.054172199219465256\n",
      "Iteration 2691, Loss: 0.05398889631032944\n",
      "Iteration 2692, Loss: 0.054171543568372726\n",
      "Iteration 2693, Loss: 0.05398862808942795\n",
      "Iteration 2694, Loss: 0.05417145416140556\n",
      "Iteration 2695, Loss: 0.0539887472987175\n",
      "Iteration 2696, Loss: 0.054171305149793625\n",
      "Iteration 2697, Loss: 0.053988825529813766\n",
      "Iteration 2698, Loss: 0.05417141318321228\n",
      "Iteration 2699, Loss: 0.05398885905742645\n",
      "Iteration 2700, Loss: 0.054171305149793625\n",
      "Iteration 2701, Loss: 0.053988903760910034\n",
      "Iteration 2702, Loss: 0.05417129397392273\n",
      "Iteration 2703, Loss: 0.05398894473910332\n",
      "Iteration 2704, Loss: 0.05417126417160034\n",
      "Iteration 2705, Loss: 0.053988978266716\n",
      "Iteration 2706, Loss: 0.054171495139598846\n",
      "Iteration 2707, Loss: 0.05398885905742645\n",
      "Iteration 2708, Loss: 0.05417146533727646\n",
      "Iteration 2709, Loss: 0.0539887472987175\n",
      "Iteration 2710, Loss: 0.05417158454656601\n",
      "Iteration 2711, Loss: 0.05398859083652496\n",
      "Iteration 2712, Loss: 0.054171688854694366\n",
      "Iteration 2713, Loss: 0.053988777101039886\n",
      "Iteration 2714, Loss: 0.0541716143488884\n",
      "Iteration 2715, Loss: 0.05398866534233093\n",
      "Iteration 2716, Loss: 0.05417145416140556\n",
      "Iteration 2717, Loss: 0.0539887472987175\n",
      "Iteration 2718, Loss: 0.05417134612798691\n",
      "Iteration 2719, Loss: 0.05398885905742645\n",
      "Iteration 2720, Loss: 0.054171301424503326\n",
      "Iteration 2721, Loss: 0.053988903760910034\n",
      "Iteration 2722, Loss: 0.05417121946811676\n",
      "Iteration 2723, Loss: 0.053989022970199585\n",
      "Iteration 2724, Loss: 0.05417115241289139\n",
      "Iteration 2725, Loss: 0.05398913472890854\n",
      "Iteration 2726, Loss: 0.05417134612798691\n",
      "Iteration 2727, Loss: 0.053988825529813766\n",
      "Iteration 2728, Loss: 0.05417153239250183\n",
      "Iteration 2729, Loss: 0.0539887398481369\n",
      "Iteration 2730, Loss: 0.05417164787650108\n",
      "Iteration 2731, Loss: 0.05398870259523392\n",
      "Iteration 2732, Loss: 0.054171379655599594\n",
      "Iteration 2733, Loss: 0.05398886650800705\n",
      "Iteration 2734, Loss: 0.0541713647544384\n",
      "Iteration 2735, Loss: 0.0539889894425869\n",
      "Iteration 2736, Loss: 0.05417117476463318\n",
      "Iteration 2737, Loss: 0.053989216685295105\n",
      "Iteration 2738, Loss: 0.05417110025882721\n",
      "Iteration 2739, Loss: 0.053989212960004807\n",
      "Iteration 2740, Loss: 0.05417121946811676\n",
      "Iteration 2741, Loss: 0.053989093750715256\n",
      "Iteration 2742, Loss: 0.054171379655599594\n",
      "Iteration 2743, Loss: 0.05398892983794212\n",
      "Iteration 2744, Loss: 0.05417173355817795\n",
      "Iteration 2745, Loss: 0.05398854613304138\n",
      "Iteration 2746, Loss: 0.0541718564927578\n",
      "Iteration 2747, Loss: 0.053988538682460785\n",
      "Iteration 2748, Loss: 0.054171934723854065\n",
      "Iteration 2749, Loss: 0.053988270461559296\n",
      "Iteration 2750, Loss: 0.054171815514564514\n",
      "Iteration 2751, Loss: 0.05398839712142944\n",
      "Iteration 2752, Loss: 0.054171573370695114\n",
      "Iteration 2753, Loss: 0.05398871749639511\n",
      "Iteration 2754, Loss: 0.054171375930309296\n",
      "Iteration 2755, Loss: 0.05398894473910332\n",
      "Iteration 2756, Loss: 0.054171182215213776\n",
      "Iteration 2757, Loss: 0.053989142179489136\n",
      "Iteration 2758, Loss: 0.054171107709407806\n",
      "Iteration 2759, Loss: 0.053989022970199585\n",
      "Iteration 2760, Loss: 0.05417117476463318\n",
      "Iteration 2761, Loss: 0.053989022970199585\n",
      "Iteration 2762, Loss: 0.054171185940504074\n",
      "Iteration 2763, Loss: 0.05398905277252197\n",
      "Iteration 2764, Loss: 0.05417141318321228\n",
      "Iteration 2765, Loss: 0.053988903760910034\n",
      "Iteration 2766, Loss: 0.054171495139598846\n",
      "Iteration 2767, Loss: 0.05398930236697197\n",
      "Iteration 2768, Loss: 0.05417153239250183\n",
      "Iteration 2769, Loss: 0.05398929864168167\n",
      "Iteration 2770, Loss: 0.05417206138372421\n",
      "Iteration 2771, Loss: 0.05398918315768242\n",
      "Iteration 2772, Loss: 0.05417206138372421\n",
      "Iteration 2773, Loss: 0.053989261388778687\n",
      "Iteration 2774, Loss: 0.054171930998563766\n",
      "Iteration 2775, Loss: 0.053989261388778687\n",
      "Iteration 2776, Loss: 0.05417182296514511\n",
      "Iteration 2777, Loss: 0.05398938059806824\n",
      "Iteration 2778, Loss: 0.054171741008758545\n",
      "Iteration 2779, Loss: 0.053989313542842865\n",
      "Iteration 2780, Loss: 0.05417170375585556\n",
      "Iteration 2781, Loss: 0.05398949980735779\n",
      "Iteration 2782, Loss: 0.054171741008758545\n",
      "Iteration 2783, Loss: 0.05398942157626152\n",
      "Iteration 2784, Loss: 0.054171741008758545\n",
      "Iteration 2785, Loss: 0.05398942157626152\n",
      "Iteration 2786, Loss: 0.05417182296514511\n",
      "Iteration 2787, Loss: 0.05398926883935928\n",
      "Iteration 2788, Loss: 0.0541718527674675\n",
      "Iteration 2789, Loss: 0.05398942157626152\n",
      "Iteration 2790, Loss: 0.054171860218048096\n",
      "Iteration 2791, Loss: 0.05398934334516525\n",
      "Iteration 2792, Loss: 0.05417182296514511\n",
      "Iteration 2793, Loss: 0.05398934334516525\n",
      "Iteration 2794, Loss: 0.054172083735466\n",
      "Iteration 2795, Loss: 0.05398918315768242\n",
      "Iteration 2796, Loss: 0.05417202040553093\n",
      "Iteration 2797, Loss: 0.053989067673683167\n",
      "Iteration 2798, Loss: 0.05417187139391899\n",
      "Iteration 2799, Loss: 0.05398929864168167\n",
      "Iteration 2800, Loss: 0.05417170748114586\n",
      "Iteration 2801, Loss: 0.05398938059806824\n",
      "Iteration 2802, Loss: 0.054171547293663025\n",
      "Iteration 2803, Loss: 0.05398949980735779\n",
      "Iteration 2804, Loss: 0.05417146906256676\n",
      "Iteration 2805, Loss: 0.053989581763744354\n",
      "Iteration 2806, Loss: 0.05417146906256676\n",
      "Iteration 2807, Loss: 0.05398955196142197\n",
      "Iteration 2808, Loss: 0.054171543568372726\n",
      "Iteration 2809, Loss: 0.05398961901664734\n",
      "Iteration 2810, Loss: 0.05417158827185631\n",
      "Iteration 2811, Loss: 0.05398949980735779\n",
      "Iteration 2812, Loss: 0.05417190119624138\n",
      "Iteration 2813, Loss: 0.05398918688297272\n",
      "Iteration 2814, Loss: 0.05417210981249809\n",
      "Iteration 2815, Loss: 0.0539889857172966\n",
      "Iteration 2816, Loss: 0.054172366857528687\n",
      "Iteration 2817, Loss: 0.05398886650800705\n",
      "Iteration 2818, Loss: 0.05417228862643242\n",
      "Iteration 2819, Loss: 0.053989022970199585\n",
      "Iteration 2820, Loss: 0.054171979427337646\n",
      "Iteration 2821, Loss: 0.053989067673683167\n",
      "Iteration 2822, Loss: 0.05417190119624138\n",
      "Iteration 2823, Loss: 0.05398938059806824\n",
      "Iteration 2824, Loss: 0.054171621799468994\n",
      "Iteration 2825, Loss: 0.05398957431316376\n",
      "Iteration 2826, Loss: 0.054171621799468994\n",
      "Iteration 2827, Loss: 0.05398954078555107\n",
      "Iteration 2828, Loss: 0.05417158454656601\n",
      "Iteration 2829, Loss: 0.05398964881896973\n",
      "Iteration 2830, Loss: 0.054171666502952576\n",
      "Iteration 2831, Loss: 0.05398938059806824\n",
      "Iteration 2832, Loss: 0.054171934723854065\n",
      "Iteration 2833, Loss: 0.05398937687277794\n",
      "Iteration 2834, Loss: 0.05417186766862869\n",
      "Iteration 2835, Loss: 0.05398914963006973\n",
      "Iteration 2836, Loss: 0.054171930998563766\n",
      "Iteration 2837, Loss: 0.05398930236697197\n",
      "Iteration 2838, Loss: 0.05417182296514511\n",
      "Iteration 2839, Loss: 0.05398926883935928\n",
      "Iteration 2840, Loss: 0.05417178198695183\n",
      "Iteration 2841, Loss: 0.05398926883935928\n",
      "Iteration 2842, Loss: 0.05417178198695183\n",
      "Iteration 2843, Loss: 0.05398930609226227\n",
      "Iteration 2844, Loss: 0.05417182296514511\n",
      "Iteration 2845, Loss: 0.05398926883935928\n",
      "Iteration 2846, Loss: 0.05417182296514511\n",
      "Iteration 2847, Loss: 0.05398937687277794\n",
      "Iteration 2848, Loss: 0.05417182296514511\n",
      "Iteration 2849, Loss: 0.05398945137858391\n",
      "Iteration 2850, Loss: 0.054171930998563766\n",
      "Iteration 2851, Loss: 0.05398930236697197\n",
      "Iteration 2852, Loss: 0.05417190119624138\n",
      "Iteration 2853, Loss: 0.05398910492658615\n",
      "Iteration 2854, Loss: 0.05417182296514511\n",
      "Iteration 2855, Loss: 0.0539892241358757\n",
      "Iteration 2856, Loss: 0.05417183041572571\n",
      "Iteration 2857, Loss: 0.05398934334516525\n",
      "Iteration 2858, Loss: 0.05417183041572571\n",
      "Iteration 2859, Loss: 0.053989261388778687\n",
      "Iteration 2860, Loss: 0.05417194217443466\n",
      "Iteration 2861, Loss: 0.05398929864168167\n",
      "Iteration 2862, Loss: 0.05417190119624138\n",
      "Iteration 2863, Loss: 0.05398929864168167\n",
      "Iteration 2864, Loss: 0.05417190119624138\n",
      "Iteration 2865, Loss: 0.05398918688297272\n",
      "Iteration 2866, Loss: 0.05417194217443466\n",
      "Iteration 2867, Loss: 0.053989194333553314\n",
      "Iteration 2868, Loss: 0.05417182296514511\n",
      "Iteration 2869, Loss: 0.053989194333553314\n",
      "Iteration 2870, Loss: 0.05417189002037048\n",
      "Iteration 2871, Loss: 0.053989261388778687\n",
      "Iteration 2872, Loss: 0.054171815514564514\n",
      "Iteration 2873, Loss: 0.05398934334516525\n",
      "Iteration 2874, Loss: 0.0541718564927578\n",
      "Iteration 2875, Loss: 0.05398942157626152\n",
      "Iteration 2876, Loss: 0.054171741008758545\n",
      "Iteration 2877, Loss: 0.05398938059806824\n",
      "Iteration 2878, Loss: 0.05417170375585556\n",
      "Iteration 2879, Loss: 0.05398949980735779\n",
      "Iteration 2880, Loss: 0.05417162925004959\n",
      "Iteration 2881, Loss: 0.05398942157626152\n",
      "Iteration 2882, Loss: 0.05417163297533989\n",
      "Iteration 2883, Loss: 0.05398938059806824\n",
      "Iteration 2884, Loss: 0.05417197197675705\n",
      "Iteration 2885, Loss: 0.05398930236697197\n",
      "Iteration 2886, Loss: 0.05417201668024063\n",
      "Iteration 2887, Loss: 0.0539892241358757\n",
      "Iteration 2888, Loss: 0.054171979427337646\n",
      "Iteration 2889, Loss: 0.053989142179489136\n",
      "Iteration 2890, Loss: 0.054171930998563766\n",
      "Iteration 2891, Loss: 0.053989261388778687\n",
      "Iteration 2892, Loss: 0.054171741008758545\n",
      "Iteration 2893, Loss: 0.053989388048648834\n",
      "Iteration 2894, Loss: 0.05417158454656601\n",
      "Iteration 2895, Loss: 0.053989510983228683\n",
      "Iteration 2896, Loss: 0.05417157709598541\n",
      "Iteration 2897, Loss: 0.053989510983228683\n",
      "Iteration 2898, Loss: 0.05417155474424362\n",
      "Iteration 2899, Loss: 0.053989510983228683\n",
      "Iteration 2900, Loss: 0.054171741008758545\n",
      "Iteration 2901, Loss: 0.053989313542842865\n",
      "Iteration 2902, Loss: 0.05417202040553093\n",
      "Iteration 2903, Loss: 0.05398903042078018\n",
      "Iteration 2904, Loss: 0.05417221039533615\n",
      "Iteration 2905, Loss: 0.05398891866207123\n",
      "Iteration 2906, Loss: 0.054172202944755554\n",
      "Iteration 2907, Loss: 0.05398910492658615\n",
      "Iteration 2908, Loss: 0.054171930998563766\n",
      "Iteration 2909, Loss: 0.05398926883935928\n",
      "Iteration 2910, Loss: 0.054171741008758545\n",
      "Iteration 2911, Loss: 0.053989313542842865\n",
      "Iteration 2912, Loss: 0.054171547293663025\n",
      "Iteration 2913, Loss: 0.053989581763744354\n",
      "Iteration 2914, Loss: 0.05417146906256676\n",
      "Iteration 2915, Loss: 0.05398949980735779\n",
      "Iteration 2916, Loss: 0.054171543568372726\n",
      "Iteration 2917, Loss: 0.05398958548903465\n",
      "Iteration 2918, Loss: 0.05417170375585556\n",
      "Iteration 2919, Loss: 0.05398949608206749\n",
      "Iteration 2920, Loss: 0.054171860218048096\n",
      "Iteration 2921, Loss: 0.05398903787136078\n",
      "Iteration 2922, Loss: 0.05417206138372421\n",
      "Iteration 2923, Loss: 0.05398903042078018\n",
      "Iteration 2924, Loss: 0.05417228862643242\n",
      "Iteration 2925, Loss: 0.05398891493678093\n",
      "Iteration 2926, Loss: 0.05417210981249809\n",
      "Iteration 2927, Loss: 0.053988873958587646\n",
      "Iteration 2928, Loss: 0.05417206138372421\n",
      "Iteration 2929, Loss: 0.05398907512426376\n",
      "Iteration 2930, Loss: 0.054171860218048096\n",
      "Iteration 2931, Loss: 0.05398915335536003\n",
      "Iteration 2932, Loss: 0.05417177826166153\n",
      "Iteration 2933, Loss: 0.05398942530155182\n",
      "Iteration 2934, Loss: 0.05417157709598541\n",
      "Iteration 2935, Loss: 0.05398965999484062\n",
      "Iteration 2936, Loss: 0.05417139455676079\n",
      "Iteration 2937, Loss: 0.05398965999484062\n",
      "Iteration 2938, Loss: 0.054171543568372726\n",
      "Iteration 2939, Loss: 0.053989581763744354\n",
      "Iteration 2940, Loss: 0.054171591997146606\n",
      "Iteration 2941, Loss: 0.053989432752132416\n",
      "Iteration 2942, Loss: 0.05417170748114586\n",
      "Iteration 2943, Loss: 0.05398938059806824\n",
      "Iteration 2944, Loss: 0.05417178198695183\n",
      "Iteration 2945, Loss: 0.05398930236697197\n",
      "Iteration 2946, Loss: 0.0541715994477272\n",
      "Iteration 2947, Loss: 0.05398934334516525\n",
      "Iteration 2948, Loss: 0.05417167395353317\n",
      "Iteration 2949, Loss: 0.05398938059806824\n",
      "Iteration 2950, Loss: 0.05417182296514511\n",
      "Iteration 2951, Loss: 0.05398938059806824\n",
      "Iteration 2952, Loss: 0.054171860218048096\n",
      "Iteration 2953, Loss: 0.05398930236697197\n",
      "Iteration 2954, Loss: 0.054171930998563766\n",
      "Iteration 2955, Loss: 0.05398934334516525\n",
      "Iteration 2956, Loss: 0.05417189747095108\n",
      "Iteration 2957, Loss: 0.053989261388778687\n",
      "Iteration 2958, Loss: 0.05417205020785332\n",
      "Iteration 2959, Loss: 0.0539892241358757\n",
      "Iteration 2960, Loss: 0.05417205020785332\n",
      "Iteration 2961, Loss: 0.05398903414607048\n",
      "Iteration 2962, Loss: 0.05417194217443466\n",
      "Iteration 2963, Loss: 0.0539892241358757\n",
      "Iteration 2964, Loss: 0.05417182296514511\n",
      "Iteration 2965, Loss: 0.05398927256464958\n",
      "Iteration 2966, Loss: 0.05417182669043541\n",
      "Iteration 2967, Loss: 0.05398930236697197\n",
      "Iteration 2968, Loss: 0.05417190119624138\n",
      "Iteration 2969, Loss: 0.0539892241358757\n",
      "Iteration 2970, Loss: 0.054171860218048096\n",
      "Iteration 2971, Loss: 0.053989261388778687\n",
      "Iteration 2972, Loss: 0.054171860218048096\n",
      "Iteration 2973, Loss: 0.05398927256464958\n",
      "Iteration 2974, Loss: 0.054171934723854065\n",
      "Iteration 2975, Loss: 0.05398966372013092\n",
      "Iteration 2976, Loss: 0.054171860218048096\n",
      "Iteration 2977, Loss: 0.053989630192518234\n",
      "Iteration 2978, Loss: 0.054171692579984665\n",
      "Iteration 2979, Loss: 0.053990092128515244\n",
      "Iteration 2980, Loss: 0.05417109653353691\n",
      "Iteration 2981, Loss: 0.05399029701948166\n",
      "Iteration 2982, Loss: 0.054170794785022736\n",
      "Iteration 2983, Loss: 0.053990256041288376\n",
      "Iteration 2984, Loss: 0.05417106673121452\n",
      "Iteration 2985, Loss: 0.05399012565612793\n",
      "Iteration 2986, Loss: 0.0541716143488884\n",
      "Iteration 2987, Loss: 0.0539894700050354\n",
      "Iteration 2988, Loss: 0.05417197197675705\n",
      "Iteration 2989, Loss: 0.05398911237716675\n",
      "Iteration 2990, Loss: 0.05417229235172272\n",
      "Iteration 2991, Loss: 0.053989022970199585\n",
      "Iteration 2992, Loss: 0.054172419011592865\n",
      "Iteration 2993, Loss: 0.053988829255104065\n",
      "Iteration 2994, Loss: 0.05417260527610779\n",
      "Iteration 2995, Loss: 0.05398879572749138\n",
      "Iteration 2996, Loss: 0.05417228862643242\n",
      "Iteration 2997, Loss: 0.05398911237716675\n",
      "Iteration 2998, Loss: 0.05417197197675705\n",
      "Iteration 2999, Loss: 0.05398942157626152\n",
      "Iteration 3000, Loss: 0.05417165905237198\n",
      "Iteration 3001, Loss: 0.05398973822593689\n",
      "Iteration 3002, Loss: 0.05417139083147049\n",
      "Iteration 3003, Loss: 0.053989775478839874\n",
      "Iteration 3004, Loss: 0.054171618074178696\n",
      "Iteration 3005, Loss: 0.05398973822593689\n",
      "Iteration 3006, Loss: 0.054171621799468994\n",
      "Iteration 3007, Loss: 0.05398949980735779\n",
      "Iteration 3008, Loss: 0.05417197570204735\n",
      "Iteration 3009, Loss: 0.053989227861166\n",
      "Iteration 3010, Loss: 0.05417216941714287\n",
      "Iteration 3011, Loss: 0.0539889931678772\n",
      "Iteration 3012, Loss: 0.05417221784591675\n",
      "Iteration 3013, Loss: 0.05398913845419884\n",
      "Iteration 3014, Loss: 0.05417213961482048\n",
      "Iteration 3015, Loss: 0.05398911237716675\n",
      "Iteration 3016, Loss: 0.0541720911860466\n",
      "Iteration 3017, Loss: 0.05398930236697197\n",
      "Iteration 3018, Loss: 0.05417178198695183\n",
      "Iteration 3019, Loss: 0.05398954078555107\n",
      "Iteration 3020, Loss: 0.05417146906256676\n",
      "Iteration 3021, Loss: 0.053989630192518234\n",
      "Iteration 3022, Loss: 0.05417150259017944\n",
      "Iteration 3023, Loss: 0.05398973822593689\n",
      "Iteration 3024, Loss: 0.05417131632566452\n",
      "Iteration 3025, Loss: 0.05398992821574211\n",
      "Iteration 3026, Loss: 0.054171498864889145\n",
      "Iteration 3027, Loss: 0.05398965999484062\n",
      "Iteration 3028, Loss: 0.05417157709598541\n",
      "Iteration 3029, Loss: 0.05398958548903465\n",
      "Iteration 3030, Loss: 0.054171692579984665\n",
      "Iteration 3031, Loss: 0.05398965999484062\n",
      "Iteration 3032, Loss: 0.05417158454656601\n",
      "Iteration 3033, Loss: 0.05398954078555107\n",
      "Iteration 3034, Loss: 0.05417158827185631\n",
      "Iteration 3035, Loss: 0.053989507257938385\n",
      "Iteration 3036, Loss: 0.05417146906256676\n",
      "Iteration 3037, Loss: 0.053989626467227936\n",
      "Iteration 3038, Loss: 0.05417150259017944\n",
      "Iteration 3039, Loss: 0.05398973822593689\n",
      "Iteration 3040, Loss: 0.05417139083147049\n",
      "Iteration 3041, Loss: 0.05398992821574211\n",
      "Iteration 3042, Loss: 0.054171498864889145\n",
      "Iteration 3043, Loss: 0.053989775478839874\n",
      "Iteration 3044, Loss: 0.054171621799468994\n",
      "Iteration 3045, Loss: 0.05398961901664734\n",
      "Iteration 3046, Loss: 0.05417177081108093\n",
      "Iteration 3047, Loss: 0.05398949980735779\n",
      "Iteration 3048, Loss: 0.05417178198695183\n",
      "Iteration 3049, Loss: 0.05398942530155182\n",
      "Iteration 3050, Loss: 0.05417170375585556\n",
      "Iteration 3051, Loss: 0.05398955196142197\n",
      "Iteration 3052, Loss: 0.054171543568372726\n",
      "Iteration 3053, Loss: 0.053989700973033905\n",
      "Iteration 3054, Loss: 0.0541716143488884\n",
      "Iteration 3055, Loss: 0.05398961901664734\n",
      "Iteration 3056, Loss: 0.054171618074178696\n",
      "Iteration 3057, Loss: 0.053989581763744354\n",
      "Iteration 3058, Loss: 0.054171472787857056\n",
      "Iteration 3059, Loss: 0.05398981273174286\n",
      "Iteration 3060, Loss: 0.05417158454656601\n",
      "Iteration 3061, Loss: 0.05398961901664734\n",
      "Iteration 3062, Loss: 0.05417170375585556\n",
      "Iteration 3063, Loss: 0.05398949980735779\n",
      "Iteration 3064, Loss: 0.054171741008758545\n",
      "Iteration 3065, Loss: 0.05398954078555107\n",
      "Iteration 3066, Loss: 0.05417170375585556\n",
      "Iteration 3067, Loss: 0.053989581763744354\n",
      "Iteration 3068, Loss: 0.054171547293663025\n",
      "Iteration 3069, Loss: 0.05398961901664734\n",
      "Iteration 3070, Loss: 0.05417146906256676\n",
      "Iteration 3071, Loss: 0.053989630192518234\n",
      "Iteration 3072, Loss: 0.054171495139598846\n",
      "Iteration 3073, Loss: 0.05398973822593689\n",
      "Iteration 3074, Loss: 0.054171349853277206\n",
      "Iteration 3075, Loss: 0.05398985743522644\n",
      "Iteration 3076, Loss: 0.054171543568372726\n",
      "Iteration 3077, Loss: 0.05398955196142197\n",
      "Iteration 3078, Loss: 0.05417170375585556\n",
      "Iteration 3079, Loss: 0.0539894625544548\n",
      "Iteration 3080, Loss: 0.05417202040553093\n",
      "Iteration 3081, Loss: 0.0539892241358757\n",
      "Iteration 3082, Loss: 0.0541720986366272\n",
      "Iteration 3083, Loss: 0.05398915335536003\n",
      "Iteration 3084, Loss: 0.05417205020785332\n",
      "Iteration 3085, Loss: 0.0539892315864563\n",
      "Iteration 3086, Loss: 0.05417170375585556\n",
      "Iteration 3087, Loss: 0.05398955196142197\n",
      "Iteration 3088, Loss: 0.054171424359083176\n",
      "Iteration 3089, Loss: 0.05398993939161301\n",
      "Iteration 3090, Loss: 0.05417122691869736\n",
      "Iteration 3091, Loss: 0.053990017622709274\n",
      "Iteration 3092, Loss: 0.05417134612798691\n",
      "Iteration 3093, Loss: 0.053989898413419724\n",
      "Iteration 3094, Loss: 0.054171424359083176\n",
      "Iteration 3095, Loss: 0.05398955196142197\n",
      "Iteration 3096, Loss: 0.054171811789274216\n",
      "Iteration 3097, Loss: 0.053989432752132416\n",
      "Iteration 3098, Loss: 0.0541718564927578\n",
      "Iteration 3099, Loss: 0.0539892315864563\n",
      "Iteration 3100, Loss: 0.054172009229660034\n",
      "Iteration 3101, Loss: 0.05398911237716675\n",
      "Iteration 3102, Loss: 0.054171930998563766\n",
      "Iteration 3103, Loss: 0.053989388048648834\n",
      "Iteration 3104, Loss: 0.05417166277766228\n",
      "Iteration 3105, Loss: 0.05398961901664734\n",
      "Iteration 3106, Loss: 0.054171305149793625\n",
      "Iteration 3107, Loss: 0.053989894688129425\n",
      "Iteration 3108, Loss: 0.05417107045650482\n",
      "Iteration 3109, Loss: 0.05398993939161301\n",
      "Iteration 3110, Loss: 0.05417114496231079\n",
      "Iteration 3111, Loss: 0.05399012565612793\n",
      "Iteration 3112, Loss: 0.05417133867740631\n",
      "Iteration 3113, Loss: 0.053989749401807785\n",
      "Iteration 3114, Loss: 0.054171543568372726\n",
      "Iteration 3115, Loss: 0.05398954451084137\n",
      "Iteration 3116, Loss: 0.054171811789274216\n",
      "Iteration 3117, Loss: 0.0539894662797451\n",
      "Iteration 3118, Loss: 0.054171741008758545\n",
      "Iteration 3119, Loss: 0.05398935079574585\n",
      "Iteration 3120, Loss: 0.05417182296514511\n",
      "Iteration 3121, Loss: 0.05398938059806824\n",
      "Iteration 3122, Loss: 0.054171934723854065\n",
      "Iteration 3123, Loss: 0.05398935079574585\n",
      "Iteration 3124, Loss: 0.05417178198695183\n",
      "Iteration 3125, Loss: 0.05398935079574585\n",
      "Iteration 3126, Loss: 0.054171621799468994\n",
      "Iteration 3127, Loss: 0.0539894700050354\n",
      "Iteration 3128, Loss: 0.054171618074178696\n",
      "Iteration 3129, Loss: 0.053989630192518234\n",
      "Iteration 3130, Loss: 0.05417146533727646\n",
      "Iteration 3131, Loss: 0.05398981273174286\n",
      "Iteration 3132, Loss: 0.054171543568372726\n",
      "Iteration 3133, Loss: 0.05398965999484062\n",
      "Iteration 3134, Loss: 0.0541716143488884\n",
      "Iteration 3135, Loss: 0.05398965999484062\n",
      "Iteration 3136, Loss: 0.05417139083147049\n",
      "Iteration 3137, Loss: 0.05398965999484062\n",
      "Iteration 3138, Loss: 0.05417150259017944\n",
      "Iteration 3139, Loss: 0.05398977920413017\n",
      "Iteration 3140, Loss: 0.054171543568372726\n",
      "Iteration 3141, Loss: 0.053989700973033905\n",
      "Iteration 3142, Loss: 0.05417146533727646\n",
      "Iteration 3143, Loss: 0.05398958921432495\n",
      "Iteration 3144, Loss: 0.05417146533727646\n",
      "Iteration 3145, Loss: 0.05398958921432495\n",
      "Iteration 3146, Loss: 0.054171543568372726\n",
      "Iteration 3147, Loss: 0.05398977920413017\n",
      "Iteration 3148, Loss: 0.0541716143488884\n",
      "Iteration 3149, Loss: 0.05398973822593689\n",
      "Iteration 3150, Loss: 0.05417165160179138\n",
      "Iteration 3151, Loss: 0.05398961901664734\n",
      "Iteration 3152, Loss: 0.054171692579984665\n",
      "Iteration 3153, Loss: 0.05398965999484062\n",
      "Iteration 3154, Loss: 0.05417166277766228\n",
      "Iteration 3155, Loss: 0.05398961901664734\n",
      "Iteration 3156, Loss: 0.05417166277766228\n",
      "Iteration 3157, Loss: 0.05398954078555107\n",
      "Iteration 3158, Loss: 0.05417171120643616\n",
      "Iteration 3159, Loss: 0.05398942530155182\n",
      "Iteration 3160, Loss: 0.05417178198695183\n",
      "Iteration 3161, Loss: 0.05398938059806824\n",
      "Iteration 3162, Loss: 0.054171979427337646\n",
      "Iteration 3163, Loss: 0.05398934707045555\n",
      "Iteration 3164, Loss: 0.05417202040553093\n",
      "Iteration 3165, Loss: 0.05398907512426376\n",
      "Iteration 3166, Loss: 0.054172366857528687\n",
      "Iteration 3167, Loss: 0.05398888140916824\n",
      "Iteration 3168, Loss: 0.05417248606681824\n",
      "Iteration 3169, Loss: 0.05398867651820183\n",
      "Iteration 3170, Loss: 0.054172419011592865\n",
      "Iteration 3171, Loss: 0.053988873958587646\n",
      "Iteration 3172, Loss: 0.05417228862643242\n",
      "Iteration 3173, Loss: 0.05398891866207123\n",
      "Iteration 3174, Loss: 0.05417201668024063\n",
      "Iteration 3175, Loss: 0.053989227861166\n",
      "Iteration 3176, Loss: 0.054171815514564514\n",
      "Iteration 3177, Loss: 0.05398954078555107\n",
      "Iteration 3178, Loss: 0.05417158827185631\n",
      "Iteration 3179, Loss: 0.05398961901664734\n",
      "Iteration 3180, Loss: 0.054171621799468994\n",
      "Iteration 3181, Loss: 0.053989700973033905\n",
      "Iteration 3182, Loss: 0.054171621799468994\n",
      "Iteration 3183, Loss: 0.05398961901664734\n",
      "Iteration 3184, Loss: 0.05417178198695183\n",
      "Iteration 3185, Loss: 0.05398935079574585\n",
      "Iteration 3186, Loss: 0.05417202040553093\n",
      "Iteration 3187, Loss: 0.05398911237716675\n",
      "Iteration 3188, Loss: 0.05417218059301376\n",
      "Iteration 3189, Loss: 0.05398903042078018\n",
      "Iteration 3190, Loss: 0.05417213961482048\n",
      "Iteration 3191, Loss: 0.05398906394839287\n",
      "Iteration 3192, Loss: 0.05417216941714287\n",
      "Iteration 3193, Loss: 0.053989142179489136\n",
      "Iteration 3194, Loss: 0.05417190119624138\n",
      "Iteration 3195, Loss: 0.0539894625544548\n",
      "Iteration 3196, Loss: 0.054171666502952576\n",
      "Iteration 3197, Loss: 0.053989510983228683\n",
      "Iteration 3198, Loss: 0.05417151376605034\n",
      "Iteration 3199, Loss: 0.05398955196142197\n",
      "Iteration 3200, Loss: 0.054171543568372726\n",
      "Iteration 3201, Loss: 0.05398973822593689\n",
      "Iteration 3202, Loss: 0.054171621799468994\n",
      "Iteration 3203, Loss: 0.053989656269550323\n",
      "Iteration 3204, Loss: 0.0541718527674675\n",
      "Iteration 3205, Loss: 0.053989388048648834\n",
      "Iteration 3206, Loss: 0.05417202040553093\n",
      "Iteration 3207, Loss: 0.05398915335536003\n",
      "Iteration 3208, Loss: 0.054172247648239136\n",
      "Iteration 3209, Loss: 0.05398906394839287\n",
      "Iteration 3210, Loss: 0.05417225882411003\n",
      "Iteration 3211, Loss: 0.0539889894425869\n",
      "Iteration 3212, Loss: 0.05417216941714287\n",
      "Iteration 3213, Loss: 0.053989142179489136\n",
      "Iteration 3214, Loss: 0.05417174845933914\n",
      "Iteration 3215, Loss: 0.05398935079574585\n",
      "Iteration 3216, Loss: 0.05417158454656601\n",
      "Iteration 3217, Loss: 0.053989700973033905\n",
      "Iteration 3218, Loss: 0.054171543568372726\n",
      "Iteration 3219, Loss: 0.05398973822593689\n",
      "Iteration 3220, Loss: 0.054171621799468994\n",
      "Iteration 3221, Loss: 0.053989700973033905\n",
      "Iteration 3222, Loss: 0.05417166277766228\n",
      "Iteration 3223, Loss: 0.05398927628993988\n",
      "Iteration 3224, Loss: 0.05417197570204735\n",
      "Iteration 3225, Loss: 0.053989119827747345\n",
      "Iteration 3226, Loss: 0.05417205020785332\n",
      "Iteration 3227, Loss: 0.0539892315864563\n",
      "Iteration 3228, Loss: 0.05417202040553093\n",
      "Iteration 3229, Loss: 0.05398915335536003\n",
      "Iteration 3230, Loss: 0.05417202040553093\n",
      "Iteration 3231, Loss: 0.05398915335536003\n",
      "Iteration 3232, Loss: 0.05417186766862869\n",
      "Iteration 3233, Loss: 0.0539892315864563\n",
      "Iteration 3234, Loss: 0.0541718564927578\n",
      "Iteration 3235, Loss: 0.05398935079574585\n",
      "Iteration 3236, Loss: 0.05417170375585556\n",
      "Iteration 3237, Loss: 0.05398954078555107\n",
      "Iteration 3238, Loss: 0.054171930998563766\n",
      "Iteration 3239, Loss: 0.053989313542842865\n",
      "Iteration 3240, Loss: 0.054172009229660034\n",
      "Iteration 3241, Loss: 0.05398930236697197\n",
      "Iteration 3242, Loss: 0.05417190119624138\n",
      "Iteration 3243, Loss: 0.05398915708065033\n",
      "Iteration 3244, Loss: 0.054171934723854065\n",
      "Iteration 3245, Loss: 0.0539894625544548\n",
      "Iteration 3246, Loss: 0.054171930998563766\n",
      "Iteration 3247, Loss: 0.05398942157626152\n",
      "Iteration 3248, Loss: 0.05417170748114586\n",
      "Iteration 3249, Loss: 0.05398954451084137\n",
      "Iteration 3250, Loss: 0.05417182296514511\n",
      "Iteration 3251, Loss: 0.05398949980735779\n",
      "Iteration 3252, Loss: 0.054171934723854065\n",
      "Iteration 3253, Loss: 0.05398927256464958\n",
      "Iteration 3254, Loss: 0.054172053933143616\n",
      "Iteration 3255, Loss: 0.05398911237716675\n",
      "Iteration 3256, Loss: 0.054172128438949585\n",
      "Iteration 3257, Loss: 0.05398903042078018\n",
      "Iteration 3258, Loss: 0.05417218059301376\n",
      "Iteration 3259, Loss: 0.053989067673683167\n",
      "Iteration 3260, Loss: 0.054172247648239136\n",
      "Iteration 3261, Loss: 0.05398907512426376\n",
      "Iteration 3262, Loss: 0.054172128438949585\n",
      "Iteration 3263, Loss: 0.053989194333553314\n",
      "Iteration 3264, Loss: 0.05417197197675705\n",
      "Iteration 3265, Loss: 0.05398934707045555\n",
      "Iteration 3266, Loss: 0.05417178198695183\n",
      "Iteration 3267, Loss: 0.05398942157626152\n",
      "Iteration 3268, Loss: 0.054171741008758545\n",
      "Iteration 3269, Loss: 0.053989581763744354\n",
      "Iteration 3270, Loss: 0.05417166277766228\n",
      "Iteration 3271, Loss: 0.053989581763744354\n",
      "Iteration 3272, Loss: 0.054171741008758545\n",
      "Iteration 3273, Loss: 0.0539894625544548\n",
      "Iteration 3274, Loss: 0.05417194217443466\n",
      "Iteration 3275, Loss: 0.053989194333553314\n",
      "Iteration 3276, Loss: 0.054172173142433167\n",
      "Iteration 3277, Loss: 0.05398903414607048\n",
      "Iteration 3278, Loss: 0.05417218059301376\n",
      "Iteration 3279, Loss: 0.05398903414607048\n",
      "Iteration 3280, Loss: 0.054172106087207794\n",
      "Iteration 3281, Loss: 0.05398911237716675\n",
      "Iteration 3282, Loss: 0.0541720911860466\n",
      "Iteration 3283, Loss: 0.053989194333553314\n",
      "Iteration 3284, Loss: 0.05417197197675705\n",
      "Iteration 3285, Loss: 0.05398934334516525\n",
      "Iteration 3286, Loss: 0.054171930998563766\n",
      "Iteration 3287, Loss: 0.0539894625544548\n",
      "Iteration 3288, Loss: 0.0541718527674675\n",
      "Iteration 3289, Loss: 0.05398949980735779\n",
      "Iteration 3290, Loss: 0.054171860218048096\n",
      "Iteration 3291, Loss: 0.05398938059806824\n",
      "Iteration 3292, Loss: 0.05417201668024063\n",
      "Iteration 3293, Loss: 0.0539892241358757\n",
      "Iteration 3294, Loss: 0.0541720949113369\n",
      "Iteration 3295, Loss: 0.05398918315768242\n",
      "Iteration 3296, Loss: 0.05417216941714287\n",
      "Iteration 3297, Loss: 0.053989067673683167\n",
      "Iteration 3298, Loss: 0.05417216941714287\n",
      "Iteration 3299, Loss: 0.05398910865187645\n",
      "Iteration 3300, Loss: 0.05417171120643616\n",
      "Iteration 3301, Loss: 0.05398938059806824\n",
      "Iteration 3302, Loss: 0.05417158827185631\n",
      "Iteration 3303, Loss: 0.05398965999484062\n",
      "Iteration 3304, Loss: 0.05417143553495407\n",
      "Iteration 3305, Loss: 0.05398967117071152\n",
      "Iteration 3306, Loss: 0.05417150259017944\n",
      "Iteration 3307, Loss: 0.05398973822593689\n",
      "Iteration 3308, Loss: 0.05417169630527496\n",
      "Iteration 3309, Loss: 0.05398954078555107\n",
      "Iteration 3310, Loss: 0.05417197197675705\n",
      "Iteration 3311, Loss: 0.053989194333553314\n",
      "Iteration 3312, Loss: 0.0541720911860466\n",
      "Iteration 3313, Loss: 0.053989194333553314\n",
      "Iteration 3314, Loss: 0.05417194217443466\n",
      "Iteration 3315, Loss: 0.053989313542842865\n",
      "Iteration 3316, Loss: 0.054172009229660034\n",
      "Iteration 3317, Loss: 0.0539892315864563\n",
      "Iteration 3318, Loss: 0.054172053933143616\n",
      "Iteration 3319, Loss: 0.05398910865187645\n",
      "Iteration 3320, Loss: 0.054172173142433167\n",
      "Iteration 3321, Loss: 0.05398918315768242\n",
      "Iteration 3322, Loss: 0.05417206138372421\n",
      "Iteration 3323, Loss: 0.05398915335536003\n",
      "Iteration 3324, Loss: 0.054172124713659286\n",
      "Iteration 3325, Loss: 0.053989194333553314\n",
      "Iteration 3326, Loss: 0.05417197197675705\n",
      "Iteration 3327, Loss: 0.05398934707045555\n",
      "Iteration 3328, Loss: 0.054171960800886154\n",
      "Iteration 3329, Loss: 0.05398942530155182\n",
      "Iteration 3330, Loss: 0.054171741008758545\n",
      "Iteration 3331, Loss: 0.0539894625544548\n",
      "Iteration 3332, Loss: 0.05417177826166153\n",
      "Iteration 3333, Loss: 0.0539894625544548\n",
      "Iteration 3334, Loss: 0.054171815514564514\n",
      "Iteration 3335, Loss: 0.0539894625544548\n",
      "Iteration 3336, Loss: 0.05417182296514511\n",
      "Iteration 3337, Loss: 0.05398949980735779\n",
      "Iteration 3338, Loss: 0.0541718527674675\n",
      "Iteration 3339, Loss: 0.05398949980735779\n",
      "Iteration 3340, Loss: 0.054171930998563766\n",
      "Iteration 3341, Loss: 0.05398985743522644\n",
      "Iteration 3342, Loss: 0.054172128438949585\n",
      "Iteration 3343, Loss: 0.053989581763744354\n",
      "Iteration 3344, Loss: 0.054172247648239136\n",
      "Iteration 3345, Loss: 0.05398958548903465\n",
      "Iteration 3346, Loss: 0.054172612726688385\n",
      "Iteration 3347, Loss: 0.05398958548903465\n",
      "Iteration 3348, Loss: 0.05417237803339958\n",
      "Iteration 3349, Loss: 0.053989749401807785\n",
      "Iteration 3350, Loss: 0.05417218059301376\n",
      "Iteration 3351, Loss: 0.053990017622709274\n",
      "Iteration 3352, Loss: 0.05417216941714287\n",
      "Iteration 3353, Loss: 0.05399005860090256\n",
      "Iteration 3354, Loss: 0.05417213961482048\n",
      "Iteration 3355, Loss: 0.05399009585380554\n",
      "Iteration 3356, Loss: 0.05417213961482048\n",
      "Iteration 3357, Loss: 0.05399013310670853\n",
      "Iteration 3358, Loss: 0.0541723296046257\n",
      "Iteration 3359, Loss: 0.05398997664451599\n",
      "Iteration 3360, Loss: 0.05417252331972122\n",
      "Iteration 3361, Loss: 0.05398973822593689\n",
      "Iteration 3362, Loss: 0.05417264997959137\n",
      "Iteration 3363, Loss: 0.0539894625544548\n",
      "Iteration 3364, Loss: 0.05417269095778465\n",
      "Iteration 3365, Loss: 0.053989432752132416\n",
      "Iteration 3366, Loss: 0.0541725680232048\n",
      "Iteration 3367, Loss: 0.05398955196142197\n",
      "Iteration 3368, Loss: 0.0541723370552063\n",
      "Iteration 3369, Loss: 0.053989969193935394\n",
      "Iteration 3370, Loss: 0.054172173142433167\n",
      "Iteration 3371, Loss: 0.053990017622709274\n",
      "Iteration 3372, Loss: 0.054171979427337646\n",
      "Iteration 3373, Loss: 0.05399017781019211\n",
      "Iteration 3374, Loss: 0.054172009229660034\n",
      "Iteration 3375, Loss: 0.053990285843610764\n",
      "Iteration 3376, Loss: 0.054171979427337646\n",
      "Iteration 3377, Loss: 0.053990062326192856\n",
      "Iteration 3378, Loss: 0.054172128438949585\n",
      "Iteration 3379, Loss: 0.05399005115032196\n",
      "Iteration 3380, Loss: 0.05417237430810928\n",
      "Iteration 3381, Loss: 0.05398974567651749\n",
      "Iteration 3382, Loss: 0.054172538220882416\n",
      "Iteration 3383, Loss: 0.053989510983228683\n",
      "Iteration 3384, Loss: 0.05417264997959137\n",
      "Iteration 3385, Loss: 0.053989432752132416\n",
      "Iteration 3386, Loss: 0.05417264625430107\n",
      "Iteration 3387, Loss: 0.0539894700050354\n",
      "Iteration 3388, Loss: 0.05417230725288391\n",
      "Iteration 3389, Loss: 0.0539897084236145\n",
      "Iteration 3390, Loss: 0.05417213961482048\n",
      "Iteration 3391, Loss: 0.05398998782038689\n",
      "Iteration 3392, Loss: 0.05417198687791824\n",
      "Iteration 3393, Loss: 0.05399010702967644\n",
      "Iteration 3394, Loss: 0.054171979427337646\n",
      "Iteration 3395, Loss: 0.053990136831998825\n",
      "Iteration 3396, Loss: 0.05417213588953018\n",
      "Iteration 3397, Loss: 0.05398998782038689\n",
      "Iteration 3398, Loss: 0.05417241156101227\n",
      "Iteration 3399, Loss: 0.0539897084236145\n",
      "Iteration 3400, Loss: 0.05417249724268913\n",
      "Iteration 3401, Loss: 0.0539894700050354\n",
      "Iteration 3402, Loss: 0.054172582924366\n",
      "Iteration 3403, Loss: 0.05398940294981003\n",
      "Iteration 3404, Loss: 0.0541725680232048\n",
      "Iteration 3405, Loss: 0.05398974567651749\n",
      "Iteration 3406, Loss: 0.05417225882411003\n",
      "Iteration 3407, Loss: 0.053989868611097336\n",
      "Iteration 3408, Loss: 0.0541720986366272\n",
      "Iteration 3409, Loss: 0.05399002879858017\n",
      "Iteration 3410, Loss: 0.0541720986366272\n",
      "Iteration 3411, Loss: 0.05399002879858017\n",
      "Iteration 3412, Loss: 0.054172106087207794\n",
      "Iteration 3413, Loss: 0.053990017622709274\n",
      "Iteration 3414, Loss: 0.054172299802303314\n",
      "Iteration 3415, Loss: 0.05398979038000107\n",
      "Iteration 3416, Loss: 0.05417249724268913\n",
      "Iteration 3417, Loss: 0.0539894700050354\n",
      "Iteration 3418, Loss: 0.0541725754737854\n",
      "Iteration 3419, Loss: 0.053989436477422714\n",
      "Iteration 3420, Loss: 0.05417260527610779\n",
      "Iteration 3421, Loss: 0.05398961901664734\n",
      "Iteration 3422, Loss: 0.0541725680232048\n",
      "Iteration 3423, Loss: 0.05398967117071152\n",
      "Iteration 3424, Loss: 0.05417240783572197\n",
      "Iteration 3425, Loss: 0.053989749401807785\n",
      "Iteration 3426, Loss: 0.054172225296497345\n",
      "Iteration 3427, Loss: 0.053989749401807785\n",
      "Iteration 3428, Loss: 0.054172299802303314\n",
      "Iteration 3429, Loss: 0.053989674896001816\n",
      "Iteration 3430, Loss: 0.05417221784591675\n",
      "Iteration 3431, Loss: 0.053989823907613754\n",
      "Iteration 3432, Loss: 0.054172333329916\n",
      "Iteration 3433, Loss: 0.053989868611097336\n",
      "Iteration 3434, Loss: 0.05417244881391525\n",
      "Iteration 3435, Loss: 0.05398985743522644\n",
      "Iteration 3436, Loss: 0.05417237803339958\n",
      "Iteration 3437, Loss: 0.05398967117071152\n",
      "Iteration 3438, Loss: 0.0541723370552063\n",
      "Iteration 3439, Loss: 0.053989700973033905\n",
      "Iteration 3440, Loss: 0.05417240783572197\n",
      "Iteration 3441, Loss: 0.05398978292942047\n",
      "Iteration 3442, Loss: 0.0541723296046257\n",
      "Iteration 3443, Loss: 0.05398986488580704\n",
      "Iteration 3444, Loss: 0.054172106087207794\n",
      "Iteration 3445, Loss: 0.05398979038000107\n",
      "Iteration 3446, Loss: 0.05417218431830406\n",
      "Iteration 3447, Loss: 0.053989898413419724\n",
      "Iteration 3448, Loss: 0.05417225882411003\n",
      "Iteration 3449, Loss: 0.053989898413419724\n",
      "Iteration 3450, Loss: 0.054172299802303314\n",
      "Iteration 3451, Loss: 0.05398985743522644\n",
      "Iteration 3452, Loss: 0.05417248606681824\n",
      "Iteration 3453, Loss: 0.053989630192518234\n",
      "Iteration 3454, Loss: 0.054172419011592865\n",
      "Iteration 3455, Loss: 0.05398958921432495\n",
      "Iteration 3456, Loss: 0.05417240783572197\n",
      "Iteration 3457, Loss: 0.053989704698324203\n",
      "Iteration 3458, Loss: 0.05417230352759361\n",
      "Iteration 3459, Loss: 0.053989704698324203\n",
      "Iteration 3460, Loss: 0.054172366857528687\n",
      "Iteration 3461, Loss: 0.05398985743522644\n",
      "Iteration 3462, Loss: 0.0541723296046257\n",
      "Iteration 3463, Loss: 0.05398990213871002\n",
      "Iteration 3464, Loss: 0.05417225882411003\n",
      "Iteration 3465, Loss: 0.05398993939161301\n",
      "Iteration 3466, Loss: 0.054172299802303314\n",
      "Iteration 3467, Loss: 0.053989898413419724\n",
      "Iteration 3468, Loss: 0.05417245626449585\n",
      "Iteration 3469, Loss: 0.053989700973033905\n",
      "Iteration 3470, Loss: 0.0541725680232048\n",
      "Iteration 3471, Loss: 0.05398955196142197\n",
      "Iteration 3472, Loss: 0.054172616451978683\n",
      "Iteration 3473, Loss: 0.05398939177393913\n",
      "Iteration 3474, Loss: 0.05417245626449585\n",
      "Iteration 3475, Loss: 0.05398973822593689\n",
      "Iteration 3476, Loss: 0.0541723370552063\n",
      "Iteration 3477, Loss: 0.05398979038000107\n",
      "Iteration 3478, Loss: 0.05417225882411003\n",
      "Iteration 3479, Loss: 0.05398993939161301\n",
      "Iteration 3480, Loss: 0.05417221412062645\n",
      "Iteration 3481, Loss: 0.05398993939161301\n",
      "Iteration 3482, Loss: 0.05417225882411003\n",
      "Iteration 3483, Loss: 0.05399000644683838\n",
      "Iteration 3484, Loss: 0.054172299802303314\n",
      "Iteration 3485, Loss: 0.053989823907613754\n",
      "Iteration 3486, Loss: 0.05417241156101227\n",
      "Iteration 3487, Loss: 0.053989820182323456\n",
      "Iteration 3488, Loss: 0.05417248606681824\n",
      "Iteration 3489, Loss: 0.05398978292942047\n",
      "Iteration 3490, Loss: 0.05417252704501152\n",
      "Iteration 3491, Loss: 0.05398977920413017\n",
      "Iteration 3492, Loss: 0.05417245253920555\n",
      "Iteration 3493, Loss: 0.05398973822593689\n",
      "Iteration 3494, Loss: 0.05417238175868988\n",
      "Iteration 3495, Loss: 0.05398973822593689\n",
      "Iteration 3496, Loss: 0.05417237430810928\n",
      "Iteration 3497, Loss: 0.053989704698324203\n",
      "Iteration 3498, Loss: 0.05417237430810928\n",
      "Iteration 3499, Loss: 0.0539897084236145\n",
      "Iteration 3500, Loss: 0.054172333329916\n",
      "Iteration 3501, Loss: 0.053989898413419724\n",
      "Iteration 3502, Loss: 0.05417218804359436\n",
      "Iteration 3503, Loss: 0.05398985743522644\n",
      "Iteration 3504, Loss: 0.05417237803339958\n",
      "Iteration 3505, Loss: 0.053989820182323456\n",
      "Iteration 3506, Loss: 0.05417248606681824\n",
      "Iteration 3507, Loss: 0.05398977920413017\n",
      "Iteration 3508, Loss: 0.05417237803339958\n",
      "Iteration 3509, Loss: 0.053989820182323456\n",
      "Iteration 3510, Loss: 0.054172419011592865\n",
      "Iteration 3511, Loss: 0.05398967117071152\n",
      "Iteration 3512, Loss: 0.05417244881391525\n",
      "Iteration 3513, Loss: 0.053989749401807785\n",
      "Iteration 3514, Loss: 0.054172333329916\n",
      "Iteration 3515, Loss: 0.05398979038000107\n",
      "Iteration 3516, Loss: 0.0541723296046257\n",
      "Iteration 3517, Loss: 0.05398993939161301\n",
      "Iteration 3518, Loss: 0.05417225509881973\n",
      "Iteration 3519, Loss: 0.05398993939161301\n",
      "Iteration 3520, Loss: 0.05417218059301376\n",
      "Iteration 3521, Loss: 0.05398985743522644\n",
      "Iteration 3522, Loss: 0.05417237430810928\n",
      "Iteration 3523, Loss: 0.053989775478839874\n",
      "Iteration 3524, Loss: 0.0541723370552063\n",
      "Iteration 3525, Loss: 0.053989700973033905\n",
      "Iteration 3526, Loss: 0.05417240783572197\n",
      "Iteration 3527, Loss: 0.053989823907613754\n",
      "Iteration 3528, Loss: 0.05417229235172272\n",
      "Iteration 3529, Loss: 0.05398985743522644\n",
      "Iteration 3530, Loss: 0.05417214334011078\n",
      "Iteration 3531, Loss: 0.05398985743522644\n",
      "Iteration 3532, Loss: 0.05417221784591675\n",
      "Iteration 3533, Loss: 0.05398993939161301\n",
      "Iteration 3534, Loss: 0.05417225882411003\n",
      "Iteration 3535, Loss: 0.05398985743522644\n",
      "Iteration 3536, Loss: 0.054172419011592865\n",
      "Iteration 3537, Loss: 0.05398974567651749\n",
      "Iteration 3538, Loss: 0.05417249724268913\n",
      "Iteration 3539, Loss: 0.05398966372013092\n",
      "Iteration 3540, Loss: 0.05417260527610779\n",
      "Iteration 3541, Loss: 0.05398954451084137\n",
      "Iteration 3542, Loss: 0.05417276918888092\n",
      "Iteration 3543, Loss: 0.05398939177393913\n",
      "Iteration 3544, Loss: 0.054172810167074203\n",
      "Iteration 3545, Loss: 0.05398938059806824\n",
      "Iteration 3546, Loss: 0.05417276918888092\n",
      "Iteration 3547, Loss: 0.0539894662797451\n",
      "Iteration 3548, Loss: 0.05417245626449585\n",
      "Iteration 3549, Loss: 0.05398967117071152\n",
      "Iteration 3550, Loss: 0.05417221039533615\n",
      "Iteration 3551, Loss: 0.05399005860090256\n",
      "Iteration 3552, Loss: 0.05417190119624138\n",
      "Iteration 3553, Loss: 0.053990330547094345\n",
      "Iteration 3554, Loss: 0.05417194217443466\n",
      "Iteration 3555, Loss: 0.05399021506309509\n",
      "Iteration 3556, Loss: 0.05417213961482048\n",
      "Iteration 3557, Loss: 0.05398986488580704\n",
      "Iteration 3558, Loss: 0.05417249724268913\n",
      "Iteration 3559, Loss: 0.05398935824632645\n",
      "Iteration 3560, Loss: 0.054172735661268234\n",
      "Iteration 3561, Loss: 0.05398930609226227\n",
      "Iteration 3562, Loss: 0.0541728138923645\n",
      "Iteration 3563, Loss: 0.05398935079574585\n",
      "Iteration 3564, Loss: 0.05417269468307495\n",
      "Iteration 3565, Loss: 0.05398939549922943\n",
      "Iteration 3566, Loss: 0.05417264625430107\n",
      "Iteration 3567, Loss: 0.05398958548903465\n",
      "Iteration 3568, Loss: 0.054172299802303314\n",
      "Iteration 3569, Loss: 0.053989823907613754\n",
      "Iteration 3570, Loss: 0.054172173142433167\n",
      "Iteration 3571, Loss: 0.05399002134799957\n",
      "Iteration 3572, Loss: 0.054171979427337646\n",
      "Iteration 3573, Loss: 0.053990136831998825\n",
      "Iteration 3574, Loss: 0.054171979427337646\n",
      "Iteration 3575, Loss: 0.053990211337804794\n",
      "Iteration 3576, Loss: 0.05417199060320854\n",
      "Iteration 3577, Loss: 0.05399009585380554\n",
      "Iteration 3578, Loss: 0.05417218059301376\n",
      "Iteration 3579, Loss: 0.05398982763290405\n",
      "Iteration 3580, Loss: 0.054172299802303314\n",
      "Iteration 3581, Loss: 0.05398977920413017\n",
      "Iteration 3582, Loss: 0.05417245253920555\n",
      "Iteration 3583, Loss: 0.053989626467227936\n",
      "Iteration 3584, Loss: 0.05417241156101227\n",
      "Iteration 3585, Loss: 0.0539897084236145\n",
      "Iteration 3586, Loss: 0.05417210981249809\n",
      "Iteration 3587, Loss: 0.053989868611097336\n",
      "Iteration 3588, Loss: 0.05417194962501526\n",
      "Iteration 3589, Loss: 0.05399002879858017\n",
      "Iteration 3590, Loss: 0.05417198687791824\n",
      "Iteration 3591, Loss: 0.05399005860090256\n",
      "Iteration 3592, Loss: 0.05417221784591675\n",
      "Iteration 3593, Loss: 0.05398982763290405\n",
      "Iteration 3594, Loss: 0.05417248606681824\n",
      "Iteration 3595, Loss: 0.05398974567651749\n",
      "Iteration 3596, Loss: 0.05417237803339958\n",
      "Iteration 3597, Loss: 0.053989704698324203\n",
      "Iteration 3598, Loss: 0.05417229235172272\n",
      "Iteration 3599, Loss: 0.05398985743522644\n",
      "Iteration 3600, Loss: 0.05417225882411003\n",
      "Iteration 3601, Loss: 0.053989823907613754\n",
      "Iteration 3602, Loss: 0.054172299802303314\n",
      "Iteration 3603, Loss: 0.05398978292942047\n",
      "Iteration 3604, Loss: 0.05417237803339958\n",
      "Iteration 3605, Loss: 0.05398973822593689\n",
      "Iteration 3606, Loss: 0.05417245626449585\n",
      "Iteration 3607, Loss: 0.05398977920413017\n",
      "Iteration 3608, Loss: 0.05417249724268913\n",
      "Iteration 3609, Loss: 0.05398965999484062\n",
      "Iteration 3610, Loss: 0.054172612726688385\n",
      "Iteration 3611, Loss: 0.05398958548903465\n",
      "Iteration 3612, Loss: 0.05417249724268913\n",
      "Iteration 3613, Loss: 0.05398958548903465\n",
      "Iteration 3614, Loss: 0.05417248606681824\n",
      "Iteration 3615, Loss: 0.053989704698324203\n",
      "Iteration 3616, Loss: 0.05417221784591675\n",
      "Iteration 3617, Loss: 0.053989868611097336\n",
      "Iteration 3618, Loss: 0.05417213961482048\n",
      "Iteration 3619, Loss: 0.05399005860090256\n",
      "Iteration 3620, Loss: 0.05417202040553093\n",
      "Iteration 3621, Loss: 0.053990136831998825\n",
      "Iteration 3622, Loss: 0.05417216941714287\n",
      "Iteration 3623, Loss: 0.05399009585380554\n",
      "Iteration 3624, Loss: 0.054172299802303314\n",
      "Iteration 3625, Loss: 0.053989820182323456\n",
      "Iteration 3626, Loss: 0.05417245626449585\n",
      "Iteration 3627, Loss: 0.05398965999484062\n",
      "Iteration 3628, Loss: 0.0541725680232048\n",
      "Iteration 3629, Loss: 0.0539894700050354\n",
      "Iteration 3630, Loss: 0.054172493517398834\n",
      "Iteration 3631, Loss: 0.05398958921432495\n",
      "Iteration 3632, Loss: 0.054172299802303314\n",
      "Iteration 3633, Loss: 0.05398982763290405\n",
      "Iteration 3634, Loss: 0.05417210981249809\n",
      "Iteration 3635, Loss: 0.05398993939161301\n",
      "Iteration 3636, Loss: 0.05417218059301376\n",
      "Iteration 3637, Loss: 0.05399004742503166\n",
      "Iteration 3638, Loss: 0.05417225882411003\n",
      "Iteration 3639, Loss: 0.053989820182323456\n",
      "Iteration 3640, Loss: 0.05417249724268913\n",
      "Iteration 3641, Loss: 0.05398973450064659\n",
      "Iteration 3642, Loss: 0.05417264997959137\n",
      "Iteration 3643, Loss: 0.05398954078555107\n",
      "Iteration 3644, Loss: 0.054172664880752563\n",
      "Iteration 3645, Loss: 0.05398939177393913\n",
      "Iteration 3646, Loss: 0.054172735661268234\n",
      "Iteration 3647, Loss: 0.053989313542842865\n",
      "Iteration 3648, Loss: 0.05417277663946152\n",
      "Iteration 3649, Loss: 0.05398935079574585\n",
      "Iteration 3650, Loss: 0.05417250096797943\n",
      "Iteration 3651, Loss: 0.05398973822593689\n",
      "Iteration 3652, Loss: 0.05417248606681824\n",
      "Iteration 3653, Loss: 0.05398973822593689\n",
      "Iteration 3654, Loss: 0.05417221784591675\n",
      "Iteration 3655, Loss: 0.05398997664451599\n",
      "Iteration 3656, Loss: 0.0541720986366272\n",
      "Iteration 3657, Loss: 0.05399002879858017\n",
      "Iteration 3658, Loss: 0.05417213961482048\n",
      "Iteration 3659, Loss: 0.05398998782038689\n",
      "Iteration 3660, Loss: 0.05417202040553093\n",
      "Iteration 3661, Loss: 0.05399002134799957\n",
      "Iteration 3662, Loss: 0.054172031581401825\n",
      "Iteration 3663, Loss: 0.05399009585380554\n",
      "Iteration 3664, Loss: 0.05417221784591675\n",
      "Iteration 3665, Loss: 0.053989898413419724\n",
      "Iteration 3666, Loss: 0.05417237803339958\n",
      "Iteration 3667, Loss: 0.05398974567651749\n",
      "Iteration 3668, Loss: 0.054172538220882416\n",
      "Iteration 3669, Loss: 0.053989581763744354\n",
      "Iteration 3670, Loss: 0.05417264997959137\n",
      "Iteration 3671, Loss: 0.053989630192518234\n",
      "Iteration 3672, Loss: 0.05417245626449585\n",
      "Iteration 3673, Loss: 0.053989749401807785\n",
      "Iteration 3674, Loss: 0.054172299802303314\n",
      "Iteration 3675, Loss: 0.05398979038000107\n",
      "Iteration 3676, Loss: 0.05417221784591675\n",
      "Iteration 3677, Loss: 0.05398993939161301\n",
      "Iteration 3678, Loss: 0.05417218059301376\n",
      "Iteration 3679, Loss: 0.05398993939161301\n",
      "Iteration 3680, Loss: 0.054172366857528687\n",
      "Iteration 3681, Loss: 0.05398979038000107\n",
      "Iteration 3682, Loss: 0.05417245253920555\n",
      "Iteration 3683, Loss: 0.05398977920413017\n",
      "Iteration 3684, Loss: 0.05417248606681824\n",
      "Iteration 3685, Loss: 0.053989555686712265\n",
      "Iteration 3686, Loss: 0.05417241156101227\n",
      "Iteration 3687, Loss: 0.05398985743522644\n",
      "Iteration 3688, Loss: 0.054172299802303314\n",
      "Iteration 3689, Loss: 0.05398992821574211\n",
      "Iteration 3690, Loss: 0.0541723370552063\n",
      "Iteration 3691, Loss: 0.053989820182323456\n",
      "Iteration 3692, Loss: 0.05417249724268913\n",
      "Iteration 3693, Loss: 0.05398954451084137\n",
      "Iteration 3694, Loss: 0.054172419011592865\n",
      "Iteration 3695, Loss: 0.05398955196142197\n",
      "Iteration 3696, Loss: 0.0541723370552063\n",
      "Iteration 3697, Loss: 0.05398966372013092\n",
      "Iteration 3698, Loss: 0.05417237803339958\n",
      "Iteration 3699, Loss: 0.05398966372013092\n",
      "Iteration 3700, Loss: 0.05417244881391525\n",
      "Iteration 3701, Loss: 0.05398977920413017\n",
      "Iteration 3702, Loss: 0.05417240411043167\n",
      "Iteration 3703, Loss: 0.05398982763290405\n",
      "Iteration 3704, Loss: 0.05417218059301376\n",
      "Iteration 3705, Loss: 0.05398993939161301\n",
      "Iteration 3706, Loss: 0.05417213961482048\n",
      "Iteration 3707, Loss: 0.05398993939161301\n",
      "Iteration 3708, Loss: 0.05417213961482048\n",
      "Iteration 3709, Loss: 0.053990017622709274\n",
      "Iteration 3710, Loss: 0.05417221412062645\n",
      "Iteration 3711, Loss: 0.05398993939161301\n",
      "Iteration 3712, Loss: 0.05417240783572197\n",
      "Iteration 3713, Loss: 0.053989898413419724\n",
      "Iteration 3714, Loss: 0.05417245626449585\n",
      "Iteration 3715, Loss: 0.05398965999484062\n",
      "Iteration 3716, Loss: 0.05417272448539734\n",
      "Iteration 3717, Loss: 0.05398949980735779\n",
      "Iteration 3718, Loss: 0.05417254567146301\n",
      "Iteration 3719, Loss: 0.0539894700050354\n",
      "Iteration 3720, Loss: 0.05417238175868988\n",
      "Iteration 3721, Loss: 0.05398966372013092\n",
      "Iteration 3722, Loss: 0.05417218059301376\n",
      "Iteration 3723, Loss: 0.05398993939161301\n",
      "Iteration 3724, Loss: 0.05417218059301376\n",
      "Iteration 3725, Loss: 0.05398993939161301\n",
      "Iteration 3726, Loss: 0.054172106087207794\n",
      "Iteration 3727, Loss: 0.053990017622709274\n",
      "Iteration 3728, Loss: 0.05417225509881973\n",
      "Iteration 3729, Loss: 0.053989868611097336\n",
      "Iteration 3730, Loss: 0.054172299802303314\n",
      "Iteration 3731, Loss: 0.053989749401807785\n",
      "Iteration 3732, Loss: 0.05417245626449585\n",
      "Iteration 3733, Loss: 0.05398967117071152\n",
      "Iteration 3734, Loss: 0.05417264997959137\n",
      "Iteration 3735, Loss: 0.05398954078555107\n",
      "Iteration 3736, Loss: 0.05417276546359062\n",
      "Iteration 3737, Loss: 0.053989432752132416\n",
      "Iteration 3738, Loss: 0.054172687232494354\n",
      "Iteration 3739, Loss: 0.0539894700050354\n",
      "Iteration 3740, Loss: 0.05417237803339958\n",
      "Iteration 3741, Loss: 0.053989749401807785\n",
      "Iteration 3742, Loss: 0.05417218059301376\n",
      "Iteration 3743, Loss: 0.05398998782038689\n",
      "Iteration 3744, Loss: 0.05417187139391899\n",
      "Iteration 3745, Loss: 0.053990136831998825\n",
      "Iteration 3746, Loss: 0.05417187511920929\n",
      "Iteration 3747, Loss: 0.05399025231599808\n",
      "Iteration 3748, Loss: 0.0541720986366272\n",
      "Iteration 3749, Loss: 0.05398993939161301\n",
      "Iteration 3750, Loss: 0.0541723370552063\n",
      "Iteration 3751, Loss: 0.05398958921432495\n",
      "Iteration 3752, Loss: 0.054172538220882416\n",
      "Iteration 3753, Loss: 0.053989581763744354\n",
      "Iteration 3754, Loss: 0.05417269095778465\n",
      "Iteration 3755, Loss: 0.05398954078555107\n",
      "Iteration 3756, Loss: 0.05417265743017197\n",
      "Iteration 3757, Loss: 0.053989432752132416\n",
      "Iteration 3758, Loss: 0.054172616451978683\n",
      "Iteration 3759, Loss: 0.0539894700050354\n",
      "Iteration 3760, Loss: 0.05417230352759361\n",
      "Iteration 3761, Loss: 0.053989674896001816\n",
      "Iteration 3762, Loss: 0.05417218059301376\n",
      "Iteration 3763, Loss: 0.05398990958929062\n",
      "Iteration 3764, Loss: 0.05417194217443466\n",
      "Iteration 3765, Loss: 0.05399010702967644\n",
      "Iteration 3766, Loss: 0.054171979427337646\n",
      "Iteration 3767, Loss: 0.05399017035961151\n",
      "Iteration 3768, Loss: 0.05417206510901451\n",
      "Iteration 3769, Loss: 0.05398997664451599\n",
      "Iteration 3770, Loss: 0.05417244881391525\n",
      "Iteration 3771, Loss: 0.05398973822593689\n",
      "Iteration 3772, Loss: 0.05417260527610779\n",
      "Iteration 3773, Loss: 0.05398954451084137\n",
      "Iteration 3774, Loss: 0.05417269095778465\n",
      "Iteration 3775, Loss: 0.0539889857172966\n",
      "Iteration 3776, Loss: 0.054172854870557785\n",
      "Iteration 3777, Loss: 0.053988706320524216\n",
      "Iteration 3778, Loss: 0.05417357012629509\n",
      "Iteration 3779, Loss: 0.05398844927549362\n",
      "Iteration 3780, Loss: 0.054173216223716736\n",
      "Iteration 3781, Loss: 0.053988873958587646\n",
      "Iteration 3782, Loss: 0.05417277663946152\n",
      "Iteration 3783, Loss: 0.05398942157626152\n",
      "Iteration 3784, Loss: 0.054172366857528687\n",
      "Iteration 3785, Loss: 0.053989820182323456\n",
      "Iteration 3786, Loss: 0.05417206138372421\n",
      "Iteration 3787, Loss: 0.053989868611097336\n",
      "Iteration 3788, Loss: 0.05417202040553093\n",
      "Iteration 3789, Loss: 0.05398993939161301\n",
      "Iteration 3790, Loss: 0.05417216941714287\n",
      "Iteration 3791, Loss: 0.053989898413419724\n",
      "Iteration 3792, Loss: 0.05417216941714287\n",
      "Iteration 3793, Loss: 0.05398997664451599\n",
      "Iteration 3794, Loss: 0.05417206138372421\n",
      "Iteration 3795, Loss: 0.053990017622709274\n",
      "Iteration 3796, Loss: 0.05417199060320854\n",
      "Iteration 3797, Loss: 0.05398997664451599\n",
      "Iteration 3798, Loss: 0.05417221784591675\n",
      "Iteration 3799, Loss: 0.053989820182323456\n",
      "Iteration 3800, Loss: 0.054172299802303314\n",
      "Iteration 3801, Loss: 0.053989581763744354\n",
      "Iteration 3802, Loss: 0.05417242273688316\n",
      "Iteration 3803, Loss: 0.05398954078555107\n",
      "Iteration 3804, Loss: 0.05417245626449585\n",
      "Iteration 3805, Loss: 0.053989510983228683\n",
      "Iteration 3806, Loss: 0.054172419011592865\n",
      "Iteration 3807, Loss: 0.05398939177393913\n",
      "Iteration 3808, Loss: 0.05417245253920555\n",
      "Iteration 3809, Loss: 0.053989581763744354\n",
      "Iteration 3810, Loss: 0.054172299802303314\n",
      "Iteration 3811, Loss: 0.05398977920413017\n",
      "Iteration 3812, Loss: 0.05417221784591675\n",
      "Iteration 3813, Loss: 0.05398993939161301\n",
      "Iteration 3814, Loss: 0.05417221039533615\n",
      "Iteration 3815, Loss: 0.05398997664451599\n",
      "Iteration 3816, Loss: 0.054171979427337646\n",
      "Iteration 3817, Loss: 0.05398990958929062\n",
      "Iteration 3818, Loss: 0.054171979427337646\n",
      "Iteration 3819, Loss: 0.053990017622709274\n",
      "Iteration 3820, Loss: 0.05417213588953018\n",
      "Iteration 3821, Loss: 0.05398985743522644\n",
      "Iteration 3822, Loss: 0.0541723370552063\n",
      "Iteration 3823, Loss: 0.05398954078555107\n",
      "Iteration 3824, Loss: 0.054172538220882416\n",
      "Iteration 3825, Loss: 0.05398934707045555\n",
      "Iteration 3826, Loss: 0.05417246371507645\n",
      "Iteration 3827, Loss: 0.053989432752132416\n",
      "Iteration 3828, Loss: 0.05417237803339958\n",
      "Iteration 3829, Loss: 0.05398965999484062\n",
      "Iteration 3830, Loss: 0.05417225882411003\n",
      "Iteration 3831, Loss: 0.053989898413419724\n",
      "Iteration 3832, Loss: 0.05417202413082123\n",
      "Iteration 3833, Loss: 0.05399004742503166\n",
      "Iteration 3834, Loss: 0.05417194589972496\n",
      "Iteration 3835, Loss: 0.053990017622709274\n",
      "Iteration 3836, Loss: 0.054171979427337646\n",
      "Iteration 3837, Loss: 0.053990017622709274\n",
      "Iteration 3838, Loss: 0.05417213588953018\n",
      "Iteration 3839, Loss: 0.053989823907613754\n",
      "Iteration 3840, Loss: 0.05417225509881973\n",
      "Iteration 3841, Loss: 0.05398973822593689\n",
      "Iteration 3842, Loss: 0.05417252704501152\n",
      "Iteration 3843, Loss: 0.053989432752132416\n",
      "Iteration 3844, Loss: 0.05417250096797943\n",
      "Iteration 3845, Loss: 0.0539894662797451\n",
      "Iteration 3846, Loss: 0.054172493517398834\n",
      "Iteration 3847, Loss: 0.053989507257938385\n",
      "Iteration 3848, Loss: 0.05417237803339958\n",
      "Iteration 3849, Loss: 0.053989700973033905\n",
      "Iteration 3850, Loss: 0.05417241156101227\n",
      "Iteration 3851, Loss: 0.053989700973033905\n",
      "Iteration 3852, Loss: 0.05417225882411003\n",
      "Iteration 3853, Loss: 0.05398977920413017\n",
      "Iteration 3854, Loss: 0.05417225882411003\n",
      "Iteration 3855, Loss: 0.05398977920413017\n",
      "Iteration 3856, Loss: 0.054172269999980927\n",
      "Iteration 3857, Loss: 0.05398965999484062\n",
      "Iteration 3858, Loss: 0.054172538220882416\n",
      "Iteration 3859, Loss: 0.053989432752132416\n",
      "Iteration 3860, Loss: 0.054172612726688385\n",
      "Iteration 3861, Loss: 0.053989432752132416\n",
      "Iteration 3862, Loss: 0.0541725754737854\n",
      "Iteration 3863, Loss: 0.05398935079574585\n",
      "Iteration 3864, Loss: 0.054172419011592865\n",
      "Iteration 3865, Loss: 0.05398954078555107\n",
      "Iteration 3866, Loss: 0.054172419011592865\n",
      "Iteration 3867, Loss: 0.053989700973033905\n",
      "Iteration 3868, Loss: 0.05417221784591675\n",
      "Iteration 3869, Loss: 0.053989898413419724\n",
      "Iteration 3870, Loss: 0.05417213961482048\n",
      "Iteration 3871, Loss: 0.05398993939161301\n",
      "Iteration 3872, Loss: 0.05417210981249809\n",
      "Iteration 3873, Loss: 0.05398977920413017\n",
      "Iteration 3874, Loss: 0.05417237803339958\n",
      "Iteration 3875, Loss: 0.05398961901664734\n",
      "Iteration 3876, Loss: 0.05417250096797943\n",
      "Iteration 3877, Loss: 0.0539894625544548\n",
      "Iteration 3878, Loss: 0.054172616451978683\n",
      "Iteration 3879, Loss: 0.05398935079574585\n",
      "Iteration 3880, Loss: 0.05417238920927048\n",
      "Iteration 3881, Loss: 0.053989581763744354\n",
      "Iteration 3882, Loss: 0.05417237803339958\n",
      "Iteration 3883, Loss: 0.05398973822593689\n",
      "Iteration 3884, Loss: 0.05417218431830406\n",
      "Iteration 3885, Loss: 0.053989898413419724\n",
      "Iteration 3886, Loss: 0.05417202040553093\n",
      "Iteration 3887, Loss: 0.053989898413419724\n",
      "Iteration 3888, Loss: 0.054172031581401825\n",
      "Iteration 3889, Loss: 0.05398986488580704\n",
      "Iteration 3890, Loss: 0.05417214334011078\n",
      "Iteration 3891, Loss: 0.0539897084236145\n",
      "Iteration 3892, Loss: 0.054172299802303314\n",
      "Iteration 3893, Loss: 0.05398958921432495\n",
      "Iteration 3894, Loss: 0.05417225882411003\n",
      "Iteration 3895, Loss: 0.05398951470851898\n",
      "Iteration 3896, Loss: 0.054172299802303314\n",
      "Iteration 3897, Loss: 0.05398958921432495\n",
      "Iteration 3898, Loss: 0.054172299802303314\n",
      "Iteration 3899, Loss: 0.053989820182323456\n",
      "Iteration 3900, Loss: 0.05417225509881973\n",
      "Iteration 3901, Loss: 0.05398977920413017\n",
      "Iteration 3902, Loss: 0.05417225882411003\n",
      "Iteration 3903, Loss: 0.0539897084236145\n",
      "Iteration 3904, Loss: 0.054172299802303314\n",
      "Iteration 3905, Loss: 0.0539897084236145\n",
      "Iteration 3906, Loss: 0.05417238920927048\n",
      "Iteration 3907, Loss: 0.053989581763744354\n",
      "Iteration 3908, Loss: 0.05417238920927048\n",
      "Iteration 3909, Loss: 0.05398949980735779\n",
      "Iteration 3910, Loss: 0.05417238920927048\n",
      "Iteration 3911, Loss: 0.05398961901664734\n",
      "Iteration 3912, Loss: 0.05417249724268913\n",
      "Iteration 3913, Loss: 0.053989510983228683\n",
      "Iteration 3914, Loss: 0.054172419011592865\n",
      "Iteration 3915, Loss: 0.05398939177393913\n",
      "Iteration 3916, Loss: 0.05417237803339958\n",
      "Iteration 3917, Loss: 0.053989581763744354\n",
      "Iteration 3918, Loss: 0.054172299802303314\n",
      "Iteration 3919, Loss: 0.053988974541425705\n",
      "Iteration 3920, Loss: 0.054172106087207794\n",
      "Iteration 3921, Loss: 0.053989242762327194\n",
      "Iteration 3922, Loss: 0.05417190492153168\n",
      "Iteration 3923, Loss: 0.05398940294981003\n",
      "Iteration 3924, Loss: 0.05417116731405258\n",
      "Iteration 3925, Loss: 0.05398951470851898\n",
      "Iteration 3926, Loss: 0.05417116731405258\n",
      "Iteration 3927, Loss: 0.05398944020271301\n",
      "Iteration 3928, Loss: 0.054171524941921234\n",
      "Iteration 3929, Loss: 0.0539892315864563\n",
      "Iteration 3930, Loss: 0.05417183041572571\n",
      "Iteration 3931, Loss: 0.05398888513445854\n",
      "Iteration 3932, Loss: 0.05417191982269287\n",
      "Iteration 3933, Loss: 0.0539887361228466\n",
      "Iteration 3934, Loss: 0.0541718415915966\n",
      "Iteration 3935, Loss: 0.05398888513445854\n",
      "Iteration 3936, Loss: 0.05417172238230705\n",
      "Iteration 3937, Loss: 0.053989045321941376\n",
      "Iteration 3938, Loss: 0.0541716068983078\n",
      "Iteration 3939, Loss: 0.05398908257484436\n",
      "Iteration 3940, Loss: 0.05417156219482422\n",
      "Iteration 3941, Loss: 0.053989164531230927\n",
      "Iteration 3942, Loss: 0.054171524941921234\n",
      "Iteration 3943, Loss: 0.05398920178413391\n",
      "Iteration 3944, Loss: 0.05417148768901825\n",
      "Iteration 3945, Loss: 0.053989049047231674\n",
      "Iteration 3946, Loss: 0.05417155846953392\n",
      "Iteration 3947, Loss: 0.053988974541425705\n",
      "Iteration 3948, Loss: 0.05417156219482422\n",
      "Iteration 3949, Loss: 0.053989049047231674\n",
      "Iteration 3950, Loss: 0.05417167767882347\n",
      "Iteration 3951, Loss: 0.053989049047231674\n",
      "Iteration 3952, Loss: 0.05417156219482422\n",
      "Iteration 3953, Loss: 0.05398900434374809\n",
      "Iteration 3954, Loss: 0.05417171120643616\n",
      "Iteration 3955, Loss: 0.05398900434374809\n",
      "Iteration 3956, Loss: 0.0541716031730175\n",
      "Iteration 3957, Loss: 0.05398919805884361\n",
      "Iteration 3958, Loss: 0.05417156219482422\n",
      "Iteration 3959, Loss: 0.05398912727832794\n",
      "Iteration 3960, Loss: 0.05417144298553467\n",
      "Iteration 3961, Loss: 0.05398917198181152\n",
      "Iteration 3962, Loss: 0.054171524941921234\n",
      "Iteration 3963, Loss: 0.05398912355303764\n",
      "Iteration 3964, Loss: 0.05417168140411377\n",
      "Iteration 3965, Loss: 0.053989045321941376\n",
      "Iteration 3966, Loss: 0.054171763360500336\n",
      "Iteration 3967, Loss: 0.05398881062865257\n",
      "Iteration 3968, Loss: 0.05417172238230705\n",
      "Iteration 3969, Loss: 0.05398895964026451\n",
      "Iteration 3970, Loss: 0.0541716068983078\n",
      "Iteration 3971, Loss: 0.05398919805884361\n",
      "Iteration 3972, Loss: 0.054171524941921234\n",
      "Iteration 3973, Loss: 0.05398912727832794\n",
      "Iteration 3974, Loss: 0.05417148396372795\n",
      "Iteration 3975, Loss: 0.05398905277252197\n",
      "Iteration 3976, Loss: 0.05417156219482422\n",
      "Iteration 3977, Loss: 0.05398901551961899\n",
      "Iteration 3978, Loss: 0.05417156219482422\n",
      "Iteration 3979, Loss: 0.053988970816135406\n",
      "Iteration 3980, Loss: 0.05417175218462944\n",
      "Iteration 3981, Loss: 0.05398900806903839\n",
      "Iteration 3982, Loss: 0.05417172238230705\n",
      "Iteration 3983, Loss: 0.05398881435394287\n",
      "Iteration 3984, Loss: 0.05417172238230705\n",
      "Iteration 3985, Loss: 0.05398893356323242\n",
      "Iteration 3986, Loss: 0.0541716031730175\n",
      "Iteration 3987, Loss: 0.05398894101381302\n",
      "Iteration 3988, Loss: 0.054171524941921234\n",
      "Iteration 3989, Loss: 0.05398908257484436\n",
      "Iteration 3990, Loss: 0.05417148023843765\n",
      "Iteration 3991, Loss: 0.05398920178413391\n",
      "Iteration 3992, Loss: 0.05417148396372795\n",
      "Iteration 3993, Loss: 0.05398912355303764\n",
      "Iteration 3994, Loss: 0.0541716143488884\n",
      "Iteration 3995, Loss: 0.053988926112651825\n",
      "Iteration 3996, Loss: 0.05417191982269287\n",
      "Iteration 3997, Loss: 0.053988732397556305\n",
      "Iteration 3998, Loss: 0.05417197197675705\n",
      "Iteration 3999, Loss: 0.05398857593536377\n",
      "Iteration 4000, Loss: 0.054171960800886154\n",
      "Iteration 4001, Loss: 0.05398865044116974\n",
      "Iteration 4002, Loss: 0.05417187511920929\n",
      "Iteration 4003, Loss: 0.053988777101039886\n",
      "Iteration 4004, Loss: 0.05417168140411377\n",
      "Iteration 4005, Loss: 0.05398912355303764\n",
      "Iteration 4006, Loss: 0.05417140945792198\n",
      "Iteration 4007, Loss: 0.05398920923471451\n",
      "Iteration 4008, Loss: 0.05417124554514885\n",
      "Iteration 4009, Loss: 0.053989291191101074\n",
      "Iteration 4010, Loss: 0.0541711263358593\n",
      "Iteration 4011, Loss: 0.05398936569690704\n",
      "Iteration 4012, Loss: 0.05417116731405258\n",
      "Iteration 4013, Loss: 0.053989436477422714\n",
      "Iteration 4014, Loss: 0.05417140945792198\n",
      "Iteration 4015, Loss: 0.05398913472890854\n",
      "Iteration 4016, Loss: 0.05417172238230705\n",
      "Iteration 4017, Loss: 0.05398900434374809\n",
      "Iteration 4018, Loss: 0.05417191982269287\n",
      "Iteration 4019, Loss: 0.05398857593536377\n",
      "Iteration 4020, Loss: 0.054172080010175705\n",
      "Iteration 4021, Loss: 0.053988538682460785\n",
      "Iteration 4022, Loss: 0.0541718453168869\n",
      "Iteration 4023, Loss: 0.053988926112651825\n",
      "Iteration 4024, Loss: 0.0541716143488884\n",
      "Iteration 4025, Loss: 0.053989045321941376\n",
      "Iteration 4026, Loss: 0.05417148396372795\n",
      "Iteration 4027, Loss: 0.053989093750715256\n",
      "Iteration 4028, Loss: 0.05417129397392273\n",
      "Iteration 4029, Loss: 0.05398932099342346\n",
      "Iteration 4030, Loss: 0.05417124554514885\n",
      "Iteration 4031, Loss: 0.053989291191101074\n",
      "Iteration 4032, Loss: 0.05417124554514885\n",
      "Iteration 4033, Loss: 0.05398932099342346\n",
      "Iteration 4034, Loss: 0.0541713647544384\n",
      "Iteration 4035, Loss: 0.053989168256521225\n",
      "Iteration 4036, Loss: 0.05417151749134064\n",
      "Iteration 4037, Loss: 0.05398905277252197\n",
      "Iteration 4038, Loss: 0.054171644151210785\n",
      "Iteration 4039, Loss: 0.053988926112651825\n",
      "Iteration 4040, Loss: 0.05417175218462944\n",
      "Iteration 4041, Loss: 0.053988777101039886\n",
      "Iteration 4042, Loss: 0.05417175590991974\n",
      "Iteration 4043, Loss: 0.05398892983794212\n",
      "Iteration 4044, Loss: 0.054171763360500336\n",
      "Iteration 4045, Loss: 0.0539887361228466\n",
      "Iteration 4046, Loss: 0.054171718657016754\n",
      "Iteration 4047, Loss: 0.053988855332136154\n",
      "Iteration 4048, Loss: 0.054171718657016754\n",
      "Iteration 4049, Loss: 0.05398889631032944\n",
      "Iteration 4050, Loss: 0.05417172238230705\n",
      "Iteration 4051, Loss: 0.05398889631032944\n",
      "Iteration 4052, Loss: 0.054171718657016754\n",
      "Iteration 4053, Loss: 0.053988855332136154\n",
      "Iteration 4054, Loss: 0.0541716068983078\n",
      "Iteration 4055, Loss: 0.05398896336555481\n",
      "Iteration 4056, Loss: 0.054171569645404816\n",
      "Iteration 4057, Loss: 0.053988974541425705\n",
      "Iteration 4058, Loss: 0.05417156219482422\n",
      "Iteration 4059, Loss: 0.053989164531230927\n",
      "Iteration 4060, Loss: 0.05417151376605034\n",
      "Iteration 4061, Loss: 0.053989164531230927\n",
      "Iteration 4062, Loss: 0.05417156219482422\n",
      "Iteration 4063, Loss: 0.05398893356323242\n",
      "Iteration 4064, Loss: 0.0541716031730175\n",
      "Iteration 4065, Loss: 0.053988855332136154\n",
      "Iteration 4066, Loss: 0.054171767085790634\n",
      "Iteration 4067, Loss: 0.05398881062865257\n",
      "Iteration 4068, Loss: 0.054171886295080185\n",
      "Iteration 4069, Loss: 0.053988732397556305\n",
      "Iteration 4070, Loss: 0.054172150790691376\n",
      "Iteration 4071, Loss: 0.053988538682460785\n",
      "Iteration 4072, Loss: 0.054171960800886154\n",
      "Iteration 4073, Loss: 0.053988661617040634\n",
      "Iteration 4074, Loss: 0.05417179688811302\n",
      "Iteration 4075, Loss: 0.05398901551961899\n",
      "Iteration 4076, Loss: 0.05417148396372795\n",
      "Iteration 4077, Loss: 0.05398928374052048\n",
      "Iteration 4078, Loss: 0.05417117476463318\n",
      "Iteration 4079, Loss: 0.053989361971616745\n",
      "Iteration 4080, Loss: 0.05417116731405258\n",
      "Iteration 4081, Loss: 0.053989559412002563\n",
      "Iteration 4082, Loss: 0.05417121574282646\n",
      "Iteration 4083, Loss: 0.05398932844400406\n",
      "Iteration 4084, Loss: 0.05417140573263168\n",
      "Iteration 4085, Loss: 0.053989045321941376\n",
      "Iteration 4086, Loss: 0.05417183041572571\n",
      "Iteration 4087, Loss: 0.053988926112651825\n",
      "Iteration 4088, Loss: 0.054171960800886154\n",
      "Iteration 4089, Loss: 0.05398854613304138\n",
      "Iteration 4090, Loss: 0.054171886295080185\n",
      "Iteration 4091, Loss: 0.053988587111234665\n",
      "Iteration 4092, Loss: 0.05417183041572571\n",
      "Iteration 4093, Loss: 0.053988855332136154\n",
      "Iteration 4094, Loss: 0.054171591997146606\n",
      "Iteration 4095, Loss: 0.053989093750715256\n",
      "Iteration 4096, Loss: 0.05417124554514885\n",
      "Iteration 4097, Loss: 0.05398944020271301\n",
      "Iteration 4098, Loss: 0.0541711263358593\n",
      "Iteration 4099, Loss: 0.05398958921432495\n",
      "Iteration 4100, Loss: 0.05417116731405258\n",
      "Iteration 4101, Loss: 0.05398960039019585\n",
      "Iteration 4102, Loss: 0.054171085357666016\n",
      "Iteration 4103, Loss: 0.05398952215909958\n",
      "Iteration 4104, Loss: 0.054171256721019745\n",
      "Iteration 4105, Loss: 0.05398920178413391\n",
      "Iteration 4106, Loss: 0.054171573370695114\n",
      "Iteration 4107, Loss: 0.053988851606845856\n",
      "Iteration 4108, Loss: 0.05417191982269287\n",
      "Iteration 4109, Loss: 0.05398857593536377\n",
      "Iteration 4110, Loss: 0.054171960800886154\n",
      "Iteration 4111, Loss: 0.053988538682460785\n",
      "Iteration 4112, Loss: 0.0541718415915966\n",
      "Iteration 4113, Loss: 0.05398889631032944\n",
      "Iteration 4114, Loss: 0.054171450436115265\n",
      "Iteration 4115, Loss: 0.05398913472890854\n",
      "Iteration 4116, Loss: 0.05417105183005333\n",
      "Iteration 4117, Loss: 0.05398937314748764\n",
      "Iteration 4118, Loss: 0.05417077988386154\n",
      "Iteration 4119, Loss: 0.0539897195994854\n",
      "Iteration 4120, Loss: 0.05417066067457199\n",
      "Iteration 4121, Loss: 0.053989723324775696\n",
      "Iteration 4122, Loss: 0.05417073890566826\n",
      "Iteration 4123, Loss: 0.053989674896001816\n",
      "Iteration 4124, Loss: 0.0541711263358593\n",
      "Iteration 4125, Loss: 0.05398940294981003\n",
      "Iteration 4126, Loss: 0.05417148396372795\n",
      "Iteration 4127, Loss: 0.05398908257484436\n",
      "Iteration 4128, Loss: 0.05417167767882347\n",
      "Iteration 4129, Loss: 0.053988851606845856\n",
      "Iteration 4130, Loss: 0.054171763360500336\n",
      "Iteration 4131, Loss: 0.05398884415626526\n",
      "Iteration 4132, Loss: 0.054171644151210785\n",
      "Iteration 4133, Loss: 0.053988974541425705\n",
      "Iteration 4134, Loss: 0.05417144298553467\n",
      "Iteration 4135, Loss: 0.05398924648761749\n",
      "Iteration 4136, Loss: 0.05417124554514885\n",
      "Iteration 4137, Loss: 0.053989477455616\n",
      "Iteration 4138, Loss: 0.05417104810476303\n",
      "Iteration 4139, Loss: 0.053989481180906296\n",
      "Iteration 4140, Loss: 0.05417093262076378\n",
      "Iteration 4141, Loss: 0.05398960039019585\n",
      "Iteration 4142, Loss: 0.05417100712656975\n",
      "Iteration 4143, Loss: 0.05398964136838913\n",
      "Iteration 4144, Loss: 0.05417100712656975\n",
      "Iteration 4145, Loss: 0.053989484906196594\n",
      "Iteration 4146, Loss: 0.05417131632566452\n",
      "Iteration 4147, Loss: 0.05398932099342346\n",
      "Iteration 4148, Loss: 0.054171398282051086\n",
      "Iteration 4149, Loss: 0.05398920178413391\n",
      "Iteration 4150, Loss: 0.054171524941921234\n",
      "Iteration 4151, Loss: 0.05398908257484436\n",
      "Iteration 4152, Loss: 0.05417163297533989\n",
      "Iteration 4153, Loss: 0.05398908257484436\n",
      "Iteration 4154, Loss: 0.05417172238230705\n",
      "Iteration 4155, Loss: 0.053988970816135406\n",
      "Iteration 4156, Loss: 0.05417183041572571\n",
      "Iteration 4157, Loss: 0.053988855332136154\n",
      "Iteration 4158, Loss: 0.05417168140411377\n",
      "Iteration 4159, Loss: 0.05398896336555481\n",
      "Iteration 4160, Loss: 0.0541716031730175\n",
      "Iteration 4161, Loss: 0.053989045321941376\n",
      "Iteration 4162, Loss: 0.05417144298553467\n",
      "Iteration 4163, Loss: 0.05398917198181152\n",
      "Iteration 4164, Loss: 0.05417116731405258\n",
      "Iteration 4165, Loss: 0.053989555686712265\n",
      "Iteration 4166, Loss: 0.054171040654182434\n",
      "Iteration 4167, Loss: 0.053989678621292114\n",
      "Iteration 4168, Loss: 0.05417080968618393\n",
      "Iteration 4169, Loss: 0.053989753127098083\n",
      "Iteration 4170, Loss: 0.05417092144489288\n",
      "Iteration 4171, Loss: 0.05398960039019585\n",
      "Iteration 4172, Loss: 0.0541711263358593\n",
      "Iteration 4173, Loss: 0.05398944020271301\n",
      "Iteration 4174, Loss: 0.0541713647544384\n",
      "Iteration 4175, Loss: 0.05398920178413391\n",
      "Iteration 4176, Loss: 0.05417148396372795\n",
      "Iteration 4177, Loss: 0.053989045321941376\n",
      "Iteration 4178, Loss: 0.05417172238230705\n",
      "Iteration 4179, Loss: 0.05398881435394287\n",
      "Iteration 4180, Loss: 0.05417175218462944\n",
      "Iteration 4181, Loss: 0.05398925393819809\n",
      "Iteration 4182, Loss: 0.054171644151210785\n",
      "Iteration 4183, Loss: 0.05398960039019585\n",
      "Iteration 4184, Loss: 0.0541708841919899\n",
      "Iteration 4185, Loss: 0.053989946842193604\n",
      "Iteration 4186, Loss: 0.054170459508895874\n",
      "Iteration 4187, Loss: 0.05399003252387047\n",
      "Iteration 4188, Loss: 0.05417072772979736\n",
      "Iteration 4189, Loss: 0.053989946842193604\n",
      "Iteration 4190, Loss: 0.054171040654182434\n",
      "Iteration 4191, Loss: 0.053989749401807785\n",
      "Iteration 4192, Loss: 0.05417124554514885\n",
      "Iteration 4193, Loss: 0.053989361971616745\n",
      "Iteration 4194, Loss: 0.05417148768901825\n",
      "Iteration 4195, Loss: 0.053989045321941376\n",
      "Iteration 4196, Loss: 0.05417168140411377\n",
      "Iteration 4197, Loss: 0.05398892983794212\n",
      "Iteration 4198, Loss: 0.054171644151210785\n",
      "Iteration 4199, Loss: 0.053989049047231674\n",
      "Iteration 4200, Loss: 0.05417143926024437\n",
      "Iteration 4201, Loss: 0.05398932099342346\n",
      "Iteration 4202, Loss: 0.054171279072761536\n",
      "Iteration 4203, Loss: 0.05398952215909958\n",
      "Iteration 4204, Loss: 0.05417104810476303\n",
      "Iteration 4205, Loss: 0.05398945137858391\n",
      "Iteration 4206, Loss: 0.054171085357666016\n",
      "Iteration 4207, Loss: 0.05398952588438988\n",
      "Iteration 4208, Loss: 0.0541711263358593\n",
      "Iteration 4209, Loss: 0.05398952215909958\n",
      "Iteration 4210, Loss: 0.05417128652334213\n",
      "Iteration 4211, Loss: 0.05398937314748764\n",
      "Iteration 4212, Loss: 0.05417140573263168\n",
      "Iteration 4213, Loss: 0.053989361971616745\n",
      "Iteration 4214, Loss: 0.05417148023843765\n",
      "Iteration 4215, Loss: 0.053989361971616745\n",
      "Iteration 4216, Loss: 0.05417141318321228\n",
      "Iteration 4217, Loss: 0.05398920923471451\n",
      "Iteration 4218, Loss: 0.05417148396372795\n",
      "Iteration 4219, Loss: 0.053989168256521225\n",
      "Iteration 4220, Loss: 0.054171524941921234\n",
      "Iteration 4221, Loss: 0.05398925393819809\n",
      "Iteration 4222, Loss: 0.05417132377624512\n",
      "Iteration 4223, Loss: 0.053989291191101074\n",
      "Iteration 4224, Loss: 0.05417132377624512\n",
      "Iteration 4225, Loss: 0.053989291191101074\n",
      "Iteration 4226, Loss: 0.054171204566955566\n",
      "Iteration 4227, Loss: 0.05398944020271301\n",
      "Iteration 4228, Loss: 0.054171204566955566\n",
      "Iteration 4229, Loss: 0.053989410400390625\n",
      "Iteration 4230, Loss: 0.05417132005095482\n",
      "Iteration 4231, Loss: 0.05398932099342346\n",
      "Iteration 4232, Loss: 0.05417148023843765\n",
      "Iteration 4233, Loss: 0.053989432752132416\n",
      "Iteration 4234, Loss: 0.05417148023843765\n",
      "Iteration 4235, Loss: 0.05398940294981003\n",
      "Iteration 4236, Loss: 0.05417144298553467\n",
      "Iteration 4237, Loss: 0.053989212960004807\n",
      "Iteration 4238, Loss: 0.05417155846953392\n",
      "Iteration 4239, Loss: 0.05398917198181152\n",
      "Iteration 4240, Loss: 0.054171450436115265\n",
      "Iteration 4241, Loss: 0.05398920178413391\n",
      "Iteration 4242, Loss: 0.05417156219482422\n",
      "Iteration 4243, Loss: 0.05398912355303764\n",
      "Iteration 4244, Loss: 0.0541716031730175\n",
      "Iteration 4245, Loss: 0.05398920178413391\n",
      "Iteration 4246, Loss: 0.05417156219482422\n",
      "Iteration 4247, Loss: 0.05398900806903839\n",
      "Iteration 4248, Loss: 0.05417171120643616\n",
      "Iteration 4249, Loss: 0.05398893356323242\n",
      "Iteration 4250, Loss: 0.054171644151210785\n",
      "Iteration 4251, Loss: 0.053988978266716\n",
      "Iteration 4252, Loss: 0.0541716031730175\n",
      "Iteration 4253, Loss: 0.05398913472890854\n",
      "Iteration 4254, Loss: 0.05417148396372795\n",
      "Iteration 4255, Loss: 0.05398924648761749\n",
      "Iteration 4256, Loss: 0.05417148396372795\n",
      "Iteration 4257, Loss: 0.05398928374052048\n",
      "Iteration 4258, Loss: 0.0541715994477272\n",
      "Iteration 4259, Loss: 0.05398912727832794\n",
      "Iteration 4260, Loss: 0.054171763360500336\n",
      "Iteration 4261, Loss: 0.05398901551961899\n",
      "Iteration 4262, Loss: 0.054171763360500336\n",
      "Iteration 4263, Loss: 0.05398892983794212\n",
      "Iteration 4264, Loss: 0.05417187511920929\n",
      "Iteration 4265, Loss: 0.05398900434374809\n",
      "Iteration 4266, Loss: 0.05417179688811302\n",
      "Iteration 4267, Loss: 0.053988855332136154\n",
      "Iteration 4268, Loss: 0.05417179316282272\n",
      "Iteration 4269, Loss: 0.05398900806903839\n",
      "Iteration 4270, Loss: 0.054171644151210785\n",
      "Iteration 4271, Loss: 0.053989045321941376\n",
      "Iteration 4272, Loss: 0.054171644151210785\n",
      "Iteration 4273, Loss: 0.053989119827747345\n",
      "Iteration 4274, Loss: 0.05417168140411377\n",
      "Iteration 4275, Loss: 0.053989045321941376\n",
      "Iteration 4276, Loss: 0.05417168140411377\n",
      "Iteration 4277, Loss: 0.05398900806903839\n",
      "Iteration 4278, Loss: 0.05417168140411377\n",
      "Iteration 4279, Loss: 0.053988974541425705\n",
      "Iteration 4280, Loss: 0.0541718415915966\n",
      "Iteration 4281, Loss: 0.05398892983794212\n",
      "Iteration 4282, Loss: 0.05417203530669212\n",
      "Iteration 4283, Loss: 0.05398854240775108\n",
      "Iteration 4284, Loss: 0.05417210981249809\n",
      "Iteration 4285, Loss: 0.05398862063884735\n",
      "Iteration 4286, Loss: 0.05417194962501526\n",
      "Iteration 4287, Loss: 0.05398882180452347\n",
      "Iteration 4288, Loss: 0.05417167767882347\n",
      "Iteration 4289, Loss: 0.05398912355303764\n",
      "Iteration 4290, Loss: 0.054171524941921234\n",
      "Iteration 4291, Loss: 0.05398925393819809\n",
      "Iteration 4292, Loss: 0.05417143926024437\n",
      "Iteration 4293, Loss: 0.05398933216929436\n",
      "Iteration 4294, Loss: 0.05417143926024437\n",
      "Iteration 4295, Loss: 0.053989287465810776\n",
      "Iteration 4296, Loss: 0.0541713684797287\n",
      "Iteration 4297, Loss: 0.05398920178413391\n",
      "Iteration 4298, Loss: 0.054171644151210785\n",
      "Iteration 4299, Loss: 0.053989049047231674\n",
      "Iteration 4300, Loss: 0.05417175218462944\n",
      "Iteration 4301, Loss: 0.05398901551961899\n",
      "Iteration 4302, Loss: 0.05417191982269287\n",
      "Iteration 4303, Loss: 0.053988926112651825\n",
      "Iteration 4304, Loss: 0.05417206883430481\n",
      "Iteration 4305, Loss: 0.05398869514465332\n",
      "Iteration 4306, Loss: 0.05417199432849884\n",
      "Iteration 4307, Loss: 0.05398881435394287\n",
      "Iteration 4308, Loss: 0.054171763360500336\n",
      "Iteration 4309, Loss: 0.05398900806903839\n",
      "Iteration 4310, Loss: 0.05417148396372795\n",
      "Iteration 4311, Loss: 0.05398920178413391\n",
      "Iteration 4312, Loss: 0.05417144298553467\n",
      "Iteration 4313, Loss: 0.05398925393819809\n",
      "Iteration 4314, Loss: 0.0541713647544384\n",
      "Iteration 4315, Loss: 0.053989287465810776\n",
      "Iteration 4316, Loss: 0.05417156219482422\n",
      "Iteration 4317, Loss: 0.05398905277252197\n",
      "Iteration 4318, Loss: 0.054171763360500336\n",
      "Iteration 4319, Loss: 0.05398882180452347\n",
      "Iteration 4320, Loss: 0.05417187139391899\n",
      "Iteration 4321, Loss: 0.05398940667510033\n",
      "Iteration 4322, Loss: 0.054171763360500336\n",
      "Iteration 4323, Loss: 0.05398949235677719\n",
      "Iteration 4324, Loss: 0.05417215824127197\n",
      "Iteration 4325, Loss: 0.053989604115486145\n",
      "Iteration 4326, Loss: 0.05417203903198242\n",
      "Iteration 4327, Loss: 0.05398957431316376\n",
      "Iteration 4328, Loss: 0.0541718415915966\n",
      "Iteration 4329, Loss: 0.05398973077535629\n",
      "Iteration 4330, Loss: 0.054171882569789886\n",
      "Iteration 4331, Loss: 0.05398983880877495\n",
      "Iteration 4332, Loss: 0.054171882569789886\n",
      "Iteration 4333, Loss: 0.05398973077535629\n",
      "Iteration 4334, Loss: 0.05417212098836899\n",
      "Iteration 4335, Loss: 0.05398960039019585\n",
      "Iteration 4336, Loss: 0.05417227745056152\n",
      "Iteration 4337, Loss: 0.053989335894584656\n",
      "Iteration 4338, Loss: 0.054172322154045105\n",
      "Iteration 4339, Loss: 0.053989216685295105\n",
      "Iteration 4340, Loss: 0.054172348231077194\n",
      "Iteration 4341, Loss: 0.05398925766348839\n",
      "Iteration 4342, Loss: 0.05417224019765854\n",
      "Iteration 4343, Loss: 0.0539894625544548\n",
      "Iteration 4344, Loss: 0.054172154515981674\n",
      "Iteration 4345, Loss: 0.05398956686258316\n",
      "Iteration 4346, Loss: 0.05417218804359436\n",
      "Iteration 4347, Loss: 0.05398957431316376\n",
      "Iteration 4348, Loss: 0.05417222902178764\n",
      "Iteration 4349, Loss: 0.053989604115486145\n",
      "Iteration 4350, Loss: 0.05417211353778839\n",
      "Iteration 4351, Loss: 0.053989604115486145\n",
      "Iteration 4352, Loss: 0.054172080010175705\n",
      "Iteration 4353, Loss: 0.053989529609680176\n",
      "Iteration 4354, Loss: 0.05417215824127197\n",
      "Iteration 4355, Loss: 0.05398945137858391\n",
      "Iteration 4356, Loss: 0.05417211353778839\n",
      "Iteration 4357, Loss: 0.053989529609680176\n",
      "Iteration 4358, Loss: 0.05417200177907944\n",
      "Iteration 4359, Loss: 0.05398961156606674\n",
      "Iteration 4360, Loss: 0.054171960800886154\n",
      "Iteration 4361, Loss: 0.05398976802825928\n",
      "Iteration 4362, Loss: 0.054171882569789886\n",
      "Iteration 4363, Loss: 0.05398976802825928\n",
      "Iteration 4364, Loss: 0.0541718527674675\n",
      "Iteration 4365, Loss: 0.05398983880877495\n",
      "Iteration 4366, Loss: 0.05417203903198242\n",
      "Iteration 4367, Loss: 0.05398961156606674\n",
      "Iteration 4368, Loss: 0.05417223274707794\n",
      "Iteration 4369, Loss: 0.05398945510387421\n",
      "Iteration 4370, Loss: 0.054172318428754807\n",
      "Iteration 4371, Loss: 0.053989291191101074\n",
      "Iteration 4372, Loss: 0.05417247116565704\n",
      "Iteration 4373, Loss: 0.053989291191101074\n",
      "Iteration 4374, Loss: 0.05417243763804436\n",
      "Iteration 4375, Loss: 0.053989291191101074\n",
      "Iteration 4376, Loss: 0.054172273725271225\n",
      "Iteration 4377, Loss: 0.05398949235677719\n",
      "Iteration 4378, Loss: 0.054172150790691376\n",
      "Iteration 4379, Loss: 0.05398968979716301\n",
      "Iteration 4380, Loss: 0.054171886295080185\n",
      "Iteration 4381, Loss: 0.05398969352245331\n",
      "Iteration 4382, Loss: 0.054171960800886154\n",
      "Iteration 4383, Loss: 0.05398968979716301\n",
      "Iteration 4384, Loss: 0.054172005504369736\n",
      "Iteration 4385, Loss: 0.05398964509367943\n",
      "Iteration 4386, Loss: 0.05417203903198242\n",
      "Iteration 4387, Loss: 0.0539897195994854\n",
      "Iteration 4388, Loss: 0.05417212098836899\n",
      "Iteration 4389, Loss: 0.05398964509367943\n",
      "Iteration 4390, Loss: 0.05417230725288391\n",
      "Iteration 4391, Loss: 0.05398945137858391\n",
      "Iteration 4392, Loss: 0.05417230725288391\n",
      "Iteration 4393, Loss: 0.05398934334516525\n",
      "Iteration 4394, Loss: 0.05417230725288391\n",
      "Iteration 4395, Loss: 0.05398938059806824\n",
      "Iteration 4396, Loss: 0.054172150790691376\n",
      "Iteration 4397, Loss: 0.05398949608206749\n",
      "Iteration 4398, Loss: 0.054171960800886154\n",
      "Iteration 4399, Loss: 0.05398964881896973\n",
      "Iteration 4400, Loss: 0.05417191982269287\n",
      "Iteration 4401, Loss: 0.05398976802825928\n",
      "Iteration 4402, Loss: 0.05417210981249809\n",
      "Iteration 4403, Loss: 0.053989656269550323\n",
      "Iteration 4404, Loss: 0.054172080010175705\n",
      "Iteration 4405, Loss: 0.05398960039019585\n",
      "Iteration 4406, Loss: 0.05417238920927048\n",
      "Iteration 4407, Loss: 0.05398925393819809\n",
      "Iteration 4408, Loss: 0.05417242646217346\n",
      "Iteration 4409, Loss: 0.05398937314748764\n",
      "Iteration 4410, Loss: 0.05417243763804436\n",
      "Iteration 4411, Loss: 0.053989291191101074\n",
      "Iteration 4412, Loss: 0.05417238920927048\n",
      "Iteration 4413, Loss: 0.05398933216929436\n",
      "Iteration 4414, Loss: 0.054172273725271225\n",
      "Iteration 4415, Loss: 0.05398938059806824\n",
      "Iteration 4416, Loss: 0.054172076284885406\n",
      "Iteration 4417, Loss: 0.05398949608206749\n",
      "Iteration 4418, Loss: 0.05417210981249809\n",
      "Iteration 4419, Loss: 0.05398968607187271\n",
      "Iteration 4420, Loss: 0.05417215824127197\n",
      "Iteration 4421, Loss: 0.05398949980735779\n",
      "Iteration 4422, Loss: 0.05417231470346451\n",
      "Iteration 4423, Loss: 0.053989261388778687\n",
      "Iteration 4424, Loss: 0.05417250841856003\n",
      "Iteration 4425, Loss: 0.053989291191101074\n",
      "Iteration 4426, Loss: 0.05417242646217346\n",
      "Iteration 4427, Loss: 0.0539892241358757\n",
      "Iteration 4428, Loss: 0.05417222902178764\n",
      "Iteration 4429, Loss: 0.05398938059806824\n",
      "Iteration 4430, Loss: 0.05417200177907944\n",
      "Iteration 4431, Loss: 0.05398984253406525\n",
      "Iteration 4432, Loss: 0.05417191982269287\n",
      "Iteration 4433, Loss: 0.05398987978696823\n",
      "Iteration 4434, Loss: 0.054171882569789886\n",
      "Iteration 4435, Loss: 0.053989723324775696\n",
      "Iteration 4436, Loss: 0.054172076284885406\n",
      "Iteration 4437, Loss: 0.053989604115486145\n",
      "Iteration 4438, Loss: 0.05417212098836899\n",
      "Iteration 4439, Loss: 0.05398956686258316\n",
      "Iteration 4440, Loss: 0.05417222902178764\n",
      "Iteration 4441, Loss: 0.053989529609680176\n",
      "Iteration 4442, Loss: 0.054172199219465256\n",
      "Iteration 4443, Loss: 0.053989335894584656\n",
      "Iteration 4444, Loss: 0.054172348231077194\n",
      "Iteration 4445, Loss: 0.05398933216929436\n",
      "Iteration 4446, Loss: 0.05417235195636749\n",
      "Iteration 4447, Loss: 0.05398933216929436\n",
      "Iteration 4448, Loss: 0.054172199219465256\n",
      "Iteration 4449, Loss: 0.05398934334516525\n",
      "Iteration 4450, Loss: 0.05417219549417496\n",
      "Iteration 4451, Loss: 0.05398949608206749\n",
      "Iteration 4452, Loss: 0.054171960800886154\n",
      "Iteration 4453, Loss: 0.05398965999484062\n",
      "Iteration 4454, Loss: 0.05417203903198242\n",
      "Iteration 4455, Loss: 0.05398964881896973\n",
      "Iteration 4456, Loss: 0.054172080010175705\n",
      "Iteration 4457, Loss: 0.053989604115486145\n",
      "Iteration 4458, Loss: 0.05417212098836899\n",
      "Iteration 4459, Loss: 0.05398945510387421\n",
      "Iteration 4460, Loss: 0.05417206883430481\n",
      "Iteration 4461, Loss: 0.05398961156606674\n",
      "Iteration 4462, Loss: 0.05417199432849884\n",
      "Iteration 4463, Loss: 0.05398961529135704\n",
      "Iteration 4464, Loss: 0.054171960800886154\n",
      "Iteration 4465, Loss: 0.05398973450064659\n",
      "Iteration 4466, Loss: 0.054171811789274216\n",
      "Iteration 4467, Loss: 0.053989797830581665\n",
      "Iteration 4468, Loss: 0.0541718415915966\n",
      "Iteration 4469, Loss: 0.05398969352245331\n",
      "Iteration 4470, Loss: 0.054171767085790634\n",
      "Iteration 4471, Loss: 0.053989849984645844\n",
      "Iteration 4472, Loss: 0.05417194962501526\n",
      "Iteration 4473, Loss: 0.05398976802825928\n",
      "Iteration 4474, Loss: 0.054171886295080185\n",
      "Iteration 4475, Loss: 0.053989656269550323\n",
      "Iteration 4476, Loss: 0.05417205020785332\n",
      "Iteration 4477, Loss: 0.053989559412002563\n",
      "Iteration 4478, Loss: 0.054172396659851074\n",
      "Iteration 4479, Loss: 0.05398925393819809\n",
      "Iteration 4480, Loss: 0.05417255684733391\n",
      "Iteration 4481, Loss: 0.053989093750715256\n",
      "Iteration 4482, Loss: 0.05417255312204361\n",
      "Iteration 4483, Loss: 0.05398917198181152\n",
      "Iteration 4484, Loss: 0.05417238920927048\n",
      "Iteration 4485, Loss: 0.053989291191101074\n",
      "Iteration 4486, Loss: 0.05417215824127197\n",
      "Iteration 4487, Loss: 0.05398961156606674\n",
      "Iteration 4488, Loss: 0.05417199060320854\n",
      "Iteration 4489, Loss: 0.05398976802825928\n",
      "Iteration 4490, Loss: 0.05417179688811302\n",
      "Iteration 4491, Loss: 0.05398992821574211\n",
      "Iteration 4492, Loss: 0.05417172238230705\n",
      "Iteration 4493, Loss: 0.053989917039871216\n",
      "Iteration 4494, Loss: 0.05417177081108093\n",
      "Iteration 4495, Loss: 0.05398973077535629\n",
      "Iteration 4496, Loss: 0.05417206883430481\n",
      "Iteration 4497, Loss: 0.05398961529135704\n",
      "Iteration 4498, Loss: 0.05417212098836899\n",
      "Iteration 4499, Loss: 0.05398945510387421\n",
      "Iteration 4500, Loss: 0.05417224019765854\n",
      "Iteration 4501, Loss: 0.05398937314748764\n",
      "Iteration 4502, Loss: 0.054172199219465256\n",
      "Iteration 4503, Loss: 0.05398937687277794\n",
      "Iteration 4504, Loss: 0.054172154515981674\n",
      "Iteration 4505, Loss: 0.05398949608206749\n",
      "Iteration 4506, Loss: 0.05417203903198242\n",
      "Iteration 4507, Loss: 0.05398949980735779\n",
      "Iteration 4508, Loss: 0.054172150790691376\n",
      "Iteration 4509, Loss: 0.05398961156606674\n",
      "Iteration 4510, Loss: 0.05417211353778839\n",
      "Iteration 4511, Loss: 0.05398968607187271\n",
      "Iteration 4512, Loss: 0.05417218804359436\n",
      "Iteration 4513, Loss: 0.053989604115486145\n",
      "Iteration 4514, Loss: 0.05417212098836899\n",
      "Iteration 4515, Loss: 0.05398957058787346\n",
      "Iteration 4516, Loss: 0.05417215824127197\n",
      "Iteration 4517, Loss: 0.05398949235677719\n",
      "Iteration 4518, Loss: 0.05417215824127197\n",
      "Iteration 4519, Loss: 0.05398964136838913\n",
      "Iteration 4520, Loss: 0.054172199219465256\n",
      "Iteration 4521, Loss: 0.053989529609680176\n",
      "Iteration 4522, Loss: 0.05417227745056152\n",
      "Iteration 4523, Loss: 0.05398944765329361\n",
      "Iteration 4524, Loss: 0.05417223274707794\n",
      "Iteration 4525, Loss: 0.05398941785097122\n",
      "Iteration 4526, Loss: 0.05417212098836899\n",
      "Iteration 4527, Loss: 0.05398949608206749\n",
      "Iteration 4528, Loss: 0.05417196452617645\n",
      "Iteration 4529, Loss: 0.05398954078555107\n",
      "Iteration 4530, Loss: 0.054172150790691376\n",
      "Iteration 4531, Loss: 0.05398961156606674\n",
      "Iteration 4532, Loss: 0.054172150790691376\n",
      "Iteration 4533, Loss: 0.05398957431316376\n",
      "Iteration 4534, Loss: 0.05417218804359436\n",
      "Iteration 4535, Loss: 0.05398949980735779\n",
      "Iteration 4536, Loss: 0.05417199432849884\n",
      "Iteration 4537, Loss: 0.05398964881896973\n",
      "Iteration 4538, Loss: 0.054171960800886154\n",
      "Iteration 4539, Loss: 0.053989797830581665\n",
      "Iteration 4540, Loss: 0.054171882569789886\n",
      "Iteration 4541, Loss: 0.05398984253406525\n",
      "Iteration 4542, Loss: 0.05417189002037048\n",
      "Iteration 4543, Loss: 0.05398983880877495\n",
      "Iteration 4544, Loss: 0.054171960800886154\n",
      "Iteration 4545, Loss: 0.053989797830581665\n",
      "Iteration 4546, Loss: 0.05417203903198242\n",
      "Iteration 4547, Loss: 0.05398964509367943\n",
      "Iteration 4548, Loss: 0.054172273725271225\n",
      "Iteration 4549, Loss: 0.05398936569690704\n",
      "Iteration 4550, Loss: 0.054172348231077194\n",
      "Iteration 4551, Loss: 0.05398933216929436\n",
      "Iteration 4552, Loss: 0.05417230725288391\n",
      "Iteration 4553, Loss: 0.05398938059806824\n",
      "Iteration 4554, Loss: 0.054172150790691376\n",
      "Iteration 4555, Loss: 0.05398961156606674\n",
      "Iteration 4556, Loss: 0.054171960800886154\n",
      "Iteration 4557, Loss: 0.05398968979716301\n",
      "Iteration 4558, Loss: 0.054171763360500336\n",
      "Iteration 4559, Loss: 0.053989820182323456\n",
      "Iteration 4560, Loss: 0.054171763360500336\n",
      "Iteration 4561, Loss: 0.05398980900645256\n",
      "Iteration 4562, Loss: 0.0541718527674675\n",
      "Iteration 4563, Loss: 0.05398976802825928\n",
      "Iteration 4564, Loss: 0.054172005504369736\n",
      "Iteration 4565, Loss: 0.053989678621292114\n",
      "Iteration 4566, Loss: 0.05417227745056152\n",
      "Iteration 4567, Loss: 0.05398933216929436\n",
      "Iteration 4568, Loss: 0.054172515869140625\n",
      "Iteration 4569, Loss: 0.053989212960004807\n",
      "Iteration 4570, Loss: 0.054172586649656296\n",
      "Iteration 4571, Loss: 0.05398917943239212\n",
      "Iteration 4572, Loss: 0.054172318428754807\n",
      "Iteration 4573, Loss: 0.05398933216929436\n",
      "Iteration 4574, Loss: 0.054172080010175705\n",
      "Iteration 4575, Loss: 0.05398961156606674\n",
      "Iteration 4576, Loss: 0.0541718527674675\n",
      "Iteration 4577, Loss: 0.05398976802825928\n",
      "Iteration 4578, Loss: 0.05417172610759735\n",
      "Iteration 4579, Loss: 0.053989775478839874\n",
      "Iteration 4580, Loss: 0.05417191982269287\n",
      "Iteration 4581, Loss: 0.053989656269550323\n",
      "Iteration 4582, Loss: 0.054172009229660034\n",
      "Iteration 4583, Loss: 0.053989529609680176\n",
      "Iteration 4584, Loss: 0.054172247648239136\n",
      "Iteration 4585, Loss: 0.0539892241358757\n",
      "Iteration 4586, Loss: 0.054172515869140625\n",
      "Iteration 4587, Loss: 0.05398917943239212\n",
      "Iteration 4588, Loss: 0.05417235940694809\n",
      "Iteration 4589, Loss: 0.053989291191101074\n",
      "Iteration 4590, Loss: 0.05417215824127197\n",
      "Iteration 4591, Loss: 0.053989529609680176\n",
      "Iteration 4592, Loss: 0.0541718527674675\n",
      "Iteration 4593, Loss: 0.053989700973033905\n",
      "Iteration 4594, Loss: 0.05417172238230705\n",
      "Iteration 4595, Loss: 0.05398988723754883\n",
      "Iteration 4596, Loss: 0.054171763360500336\n",
      "Iteration 4597, Loss: 0.053989849984645844\n",
      "Iteration 4598, Loss: 0.05417191982269287\n",
      "Iteration 4599, Loss: 0.0539897195994854\n",
      "Iteration 4600, Loss: 0.054172128438949585\n",
      "Iteration 4601, Loss: 0.05398937687277794\n",
      "Iteration 4602, Loss: 0.05417240783572197\n",
      "Iteration 4603, Loss: 0.05398924648761749\n",
      "Iteration 4604, Loss: 0.05417255684733391\n",
      "Iteration 4605, Loss: 0.05398913472890854\n",
      "Iteration 4606, Loss: 0.054172515869140625\n",
      "Iteration 4607, Loss: 0.05398913845419884\n",
      "Iteration 4608, Loss: 0.054172318428754807\n",
      "Iteration 4609, Loss: 0.05398949235677719\n",
      "Iteration 4610, Loss: 0.054172080010175705\n",
      "Iteration 4611, Loss: 0.05398949235677719\n",
      "Iteration 4612, Loss: 0.05417203530669212\n",
      "Iteration 4613, Loss: 0.05398964881896973\n",
      "Iteration 4614, Loss: 0.05417191982269287\n",
      "Iteration 4615, Loss: 0.05398968979716301\n",
      "Iteration 4616, Loss: 0.054171960800886154\n",
      "Iteration 4617, Loss: 0.053990088403224945\n",
      "Iteration 4618, Loss: 0.054172080010175705\n",
      "Iteration 4619, Loss: 0.05399000644683838\n",
      "Iteration 4620, Loss: 0.05417263135313988\n",
      "Iteration 4621, Loss: 0.053989820182323456\n",
      "Iteration 4622, Loss: 0.05417262762784958\n",
      "Iteration 4623, Loss: 0.05399000644683838\n",
      "Iteration 4624, Loss: 0.05417235940694809\n",
      "Iteration 4625, Loss: 0.05399024486541748\n",
      "Iteration 4626, Loss: 0.05417215824127197\n",
      "Iteration 4627, Loss: 0.053990475833415985\n",
      "Iteration 4628, Loss: 0.05417200177907944\n",
      "Iteration 4629, Loss: 0.05399055778980255\n",
      "Iteration 4630, Loss: 0.0541720911860466\n",
      "Iteration 4631, Loss: 0.05399036034941673\n",
      "Iteration 4632, Loss: 0.054172173142433167\n",
      "Iteration 4633, Loss: 0.05399012193083763\n",
      "Iteration 4634, Loss: 0.05417260527610779\n",
      "Iteration 4635, Loss: 0.05398987978696823\n",
      "Iteration 4636, Loss: 0.05417299270629883\n",
      "Iteration 4637, Loss: 0.05398949980735779\n",
      "Iteration 4638, Loss: 0.0541730672121048\n",
      "Iteration 4639, Loss: 0.05398938059806824\n",
      "Iteration 4640, Loss: 0.05417291447520256\n",
      "Iteration 4641, Loss: 0.05398973450064659\n",
      "Iteration 4642, Loss: 0.05417267233133316\n",
      "Iteration 4643, Loss: 0.05398993194103241\n",
      "Iteration 4644, Loss: 0.05417235940694809\n",
      "Iteration 4645, Loss: 0.05399024486541748\n",
      "Iteration 4646, Loss: 0.054172124713659286\n",
      "Iteration 4647, Loss: 0.053990475833415985\n",
      "Iteration 4648, Loss: 0.054172005504369736\n",
      "Iteration 4649, Loss: 0.05399051308631897\n",
      "Iteration 4650, Loss: 0.054172154515981674\n",
      "Iteration 4651, Loss: 0.05399051308631897\n",
      "Iteration 4652, Loss: 0.05417213588953018\n",
      "Iteration 4653, Loss: 0.05399027466773987\n",
      "Iteration 4654, Loss: 0.0541725680232048\n",
      "Iteration 4655, Loss: 0.05398976802825928\n",
      "Iteration 4656, Loss: 0.05417299270629883\n",
      "Iteration 4657, Loss: 0.05398961156606674\n",
      "Iteration 4658, Loss: 0.05417299270629883\n",
      "Iteration 4659, Loss: 0.05398957431316376\n",
      "Iteration 4660, Loss: 0.05417283624410629\n",
      "Iteration 4661, Loss: 0.05398968979716301\n",
      "Iteration 4662, Loss: 0.05417275428771973\n",
      "Iteration 4663, Loss: 0.0539899617433548\n",
      "Iteration 4664, Loss: 0.05417259782552719\n",
      "Iteration 4665, Loss: 0.053989969193935394\n",
      "Iteration 4666, Loss: 0.05417255684733391\n",
      "Iteration 4667, Loss: 0.05399004742503166\n",
      "Iteration 4668, Loss: 0.05417260155081749\n",
      "Iteration 4669, Loss: 0.05398988723754883\n",
      "Iteration 4670, Loss: 0.05417272076010704\n",
      "Iteration 4671, Loss: 0.053989775478839874\n",
      "Iteration 4672, Loss: 0.05417275428771973\n",
      "Iteration 4673, Loss: 0.053989849984645844\n",
      "Iteration 4674, Loss: 0.05417279899120331\n",
      "Iteration 4675, Loss: 0.05398976802825928\n",
      "Iteration 4676, Loss: 0.05417287349700928\n",
      "Iteration 4677, Loss: 0.05398973077535629\n",
      "Iteration 4678, Loss: 0.05417286604642868\n",
      "Iteration 4679, Loss: 0.05398969352245331\n",
      "Iteration 4680, Loss: 0.0541728250682354\n",
      "Iteration 4681, Loss: 0.053989969193935394\n",
      "Iteration 4682, Loss: 0.05417255312204361\n",
      "Iteration 4683, Loss: 0.053989969193935394\n",
      "Iteration 4684, Loss: 0.054172441363334656\n",
      "Iteration 4685, Loss: 0.05399012565612793\n",
      "Iteration 4686, Loss: 0.05417247861623764\n",
      "Iteration 4687, Loss: 0.053990088403224945\n",
      "Iteration 4688, Loss: 0.05417244881391525\n",
      "Iteration 4689, Loss: 0.05399000644683838\n",
      "Iteration 4690, Loss: 0.054172635078430176\n",
      "Iteration 4691, Loss: 0.05398992821574211\n",
      "Iteration 4692, Loss: 0.054172635078430176\n",
      "Iteration 4693, Loss: 0.05398977920413017\n",
      "Iteration 4694, Loss: 0.05417259782552719\n",
      "Iteration 4695, Loss: 0.05399004742503166\n",
      "Iteration 4696, Loss: 0.05417247861623764\n",
      "Iteration 4697, Loss: 0.05399012565612793\n",
      "Iteration 4698, Loss: 0.05417235940694809\n",
      "Iteration 4699, Loss: 0.05399016663432121\n",
      "Iteration 4700, Loss: 0.054172396659851074\n",
      "Iteration 4701, Loss: 0.05399012565612793\n",
      "Iteration 4702, Loss: 0.05417247861623764\n",
      "Iteration 4703, Loss: 0.05399012565612793\n",
      "Iteration 4704, Loss: 0.05417255684733391\n",
      "Iteration 4705, Loss: 0.05399004742503166\n",
      "Iteration 4706, Loss: 0.05417267233133316\n",
      "Iteration 4707, Loss: 0.05398980900645256\n",
      "Iteration 4708, Loss: 0.054172784090042114\n",
      "Iteration 4709, Loss: 0.053989849984645844\n",
      "Iteration 4710, Loss: 0.05417275056242943\n",
      "Iteration 4711, Loss: 0.053989775478839874\n",
      "Iteration 4712, Loss: 0.054172635078430176\n",
      "Iteration 4713, Loss: 0.05398988723754883\n",
      "Iteration 4714, Loss: 0.05417255684733391\n",
      "Iteration 4715, Loss: 0.05399000272154808\n",
      "Iteration 4716, Loss: 0.05417251214385033\n",
      "Iteration 4717, Loss: 0.0539902001619339\n",
      "Iteration 4718, Loss: 0.054172322154045105\n",
      "Iteration 4719, Loss: 0.053990356624126434\n",
      "Iteration 4720, Loss: 0.05417243763804436\n",
      "Iteration 4721, Loss: 0.05399012565612793\n",
      "Iteration 4722, Loss: 0.054172515869140625\n",
      "Iteration 4723, Loss: 0.053989969193935394\n",
      "Iteration 4724, Loss: 0.05417272076010704\n",
      "Iteration 4725, Loss: 0.05398961901664734\n",
      "Iteration 4726, Loss: 0.054172955453395844\n",
      "Iteration 4727, Loss: 0.053989581763744354\n",
      "Iteration 4728, Loss: 0.05417298898100853\n",
      "Iteration 4729, Loss: 0.05398957431316376\n",
      "Iteration 4730, Loss: 0.05417291074991226\n",
      "Iteration 4731, Loss: 0.05398968979716301\n",
      "Iteration 4732, Loss: 0.05417259782552719\n",
      "Iteration 4733, Loss: 0.05399015545845032\n",
      "Iteration 4734, Loss: 0.054172366857528687\n",
      "Iteration 4735, Loss: 0.05399027466773987\n",
      "Iteration 4736, Loss: 0.05417224019765854\n",
      "Iteration 4737, Loss: 0.05399032682180405\n",
      "Iteration 4738, Loss: 0.05417216941714287\n",
      "Iteration 4739, Loss: 0.05399039387702942\n",
      "Iteration 4740, Loss: 0.054172247648239136\n",
      "Iteration 4741, Loss: 0.05399027094244957\n",
      "Iteration 4742, Loss: 0.05417264997959137\n",
      "Iteration 4743, Loss: 0.05398976802825928\n",
      "Iteration 4744, Loss: 0.054172955453395844\n",
      "Iteration 4745, Loss: 0.05398957058787346\n",
      "Iteration 4746, Loss: 0.054173074662685394\n",
      "Iteration 4747, Loss: 0.05398957431316376\n",
      "Iteration 4748, Loss: 0.05417299270629883\n",
      "Iteration 4749, Loss: 0.05398968979716301\n",
      "Iteration 4750, Loss: 0.05417271703481674\n",
      "Iteration 4751, Loss: 0.05398992821574211\n",
      "Iteration 4752, Loss: 0.05417247861623764\n",
      "Iteration 4753, Loss: 0.053990088403224945\n",
      "Iteration 4754, Loss: 0.054172247648239136\n",
      "Iteration 4755, Loss: 0.053990285843610764\n",
      "Iteration 4756, Loss: 0.054172202944755554\n",
      "Iteration 4757, Loss: 0.05399031937122345\n",
      "Iteration 4758, Loss: 0.05417235940694809\n",
      "Iteration 4759, Loss: 0.0539901964366436\n",
      "Iteration 4760, Loss: 0.054172687232494354\n",
      "Iteration 4761, Loss: 0.05398980900645256\n",
      "Iteration 4762, Loss: 0.054172955453395844\n",
      "Iteration 4763, Loss: 0.05398964881896973\n",
      "Iteration 4764, Loss: 0.05417279899120331\n",
      "Iteration 4765, Loss: 0.05398964881896973\n",
      "Iteration 4766, Loss: 0.05417275056242943\n",
      "Iteration 4767, Loss: 0.05398992821574211\n",
      "Iteration 4768, Loss: 0.05417255684733391\n",
      "Iteration 4769, Loss: 0.053990088403224945\n",
      "Iteration 4770, Loss: 0.0541723296046257\n",
      "Iteration 4771, Loss: 0.05399027466773987\n",
      "Iteration 4772, Loss: 0.05417228490114212\n",
      "Iteration 4773, Loss: 0.053990207612514496\n",
      "Iteration 4774, Loss: 0.05417235940694809\n",
      "Iteration 4775, Loss: 0.05399024486541748\n",
      "Iteration 4776, Loss: 0.054172322154045105\n",
      "Iteration 4777, Loss: 0.053990088403224945\n",
      "Iteration 4778, Loss: 0.054172635078430176\n",
      "Iteration 4779, Loss: 0.05399004742503166\n",
      "Iteration 4780, Loss: 0.05417267605662346\n",
      "Iteration 4781, Loss: 0.05398985370993614\n",
      "Iteration 4782, Loss: 0.05417279154062271\n",
      "Iteration 4783, Loss: 0.053989775478839874\n",
      "Iteration 4784, Loss: 0.05417264625430107\n",
      "Iteration 4785, Loss: 0.05398976802825928\n",
      "Iteration 4786, Loss: 0.05417267605662346\n",
      "Iteration 4787, Loss: 0.05398988723754883\n",
      "Iteration 4788, Loss: 0.054172784090042114\n",
      "Iteration 4789, Loss: 0.05398992821574211\n",
      "Iteration 4790, Loss: 0.054172709584236145\n",
      "Iteration 4791, Loss: 0.05398981273174286\n",
      "Iteration 4792, Loss: 0.05417267233133316\n",
      "Iteration 4793, Loss: 0.05398992821574211\n",
      "Iteration 4794, Loss: 0.054172664880752563\n",
      "Iteration 4795, Loss: 0.05399000272154808\n",
      "Iteration 4796, Loss: 0.05417263135313988\n",
      "Iteration 4797, Loss: 0.05398988723754883\n",
      "Iteration 4798, Loss: 0.054172515869140625\n",
      "Iteration 4799, Loss: 0.053989969193935394\n",
      "Iteration 4800, Loss: 0.054172396659851074\n",
      "Iteration 4801, Loss: 0.05399004742503166\n",
      "Iteration 4802, Loss: 0.05417228862643242\n",
      "Iteration 4803, Loss: 0.053990356624126434\n",
      "Iteration 4804, Loss: 0.05417235940694809\n",
      "Iteration 4805, Loss: 0.053990285843610764\n",
      "Iteration 4806, Loss: 0.05417235940694809\n",
      "Iteration 4807, Loss: 0.05399012565612793\n",
      "Iteration 4808, Loss: 0.054172396659851074\n",
      "Iteration 4809, Loss: 0.05399027466773987\n",
      "Iteration 4810, Loss: 0.05417255312204361\n",
      "Iteration 4811, Loss: 0.05399004742503166\n",
      "Iteration 4812, Loss: 0.054172515869140625\n",
      "Iteration 4813, Loss: 0.053990043699741364\n",
      "Iteration 4814, Loss: 0.05417271703481674\n",
      "Iteration 4815, Loss: 0.05398980900645256\n",
      "Iteration 4816, Loss: 0.05417291447520256\n",
      "Iteration 4817, Loss: 0.05398968979716301\n",
      "Iteration 4818, Loss: 0.05417291447520256\n",
      "Iteration 4819, Loss: 0.05398968979716301\n",
      "Iteration 4820, Loss: 0.054172828793525696\n",
      "Iteration 4821, Loss: 0.05398968979716301\n",
      "Iteration 4822, Loss: 0.05417259782552719\n",
      "Iteration 4823, Loss: 0.053989969193935394\n",
      "Iteration 4824, Loss: 0.05417229235172272\n",
      "Iteration 4825, Loss: 0.053990207612514496\n",
      "Iteration 4826, Loss: 0.05417224019765854\n",
      "Iteration 4827, Loss: 0.05399032682180405\n",
      "Iteration 4828, Loss: 0.05417221039533615\n",
      "Iteration 4829, Loss: 0.05399031564593315\n",
      "Iteration 4830, Loss: 0.05417240411043167\n",
      "Iteration 4831, Loss: 0.05399000644683838\n",
      "Iteration 4832, Loss: 0.054172635078430176\n",
      "Iteration 4833, Loss: 0.05398988351225853\n",
      "Iteration 4834, Loss: 0.05417267605662346\n",
      "Iteration 4835, Loss: 0.053989849984645844\n",
      "Iteration 4836, Loss: 0.054172709584236145\n",
      "Iteration 4837, Loss: 0.05398992821574211\n",
      "Iteration 4838, Loss: 0.05417259782552719\n",
      "Iteration 4839, Loss: 0.053990043699741364\n",
      "Iteration 4840, Loss: 0.05417254567146301\n",
      "Iteration 4841, Loss: 0.05399000644683838\n",
      "Iteration 4842, Loss: 0.054172366857528687\n",
      "Iteration 4843, Loss: 0.053990207612514496\n",
      "Iteration 4844, Loss: 0.054172322154045105\n",
      "Iteration 4845, Loss: 0.05399024486541748\n",
      "Iteration 4846, Loss: 0.054172396659851074\n",
      "Iteration 4847, Loss: 0.05399031564593315\n",
      "Iteration 4848, Loss: 0.05417255684733391\n",
      "Iteration 4849, Loss: 0.053989969193935394\n",
      "Iteration 4850, Loss: 0.05417275428771973\n",
      "Iteration 4851, Loss: 0.05398973077535629\n",
      "Iteration 4852, Loss: 0.05417275428771973\n",
      "Iteration 4853, Loss: 0.05398973077535629\n",
      "Iteration 4854, Loss: 0.054172635078430176\n",
      "Iteration 4855, Loss: 0.05399011820554733\n",
      "Iteration 4856, Loss: 0.05417247861623764\n",
      "Iteration 4857, Loss: 0.05399024114012718\n",
      "Iteration 4858, Loss: 0.05417228490114212\n",
      "Iteration 4859, Loss: 0.053990356624126434\n",
      "Iteration 4860, Loss: 0.054172396659851074\n",
      "Iteration 4861, Loss: 0.053990207612514496\n",
      "Iteration 4862, Loss: 0.05417240783572197\n",
      "Iteration 4863, Loss: 0.053989969193935394\n",
      "Iteration 4864, Loss: 0.05417267978191376\n",
      "Iteration 4865, Loss: 0.05398973077535629\n",
      "Iteration 4866, Loss: 0.054172955453395844\n",
      "Iteration 4867, Loss: 0.05398961156606674\n",
      "Iteration 4868, Loss: 0.05417299270629883\n",
      "Iteration 4869, Loss: 0.05398961901664734\n",
      "Iteration 4870, Loss: 0.05417291447520256\n",
      "Iteration 4871, Loss: 0.053989581763744354\n",
      "Iteration 4872, Loss: 0.05417279154062271\n",
      "Iteration 4873, Loss: 0.05398976802825928\n",
      "Iteration 4874, Loss: 0.05417259782552719\n",
      "Iteration 4875, Loss: 0.05399004742503166\n",
      "Iteration 4876, Loss: 0.0541723296046257\n",
      "Iteration 4877, Loss: 0.05399024114012718\n",
      "Iteration 4878, Loss: 0.054172318428754807\n",
      "Iteration 4879, Loss: 0.05399039387702942\n",
      "Iteration 4880, Loss: 0.05417216569185257\n",
      "Iteration 4881, Loss: 0.05399032682180405\n",
      "Iteration 4882, Loss: 0.05417224019765854\n",
      "Iteration 4883, Loss: 0.053990356624126434\n",
      "Iteration 4884, Loss: 0.05417247116565704\n",
      "Iteration 4885, Loss: 0.05399004742503166\n",
      "Iteration 4886, Loss: 0.05417259782552719\n",
      "Iteration 4887, Loss: 0.05399000644683838\n",
      "Iteration 4888, Loss: 0.054172635078430176\n",
      "Iteration 4889, Loss: 0.05398981273174286\n",
      "Iteration 4890, Loss: 0.05417279154062271\n",
      "Iteration 4891, Loss: 0.05398988723754883\n",
      "Iteration 4892, Loss: 0.05417267605662346\n",
      "Iteration 4893, Loss: 0.05398988723754883\n",
      "Iteration 4894, Loss: 0.05417255684733391\n",
      "Iteration 4895, Loss: 0.05398988723754883\n",
      "Iteration 4896, Loss: 0.05417255684733391\n",
      "Iteration 4897, Loss: 0.053989894688129425\n",
      "Iteration 4898, Loss: 0.05417259782552719\n",
      "Iteration 4899, Loss: 0.05399000272154808\n",
      "Iteration 4900, Loss: 0.05417263135313988\n",
      "Iteration 4901, Loss: 0.053989969193935394\n",
      "Iteration 4902, Loss: 0.05417263135313988\n",
      "Iteration 4903, Loss: 0.053989969193935394\n",
      "Iteration 4904, Loss: 0.05417262762784958\n",
      "Iteration 4905, Loss: 0.053989969193935394\n",
      "Iteration 4906, Loss: 0.05417262762784958\n",
      "Iteration 4907, Loss: 0.053989969193935394\n",
      "Iteration 4908, Loss: 0.05417262762784958\n",
      "Iteration 4909, Loss: 0.053989969193935394\n",
      "Iteration 4910, Loss: 0.054172590374946594\n",
      "Iteration 4911, Loss: 0.053989969193935394\n",
      "Iteration 4912, Loss: 0.05417255684733391\n",
      "Iteration 4913, Loss: 0.053990088403224945\n",
      "Iteration 4914, Loss: 0.0541725680232048\n",
      "Iteration 4915, Loss: 0.05398992821574211\n",
      "Iteration 4916, Loss: 0.054172687232494354\n",
      "Iteration 4917, Loss: 0.053989656269550323\n",
      "Iteration 4918, Loss: 0.054172880947589874\n",
      "Iteration 4919, Loss: 0.05398949980735779\n",
      "Iteration 4920, Loss: 0.054172955453395844\n",
      "Iteration 4921, Loss: 0.05398957431316376\n",
      "Iteration 4922, Loss: 0.05417275428771973\n",
      "Iteration 4923, Loss: 0.05398980900645256\n",
      "Iteration 4924, Loss: 0.054172664880752563\n",
      "Iteration 4925, Loss: 0.053989969193935394\n",
      "Iteration 4926, Loss: 0.0541723296046257\n",
      "Iteration 4927, Loss: 0.05399024114012718\n",
      "Iteration 4928, Loss: 0.05417227745056152\n",
      "Iteration 4929, Loss: 0.053990401327610016\n",
      "Iteration 4930, Loss: 0.054172199219465256\n",
      "Iteration 4931, Loss: 0.05399024486541748\n",
      "Iteration 4932, Loss: 0.05417235940694809\n",
      "Iteration 4933, Loss: 0.053990282118320465\n",
      "Iteration 4934, Loss: 0.05417236313223839\n",
      "Iteration 4935, Loss: 0.053990088403224945\n",
      "Iteration 4936, Loss: 0.0541725680232048\n",
      "Iteration 4937, Loss: 0.05398980900645256\n",
      "Iteration 4938, Loss: 0.05417279526591301\n",
      "Iteration 4939, Loss: 0.05398968979716301\n",
      "Iteration 4940, Loss: 0.05417279154062271\n",
      "Iteration 4941, Loss: 0.05398973822593689\n",
      "Iteration 4942, Loss: 0.05417255684733391\n",
      "Iteration 4943, Loss: 0.053990043699741364\n",
      "Iteration 4944, Loss: 0.05417255312204361\n",
      "Iteration 4945, Loss: 0.05399011820554733\n",
      "Iteration 4946, Loss: 0.05417236313223839\n",
      "Iteration 4947, Loss: 0.053990088403224945\n",
      "Iteration 4948, Loss: 0.05417235940694809\n",
      "Iteration 4949, Loss: 0.053990282118320465\n",
      "Iteration 4950, Loss: 0.05417247861623764\n",
      "Iteration 4951, Loss: 0.05399016663432121\n",
      "Iteration 4952, Loss: 0.054172709584236145\n",
      "Iteration 4953, Loss: 0.053989849984645844\n",
      "Iteration 4954, Loss: 0.05417279526591301\n",
      "Iteration 4955, Loss: 0.05398976802825928\n",
      "Iteration 4956, Loss: 0.05417286977171898\n",
      "Iteration 4957, Loss: 0.05398973822593689\n",
      "Iteration 4958, Loss: 0.05417275428771973\n",
      "Iteration 4959, Loss: 0.05398988723754883\n",
      "Iteration 4960, Loss: 0.054172709584236145\n",
      "Iteration 4961, Loss: 0.0539899617433548\n",
      "Iteration 4962, Loss: 0.05417240783572197\n",
      "Iteration 4963, Loss: 0.05399007722735405\n",
      "Iteration 4964, Loss: 0.0541723296046257\n",
      "Iteration 4965, Loss: 0.053990088403224945\n",
      "Iteration 4966, Loss: 0.05417243763804436\n",
      "Iteration 4967, Loss: 0.05399012565612793\n",
      "Iteration 4968, Loss: 0.05417243763804436\n",
      "Iteration 4969, Loss: 0.053990092128515244\n",
      "Iteration 4970, Loss: 0.05417248234152794\n",
      "Iteration 4971, Loss: 0.05399004742503166\n",
      "Iteration 4972, Loss: 0.05417260155081749\n",
      "Iteration 4973, Loss: 0.0539892241358757\n",
      "Iteration 4974, Loss: 0.0541725680232048\n",
      "Iteration 4975, Loss: 0.053989119827747345\n",
      "Iteration 4976, Loss: 0.05417267605662346\n",
      "Iteration 4977, Loss: 0.05398915708065033\n",
      "Iteration 4978, Loss: 0.05417194217443466\n",
      "Iteration 4979, Loss: 0.053989261388778687\n",
      "Iteration 4980, Loss: 0.05417194217443466\n",
      "Iteration 4981, Loss: 0.05398930236697197\n",
      "Iteration 4982, Loss: 0.054171934723854065\n",
      "Iteration 4983, Loss: 0.05398930236697197\n",
      "Iteration 4984, Loss: 0.05417182669043541\n",
      "Iteration 4985, Loss: 0.0539892315864563\n",
      "Iteration 4986, Loss: 0.05417190119624138\n",
      "Iteration 4987, Loss: 0.053989261388778687\n",
      "Iteration 4988, Loss: 0.05417190119624138\n",
      "Iteration 4989, Loss: 0.05398930236697197\n",
      "Iteration 4990, Loss: 0.05417182296514511\n",
      "Iteration 4991, Loss: 0.053989261388778687\n",
      "Iteration 4992, Loss: 0.054171785712242126\n",
      "Iteration 4993, Loss: 0.053989261388778687\n",
      "Iteration 4994, Loss: 0.05417182296514511\n",
      "Iteration 4995, Loss: 0.0539892241358757\n",
      "Iteration 4996, Loss: 0.0541718564927578\n",
      "Iteration 4997, Loss: 0.05398938059806824\n",
      "Iteration 4998, Loss: 0.05417170375585556\n",
      "Iteration 4999, Loss: 0.05398930609226227\n",
      "Iteration 5000, Loss: 0.05417166277766228\n",
      "Iteration 5001, Loss: 0.053989432752132416\n",
      "Iteration 5002, Loss: 0.05417158827185631\n",
      "Iteration 5003, Loss: 0.0539894662797451\n",
      "Iteration 5004, Loss: 0.054171547293663025\n",
      "Iteration 5005, Loss: 0.053989510983228683\n",
      "Iteration 5006, Loss: 0.05417170375585556\n",
      "Iteration 5007, Loss: 0.05398949608206749\n",
      "Iteration 5008, Loss: 0.05417179316282272\n",
      "Iteration 5009, Loss: 0.053989142179489136\n",
      "Iteration 5010, Loss: 0.05417221784591675\n",
      "Iteration 5011, Loss: 0.05398891493678093\n",
      "Iteration 5012, Loss: 0.05417225882411003\n",
      "Iteration 5013, Loss: 0.05398891493678093\n",
      "Iteration 5014, Loss: 0.05417206138372421\n",
      "Iteration 5015, Loss: 0.053989067673683167\n",
      "Iteration 5016, Loss: 0.05417190119624138\n",
      "Iteration 5017, Loss: 0.05398934707045555\n",
      "Iteration 5018, Loss: 0.054171621799468994\n",
      "Iteration 5019, Loss: 0.05398954078555107\n",
      "Iteration 5020, Loss: 0.05417146533727646\n",
      "Iteration 5021, Loss: 0.053989704698324203\n",
      "Iteration 5022, Loss: 0.05417127534747124\n",
      "Iteration 5023, Loss: 0.053989820182323456\n",
      "Iteration 5024, Loss: 0.054171472787857056\n",
      "Iteration 5025, Loss: 0.05398954078555107\n",
      "Iteration 5026, Loss: 0.054171860218048096\n",
      "Iteration 5027, Loss: 0.05398911237716675\n",
      "Iteration 5028, Loss: 0.05417213961482048\n",
      "Iteration 5029, Loss: 0.05398891493678093\n",
      "Iteration 5030, Loss: 0.05417225882411003\n",
      "Iteration 5031, Loss: 0.05398883670568466\n",
      "Iteration 5032, Loss: 0.05417221784591675\n",
      "Iteration 5033, Loss: 0.053989022970199585\n",
      "Iteration 5034, Loss: 0.05417213961482048\n",
      "Iteration 5035, Loss: 0.053989142179489136\n",
      "Iteration 5036, Loss: 0.05417194217443466\n",
      "Iteration 5037, Loss: 0.05398930236697197\n",
      "Iteration 5038, Loss: 0.0541718527674675\n",
      "Iteration 5039, Loss: 0.05398942157626152\n",
      "Iteration 5040, Loss: 0.05417178198695183\n",
      "Iteration 5041, Loss: 0.05398938059806824\n",
      "Iteration 5042, Loss: 0.05417182296514511\n",
      "Iteration 5043, Loss: 0.053989261388778687\n",
      "Iteration 5044, Loss: 0.05417194217443466\n",
      "Iteration 5045, Loss: 0.05398918315768242\n",
      "Iteration 5046, Loss: 0.05417210981249809\n",
      "Iteration 5047, Loss: 0.05398895591497421\n",
      "Iteration 5048, Loss: 0.05417218804359436\n",
      "Iteration 5049, Loss: 0.05398886650800705\n",
      "Iteration 5050, Loss: 0.05417226254940033\n",
      "Iteration 5051, Loss: 0.053988825529813766\n",
      "Iteration 5052, Loss: 0.05417225882411003\n",
      "Iteration 5053, Loss: 0.05398883670568466\n",
      "Iteration 5054, Loss: 0.0541720949113369\n",
      "Iteration 5055, Loss: 0.05398911237716675\n",
      "Iteration 5056, Loss: 0.05417162925004959\n",
      "Iteration 5057, Loss: 0.05398954078555107\n",
      "Iteration 5058, Loss: 0.05417146533727646\n",
      "Iteration 5059, Loss: 0.05398965999484062\n",
      "Iteration 5060, Loss: 0.05417146533727646\n",
      "Iteration 5061, Loss: 0.05398965999484062\n",
      "Iteration 5062, Loss: 0.05417158454656601\n",
      "Iteration 5063, Loss: 0.053989581763744354\n",
      "Iteration 5064, Loss: 0.054171591997146606\n",
      "Iteration 5065, Loss: 0.0539894625544548\n",
      "Iteration 5066, Loss: 0.054171741008758545\n",
      "Iteration 5067, Loss: 0.05398942530155182\n",
      "Iteration 5068, Loss: 0.054171860218048096\n",
      "Iteration 5069, Loss: 0.0539892315864563\n",
      "Iteration 5070, Loss: 0.05417194217443466\n",
      "Iteration 5071, Loss: 0.05398910492658615\n",
      "Iteration 5072, Loss: 0.054172031581401825\n",
      "Iteration 5073, Loss: 0.0539889894425869\n",
      "Iteration 5074, Loss: 0.05417210981249809\n",
      "Iteration 5075, Loss: 0.05398883670568466\n",
      "Iteration 5076, Loss: 0.05417225882411003\n",
      "Iteration 5077, Loss: 0.053988873958587646\n",
      "Iteration 5078, Loss: 0.05417221784591675\n",
      "Iteration 5079, Loss: 0.05398903042078018\n",
      "Iteration 5080, Loss: 0.05417194217443466\n",
      "Iteration 5081, Loss: 0.05398915708065033\n",
      "Iteration 5082, Loss: 0.054171741008758545\n",
      "Iteration 5083, Loss: 0.053989432752132416\n",
      "Iteration 5084, Loss: 0.054171543568372726\n",
      "Iteration 5085, Loss: 0.05398961901664734\n",
      "Iteration 5086, Loss: 0.05417134612798691\n",
      "Iteration 5087, Loss: 0.05398966372013092\n",
      "Iteration 5088, Loss: 0.05417134612798691\n",
      "Iteration 5089, Loss: 0.05398977920413017\n",
      "Iteration 5090, Loss: 0.054171428084373474\n",
      "Iteration 5091, Loss: 0.05398961901664734\n",
      "Iteration 5092, Loss: 0.05417155846953392\n",
      "Iteration 5093, Loss: 0.053989313542842865\n",
      "Iteration 5094, Loss: 0.05417183041572571\n",
      "Iteration 5095, Loss: 0.05398895591497421\n",
      "Iteration 5096, Loss: 0.05417218059301376\n",
      "Iteration 5097, Loss: 0.05398891121149063\n",
      "Iteration 5098, Loss: 0.054172299802303314\n",
      "Iteration 5099, Loss: 0.053988754749298096\n",
      "Iteration 5100, Loss: 0.05417221784591675\n",
      "Iteration 5101, Loss: 0.053988873958587646\n",
      "Iteration 5102, Loss: 0.05417202040553093\n",
      "Iteration 5103, Loss: 0.05398915335536003\n",
      "Iteration 5104, Loss: 0.05417170748114586\n",
      "Iteration 5105, Loss: 0.05398939177393913\n",
      "Iteration 5106, Loss: 0.05417150259017944\n",
      "Iteration 5107, Loss: 0.05398973822593689\n",
      "Iteration 5108, Loss: 0.05417130887508392\n",
      "Iteration 5109, Loss: 0.053989704698324203\n",
      "Iteration 5110, Loss: 0.054171230643987656\n",
      "Iteration 5111, Loss: 0.05398967117071152\n",
      "Iteration 5112, Loss: 0.05417146906256676\n",
      "Iteration 5113, Loss: 0.053989432752132416\n",
      "Iteration 5114, Loss: 0.0541715994477272\n",
      "Iteration 5115, Loss: 0.05398903787136078\n",
      "Iteration 5116, Loss: 0.054172106087207794\n",
      "Iteration 5117, Loss: 0.05398883670568466\n",
      "Iteration 5118, Loss: 0.05417218059301376\n",
      "Iteration 5119, Loss: 0.05398883670568466\n",
      "Iteration 5120, Loss: 0.05417225882411003\n",
      "Iteration 5121, Loss: 0.05398903414607048\n",
      "Iteration 5122, Loss: 0.0541720949113369\n",
      "Iteration 5123, Loss: 0.05398911237716675\n",
      "Iteration 5124, Loss: 0.054171930998563766\n",
      "Iteration 5125, Loss: 0.05398927256464958\n",
      "Iteration 5126, Loss: 0.05417182296514511\n",
      "Iteration 5127, Loss: 0.05398934707045555\n",
      "Iteration 5128, Loss: 0.05417170748114586\n",
      "Iteration 5129, Loss: 0.053989388048648834\n",
      "Iteration 5130, Loss: 0.05417182296514511\n",
      "Iteration 5131, Loss: 0.053989313542842865\n",
      "Iteration 5132, Loss: 0.054171860218048096\n",
      "Iteration 5133, Loss: 0.0539892315864563\n",
      "Iteration 5134, Loss: 0.05417186766862869\n",
      "Iteration 5135, Loss: 0.053989119827747345\n",
      "Iteration 5136, Loss: 0.05417190492153168\n",
      "Iteration 5137, Loss: 0.05398907884955406\n",
      "Iteration 5138, Loss: 0.054171934723854065\n",
      "Iteration 5139, Loss: 0.05398915335536003\n",
      "Iteration 5140, Loss: 0.05417189747095108\n",
      "Iteration 5141, Loss: 0.05398930236697197\n",
      "Iteration 5142, Loss: 0.05417178198695183\n",
      "Iteration 5143, Loss: 0.0539892315864563\n",
      "Iteration 5144, Loss: 0.054171741008758545\n",
      "Iteration 5145, Loss: 0.05398942157626152\n",
      "Iteration 5146, Loss: 0.054171785712242126\n",
      "Iteration 5147, Loss: 0.05398915708065033\n",
      "Iteration 5148, Loss: 0.05417182669043541\n",
      "Iteration 5149, Loss: 0.05398911237716675\n",
      "Iteration 5150, Loss: 0.05417194217443466\n",
      "Iteration 5151, Loss: 0.05398911237716675\n",
      "Iteration 5152, Loss: 0.054171979427337646\n",
      "Iteration 5153, Loss: 0.05398896336555481\n",
      "Iteration 5154, Loss: 0.05417190119624138\n",
      "Iteration 5155, Loss: 0.0539892315864563\n",
      "Iteration 5156, Loss: 0.05417182296514511\n",
      "Iteration 5157, Loss: 0.05398942157626152\n",
      "Iteration 5158, Loss: 0.054171666502952576\n",
      "Iteration 5159, Loss: 0.05398938059806824\n",
      "Iteration 5160, Loss: 0.05417170748114586\n",
      "Iteration 5161, Loss: 0.05398927256464958\n",
      "Iteration 5162, Loss: 0.05417175218462944\n",
      "Iteration 5163, Loss: 0.0539892315864563\n",
      "Iteration 5164, Loss: 0.05417179316282272\n",
      "Iteration 5165, Loss: 0.053989261388778687\n",
      "Iteration 5166, Loss: 0.05417182669043541\n",
      "Iteration 5167, Loss: 0.053989119827747345\n",
      "Iteration 5168, Loss: 0.05417201668024063\n",
      "Iteration 5169, Loss: 0.05398907884955406\n",
      "Iteration 5170, Loss: 0.05417205020785332\n",
      "Iteration 5171, Loss: 0.05398907884955406\n",
      "Iteration 5172, Loss: 0.054171934723854065\n",
      "Iteration 5173, Loss: 0.0539892241358757\n",
      "Iteration 5174, Loss: 0.054171860218048096\n",
      "Iteration 5175, Loss: 0.05398930236697197\n",
      "Iteration 5176, Loss: 0.05417178198695183\n",
      "Iteration 5177, Loss: 0.05398930609226227\n",
      "Iteration 5178, Loss: 0.05417182296514511\n",
      "Iteration 5179, Loss: 0.05398935079574585\n",
      "Iteration 5180, Loss: 0.05417194217443466\n",
      "Iteration 5181, Loss: 0.05398911237716675\n",
      "Iteration 5182, Loss: 0.05417194217443466\n",
      "Iteration 5183, Loss: 0.05398903787136078\n",
      "Iteration 5184, Loss: 0.05417202040553093\n",
      "Iteration 5185, Loss: 0.05398903042078018\n",
      "Iteration 5186, Loss: 0.05417206138372421\n",
      "Iteration 5187, Loss: 0.05398903042078018\n",
      "Iteration 5188, Loss: 0.05417194217443466\n",
      "Iteration 5189, Loss: 0.0539892315864563\n",
      "Iteration 5190, Loss: 0.05417182296514511\n",
      "Iteration 5191, Loss: 0.05398938059806824\n",
      "Iteration 5192, Loss: 0.05417166277766228\n",
      "Iteration 5193, Loss: 0.0539894625544548\n",
      "Iteration 5194, Loss: 0.054171591997146606\n",
      "Iteration 5195, Loss: 0.053989388048648834\n",
      "Iteration 5196, Loss: 0.054171666502952576\n",
      "Iteration 5197, Loss: 0.053989388048648834\n",
      "Iteration 5198, Loss: 0.05417170375585556\n",
      "Iteration 5199, Loss: 0.05398935079574585\n",
      "Iteration 5200, Loss: 0.05417162925004959\n",
      "Iteration 5201, Loss: 0.05398939177393913\n",
      "Iteration 5202, Loss: 0.05417162925004959\n",
      "Iteration 5203, Loss: 0.05398927256464958\n",
      "Iteration 5204, Loss: 0.054171621799468994\n",
      "Iteration 5205, Loss: 0.053989313542842865\n",
      "Iteration 5206, Loss: 0.054171621799468994\n",
      "Iteration 5207, Loss: 0.05398954078555107\n",
      "Iteration 5208, Loss: 0.054171349853277206\n",
      "Iteration 5209, Loss: 0.05398977920413017\n",
      "Iteration 5210, Loss: 0.05417126417160034\n",
      "Iteration 5211, Loss: 0.053989820182323456\n",
      "Iteration 5212, Loss: 0.05417122691869736\n",
      "Iteration 5213, Loss: 0.05398977920413017\n",
      "Iteration 5214, Loss: 0.05417138338088989\n",
      "Iteration 5215, Loss: 0.053989700973033905\n",
      "Iteration 5216, Loss: 0.05417143553495407\n",
      "Iteration 5217, Loss: 0.053989581763744354\n",
      "Iteration 5218, Loss: 0.05417163297533989\n",
      "Iteration 5219, Loss: 0.05398934334516525\n",
      "Iteration 5220, Loss: 0.05417186766862869\n",
      "Iteration 5221, Loss: 0.05398910492658615\n",
      "Iteration 5222, Loss: 0.0541720986366272\n",
      "Iteration 5223, Loss: 0.053988806903362274\n",
      "Iteration 5224, Loss: 0.05417202413082123\n",
      "Iteration 5225, Loss: 0.05398888513445854\n",
      "Iteration 5226, Loss: 0.05417194217443466\n",
      "Iteration 5227, Loss: 0.053989164531230927\n",
      "Iteration 5228, Loss: 0.05417169630527496\n",
      "Iteration 5229, Loss: 0.05398954078555107\n",
      "Iteration 5230, Loss: 0.054171353578567505\n",
      "Iteration 5231, Loss: 0.053989700973033905\n",
      "Iteration 5232, Loss: 0.054171230643987656\n",
      "Iteration 5233, Loss: 0.053989749401807785\n",
      "Iteration 5234, Loss: 0.054171185940504074\n",
      "Iteration 5235, Loss: 0.053989749401807785\n",
      "Iteration 5236, Loss: 0.054171230643987656\n",
      "Iteration 5237, Loss: 0.05398973822593689\n",
      "Iteration 5238, Loss: 0.054171424359083176\n",
      "Iteration 5239, Loss: 0.053989626467227936\n",
      "Iteration 5240, Loss: 0.054171543568372726\n",
      "Iteration 5241, Loss: 0.0539894700050354\n",
      "Iteration 5242, Loss: 0.05417155474424362\n",
      "Iteration 5243, Loss: 0.05398935079574585\n",
      "Iteration 5244, Loss: 0.05417162925004959\n",
      "Iteration 5245, Loss: 0.05398938059806824\n",
      "Iteration 5246, Loss: 0.054171472787857056\n",
      "Iteration 5247, Loss: 0.05398939177393913\n",
      "Iteration 5248, Loss: 0.05417143553495407\n",
      "Iteration 5249, Loss: 0.05398954451084137\n",
      "Iteration 5250, Loss: 0.05417139455676079\n",
      "Iteration 5251, Loss: 0.0539894700050354\n",
      "Iteration 5252, Loss: 0.054171543568372726\n",
      "Iteration 5253, Loss: 0.05398954078555107\n",
      "Iteration 5254, Loss: 0.054171543568372726\n",
      "Iteration 5255, Loss: 0.05398942530155182\n",
      "Iteration 5256, Loss: 0.05417162925004959\n",
      "Iteration 5257, Loss: 0.05398935079574585\n",
      "Iteration 5258, Loss: 0.054171591997146606\n",
      "Iteration 5259, Loss: 0.053989313542842865\n",
      "Iteration 5260, Loss: 0.054171621799468994\n",
      "Iteration 5261, Loss: 0.053989507257938385\n",
      "Iteration 5262, Loss: 0.05417151004076004\n",
      "Iteration 5263, Loss: 0.05398949980735779\n",
      "Iteration 5264, Loss: 0.054171543568372726\n",
      "Iteration 5265, Loss: 0.05398954078555107\n",
      "Iteration 5266, Loss: 0.05417143553495407\n",
      "Iteration 5267, Loss: 0.0539894662797451\n",
      "Iteration 5268, Loss: 0.054171472787857056\n",
      "Iteration 5269, Loss: 0.053989432752132416\n",
      "Iteration 5270, Loss: 0.054171591997146606\n",
      "Iteration 5271, Loss: 0.05398942157626152\n",
      "Iteration 5272, Loss: 0.054171591997146606\n",
      "Iteration 5273, Loss: 0.05398942157626152\n",
      "Iteration 5274, Loss: 0.05417158454656601\n",
      "Iteration 5275, Loss: 0.05398939177393913\n",
      "Iteration 5276, Loss: 0.054171543568372726\n",
      "Iteration 5277, Loss: 0.05398954451084137\n",
      "Iteration 5278, Loss: 0.054171543568372726\n",
      "Iteration 5279, Loss: 0.053989581763744354\n",
      "Iteration 5280, Loss: 0.054171428084373474\n",
      "Iteration 5281, Loss: 0.05398949980735779\n",
      "Iteration 5282, Loss: 0.054171621799468994\n",
      "Iteration 5283, Loss: 0.053989432752132416\n",
      "Iteration 5284, Loss: 0.05417151376605034\n",
      "Iteration 5285, Loss: 0.05398935079574585\n",
      "Iteration 5286, Loss: 0.054171591997146606\n",
      "Iteration 5287, Loss: 0.053989432752132416\n",
      "Iteration 5288, Loss: 0.05417170748114586\n",
      "Iteration 5289, Loss: 0.05398919805884361\n",
      "Iteration 5290, Loss: 0.05417182669043541\n",
      "Iteration 5291, Loss: 0.05398911237716675\n",
      "Iteration 5292, Loss: 0.05417190119624138\n",
      "Iteration 5293, Loss: 0.053989194333553314\n",
      "Iteration 5294, Loss: 0.05417170375585556\n",
      "Iteration 5295, Loss: 0.053989313542842865\n",
      "Iteration 5296, Loss: 0.05417139455676079\n",
      "Iteration 5297, Loss: 0.05398955196142197\n",
      "Iteration 5298, Loss: 0.054171424359083176\n",
      "Iteration 5299, Loss: 0.05398961901664734\n",
      "Iteration 5300, Loss: 0.05417133495211601\n",
      "Iteration 5301, Loss: 0.053989700973033905\n",
      "Iteration 5302, Loss: 0.054171305149793625\n",
      "Iteration 5303, Loss: 0.05398973822593689\n",
      "Iteration 5304, Loss: 0.054171498864889145\n",
      "Iteration 5305, Loss: 0.05398973450064659\n",
      "Iteration 5306, Loss: 0.05417143553495407\n",
      "Iteration 5307, Loss: 0.0539894700050354\n",
      "Iteration 5308, Loss: 0.05417178198695183\n",
      "Iteration 5309, Loss: 0.053989313542842865\n",
      "Iteration 5310, Loss: 0.05417197197675705\n",
      "Iteration 5311, Loss: 0.05398907512426376\n",
      "Iteration 5312, Loss: 0.054172053933143616\n",
      "Iteration 5313, Loss: 0.05398903414607048\n",
      "Iteration 5314, Loss: 0.054171930998563766\n",
      "Iteration 5315, Loss: 0.05398915335536003\n",
      "Iteration 5316, Loss: 0.054171741008758545\n",
      "Iteration 5317, Loss: 0.0539894625544548\n",
      "Iteration 5318, Loss: 0.05417146533727646\n",
      "Iteration 5319, Loss: 0.053989820182323456\n",
      "Iteration 5320, Loss: 0.054171305149793625\n",
      "Iteration 5321, Loss: 0.053989894688129425\n",
      "Iteration 5322, Loss: 0.05417130887508392\n",
      "Iteration 5323, Loss: 0.053989849984645844\n",
      "Iteration 5324, Loss: 0.05417155474424362\n",
      "Iteration 5325, Loss: 0.053989529609680176\n",
      "Iteration 5326, Loss: 0.05417190119624138\n",
      "Iteration 5327, Loss: 0.0539892241358757\n",
      "Iteration 5328, Loss: 0.05417206138372421\n",
      "Iteration 5329, Loss: 0.05398911237716675\n",
      "Iteration 5330, Loss: 0.0541720986366272\n",
      "Iteration 5331, Loss: 0.053988926112651825\n",
      "Iteration 5332, Loss: 0.05417218059301376\n",
      "Iteration 5333, Loss: 0.05398883670568466\n",
      "Iteration 5334, Loss: 0.05417225882411003\n",
      "Iteration 5335, Loss: 0.05398883670568466\n",
      "Iteration 5336, Loss: 0.0541720986366272\n",
      "Iteration 5337, Loss: 0.05398903414607048\n",
      "Iteration 5338, Loss: 0.054171979427337646\n",
      "Iteration 5339, Loss: 0.05398927256464958\n",
      "Iteration 5340, Loss: 0.054171741008758545\n",
      "Iteration 5341, Loss: 0.05398942157626152\n",
      "Iteration 5342, Loss: 0.05417177826166153\n",
      "Iteration 5343, Loss: 0.05398938059806824\n",
      "Iteration 5344, Loss: 0.05417182296514511\n",
      "Iteration 5345, Loss: 0.05398927256464958\n",
      "Iteration 5346, Loss: 0.05417197570204735\n",
      "Iteration 5347, Loss: 0.05398915335536003\n",
      "Iteration 5348, Loss: 0.054172009229660034\n",
      "Iteration 5349, Loss: 0.053989261388778687\n",
      "Iteration 5350, Loss: 0.05417182669043541\n",
      "Iteration 5351, Loss: 0.05398930236697197\n",
      "Iteration 5352, Loss: 0.05417178198695183\n",
      "Iteration 5353, Loss: 0.05398938059806824\n",
      "Iteration 5354, Loss: 0.05417170375585556\n",
      "Iteration 5355, Loss: 0.053989388048648834\n",
      "Iteration 5356, Loss: 0.054171741008758545\n",
      "Iteration 5357, Loss: 0.05398938059806824\n",
      "Iteration 5358, Loss: 0.05417178198695183\n",
      "Iteration 5359, Loss: 0.05398939177393913\n",
      "Iteration 5360, Loss: 0.054171860218048096\n",
      "Iteration 5361, Loss: 0.05398915335536003\n",
      "Iteration 5362, Loss: 0.05417190119624138\n",
      "Iteration 5363, Loss: 0.053989194333553314\n",
      "Iteration 5364, Loss: 0.05417194217443466\n",
      "Iteration 5365, Loss: 0.05398915335536003\n",
      "Iteration 5366, Loss: 0.054171930998563766\n",
      "Iteration 5367, Loss: 0.05398927256464958\n",
      "Iteration 5368, Loss: 0.05417178198695183\n",
      "Iteration 5369, Loss: 0.05398927256464958\n",
      "Iteration 5370, Loss: 0.05417182296514511\n",
      "Iteration 5371, Loss: 0.05398938059806824\n",
      "Iteration 5372, Loss: 0.05417177081108093\n",
      "Iteration 5373, Loss: 0.05398942530155182\n",
      "Iteration 5374, Loss: 0.05417177081108093\n",
      "Iteration 5375, Loss: 0.05398939177393913\n",
      "Iteration 5376, Loss: 0.05417182296514511\n",
      "Iteration 5377, Loss: 0.05398927256464958\n",
      "Iteration 5378, Loss: 0.05417183041572571\n",
      "Iteration 5379, Loss: 0.05398918688297272\n",
      "Iteration 5380, Loss: 0.05417202040553093\n",
      "Iteration 5381, Loss: 0.05398895591497421\n",
      "Iteration 5382, Loss: 0.05417218059301376\n",
      "Iteration 5383, Loss: 0.05398891493678093\n",
      "Iteration 5384, Loss: 0.054172106087207794\n",
      "Iteration 5385, Loss: 0.053989432752132416\n",
      "Iteration 5386, Loss: 0.054172009229660034\n",
      "Iteration 5387, Loss: 0.05398977920413017\n",
      "Iteration 5388, Loss: 0.05417138338088989\n",
      "Iteration 5389, Loss: 0.053990334272384644\n",
      "Iteration 5390, Loss: 0.05417056009173393\n",
      "Iteration 5391, Loss: 0.05399053543806076\n",
      "Iteration 5392, Loss: 0.05417070910334587\n",
      "Iteration 5393, Loss: 0.0539904460310936\n",
      "Iteration 5394, Loss: 0.05417099595069885\n",
      "Iteration 5395, Loss: 0.053989898413419724\n",
      "Iteration 5396, Loss: 0.054171860218048096\n",
      "Iteration 5397, Loss: 0.05398911237716675\n",
      "Iteration 5398, Loss: 0.054172269999980927\n",
      "Iteration 5399, Loss: 0.05398871749639511\n",
      "Iteration 5400, Loss: 0.05417269095778465\n",
      "Iteration 5401, Loss: 0.05398837476968765\n",
      "Iteration 5402, Loss: 0.054172616451978683\n",
      "Iteration 5403, Loss: 0.053988516330718994\n",
      "Iteration 5404, Loss: 0.054172419011592865\n",
      "Iteration 5405, Loss: 0.053988873958587646\n",
      "Iteration 5406, Loss: 0.054171979427337646\n",
      "Iteration 5407, Loss: 0.05398930236697197\n",
      "Iteration 5408, Loss: 0.05417155474424362\n",
      "Iteration 5409, Loss: 0.05398958921432495\n",
      "Iteration 5410, Loss: 0.05417138338088989\n",
      "Iteration 5411, Loss: 0.0539897084236145\n",
      "Iteration 5412, Loss: 0.054171424359083176\n",
      "Iteration 5413, Loss: 0.053989820182323456\n",
      "Iteration 5414, Loss: 0.05417150259017944\n",
      "Iteration 5415, Loss: 0.053989581763744354\n",
      "Iteration 5416, Loss: 0.054171741008758545\n",
      "Iteration 5417, Loss: 0.05398935079574585\n",
      "Iteration 5418, Loss: 0.054171815514564514\n",
      "Iteration 5419, Loss: 0.05398935079574585\n",
      "Iteration 5420, Loss: 0.05417189002037048\n",
      "Iteration 5421, Loss: 0.05398935079574585\n",
      "Iteration 5422, Loss: 0.05417190119624138\n",
      "Iteration 5423, Loss: 0.053988754749298096\n",
      "Iteration 5424, Loss: 0.05417179316282272\n",
      "Iteration 5425, Loss: 0.053988516330718994\n",
      "Iteration 5426, Loss: 0.05417262762784958\n",
      "Iteration 5427, Loss: 0.05398839712142944\n",
      "Iteration 5428, Loss: 0.05417269468307495\n",
      "Iteration 5429, Loss: 0.053988441824913025\n",
      "Iteration 5430, Loss: 0.05417237803339958\n",
      "Iteration 5431, Loss: 0.053988903760910034\n",
      "Iteration 5432, Loss: 0.05417212098836899\n",
      "Iteration 5433, Loss: 0.05398937687277794\n",
      "Iteration 5434, Loss: 0.054171621799468994\n",
      "Iteration 5435, Loss: 0.053989507257938385\n",
      "Iteration 5436, Loss: 0.05417130887508392\n",
      "Iteration 5437, Loss: 0.05398973822593689\n",
      "Iteration 5438, Loss: 0.05417134612798691\n",
      "Iteration 5439, Loss: 0.053989700973033905\n",
      "Iteration 5440, Loss: 0.05417143553495407\n",
      "Iteration 5441, Loss: 0.05398954078555107\n",
      "Iteration 5442, Loss: 0.05417155474424362\n",
      "Iteration 5443, Loss: 0.05398927256464958\n",
      "Iteration 5444, Loss: 0.05417194217443466\n",
      "Iteration 5445, Loss: 0.05398911237716675\n",
      "Iteration 5446, Loss: 0.05417206138372421\n",
      "Iteration 5447, Loss: 0.05398910492658615\n",
      "Iteration 5448, Loss: 0.054172173142433167\n",
      "Iteration 5449, Loss: 0.05398891493678093\n",
      "Iteration 5450, Loss: 0.05417213588953018\n",
      "Iteration 5451, Loss: 0.0539889931678772\n",
      "Iteration 5452, Loss: 0.05417202040553093\n",
      "Iteration 5453, Loss: 0.05398910865187645\n",
      "Iteration 5454, Loss: 0.05417182296514511\n",
      "Iteration 5455, Loss: 0.0539892315864563\n",
      "Iteration 5456, Loss: 0.0541718527674675\n",
      "Iteration 5457, Loss: 0.05398930609226227\n",
      "Iteration 5458, Loss: 0.05417178198695183\n",
      "Iteration 5459, Loss: 0.05398927256464958\n",
      "Iteration 5460, Loss: 0.05417170375585556\n",
      "Iteration 5461, Loss: 0.053989313542842865\n",
      "Iteration 5462, Loss: 0.05417174845933914\n",
      "Iteration 5463, Loss: 0.05398926883935928\n",
      "Iteration 5464, Loss: 0.05417179316282272\n",
      "Iteration 5465, Loss: 0.0539892241358757\n",
      "Iteration 5466, Loss: 0.05417194217443466\n",
      "Iteration 5467, Loss: 0.053989142179489136\n",
      "Iteration 5468, Loss: 0.054171979427337646\n",
      "Iteration 5469, Loss: 0.05398907512426376\n",
      "Iteration 5470, Loss: 0.05417190119624138\n",
      "Iteration 5471, Loss: 0.05398911237716675\n",
      "Iteration 5472, Loss: 0.0541718527674675\n",
      "Iteration 5473, Loss: 0.05398926883935928\n",
      "Iteration 5474, Loss: 0.05417165905237198\n",
      "Iteration 5475, Loss: 0.05398949980735779\n",
      "Iteration 5476, Loss: 0.054171618074178696\n",
      "Iteration 5477, Loss: 0.05398961901664734\n",
      "Iteration 5478, Loss: 0.054171618074178696\n",
      "Iteration 5479, Loss: 0.053989510983228683\n",
      "Iteration 5480, Loss: 0.05417166277766228\n",
      "Iteration 5481, Loss: 0.0539894625544548\n",
      "Iteration 5482, Loss: 0.054171815514564514\n",
      "Iteration 5483, Loss: 0.05398938059806824\n",
      "Iteration 5484, Loss: 0.05417190119624138\n",
      "Iteration 5485, Loss: 0.05398907512426376\n",
      "Iteration 5486, Loss: 0.0541720986366272\n",
      "Iteration 5487, Loss: 0.05398879945278168\n",
      "Iteration 5488, Loss: 0.0541723296046257\n",
      "Iteration 5489, Loss: 0.05398879572749138\n",
      "Iteration 5490, Loss: 0.054172247648239136\n",
      "Iteration 5491, Loss: 0.05398891493678093\n",
      "Iteration 5492, Loss: 0.054172009229660034\n",
      "Iteration 5493, Loss: 0.053989227861166\n",
      "Iteration 5494, Loss: 0.05417166277766228\n",
      "Iteration 5495, Loss: 0.05398957431316376\n",
      "Iteration 5496, Loss: 0.054171424359083176\n",
      "Iteration 5497, Loss: 0.05398977920413017\n",
      "Iteration 5498, Loss: 0.05417133495211601\n",
      "Iteration 5499, Loss: 0.05398993939161301\n",
      "Iteration 5500, Loss: 0.054171305149793625\n",
      "Iteration 5501, Loss: 0.05398977920413017\n",
      "Iteration 5502, Loss: 0.05417151376605034\n",
      "Iteration 5503, Loss: 0.05398954078555107\n",
      "Iteration 5504, Loss: 0.05417174845933914\n",
      "Iteration 5505, Loss: 0.0539892315864563\n",
      "Iteration 5506, Loss: 0.05417194589972496\n",
      "Iteration 5507, Loss: 0.0539889931678772\n",
      "Iteration 5508, Loss: 0.05417218059301376\n",
      "Iteration 5509, Loss: 0.05398887023329735\n",
      "Iteration 5510, Loss: 0.0541723296046257\n",
      "Iteration 5511, Loss: 0.05398883670568466\n",
      "Iteration 5512, Loss: 0.05417221039533615\n",
      "Iteration 5513, Loss: 0.05398891493678093\n",
      "Iteration 5514, Loss: 0.054172128438949585\n",
      "Iteration 5515, Loss: 0.053989194333553314\n",
      "Iteration 5516, Loss: 0.05417177081108093\n",
      "Iteration 5517, Loss: 0.05398949980735779\n",
      "Iteration 5518, Loss: 0.05417158827185631\n",
      "Iteration 5519, Loss: 0.05398961901664734\n",
      "Iteration 5520, Loss: 0.05417158454656601\n",
      "Iteration 5521, Loss: 0.05398955196142197\n",
      "Iteration 5522, Loss: 0.054171621799468994\n",
      "Iteration 5523, Loss: 0.053989388048648834\n",
      "Iteration 5524, Loss: 0.05417190119624138\n",
      "Iteration 5525, Loss: 0.05398914963006973\n",
      "Iteration 5526, Loss: 0.05417206138372421\n",
      "Iteration 5527, Loss: 0.053988873958587646\n",
      "Iteration 5528, Loss: 0.05417221784591675\n",
      "Iteration 5529, Loss: 0.05398883670568466\n",
      "Iteration 5530, Loss: 0.05417221039533615\n",
      "Iteration 5531, Loss: 0.05398891493678093\n",
      "Iteration 5532, Loss: 0.05417221039533615\n",
      "Iteration 5533, Loss: 0.0539889931678772\n",
      "Iteration 5534, Loss: 0.054171860218048096\n",
      "Iteration 5535, Loss: 0.053989227861166\n",
      "Iteration 5536, Loss: 0.05417177081108093\n",
      "Iteration 5537, Loss: 0.05398942530155182\n",
      "Iteration 5538, Loss: 0.054171621799468994\n",
      "Iteration 5539, Loss: 0.05398954078555107\n",
      "Iteration 5540, Loss: 0.054171621799468994\n",
      "Iteration 5541, Loss: 0.053989581763744354\n",
      "Iteration 5542, Loss: 0.054171621799468994\n",
      "Iteration 5543, Loss: 0.05398942530155182\n",
      "Iteration 5544, Loss: 0.05417158454656601\n",
      "Iteration 5545, Loss: 0.05398961529135704\n",
      "Iteration 5546, Loss: 0.05417150259017944\n",
      "Iteration 5547, Loss: 0.053989581763744354\n",
      "Iteration 5548, Loss: 0.05417150259017944\n",
      "Iteration 5549, Loss: 0.05398961901664734\n",
      "Iteration 5550, Loss: 0.054171621799468994\n",
      "Iteration 5551, Loss: 0.05398954078555107\n",
      "Iteration 5552, Loss: 0.05417158454656601\n",
      "Iteration 5553, Loss: 0.05398938059806824\n",
      "Iteration 5554, Loss: 0.05417174845933914\n",
      "Iteration 5555, Loss: 0.053989194333553314\n",
      "Iteration 5556, Loss: 0.05417202040553093\n",
      "Iteration 5557, Loss: 0.0539889931678772\n",
      "Iteration 5558, Loss: 0.054172247648239136\n",
      "Iteration 5559, Loss: 0.05398888140916824\n",
      "Iteration 5560, Loss: 0.05417213588953018\n",
      "Iteration 5561, Loss: 0.05398903042078018\n",
      "Iteration 5562, Loss: 0.05417182296514511\n",
      "Iteration 5563, Loss: 0.053989227861166\n",
      "Iteration 5564, Loss: 0.054171811789274216\n",
      "Iteration 5565, Loss: 0.05398934707045555\n",
      "Iteration 5566, Loss: 0.05417158454656601\n",
      "Iteration 5567, Loss: 0.053989581763744354\n",
      "Iteration 5568, Loss: 0.054171543568372726\n",
      "Iteration 5569, Loss: 0.05398965999484062\n",
      "Iteration 5570, Loss: 0.054171621799468994\n",
      "Iteration 5571, Loss: 0.0539894625544548\n",
      "Iteration 5572, Loss: 0.05417178198695183\n",
      "Iteration 5573, Loss: 0.053989194333553314\n",
      "Iteration 5574, Loss: 0.054172031581401825\n",
      "Iteration 5575, Loss: 0.053988873958587646\n",
      "Iteration 5576, Loss: 0.05417221784591675\n",
      "Iteration 5577, Loss: 0.05398879200220108\n",
      "Iteration 5578, Loss: 0.05417244881391525\n",
      "Iteration 5579, Loss: 0.05398871749639511\n",
      "Iteration 5580, Loss: 0.054172173142433167\n",
      "Iteration 5581, Loss: 0.05398891493678093\n",
      "Iteration 5582, Loss: 0.054172009229660034\n",
      "Iteration 5583, Loss: 0.05398918315768242\n",
      "Iteration 5584, Loss: 0.05417189002037048\n",
      "Iteration 5585, Loss: 0.05398930609226227\n",
      "Iteration 5586, Loss: 0.05417166277766228\n",
      "Iteration 5587, Loss: 0.053989507257938385\n",
      "Iteration 5588, Loss: 0.05417166277766228\n",
      "Iteration 5589, Loss: 0.05398949980735779\n",
      "Iteration 5590, Loss: 0.054171741008758545\n",
      "Iteration 5591, Loss: 0.05398942157626152\n",
      "Iteration 5592, Loss: 0.054171934723854065\n",
      "Iteration 5593, Loss: 0.05398915335536003\n",
      "Iteration 5594, Loss: 0.0541720986366272\n",
      "Iteration 5595, Loss: 0.05398884415626526\n",
      "Iteration 5596, Loss: 0.054172299802303314\n",
      "Iteration 5597, Loss: 0.05398871749639511\n",
      "Iteration 5598, Loss: 0.05417244881391525\n",
      "Iteration 5599, Loss: 0.053988754749298096\n",
      "Iteration 5600, Loss: 0.05417213961482048\n",
      "Iteration 5601, Loss: 0.05398891493678093\n",
      "Iteration 5602, Loss: 0.05417190119624138\n",
      "Iteration 5603, Loss: 0.05398927256464958\n",
      "Iteration 5604, Loss: 0.05417158454656601\n",
      "Iteration 5605, Loss: 0.05398965999484062\n",
      "Iteration 5606, Loss: 0.05417138338088989\n",
      "Iteration 5607, Loss: 0.05398977920413017\n",
      "Iteration 5608, Loss: 0.054171424359083176\n",
      "Iteration 5609, Loss: 0.05398961901664734\n",
      "Iteration 5610, Loss: 0.05417155474424362\n",
      "Iteration 5611, Loss: 0.05398934707045555\n",
      "Iteration 5612, Loss: 0.054171860218048096\n",
      "Iteration 5613, Loss: 0.0539892241358757\n",
      "Iteration 5614, Loss: 0.054172009229660034\n",
      "Iteration 5615, Loss: 0.05398915708065033\n",
      "Iteration 5616, Loss: 0.054171930998563766\n",
      "Iteration 5617, Loss: 0.0539892315864563\n",
      "Iteration 5618, Loss: 0.0541718527674675\n",
      "Iteration 5619, Loss: 0.05398930609226227\n",
      "Iteration 5620, Loss: 0.05417166277766228\n",
      "Iteration 5621, Loss: 0.05398949980735779\n",
      "Iteration 5622, Loss: 0.054171621799468994\n",
      "Iteration 5623, Loss: 0.053989432752132416\n",
      "Iteration 5624, Loss: 0.05417170375585556\n",
      "Iteration 5625, Loss: 0.053989388048648834\n",
      "Iteration 5626, Loss: 0.0541718527674675\n",
      "Iteration 5627, Loss: 0.05398938059806824\n",
      "Iteration 5628, Loss: 0.05417182296514511\n",
      "Iteration 5629, Loss: 0.053989261388778687\n",
      "Iteration 5630, Loss: 0.05417189747095108\n",
      "Iteration 5631, Loss: 0.0539892315864563\n",
      "Iteration 5632, Loss: 0.0541718527674675\n",
      "Iteration 5633, Loss: 0.05398949235677719\n",
      "Iteration 5634, Loss: 0.05417170375585556\n",
      "Iteration 5635, Loss: 0.05398945510387421\n",
      "Iteration 5636, Loss: 0.05417158454656601\n",
      "Iteration 5637, Loss: 0.05398954078555107\n",
      "Iteration 5638, Loss: 0.0541716143488884\n",
      "Iteration 5639, Loss: 0.05398969352245331\n",
      "Iteration 5640, Loss: 0.05417157709598541\n",
      "Iteration 5641, Loss: 0.05398961901664734\n",
      "Iteration 5642, Loss: 0.05417166277766228\n",
      "Iteration 5643, Loss: 0.05398938059806824\n",
      "Iteration 5644, Loss: 0.05417182296514511\n",
      "Iteration 5645, Loss: 0.05398914963006973\n",
      "Iteration 5646, Loss: 0.05417190492153168\n",
      "Iteration 5647, Loss: 0.05398891493678093\n",
      "Iteration 5648, Loss: 0.05417202040553093\n",
      "Iteration 5649, Loss: 0.05398915335536003\n",
      "Iteration 5650, Loss: 0.05417194217443466\n",
      "Iteration 5651, Loss: 0.053989261388778687\n",
      "Iteration 5652, Loss: 0.05417189002037048\n",
      "Iteration 5653, Loss: 0.05398945510387421\n",
      "Iteration 5654, Loss: 0.05417166277766228\n",
      "Iteration 5655, Loss: 0.05398949980735779\n",
      "Iteration 5656, Loss: 0.054171737283468246\n",
      "Iteration 5657, Loss: 0.05398949980735779\n",
      "Iteration 5658, Loss: 0.05417166277766228\n",
      "Iteration 5659, Loss: 0.0539894625544548\n",
      "Iteration 5660, Loss: 0.05417182296514511\n",
      "Iteration 5661, Loss: 0.05398934334516525\n",
      "Iteration 5662, Loss: 0.05417190119624138\n",
      "Iteration 5663, Loss: 0.05398910865187645\n",
      "Iteration 5664, Loss: 0.05417194217443466\n",
      "Iteration 5665, Loss: 0.05398903414607048\n",
      "Iteration 5666, Loss: 0.05417201668024063\n",
      "Iteration 5667, Loss: 0.05398907512426376\n",
      "Iteration 5668, Loss: 0.054171815514564514\n",
      "Iteration 5669, Loss: 0.05398930236697197\n",
      "Iteration 5670, Loss: 0.05417165905237198\n",
      "Iteration 5671, Loss: 0.0539894625544548\n",
      "Iteration 5672, Loss: 0.05417151004076004\n",
      "Iteration 5673, Loss: 0.05398958548903465\n",
      "Iteration 5674, Loss: 0.054171621799468994\n",
      "Iteration 5675, Loss: 0.05398881062865257\n",
      "Iteration 5676, Loss: 0.05417170375585556\n",
      "Iteration 5677, Loss: 0.05398872494697571\n",
      "Iteration 5678, Loss: 0.054171159863471985\n",
      "Iteration 5679, Loss: 0.05398876592516899\n",
      "Iteration 5680, Loss: 0.054171353578567505\n",
      "Iteration 5681, Loss: 0.05398841202259064\n",
      "Iteration 5682, Loss: 0.05417144298553467\n",
      "Iteration 5683, Loss: 0.05398828908801079\n",
      "Iteration 5684, Loss: 0.05417155474424362\n",
      "Iteration 5685, Loss: 0.05398821830749512\n",
      "Iteration 5686, Loss: 0.054171591997146606\n",
      "Iteration 5687, Loss: 0.053988367319107056\n",
      "Iteration 5688, Loss: 0.05417143553495407\n",
      "Iteration 5689, Loss: 0.05398837849497795\n",
      "Iteration 5690, Loss: 0.054171156138181686\n",
      "Iteration 5691, Loss: 0.05398865044116974\n",
      "Iteration 5692, Loss: 0.05417100712656975\n",
      "Iteration 5693, Loss: 0.053988732397556305\n",
      "Iteration 5694, Loss: 0.0541711263358593\n",
      "Iteration 5695, Loss: 0.053988657891750336\n",
      "Iteration 5696, Loss: 0.0541711263358593\n",
      "Iteration 5697, Loss: 0.05398860573768616\n",
      "Iteration 5698, Loss: 0.05417131632566452\n",
      "Iteration 5699, Loss: 0.05398860573768616\n",
      "Iteration 5700, Loss: 0.054171234369277954\n",
      "Iteration 5701, Loss: 0.053988657891750336\n",
      "Iteration 5702, Loss: 0.05417127162218094\n",
      "Iteration 5703, Loss: 0.05398861691355705\n",
      "Iteration 5704, Loss: 0.05417100340127945\n",
      "Iteration 5705, Loss: 0.05398876592516899\n",
      "Iteration 5706, Loss: 0.05417099595069885\n",
      "Iteration 5707, Loss: 0.053988926112651825\n",
      "Iteration 5708, Loss: 0.05417095869779587\n",
      "Iteration 5709, Loss: 0.05398900434374809\n",
      "Iteration 5710, Loss: 0.0541708879172802\n",
      "Iteration 5711, Loss: 0.05398903414607048\n",
      "Iteration 5712, Loss: 0.05417093634605408\n",
      "Iteration 5713, Loss: 0.053988613188266754\n",
      "Iteration 5714, Loss: 0.05417121201753616\n",
      "Iteration 5715, Loss: 0.05398837476968765\n",
      "Iteration 5716, Loss: 0.05417140945792198\n",
      "Iteration 5717, Loss: 0.05398821830749512\n",
      "Iteration 5718, Loss: 0.0541715994477272\n",
      "Iteration 5719, Loss: 0.05398814007639885\n",
      "Iteration 5720, Loss: 0.05417151004076004\n",
      "Iteration 5721, Loss: 0.05398845300078392\n",
      "Iteration 5722, Loss: 0.05417119711637497\n",
      "Iteration 5723, Loss: 0.05398872494697571\n",
      "Iteration 5724, Loss: 0.054171156138181686\n",
      "Iteration 5725, Loss: 0.05398876592516899\n",
      "Iteration 5726, Loss: 0.05417104810476303\n",
      "Iteration 5727, Loss: 0.05398876592516899\n",
      "Iteration 5728, Loss: 0.05417127534747124\n",
      "Iteration 5729, Loss: 0.053988538682460785\n",
      "Iteration 5730, Loss: 0.05417127534747124\n",
      "Iteration 5731, Loss: 0.05398856848478317\n",
      "Iteration 5732, Loss: 0.054171204566955566\n",
      "Iteration 5733, Loss: 0.0539884939789772\n",
      "Iteration 5734, Loss: 0.05417139455676079\n",
      "Iteration 5735, Loss: 0.05398833751678467\n",
      "Iteration 5736, Loss: 0.05417139455676079\n",
      "Iteration 5737, Loss: 0.053988419473171234\n",
      "Iteration 5738, Loss: 0.054171156138181686\n",
      "Iteration 5739, Loss: 0.05398872494697571\n",
      "Iteration 5740, Loss: 0.05417085811495781\n",
      "Iteration 5741, Loss: 0.05398888513445854\n",
      "Iteration 5742, Loss: 0.05417092889547348\n",
      "Iteration 5743, Loss: 0.053988851606845856\n",
      "Iteration 5744, Loss: 0.05417092889547348\n",
      "Iteration 5745, Loss: 0.05398900434374809\n",
      "Iteration 5746, Loss: 0.054170966148376465\n",
      "Iteration 5747, Loss: 0.053988732397556305\n",
      "Iteration 5748, Loss: 0.054171085357666016\n",
      "Iteration 5749, Loss: 0.053988538682460785\n",
      "Iteration 5750, Loss: 0.05417124554514885\n",
      "Iteration 5751, Loss: 0.053988419473171234\n",
      "Iteration 5752, Loss: 0.0541713647544384\n",
      "Iteration 5753, Loss: 0.0539882592856884\n",
      "Iteration 5754, Loss: 0.054171331226825714\n",
      "Iteration 5755, Loss: 0.05398822948336601\n",
      "Iteration 5756, Loss: 0.0541713647544384\n",
      "Iteration 5757, Loss: 0.05398845672607422\n",
      "Iteration 5758, Loss: 0.054171279072761536\n",
      "Iteration 5759, Loss: 0.0539884977042675\n",
      "Iteration 5760, Loss: 0.05417116731405258\n",
      "Iteration 5761, Loss: 0.05398864671587944\n",
      "Iteration 5762, Loss: 0.0541711263358593\n",
      "Iteration 5763, Loss: 0.05398869514465332\n",
      "Iteration 5764, Loss: 0.054171156138181686\n",
      "Iteration 5765, Loss: 0.05398869141936302\n",
      "Iteration 5766, Loss: 0.05417100712656975\n",
      "Iteration 5767, Loss: 0.05398876965045929\n",
      "Iteration 5768, Loss: 0.05417104810476303\n",
      "Iteration 5769, Loss: 0.05398868769407272\n",
      "Iteration 5770, Loss: 0.05417120084166527\n",
      "Iteration 5771, Loss: 0.053988613188266754\n",
      "Iteration 5772, Loss: 0.05417124554514885\n",
      "Iteration 5773, Loss: 0.053988419473171234\n",
      "Iteration 5774, Loss: 0.054171279072761536\n",
      "Iteration 5775, Loss: 0.05398845672607422\n",
      "Iteration 5776, Loss: 0.054171085357666016\n",
      "Iteration 5777, Loss: 0.053988538682460785\n",
      "Iteration 5778, Loss: 0.05417104810476303\n",
      "Iteration 5779, Loss: 0.05398868769407272\n",
      "Iteration 5780, Loss: 0.05417092889547348\n",
      "Iteration 5781, Loss: 0.05398888885974884\n",
      "Iteration 5782, Loss: 0.0541708879172802\n",
      "Iteration 5783, Loss: 0.05398884415626526\n",
      "Iteration 5784, Loss: 0.05417093634605408\n",
      "Iteration 5785, Loss: 0.053988806903362274\n",
      "Iteration 5786, Loss: 0.05417124181985855\n",
      "Iteration 5787, Loss: 0.05398845672607422\n",
      "Iteration 5788, Loss: 0.05417139455676079\n",
      "Iteration 5789, Loss: 0.05398837849497795\n",
      "Iteration 5790, Loss: 0.05417140573263168\n",
      "Iteration 5791, Loss: 0.05398833751678467\n",
      "Iteration 5792, Loss: 0.05417132377624512\n",
      "Iteration 5793, Loss: 0.05398833751678467\n",
      "Iteration 5794, Loss: 0.05417124554514885\n",
      "Iteration 5795, Loss: 0.0539884977042675\n",
      "Iteration 5796, Loss: 0.0541711263358593\n",
      "Iteration 5797, Loss: 0.05398868769407272\n",
      "Iteration 5798, Loss: 0.05417100712656975\n",
      "Iteration 5799, Loss: 0.053988806903362274\n",
      "Iteration 5800, Loss: 0.054170966148376465\n",
      "Iteration 5801, Loss: 0.053988777101039886\n",
      "Iteration 5802, Loss: 0.054170966148376465\n",
      "Iteration 5803, Loss: 0.053988657891750336\n",
      "Iteration 5804, Loss: 0.05417104810476303\n",
      "Iteration 5805, Loss: 0.05398865044116974\n",
      "Iteration 5806, Loss: 0.054171204566955566\n",
      "Iteration 5807, Loss: 0.053988419473171234\n",
      "Iteration 5808, Loss: 0.0541713647544384\n",
      "Iteration 5809, Loss: 0.05398821830749512\n",
      "Iteration 5810, Loss: 0.05417148396372795\n",
      "Iteration 5811, Loss: 0.05398821830749512\n",
      "Iteration 5812, Loss: 0.05417156219482422\n",
      "Iteration 5813, Loss: 0.05398830026388168\n",
      "Iteration 5814, Loss: 0.05417151749134064\n",
      "Iteration 5815, Loss: 0.05398821830749512\n",
      "Iteration 5816, Loss: 0.0541713610291481\n",
      "Iteration 5817, Loss: 0.05398845672607422\n",
      "Iteration 5818, Loss: 0.05417116731405258\n",
      "Iteration 5819, Loss: 0.05398857593536377\n",
      "Iteration 5820, Loss: 0.05417116731405258\n",
      "Iteration 5821, Loss: 0.053988538682460785\n",
      "Iteration 5822, Loss: 0.05417119711637497\n",
      "Iteration 5823, Loss: 0.05398857593536377\n",
      "Iteration 5824, Loss: 0.05417100712656975\n",
      "Iteration 5825, Loss: 0.053988657891750336\n",
      "Iteration 5826, Loss: 0.0541708879172802\n",
      "Iteration 5827, Loss: 0.053988851606845856\n",
      "Iteration 5828, Loss: 0.054170846939086914\n",
      "Iteration 5829, Loss: 0.05398896336555481\n",
      "Iteration 5830, Loss: 0.05417092144489288\n",
      "Iteration 5831, Loss: 0.053989000618457794\n",
      "Iteration 5832, Loss: 0.05417100712656975\n",
      "Iteration 5833, Loss: 0.05398876592516899\n",
      "Iteration 5834, Loss: 0.05417117103934288\n",
      "Iteration 5835, Loss: 0.05398845672607422\n",
      "Iteration 5836, Loss: 0.05417140945792198\n",
      "Iteration 5837, Loss: 0.053988292813301086\n",
      "Iteration 5838, Loss: 0.05417148768901825\n",
      "Iteration 5839, Loss: 0.05398810654878616\n",
      "Iteration 5840, Loss: 0.05417156219482422\n",
      "Iteration 5841, Loss: 0.05398821830749512\n",
      "Iteration 5842, Loss: 0.05417143553495407\n",
      "Iteration 5843, Loss: 0.053988486528396606\n",
      "Iteration 5844, Loss: 0.0541711151599884\n",
      "Iteration 5845, Loss: 0.05398876965045929\n",
      "Iteration 5846, Loss: 0.0541708841919899\n",
      "Iteration 5847, Loss: 0.05398888513445854\n",
      "Iteration 5848, Loss: 0.054170768707990646\n",
      "Iteration 5849, Loss: 0.05398900434374809\n",
      "Iteration 5850, Loss: 0.054170768707990646\n",
      "Iteration 5851, Loss: 0.05398892983794212\n",
      "Iteration 5852, Loss: 0.0541708879172802\n",
      "Iteration 5853, Loss: 0.05398891493678093\n",
      "Iteration 5854, Loss: 0.05417092889547348\n",
      "Iteration 5855, Loss: 0.05398876592516899\n",
      "Iteration 5856, Loss: 0.054171085357666016\n",
      "Iteration 5857, Loss: 0.05398864671587944\n",
      "Iteration 5858, Loss: 0.05417124554514885\n",
      "Iteration 5859, Loss: 0.053988419473171234\n",
      "Iteration 5860, Loss: 0.0541713647544384\n",
      "Iteration 5861, Loss: 0.05398830398917198\n",
      "Iteration 5862, Loss: 0.05417124554514885\n",
      "Iteration 5863, Loss: 0.05398845672607422\n",
      "Iteration 5864, Loss: 0.05417124554514885\n",
      "Iteration 5865, Loss: 0.05398845672607422\n",
      "Iteration 5866, Loss: 0.0541711263358593\n",
      "Iteration 5867, Loss: 0.05398861691355705\n",
      "Iteration 5868, Loss: 0.05417100712656975\n",
      "Iteration 5869, Loss: 0.053988806903362274\n",
      "Iteration 5870, Loss: 0.05417092889547348\n",
      "Iteration 5871, Loss: 0.05398876592516899\n",
      "Iteration 5872, Loss: 0.05417104810476303\n",
      "Iteration 5873, Loss: 0.05398864671587944\n",
      "Iteration 5874, Loss: 0.054171279072761536\n",
      "Iteration 5875, Loss: 0.053988486528396606\n",
      "Iteration 5876, Loss: 0.05417131632566452\n",
      "Iteration 5877, Loss: 0.05398901551961899\n",
      "Iteration 5878, Loss: 0.05417120084166527\n",
      "Iteration 5879, Loss: 0.05398900806903839\n",
      "Iteration 5880, Loss: 0.054171644151210785\n",
      "Iteration 5881, Loss: 0.053988974541425705\n",
      "Iteration 5882, Loss: 0.05417168140411377\n",
      "Iteration 5883, Loss: 0.05398905277252197\n",
      "Iteration 5884, Loss: 0.0541716031730175\n",
      "Iteration 5885, Loss: 0.053989093750715256\n",
      "Iteration 5886, Loss: 0.05417148396372795\n",
      "Iteration 5887, Loss: 0.05398924648761749\n",
      "Iteration 5888, Loss: 0.05417140573263168\n",
      "Iteration 5889, Loss: 0.05398929864168167\n",
      "Iteration 5890, Loss: 0.05417129397392273\n",
      "Iteration 5891, Loss: 0.05398976802825928\n",
      "Iteration 5892, Loss: 0.05417144298553467\n",
      "Iteration 5893, Loss: 0.05398961156606674\n",
      "Iteration 5894, Loss: 0.05417200177907944\n",
      "Iteration 5895, Loss: 0.05398961156606674\n",
      "Iteration 5896, Loss: 0.054172076284885406\n",
      "Iteration 5897, Loss: 0.05398956686258316\n",
      "Iteration 5898, Loss: 0.05417210981249809\n",
      "Iteration 5899, Loss: 0.05398961156606674\n",
      "Iteration 5900, Loss: 0.05417212098836899\n",
      "Iteration 5901, Loss: 0.05398952588438988\n",
      "Iteration 5902, Loss: 0.05417215824127197\n",
      "Iteration 5903, Loss: 0.053989484906196594\n",
      "Iteration 5904, Loss: 0.054172202944755554\n",
      "Iteration 5905, Loss: 0.05398940667510033\n",
      "Iteration 5906, Loss: 0.05417224019765854\n",
      "Iteration 5907, Loss: 0.05398933216929436\n",
      "Iteration 5908, Loss: 0.05417216941714287\n",
      "Iteration 5909, Loss: 0.05398932844400406\n",
      "Iteration 5910, Loss: 0.05417227745056152\n",
      "Iteration 5911, Loss: 0.05398929864168167\n",
      "Iteration 5912, Loss: 0.05417224019765854\n",
      "Iteration 5913, Loss: 0.05398945137858391\n",
      "Iteration 5914, Loss: 0.05417218804359436\n",
      "Iteration 5915, Loss: 0.05398957058787346\n",
      "Iteration 5916, Loss: 0.054172080010175705\n",
      "Iteration 5917, Loss: 0.05398964136838913\n",
      "Iteration 5918, Loss: 0.05417210981249809\n",
      "Iteration 5919, Loss: 0.0539897195994854\n",
      "Iteration 5920, Loss: 0.054171960800886154\n",
      "Iteration 5921, Loss: 0.05398983508348465\n",
      "Iteration 5922, Loss: 0.05417199060320854\n",
      "Iteration 5923, Loss: 0.05398968979716301\n",
      "Iteration 5924, Loss: 0.05417200177907944\n",
      "Iteration 5925, Loss: 0.05398957058787346\n",
      "Iteration 5926, Loss: 0.05417211353778839\n",
      "Iteration 5927, Loss: 0.053989529609680176\n",
      "Iteration 5928, Loss: 0.05417215824127197\n",
      "Iteration 5929, Loss: 0.053989484906196594\n",
      "Iteration 5930, Loss: 0.05417223274707794\n",
      "Iteration 5931, Loss: 0.05398933216929436\n",
      "Iteration 5932, Loss: 0.05417223274707794\n",
      "Iteration 5933, Loss: 0.053989410400390625\n",
      "Iteration 5934, Loss: 0.05417218804359436\n",
      "Iteration 5935, Loss: 0.053989674896001816\n",
      "Iteration 5936, Loss: 0.05417200177907944\n",
      "Iteration 5937, Loss: 0.053989678621292114\n",
      "Iteration 5938, Loss: 0.054171886295080185\n",
      "Iteration 5939, Loss: 0.053989678621292114\n",
      "Iteration 5940, Loss: 0.05417191982269287\n",
      "Iteration 5941, Loss: 0.05398968607187271\n",
      "Iteration 5942, Loss: 0.054171960800886154\n",
      "Iteration 5943, Loss: 0.05398961156606674\n",
      "Iteration 5944, Loss: 0.054172009229660034\n",
      "Iteration 5945, Loss: 0.05398941785097122\n",
      "Iteration 5946, Loss: 0.05417228862643242\n",
      "Iteration 5947, Loss: 0.05398917198181152\n",
      "Iteration 5948, Loss: 0.05417247861623764\n",
      "Iteration 5949, Loss: 0.05398905277252197\n",
      "Iteration 5950, Loss: 0.05417262762784958\n",
      "Iteration 5951, Loss: 0.053989093750715256\n",
      "Iteration 5952, Loss: 0.05417235940694809\n",
      "Iteration 5953, Loss: 0.053989291191101074\n",
      "Iteration 5954, Loss: 0.05417219549417496\n",
      "Iteration 5955, Loss: 0.053989559412002563\n",
      "Iteration 5956, Loss: 0.054172076284885406\n",
      "Iteration 5957, Loss: 0.05398961156606674\n",
      "Iteration 5958, Loss: 0.05417200177907944\n",
      "Iteration 5959, Loss: 0.053989723324775696\n",
      "Iteration 5960, Loss: 0.054171886295080185\n",
      "Iteration 5961, Loss: 0.05398964509367943\n",
      "Iteration 5962, Loss: 0.05417196452617645\n",
      "Iteration 5963, Loss: 0.05398960039019585\n",
      "Iteration 5964, Loss: 0.0541720911860466\n",
      "Iteration 5965, Loss: 0.05398933216929436\n",
      "Iteration 5966, Loss: 0.054172318428754807\n",
      "Iteration 5967, Loss: 0.053989287465810776\n",
      "Iteration 5968, Loss: 0.054172396659851074\n",
      "Iteration 5969, Loss: 0.05398924648761749\n",
      "Iteration 5970, Loss: 0.054172396659851074\n",
      "Iteration 5971, Loss: 0.053989291191101074\n",
      "Iteration 5972, Loss: 0.05417219549417496\n",
      "Iteration 5973, Loss: 0.053989559412002563\n",
      "Iteration 5974, Loss: 0.05417203903198242\n",
      "Iteration 5975, Loss: 0.05398961156606674\n",
      "Iteration 5976, Loss: 0.05417203903198242\n",
      "Iteration 5977, Loss: 0.05398961156606674\n",
      "Iteration 5978, Loss: 0.054172009229660034\n",
      "Iteration 5979, Loss: 0.05398956686258316\n",
      "Iteration 5980, Loss: 0.05417212098836899\n",
      "Iteration 5981, Loss: 0.05398937314748764\n",
      "Iteration 5982, Loss: 0.05417223274707794\n",
      "Iteration 5983, Loss: 0.05398937314748764\n",
      "Iteration 5984, Loss: 0.05417223274707794\n",
      "Iteration 5985, Loss: 0.05398940667510033\n",
      "Iteration 5986, Loss: 0.05417230725288391\n",
      "Iteration 5987, Loss: 0.053989484906196594\n",
      "Iteration 5988, Loss: 0.05417224019765854\n",
      "Iteration 5989, Loss: 0.05398940294981003\n",
      "Iteration 5990, Loss: 0.05417215824127197\n",
      "Iteration 5991, Loss: 0.053989410400390625\n",
      "Iteration 5992, Loss: 0.05417224019765854\n",
      "Iteration 5993, Loss: 0.053989559412002563\n",
      "Iteration 5994, Loss: 0.05417203903198242\n",
      "Iteration 5995, Loss: 0.053989678621292114\n",
      "Iteration 5996, Loss: 0.05417200177907944\n",
      "Iteration 5997, Loss: 0.05398964509367943\n",
      "Iteration 5998, Loss: 0.05417200177907944\n",
      "Iteration 5999, Loss: 0.053989723324775696\n",
      "Iteration 6000, Loss: 0.05417191982269287\n",
      "Iteration 6001, Loss: 0.05398957058787346\n",
      "Iteration 6002, Loss: 0.05417200177907944\n",
      "Iteration 6003, Loss: 0.05398957058787346\n",
      "Iteration 6004, Loss: 0.05417203903198242\n",
      "Iteration 6005, Loss: 0.053989604115486145\n",
      "Iteration 6006, Loss: 0.05417211353778839\n",
      "Iteration 6007, Loss: 0.05398960039019585\n",
      "Iteration 6008, Loss: 0.05417211353778839\n",
      "Iteration 6009, Loss: 0.05398960039019585\n",
      "Iteration 6010, Loss: 0.05417205020785332\n",
      "Iteration 6011, Loss: 0.05398949235677719\n",
      "Iteration 6012, Loss: 0.054172199219465256\n",
      "Iteration 6013, Loss: 0.05398937314748764\n",
      "Iteration 6014, Loss: 0.05417227745056152\n",
      "Iteration 6015, Loss: 0.053989291191101074\n",
      "Iteration 6016, Loss: 0.054172318428754807\n",
      "Iteration 6017, Loss: 0.05398933216929436\n",
      "Iteration 6018, Loss: 0.05417224019765854\n",
      "Iteration 6019, Loss: 0.05398945137858391\n",
      "Iteration 6020, Loss: 0.054172080010175705\n",
      "Iteration 6021, Loss: 0.053989529609680176\n",
      "Iteration 6022, Loss: 0.054171960800886154\n",
      "Iteration 6023, Loss: 0.0539897195994854\n",
      "Iteration 6024, Loss: 0.0541718415915966\n",
      "Iteration 6025, Loss: 0.053989723324775696\n",
      "Iteration 6026, Loss: 0.054171767085790634\n",
      "Iteration 6027, Loss: 0.05398964881896973\n",
      "Iteration 6028, Loss: 0.05417192727327347\n",
      "Iteration 6029, Loss: 0.05398964136838913\n",
      "Iteration 6030, Loss: 0.05417212098836899\n",
      "Iteration 6031, Loss: 0.05398944765329361\n",
      "Iteration 6032, Loss: 0.0541720911860466\n",
      "Iteration 6033, Loss: 0.05398937314748764\n",
      "Iteration 6034, Loss: 0.054172318428754807\n",
      "Iteration 6035, Loss: 0.053989212960004807\n",
      "Iteration 6036, Loss: 0.05417228862643242\n",
      "Iteration 6037, Loss: 0.05398910492658615\n",
      "Iteration 6038, Loss: 0.05417243391275406\n",
      "Iteration 6039, Loss: 0.05398925393819809\n",
      "Iteration 6040, Loss: 0.05417242646217346\n",
      "Iteration 6041, Loss: 0.053989361971616745\n",
      "Iteration 6042, Loss: 0.05417230352759361\n",
      "Iteration 6043, Loss: 0.053989559412002563\n",
      "Iteration 6044, Loss: 0.05417189002037048\n",
      "Iteration 6045, Loss: 0.05398976057767868\n",
      "Iteration 6046, Loss: 0.05417191982269287\n",
      "Iteration 6047, Loss: 0.05398983880877495\n",
      "Iteration 6048, Loss: 0.05417180806398392\n",
      "Iteration 6049, Loss: 0.05398976057767868\n",
      "Iteration 6050, Loss: 0.0541718527674675\n",
      "Iteration 6051, Loss: 0.05398964509367943\n",
      "Iteration 6052, Loss: 0.05417203903198242\n",
      "Iteration 6053, Loss: 0.05398957058787346\n",
      "Iteration 6054, Loss: 0.05417197197675705\n",
      "Iteration 6055, Loss: 0.05398956686258316\n",
      "Iteration 6056, Loss: 0.054172009229660034\n",
      "Iteration 6057, Loss: 0.05398949235677719\n",
      "Iteration 6058, Loss: 0.054172080010175705\n",
      "Iteration 6059, Loss: 0.053989529609680176\n",
      "Iteration 6060, Loss: 0.05417203903198242\n",
      "Iteration 6061, Loss: 0.053989678621292114\n",
      "Iteration 6062, Loss: 0.05417200177907944\n",
      "Iteration 6063, Loss: 0.05398957058787346\n",
      "Iteration 6064, Loss: 0.05417200177907944\n",
      "Iteration 6065, Loss: 0.05398961156606674\n",
      "Iteration 6066, Loss: 0.05417196452617645\n",
      "Iteration 6067, Loss: 0.05398957058787346\n",
      "Iteration 6068, Loss: 0.054172009229660034\n",
      "Iteration 6069, Loss: 0.05398956686258316\n",
      "Iteration 6070, Loss: 0.05417212098836899\n",
      "Iteration 6071, Loss: 0.05398940667510033\n",
      "Iteration 6072, Loss: 0.05417212098836899\n",
      "Iteration 6073, Loss: 0.05398940667510033\n",
      "Iteration 6074, Loss: 0.05417215824127197\n",
      "Iteration 6075, Loss: 0.053989484906196594\n",
      "Iteration 6076, Loss: 0.05417216569185257\n",
      "Iteration 6077, Loss: 0.05398940667510033\n",
      "Iteration 6078, Loss: 0.05417215824127197\n",
      "Iteration 6079, Loss: 0.053989410400390625\n",
      "Iteration 6080, Loss: 0.05417197197675705\n",
      "Iteration 6081, Loss: 0.053989559412002563\n",
      "Iteration 6082, Loss: 0.05417203903198242\n",
      "Iteration 6083, Loss: 0.05398961156606674\n",
      "Iteration 6084, Loss: 0.05417203903198242\n",
      "Iteration 6085, Loss: 0.05398961156606674\n",
      "Iteration 6086, Loss: 0.05417203903198242\n",
      "Iteration 6087, Loss: 0.05398957058787346\n",
      "Iteration 6088, Loss: 0.05417191982269287\n",
      "Iteration 6089, Loss: 0.05398949980735779\n",
      "Iteration 6090, Loss: 0.054171960800886154\n",
      "Iteration 6091, Loss: 0.05398968607187271\n",
      "Iteration 6092, Loss: 0.054171882569789886\n",
      "Iteration 6093, Loss: 0.05398976057767868\n",
      "Iteration 6094, Loss: 0.05417191982269287\n",
      "Iteration 6095, Loss: 0.05398991331458092\n",
      "Iteration 6096, Loss: 0.0541718415915966\n",
      "Iteration 6097, Loss: 0.053989917039871216\n",
      "Iteration 6098, Loss: 0.054171767085790634\n",
      "Iteration 6099, Loss: 0.05398987978696823\n",
      "Iteration 6100, Loss: 0.054171737283468246\n",
      "Iteration 6101, Loss: 0.05398976057767868\n",
      "Iteration 6102, Loss: 0.05417196452617645\n",
      "Iteration 6103, Loss: 0.053989674896001816\n",
      "Iteration 6104, Loss: 0.0541720911860466\n",
      "Iteration 6105, Loss: 0.05398940294981003\n",
      "Iteration 6106, Loss: 0.05417228862643242\n",
      "Iteration 6107, Loss: 0.05398917198181152\n",
      "Iteration 6108, Loss: 0.054172396659851074\n",
      "Iteration 6109, Loss: 0.05398917943239212\n",
      "Iteration 6110, Loss: 0.05417215824127197\n",
      "Iteration 6111, Loss: 0.05398952215909958\n",
      "Iteration 6112, Loss: 0.05417200177907944\n",
      "Iteration 6113, Loss: 0.05398964509367943\n",
      "Iteration 6114, Loss: 0.054171811789274216\n",
      "Iteration 6115, Loss: 0.053989797830581665\n",
      "Iteration 6116, Loss: 0.05417177081108093\n",
      "Iteration 6117, Loss: 0.05398991331458092\n",
      "Iteration 6118, Loss: 0.05417177081108093\n",
      "Iteration 6119, Loss: 0.053989917039871216\n",
      "Iteration 6120, Loss: 0.054171882569789886\n",
      "Iteration 6121, Loss: 0.05398987978696823\n",
      "Iteration 6122, Loss: 0.054171811789274216\n",
      "Iteration 6123, Loss: 0.05398964136838913\n",
      "Iteration 6124, Loss: 0.05417212098836899\n",
      "Iteration 6125, Loss: 0.05398933216929436\n",
      "Iteration 6126, Loss: 0.0541720911860466\n",
      "Iteration 6127, Loss: 0.05398933216929436\n",
      "Iteration 6128, Loss: 0.05417216569185257\n",
      "Iteration 6129, Loss: 0.05398944765329361\n",
      "Iteration 6130, Loss: 0.05417197197675705\n",
      "Iteration 6131, Loss: 0.05398961156606674\n",
      "Iteration 6132, Loss: 0.05417191982269287\n",
      "Iteration 6133, Loss: 0.05398976057767868\n",
      "Iteration 6134, Loss: 0.0541718527674675\n",
      "Iteration 6135, Loss: 0.0539897195994854\n",
      "Iteration 6136, Loss: 0.054171930998563766\n",
      "Iteration 6137, Loss: 0.053989604115486145\n",
      "Iteration 6138, Loss: 0.05417197197675705\n",
      "Iteration 6139, Loss: 0.053989484906196594\n",
      "Iteration 6140, Loss: 0.054172202944755554\n",
      "Iteration 6141, Loss: 0.05398936569690704\n",
      "Iteration 6142, Loss: 0.05417216569185257\n",
      "Iteration 6143, Loss: 0.053989410400390625\n",
      "Iteration 6144, Loss: 0.05417224019765854\n",
      "Iteration 6145, Loss: 0.053989484906196594\n",
      "Iteration 6146, Loss: 0.054172199219465256\n",
      "Iteration 6147, Loss: 0.053989484906196594\n",
      "Iteration 6148, Loss: 0.05417212098836899\n",
      "Iteration 6149, Loss: 0.05398949235677719\n",
      "Iteration 6150, Loss: 0.05417200177907944\n",
      "Iteration 6151, Loss: 0.0539897195994854\n",
      "Iteration 6152, Loss: 0.05417191982269287\n",
      "Iteration 6153, Loss: 0.05398987978696823\n",
      "Iteration 6154, Loss: 0.0541718527674675\n",
      "Iteration 6155, Loss: 0.05398964881896973\n",
      "Iteration 6156, Loss: 0.054171930998563766\n",
      "Iteration 6157, Loss: 0.05398964136838913\n",
      "Iteration 6158, Loss: 0.05417204648256302\n",
      "Iteration 6159, Loss: 0.05398944765329361\n",
      "Iteration 6160, Loss: 0.05417215824127197\n",
      "Iteration 6161, Loss: 0.05398942157626152\n",
      "Iteration 6162, Loss: 0.054172154515981674\n",
      "Iteration 6163, Loss: 0.05398949235677719\n",
      "Iteration 6164, Loss: 0.054172080010175705\n",
      "Iteration 6165, Loss: 0.05398949235677719\n",
      "Iteration 6166, Loss: 0.05417203903198242\n",
      "Iteration 6167, Loss: 0.053989604115486145\n",
      "Iteration 6168, Loss: 0.05417191982269287\n",
      "Iteration 6169, Loss: 0.05398968979716301\n",
      "Iteration 6170, Loss: 0.054171882569789886\n",
      "Iteration 6171, Loss: 0.05398976057767868\n",
      "Iteration 6172, Loss: 0.05417199432849884\n",
      "Iteration 6173, Loss: 0.05398964881896973\n",
      "Iteration 6174, Loss: 0.054171960800886154\n",
      "Iteration 6175, Loss: 0.05398968607187271\n",
      "Iteration 6176, Loss: 0.05417199432849884\n",
      "Iteration 6177, Loss: 0.05398968607187271\n",
      "Iteration 6178, Loss: 0.05417200177907944\n",
      "Iteration 6179, Loss: 0.05398953706026077\n",
      "Iteration 6180, Loss: 0.054171930998563766\n",
      "Iteration 6181, Loss: 0.05398956686258316\n",
      "Iteration 6182, Loss: 0.05417205020785332\n",
      "Iteration 6183, Loss: 0.053989484906196594\n",
      "Iteration 6184, Loss: 0.05417205020785332\n",
      "Iteration 6185, Loss: 0.05398945137858391\n",
      "Iteration 6186, Loss: 0.054172128438949585\n",
      "Iteration 6187, Loss: 0.05398952215909958\n",
      "Iteration 6188, Loss: 0.05417215824127197\n",
      "Iteration 6189, Loss: 0.05398944765329361\n",
      "Iteration 6190, Loss: 0.054172154515981674\n",
      "Iteration 6191, Loss: 0.05398957058787346\n",
      "Iteration 6192, Loss: 0.05417192727327347\n",
      "Iteration 6193, Loss: 0.05398961156606674\n",
      "Iteration 6194, Loss: 0.05417191982269287\n",
      "Iteration 6195, Loss: 0.053989797830581665\n",
      "Iteration 6196, Loss: 0.054171886295080185\n",
      "Iteration 6197, Loss: 0.05398964881896973\n",
      "Iteration 6198, Loss: 0.054171886295080185\n",
      "Iteration 6199, Loss: 0.05398964881896973\n",
      "Iteration 6200, Loss: 0.05417199432849884\n",
      "Iteration 6201, Loss: 0.053989678621292114\n",
      "Iteration 6202, Loss: 0.05417196452617645\n",
      "Iteration 6203, Loss: 0.05398952588438988\n",
      "Iteration 6204, Loss: 0.054172009229660034\n",
      "Iteration 6205, Loss: 0.053989410400390625\n",
      "Iteration 6206, Loss: 0.05417224019765854\n",
      "Iteration 6207, Loss: 0.053989410400390625\n",
      "Iteration 6208, Loss: 0.054172199219465256\n",
      "Iteration 6209, Loss: 0.05398937314748764\n",
      "Iteration 6210, Loss: 0.05417213588953018\n",
      "Iteration 6211, Loss: 0.05398933216929436\n",
      "Iteration 6212, Loss: 0.05417227745056152\n",
      "Iteration 6213, Loss: 0.05398933216929436\n",
      "Iteration 6214, Loss: 0.05417212098836899\n",
      "Iteration 6215, Loss: 0.053989484906196594\n",
      "Iteration 6216, Loss: 0.05417200177907944\n",
      "Iteration 6217, Loss: 0.05398976057767868\n",
      "Iteration 6218, Loss: 0.0541718415915966\n",
      "Iteration 6219, Loss: 0.05398984253406525\n",
      "Iteration 6220, Loss: 0.05417172238230705\n",
      "Iteration 6221, Loss: 0.05398980900645256\n",
      "Iteration 6222, Loss: 0.05417180806398392\n",
      "Iteration 6223, Loss: 0.05398964881896973\n",
      "Iteration 6224, Loss: 0.0541718527674675\n",
      "Iteration 6225, Loss: 0.053989529609680176\n",
      "Iteration 6226, Loss: 0.0541720911860466\n",
      "Iteration 6227, Loss: 0.05398937314748764\n",
      "Iteration 6228, Loss: 0.0541723296046257\n",
      "Iteration 6229, Loss: 0.05398917198181152\n",
      "Iteration 6230, Loss: 0.05417247861623764\n",
      "Iteration 6231, Loss: 0.05398913472890854\n",
      "Iteration 6232, Loss: 0.05417255312204361\n",
      "Iteration 6233, Loss: 0.053989093750715256\n",
      "Iteration 6234, Loss: 0.054172396659851074\n",
      "Iteration 6235, Loss: 0.053989291191101074\n",
      "Iteration 6236, Loss: 0.054172154515981674\n",
      "Iteration 6237, Loss: 0.05398957058787346\n",
      "Iteration 6238, Loss: 0.05417192727327347\n",
      "Iteration 6239, Loss: 0.05398973077535629\n",
      "Iteration 6240, Loss: 0.05417173355817795\n",
      "Iteration 6241, Loss: 0.053989797830581665\n",
      "Iteration 6242, Loss: 0.05417192727327347\n",
      "Iteration 6243, Loss: 0.05398960039019585\n",
      "Iteration 6244, Loss: 0.0541720911860466\n",
      "Iteration 6245, Loss: 0.05398937314748764\n",
      "Iteration 6246, Loss: 0.054172247648239136\n",
      "Iteration 6247, Loss: 0.05398917198181152\n",
      "Iteration 6248, Loss: 0.05417243763804436\n",
      "Iteration 6249, Loss: 0.05398925393819809\n",
      "Iteration 6250, Loss: 0.05417235940694809\n",
      "Iteration 6251, Loss: 0.0539892241358757\n",
      "Iteration 6252, Loss: 0.05417203903198242\n",
      "Iteration 6253, Loss: 0.05398964136838913\n",
      "Iteration 6254, Loss: 0.054171882569789886\n",
      "Iteration 6255, Loss: 0.05398987978696823\n",
      "Iteration 6256, Loss: 0.05417180061340332\n",
      "Iteration 6257, Loss: 0.053989917039871216\n",
      "Iteration 6258, Loss: 0.05417180061340332\n",
      "Iteration 6259, Loss: 0.053989849984645844\n",
      "Iteration 6260, Loss: 0.05417180061340332\n",
      "Iteration 6261, Loss: 0.05398984253406525\n",
      "Iteration 6262, Loss: 0.05417189002037048\n",
      "Iteration 6263, Loss: 0.05398956686258316\n",
      "Iteration 6264, Loss: 0.054172199219465256\n",
      "Iteration 6265, Loss: 0.053989410400390625\n",
      "Iteration 6266, Loss: 0.05417221039533615\n",
      "Iteration 6267, Loss: 0.05398918315768242\n",
      "Iteration 6268, Loss: 0.05417221039533615\n",
      "Iteration 6269, Loss: 0.053989291191101074\n",
      "Iteration 6270, Loss: 0.054172202944755554\n",
      "Iteration 6271, Loss: 0.053989559412002563\n",
      "Iteration 6272, Loss: 0.05417212098836899\n",
      "Iteration 6273, Loss: 0.05398960039019585\n",
      "Iteration 6274, Loss: 0.05417191982269287\n",
      "Iteration 6275, Loss: 0.05398968607187271\n",
      "Iteration 6276, Loss: 0.05417191982269287\n",
      "Iteration 6277, Loss: 0.05398961156606674\n",
      "Iteration 6278, Loss: 0.05417191982269287\n",
      "Iteration 6279, Loss: 0.05398968979716301\n",
      "Iteration 6280, Loss: 0.054171811789274216\n",
      "Iteration 6281, Loss: 0.05398957058787346\n",
      "Iteration 6282, Loss: 0.054171960800886154\n",
      "Iteration 6283, Loss: 0.053989529609680176\n",
      "Iteration 6284, Loss: 0.054171960800886154\n",
      "Iteration 6285, Loss: 0.05398973077535629\n",
      "Iteration 6286, Loss: 0.054171763360500336\n",
      "Iteration 6287, Loss: 0.05398980900645256\n",
      "Iteration 6288, Loss: 0.05417179316282272\n",
      "Iteration 6289, Loss: 0.053989917039871216\n",
      "Iteration 6290, Loss: 0.05417175218462944\n",
      "Iteration 6291, Loss: 0.053989849984645844\n",
      "Iteration 6292, Loss: 0.05417175590991974\n",
      "Iteration 6293, Loss: 0.05398999899625778\n",
      "Iteration 6294, Loss: 0.05417165160179138\n",
      "Iteration 6295, Loss: 0.05398987978696823\n",
      "Iteration 6296, Loss: 0.054171811789274216\n",
      "Iteration 6297, Loss: 0.05398964509367943\n",
      "Iteration 6298, Loss: 0.054171960800886154\n",
      "Iteration 6299, Loss: 0.05398964136838913\n",
      "Iteration 6300, Loss: 0.054172080010175705\n",
      "Iteration 6301, Loss: 0.053989410400390625\n",
      "Iteration 6302, Loss: 0.05417210981249809\n",
      "Iteration 6303, Loss: 0.05398957058787346\n",
      "Iteration 6304, Loss: 0.05417177081108093\n",
      "Iteration 6305, Loss: 0.05398976802825928\n",
      "Iteration 6306, Loss: 0.05417172238230705\n",
      "Iteration 6307, Loss: 0.05398988723754883\n",
      "Iteration 6308, Loss: 0.05417148396372795\n",
      "Iteration 6309, Loss: 0.05399007722735405\n",
      "Iteration 6310, Loss: 0.05417140573263168\n",
      "Iteration 6311, Loss: 0.05399015545845032\n",
      "Iteration 6312, Loss: 0.05417140573263168\n",
      "Iteration 6313, Loss: 0.05399000644683838\n",
      "Iteration 6314, Loss: 0.05417144298553467\n",
      "Iteration 6315, Loss: 0.05399011820554733\n",
      "Iteration 6316, Loss: 0.05417165160179138\n",
      "Iteration 6317, Loss: 0.05398983880877495\n",
      "Iteration 6318, Loss: 0.054171886295080185\n",
      "Iteration 6319, Loss: 0.05398961156606674\n",
      "Iteration 6320, Loss: 0.054172076284885406\n",
      "Iteration 6321, Loss: 0.05398945510387421\n",
      "Iteration 6322, Loss: 0.05417210981249809\n",
      "Iteration 6323, Loss: 0.0539897195994854\n",
      "Iteration 6324, Loss: 0.054171692579984665\n",
      "Iteration 6325, Loss: 0.05398987978696823\n",
      "Iteration 6326, Loss: 0.054171524941921234\n",
      "Iteration 6327, Loss: 0.05398992821574211\n",
      "Iteration 6328, Loss: 0.054171450436115265\n",
      "Iteration 6329, Loss: 0.05399007722735405\n",
      "Iteration 6330, Loss: 0.054171450436115265\n",
      "Iteration 6331, Loss: 0.05399000272154808\n",
      "Iteration 6332, Loss: 0.05417145416140556\n",
      "Iteration 6333, Loss: 0.05399000272154808\n",
      "Iteration 6334, Loss: 0.0541716143488884\n",
      "Iteration 6335, Loss: 0.05398976802825928\n",
      "Iteration 6336, Loss: 0.054171811789274216\n",
      "Iteration 6337, Loss: 0.05398964509367943\n",
      "Iteration 6338, Loss: 0.054171930998563766\n",
      "Iteration 6339, Loss: 0.05398953706026077\n",
      "Iteration 6340, Loss: 0.05417203903198242\n",
      "Iteration 6341, Loss: 0.053989559412002563\n",
      "Iteration 6342, Loss: 0.05417192727327347\n",
      "Iteration 6343, Loss: 0.05398964136838913\n",
      "Iteration 6344, Loss: 0.05417199432849884\n",
      "Iteration 6345, Loss: 0.053989723324775696\n",
      "Iteration 6346, Loss: 0.0541718415915966\n",
      "Iteration 6347, Loss: 0.05398976057767868\n",
      "Iteration 6348, Loss: 0.05417168140411377\n",
      "Iteration 6349, Loss: 0.05398984253406525\n",
      "Iteration 6350, Loss: 0.05417164787650108\n",
      "Iteration 6351, Loss: 0.0539899580180645\n",
      "Iteration 6352, Loss: 0.05417172610759735\n",
      "Iteration 6353, Loss: 0.05398968979716301\n",
      "Iteration 6354, Loss: 0.0541718415915966\n",
      "Iteration 6355, Loss: 0.05398968979716301\n",
      "Iteration 6356, Loss: 0.05417180061340332\n",
      "Iteration 6357, Loss: 0.05398973077535629\n",
      "Iteration 6358, Loss: 0.0541716068983078\n",
      "Iteration 6359, Loss: 0.05398980900645256\n",
      "Iteration 6360, Loss: 0.05417171120643616\n",
      "Iteration 6361, Loss: 0.05399003252387047\n",
      "Iteration 6362, Loss: 0.05417164787650108\n",
      "Iteration 6363, Loss: 0.05398980528116226\n",
      "Iteration 6364, Loss: 0.05417180061340332\n",
      "Iteration 6365, Loss: 0.05398968979716301\n",
      "Iteration 6366, Loss: 0.054171882569789886\n",
      "Iteration 6367, Loss: 0.05398976057767868\n",
      "Iteration 6368, Loss: 0.0541718453168869\n",
      "Iteration 6369, Loss: 0.053989723324775696\n",
      "Iteration 6370, Loss: 0.0541718415915966\n",
      "Iteration 6371, Loss: 0.05398973077535629\n",
      "Iteration 6372, Loss: 0.05417179316282272\n",
      "Iteration 6373, Loss: 0.053989917039871216\n",
      "Iteration 6374, Loss: 0.05417179316282272\n",
      "Iteration 6375, Loss: 0.05398988351225853\n",
      "Iteration 6376, Loss: 0.0541716068983078\n",
      "Iteration 6377, Loss: 0.05398988723754883\n",
      "Iteration 6378, Loss: 0.05417168140411377\n",
      "Iteration 6379, Loss: 0.05398999899625778\n",
      "Iteration 6380, Loss: 0.054171495139598846\n",
      "Iteration 6381, Loss: 0.0539899617433548\n",
      "Iteration 6382, Loss: 0.0541716143488884\n",
      "Iteration 6383, Loss: 0.053989849984645844\n",
      "Iteration 6384, Loss: 0.054171767085790634\n",
      "Iteration 6385, Loss: 0.05398968979716301\n",
      "Iteration 6386, Loss: 0.054171960800886154\n",
      "Iteration 6387, Loss: 0.053989529609680176\n",
      "Iteration 6388, Loss: 0.05417212098836899\n",
      "Iteration 6389, Loss: 0.05398933216929436\n",
      "Iteration 6390, Loss: 0.054172199219465256\n",
      "Iteration 6391, Loss: 0.05398937314748764\n",
      "Iteration 6392, Loss: 0.05417218804359436\n",
      "Iteration 6393, Loss: 0.05398937314748764\n",
      "Iteration 6394, Loss: 0.054172150790691376\n",
      "Iteration 6395, Loss: 0.05398964136838913\n",
      "Iteration 6396, Loss: 0.0541718415915966\n",
      "Iteration 6397, Loss: 0.053989797830581665\n",
      "Iteration 6398, Loss: 0.054171644151210785\n",
      "Iteration 6399, Loss: 0.05398988723754883\n",
      "Iteration 6400, Loss: 0.0541716031730175\n",
      "Iteration 6401, Loss: 0.05398999899625778\n",
      "Iteration 6402, Loss: 0.054171495139598846\n",
      "Iteration 6403, Loss: 0.05399007722735405\n",
      "Iteration 6404, Loss: 0.054171621799468994\n",
      "Iteration 6405, Loss: 0.0539897195994854\n",
      "Iteration 6406, Loss: 0.054172128438949585\n",
      "Iteration 6407, Loss: 0.05398918315768242\n",
      "Iteration 6408, Loss: 0.054172441363334656\n",
      "Iteration 6409, Loss: 0.053989019244909286\n",
      "Iteration 6410, Loss: 0.05417244881391525\n",
      "Iteration 6411, Loss: 0.05398909002542496\n",
      "Iteration 6412, Loss: 0.05417243763804436\n",
      "Iteration 6413, Loss: 0.05398913472890854\n",
      "Iteration 6414, Loss: 0.05417227745056152\n",
      "Iteration 6415, Loss: 0.05398933216929436\n",
      "Iteration 6416, Loss: 0.05417200177907944\n",
      "Iteration 6417, Loss: 0.05398961156606674\n",
      "Iteration 6418, Loss: 0.05417191609740257\n",
      "Iteration 6419, Loss: 0.05398987978696823\n",
      "Iteration 6420, Loss: 0.05417172238230705\n",
      "Iteration 6421, Loss: 0.053989917039871216\n",
      "Iteration 6422, Loss: 0.054171573370695114\n",
      "Iteration 6423, Loss: 0.05398984253406525\n",
      "Iteration 6424, Loss: 0.0541718527674675\n",
      "Iteration 6425, Loss: 0.05398968979716301\n",
      "Iteration 6426, Loss: 0.05417191982269287\n",
      "Iteration 6427, Loss: 0.05398964881896973\n",
      "Iteration 6428, Loss: 0.054171882569789886\n",
      "Iteration 6429, Loss: 0.05398964136838913\n",
      "Iteration 6430, Loss: 0.05417203903198242\n",
      "Iteration 6431, Loss: 0.05398956686258316\n",
      "Iteration 6432, Loss: 0.05417197197675705\n",
      "Iteration 6433, Loss: 0.053989559412002563\n",
      "Iteration 6434, Loss: 0.05417203530669212\n",
      "Iteration 6435, Loss: 0.05398961156606674\n",
      "Iteration 6436, Loss: 0.054171930998563766\n",
      "Iteration 6437, Loss: 0.05398964136838913\n",
      "Iteration 6438, Loss: 0.05417197197675705\n",
      "Iteration 6439, Loss: 0.05398964136838913\n",
      "Iteration 6440, Loss: 0.05417189002037048\n",
      "Iteration 6441, Loss: 0.05398942157626152\n",
      "Iteration 6442, Loss: 0.054172080010175705\n",
      "Iteration 6443, Loss: 0.05398945510387421\n",
      "Iteration 6444, Loss: 0.054171960800886154\n",
      "Iteration 6445, Loss: 0.053989529609680176\n",
      "Iteration 6446, Loss: 0.05417200177907944\n",
      "Iteration 6447, Loss: 0.053989529609680176\n",
      "Iteration 6448, Loss: 0.05417189002037048\n",
      "Iteration 6449, Loss: 0.05398964136838913\n",
      "Iteration 6450, Loss: 0.05417210981249809\n",
      "Iteration 6451, Loss: 0.053989529609680176\n",
      "Iteration 6452, Loss: 0.054172080010175705\n",
      "Iteration 6453, Loss: 0.05398945137858391\n",
      "Iteration 6454, Loss: 0.054172199219465256\n",
      "Iteration 6455, Loss: 0.05398945137858391\n",
      "Iteration 6456, Loss: 0.05417212098836899\n",
      "Iteration 6457, Loss: 0.05398956686258316\n",
      "Iteration 6458, Loss: 0.05417197197675705\n",
      "Iteration 6459, Loss: 0.05398957058787346\n",
      "Iteration 6460, Loss: 0.054171930998563766\n",
      "Iteration 6461, Loss: 0.05398956686258316\n",
      "Iteration 6462, Loss: 0.054172076284885406\n",
      "Iteration 6463, Loss: 0.05398941785097122\n",
      "Iteration 6464, Loss: 0.05417199432849884\n",
      "Iteration 6465, Loss: 0.05398957058787346\n",
      "Iteration 6466, Loss: 0.05417191982269287\n",
      "Iteration 6467, Loss: 0.05398976057767868\n",
      "Iteration 6468, Loss: 0.05417180806398392\n",
      "Iteration 6469, Loss: 0.05398973077535629\n",
      "Iteration 6470, Loss: 0.05417177081108093\n",
      "Iteration 6471, Loss: 0.05398964881896973\n",
      "Iteration 6472, Loss: 0.0541718527674675\n",
      "Iteration 6473, Loss: 0.053989678621292114\n",
      "Iteration 6474, Loss: 0.054171960800886154\n",
      "Iteration 6475, Loss: 0.05398964136838913\n",
      "Iteration 6476, Loss: 0.054172080010175705\n",
      "Iteration 6477, Loss: 0.05398949235677719\n",
      "Iteration 6478, Loss: 0.05417215824127197\n",
      "Iteration 6479, Loss: 0.05398944765329361\n",
      "Iteration 6480, Loss: 0.054172080010175705\n",
      "Iteration 6481, Loss: 0.05398961156606674\n",
      "Iteration 6482, Loss: 0.05417191982269287\n",
      "Iteration 6483, Loss: 0.05398976057767868\n",
      "Iteration 6484, Loss: 0.05417180061340332\n",
      "Iteration 6485, Loss: 0.05398976057767868\n",
      "Iteration 6486, Loss: 0.05417180061340332\n",
      "Iteration 6487, Loss: 0.05398980528116226\n",
      "Iteration 6488, Loss: 0.054171763360500336\n",
      "Iteration 6489, Loss: 0.05398983880877495\n",
      "Iteration 6490, Loss: 0.05417173355817795\n",
      "Iteration 6491, Loss: 0.053989872336387634\n",
      "Iteration 6492, Loss: 0.0541718527674675\n",
      "Iteration 6493, Loss: 0.05398976057767868\n",
      "Iteration 6494, Loss: 0.0541720911860466\n",
      "Iteration 6495, Loss: 0.05398944020271301\n",
      "Iteration 6496, Loss: 0.05417221039533615\n",
      "Iteration 6497, Loss: 0.053989291191101074\n",
      "Iteration 6498, Loss: 0.05417227745056152\n",
      "Iteration 6499, Loss: 0.05398918315768242\n",
      "Iteration 6500, Loss: 0.054172269999980927\n",
      "Iteration 6501, Loss: 0.053989529609680176\n",
      "Iteration 6502, Loss: 0.05417206883430481\n",
      "Iteration 6503, Loss: 0.0539897195994854\n",
      "Iteration 6504, Loss: 0.05417180061340332\n",
      "Iteration 6505, Loss: 0.05398976057767868\n",
      "Iteration 6506, Loss: 0.05417194962501526\n",
      "Iteration 6507, Loss: 0.05398976802825928\n",
      "Iteration 6508, Loss: 0.0541718453168869\n",
      "Iteration 6509, Loss: 0.053989797830581665\n",
      "Iteration 6510, Loss: 0.054171960800886154\n",
      "Iteration 6511, Loss: 0.05398961156606674\n",
      "Iteration 6512, Loss: 0.05417203903198242\n",
      "Iteration 6513, Loss: 0.05398957058787346\n",
      "Iteration 6514, Loss: 0.05417200177907944\n",
      "Iteration 6515, Loss: 0.05398960039019585\n",
      "Iteration 6516, Loss: 0.054172080010175705\n",
      "Iteration 6517, Loss: 0.05398964136838913\n",
      "Iteration 6518, Loss: 0.054172080010175705\n",
      "Iteration 6519, Loss: 0.05398952588438988\n",
      "Iteration 6520, Loss: 0.05417212098836899\n",
      "Iteration 6521, Loss: 0.053989484906196594\n",
      "Iteration 6522, Loss: 0.054172154515981674\n",
      "Iteration 6523, Loss: 0.053989559412002563\n",
      "Iteration 6524, Loss: 0.05417204648256302\n",
      "Iteration 6525, Loss: 0.05398957058787346\n",
      "Iteration 6526, Loss: 0.0541720911860466\n",
      "Iteration 6527, Loss: 0.05398941785097122\n",
      "Iteration 6528, Loss: 0.05417221039533615\n",
      "Iteration 6529, Loss: 0.05398933216929436\n",
      "Iteration 6530, Loss: 0.05417235940694809\n",
      "Iteration 6531, Loss: 0.053989242762327194\n",
      "Iteration 6532, Loss: 0.05417247861623764\n",
      "Iteration 6533, Loss: 0.0539889857172966\n",
      "Iteration 6534, Loss: 0.05417247861623764\n",
      "Iteration 6535, Loss: 0.05398917198181152\n",
      "Iteration 6536, Loss: 0.05417235940694809\n",
      "Iteration 6537, Loss: 0.05398939177393913\n",
      "Iteration 6538, Loss: 0.054172199219465256\n",
      "Iteration 6539, Loss: 0.05398944020271301\n",
      "Iteration 6540, Loss: 0.05417224019765854\n",
      "Iteration 6541, Loss: 0.053989484906196594\n",
      "Iteration 6542, Loss: 0.05417215824127197\n",
      "Iteration 6543, Loss: 0.05398952588438988\n",
      "Iteration 6544, Loss: 0.054172124713659286\n",
      "Iteration 6545, Loss: 0.053989559412002563\n",
      "Iteration 6546, Loss: 0.0541720911860466\n",
      "Iteration 6547, Loss: 0.053989559412002563\n",
      "Iteration 6548, Loss: 0.05417215824127197\n",
      "Iteration 6549, Loss: 0.05398944765329361\n",
      "Iteration 6550, Loss: 0.054172199219465256\n",
      "Iteration 6551, Loss: 0.053989410400390625\n",
      "Iteration 6552, Loss: 0.05417218804359436\n",
      "Iteration 6553, Loss: 0.05398949235677719\n",
      "Iteration 6554, Loss: 0.05417191982269287\n",
      "Iteration 6555, Loss: 0.05398957058787346\n",
      "Iteration 6556, Loss: 0.05417199432849884\n",
      "Iteration 6557, Loss: 0.05398964881896973\n",
      "Iteration 6558, Loss: 0.05417192727327347\n",
      "Iteration 6559, Loss: 0.053989678621292114\n",
      "Iteration 6560, Loss: 0.054172083735466\n",
      "Iteration 6561, Loss: 0.053989559412002563\n",
      "Iteration 6562, Loss: 0.054172396659851074\n",
      "Iteration 6563, Loss: 0.05398933216929436\n",
      "Iteration 6564, Loss: 0.05417247861623764\n",
      "Iteration 6565, Loss: 0.053989212960004807\n",
      "Iteration 6566, Loss: 0.0541725754737854\n",
      "Iteration 6567, Loss: 0.053989361971616745\n",
      "Iteration 6568, Loss: 0.05417237803339958\n",
      "Iteration 6569, Loss: 0.053989559412002563\n",
      "Iteration 6570, Loss: 0.054172080010175705\n",
      "Iteration 6571, Loss: 0.0539897195994854\n",
      "Iteration 6572, Loss: 0.0541718415915966\n",
      "Iteration 6573, Loss: 0.05398987978696823\n",
      "Iteration 6574, Loss: 0.05417180061340332\n",
      "Iteration 6575, Loss: 0.05398987978696823\n",
      "Iteration 6576, Loss: 0.0541718415915966\n",
      "Iteration 6577, Loss: 0.05398983880877495\n",
      "Iteration 6578, Loss: 0.05417177081108093\n",
      "Iteration 6579, Loss: 0.05398976057767868\n",
      "Iteration 6580, Loss: 0.05417196452617645\n",
      "Iteration 6581, Loss: 0.05398961156606674\n",
      "Iteration 6582, Loss: 0.05417215824127197\n",
      "Iteration 6583, Loss: 0.053989410400390625\n",
      "Iteration 6584, Loss: 0.054172318428754807\n",
      "Iteration 6585, Loss: 0.05398925393819809\n",
      "Iteration 6586, Loss: 0.054172392934560776\n",
      "Iteration 6587, Loss: 0.05398936569690704\n",
      "Iteration 6588, Loss: 0.054172348231077194\n",
      "Iteration 6589, Loss: 0.05398940294981003\n",
      "Iteration 6590, Loss: 0.054172269999980927\n",
      "Iteration 6591, Loss: 0.05398992449045181\n",
      "Iteration 6592, Loss: 0.05417210981249809\n",
      "Iteration 6593, Loss: 0.05399015545845032\n",
      "Iteration 6594, Loss: 0.05417128652334213\n",
      "Iteration 6595, Loss: 0.0539904348552227\n",
      "Iteration 6596, Loss: 0.05417131632566452\n",
      "Iteration 6597, Loss: 0.05399039387702942\n",
      "Iteration 6598, Loss: 0.05417148396372795\n",
      "Iteration 6599, Loss: 0.05399011820554733\n",
      "Iteration 6600, Loss: 0.054171811789274216\n",
      "Iteration 6601, Loss: 0.053989678621292114\n",
      "Iteration 6602, Loss: 0.05417227745056152\n",
      "Iteration 6603, Loss: 0.05398936569690704\n",
      "Iteration 6604, Loss: 0.054172590374946594\n",
      "Iteration 6605, Loss: 0.05398917198181152\n",
      "Iteration 6606, Loss: 0.054172586649656296\n",
      "Iteration 6607, Loss: 0.053989212960004807\n",
      "Iteration 6608, Loss: 0.05417250841856003\n",
      "Iteration 6609, Loss: 0.05398937314748764\n",
      "Iteration 6610, Loss: 0.05417218804359436\n",
      "Iteration 6611, Loss: 0.05398964509367943\n",
      "Iteration 6612, Loss: 0.05417199060320854\n",
      "Iteration 6613, Loss: 0.05398987978696823\n",
      "Iteration 6614, Loss: 0.054171882569789886\n",
      "Iteration 6615, Loss: 0.05398992449045181\n",
      "Iteration 6616, Loss: 0.05417191982269287\n",
      "Iteration 6617, Loss: 0.05398976430296898\n",
      "Iteration 6618, Loss: 0.05417200177907944\n",
      "Iteration 6619, Loss: 0.053989581763744354\n",
      "Iteration 6620, Loss: 0.05417212098836899\n",
      "Iteration 6621, Loss: 0.05398945137858391\n",
      "Iteration 6622, Loss: 0.05417238920927048\n",
      "Iteration 6623, Loss: 0.053989484906196594\n",
      "Iteration 6624, Loss: 0.05417242646217346\n",
      "Iteration 6625, Loss: 0.05398940294981003\n",
      "Iteration 6626, Loss: 0.05417235940694809\n",
      "Iteration 6627, Loss: 0.05398940667510033\n",
      "Iteration 6628, Loss: 0.05417238920927048\n",
      "Iteration 6629, Loss: 0.053989410400390625\n",
      "Iteration 6630, Loss: 0.054172348231077194\n",
      "Iteration 6631, Loss: 0.05398952215909958\n",
      "Iteration 6632, Loss: 0.05417230725288391\n",
      "Iteration 6633, Loss: 0.05398952588438988\n",
      "Iteration 6634, Loss: 0.05417223274707794\n",
      "Iteration 6635, Loss: 0.053989529609680176\n",
      "Iteration 6636, Loss: 0.05417222902178764\n",
      "Iteration 6637, Loss: 0.05398949235677719\n",
      "Iteration 6638, Loss: 0.054172269999980927\n",
      "Iteration 6639, Loss: 0.05398957058787346\n",
      "Iteration 6640, Loss: 0.054172150790691376\n",
      "Iteration 6641, Loss: 0.05398961156606674\n",
      "Iteration 6642, Loss: 0.05417210981249809\n",
      "Iteration 6643, Loss: 0.05398968607187271\n",
      "Iteration 6644, Loss: 0.05417204648256302\n",
      "Iteration 6645, Loss: 0.05398961156606674\n",
      "Iteration 6646, Loss: 0.05417203903198242\n",
      "Iteration 6647, Loss: 0.05398976057767868\n",
      "Iteration 6648, Loss: 0.05417203903198242\n",
      "Iteration 6649, Loss: 0.05398968607187271\n",
      "Iteration 6650, Loss: 0.054172150790691376\n",
      "Iteration 6651, Loss: 0.05398968607187271\n",
      "Iteration 6652, Loss: 0.054172225296497345\n",
      "Iteration 6653, Loss: 0.05398964881896973\n",
      "Iteration 6654, Loss: 0.05417203903198242\n",
      "Iteration 6655, Loss: 0.0539897195994854\n",
      "Iteration 6656, Loss: 0.05417203903198242\n",
      "Iteration 6657, Loss: 0.05398961156606674\n",
      "Iteration 6658, Loss: 0.05417210981249809\n",
      "Iteration 6659, Loss: 0.05398961156606674\n",
      "Iteration 6660, Loss: 0.05417210981249809\n",
      "Iteration 6661, Loss: 0.053989678621292114\n",
      "Iteration 6662, Loss: 0.054172080010175705\n",
      "Iteration 6663, Loss: 0.053989678621292114\n",
      "Iteration 6664, Loss: 0.054172080010175705\n",
      "Iteration 6665, Loss: 0.0539897195994854\n",
      "Iteration 6666, Loss: 0.054172124713659286\n",
      "Iteration 6667, Loss: 0.05398957058787346\n",
      "Iteration 6668, Loss: 0.054172199219465256\n",
      "Iteration 6669, Loss: 0.053989484906196594\n",
      "Iteration 6670, Loss: 0.054172199219465256\n",
      "Iteration 6671, Loss: 0.05398937314748764\n",
      "Iteration 6672, Loss: 0.05417247116565704\n",
      "Iteration 6673, Loss: 0.05398933216929436\n",
      "Iteration 6674, Loss: 0.05417242646217346\n",
      "Iteration 6675, Loss: 0.05398933216929436\n",
      "Iteration 6676, Loss: 0.05417237803339958\n",
      "Iteration 6677, Loss: 0.053989529609680176\n",
      "Iteration 6678, Loss: 0.054172031581401825\n",
      "Iteration 6679, Loss: 0.05398980900645256\n",
      "Iteration 6680, Loss: 0.05417187139391899\n",
      "Iteration 6681, Loss: 0.053989969193935394\n",
      "Iteration 6682, Loss: 0.05417187139391899\n",
      "Iteration 6683, Loss: 0.0539899580180645\n",
      "Iteration 6684, Loss: 0.0541718527674675\n",
      "Iteration 6685, Loss: 0.05398968979716301\n",
      "Iteration 6686, Loss: 0.05417203903198242\n",
      "Iteration 6687, Loss: 0.05398953706026077\n",
      "Iteration 6688, Loss: 0.05417227745056152\n",
      "Iteration 6689, Loss: 0.05398932844400406\n",
      "Iteration 6690, Loss: 0.054172396659851074\n",
      "Iteration 6691, Loss: 0.05398913845419884\n",
      "Iteration 6692, Loss: 0.054172538220882416\n",
      "Iteration 6693, Loss: 0.05398933216929436\n",
      "Iteration 6694, Loss: 0.05417226254940033\n",
      "Iteration 6695, Loss: 0.05398964509367943\n",
      "Iteration 6696, Loss: 0.05417199060320854\n",
      "Iteration 6697, Loss: 0.053989849984645844\n",
      "Iteration 6698, Loss: 0.05417187511920929\n",
      "Iteration 6699, Loss: 0.0539899580180645\n",
      "Iteration 6700, Loss: 0.05417194962501526\n",
      "Iteration 6701, Loss: 0.05398992449045181\n",
      "Iteration 6702, Loss: 0.054171960800886154\n",
      "Iteration 6703, Loss: 0.05398976057767868\n",
      "Iteration 6704, Loss: 0.05417224019765854\n",
      "Iteration 6705, Loss: 0.05398944020271301\n",
      "Iteration 6706, Loss: 0.05417240411043167\n",
      "Iteration 6707, Loss: 0.053989212960004807\n",
      "Iteration 6708, Loss: 0.05417259782552719\n",
      "Iteration 6709, Loss: 0.05398913472890854\n",
      "Iteration 6710, Loss: 0.05417251214385033\n",
      "Iteration 6711, Loss: 0.053989212960004807\n",
      "Iteration 6712, Loss: 0.05417238920927048\n",
      "Iteration 6713, Loss: 0.053989481180906296\n",
      "Iteration 6714, Loss: 0.05417215824127197\n",
      "Iteration 6715, Loss: 0.05398964136838913\n",
      "Iteration 6716, Loss: 0.05417199432849884\n",
      "Iteration 6717, Loss: 0.05398982763290405\n",
      "Iteration 6718, Loss: 0.05417187139391899\n",
      "Iteration 6719, Loss: 0.053989917039871216\n",
      "Iteration 6720, Loss: 0.05417187139391899\n",
      "Iteration 6721, Loss: 0.05398988351225853\n",
      "Iteration 6722, Loss: 0.0541718415915966\n",
      "Iteration 6723, Loss: 0.053989849984645844\n",
      "Iteration 6724, Loss: 0.054172031581401825\n",
      "Iteration 6725, Loss: 0.05398976430296898\n",
      "Iteration 6726, Loss: 0.05417191982269287\n",
      "Iteration 6727, Loss: 0.05398968979716301\n",
      "Iteration 6728, Loss: 0.05417196452617645\n",
      "Iteration 6729, Loss: 0.05398968607187271\n",
      "Iteration 6730, Loss: 0.05417203530669212\n",
      "Iteration 6731, Loss: 0.05398964881896973\n",
      "Iteration 6732, Loss: 0.054172076284885406\n",
      "Iteration 6733, Loss: 0.053989797830581665\n",
      "Iteration 6734, Loss: 0.05417211353778839\n",
      "Iteration 6735, Loss: 0.053989604115486145\n",
      "Iteration 6736, Loss: 0.05417215824127197\n",
      "Iteration 6737, Loss: 0.05398964136838913\n",
      "Iteration 6738, Loss: 0.05417224019765854\n",
      "Iteration 6739, Loss: 0.05398960039019585\n",
      "Iteration 6740, Loss: 0.05417215824127197\n",
      "Iteration 6741, Loss: 0.05398952215909958\n",
      "Iteration 6742, Loss: 0.05417231470346451\n",
      "Iteration 6743, Loss: 0.05398933216929436\n",
      "Iteration 6744, Loss: 0.05417227745056152\n",
      "Iteration 6745, Loss: 0.05398952215909958\n",
      "Iteration 6746, Loss: 0.05417222902178764\n",
      "Iteration 6747, Loss: 0.053989529609680176\n",
      "Iteration 6748, Loss: 0.05417210981249809\n",
      "Iteration 6749, Loss: 0.05398968979716301\n",
      "Iteration 6750, Loss: 0.054172031581401825\n",
      "Iteration 6751, Loss: 0.05398976802825928\n",
      "Iteration 6752, Loss: 0.05417199060320854\n",
      "Iteration 6753, Loss: 0.053989849984645844\n",
      "Iteration 6754, Loss: 0.054171960800886154\n",
      "Iteration 6755, Loss: 0.05398980528116226\n",
      "Iteration 6756, Loss: 0.054172005504369736\n",
      "Iteration 6757, Loss: 0.05398957058787346\n",
      "Iteration 6758, Loss: 0.054172247648239136\n",
      "Iteration 6759, Loss: 0.05398936569690704\n",
      "Iteration 6760, Loss: 0.05417251214385033\n",
      "Iteration 6761, Loss: 0.05398928374052048\n",
      "Iteration 6762, Loss: 0.05417247116565704\n",
      "Iteration 6763, Loss: 0.05398932844400406\n",
      "Iteration 6764, Loss: 0.05417238920927048\n",
      "Iteration 6765, Loss: 0.053989410400390625\n",
      "Iteration 6766, Loss: 0.05417227745056152\n",
      "Iteration 6767, Loss: 0.053989674896001816\n",
      "Iteration 6768, Loss: 0.05417210981249809\n",
      "Iteration 6769, Loss: 0.0539897195994854\n",
      "Iteration 6770, Loss: 0.054171960800886154\n",
      "Iteration 6771, Loss: 0.05398983880877495\n",
      "Iteration 6772, Loss: 0.05417200177907944\n",
      "Iteration 6773, Loss: 0.05398980528116226\n",
      "Iteration 6774, Loss: 0.05417200177907944\n",
      "Iteration 6775, Loss: 0.05398976430296898\n",
      "Iteration 6776, Loss: 0.05417212098836899\n",
      "Iteration 6777, Loss: 0.05398960039019585\n",
      "Iteration 6778, Loss: 0.05417215824127197\n",
      "Iteration 6779, Loss: 0.05398949235677719\n",
      "Iteration 6780, Loss: 0.05417227745056152\n",
      "Iteration 6781, Loss: 0.05398929864168167\n",
      "Iteration 6782, Loss: 0.054172318428754807\n",
      "Iteration 6783, Loss: 0.05398937314748764\n",
      "Iteration 6784, Loss: 0.054172344505786896\n",
      "Iteration 6785, Loss: 0.05398952215909958\n",
      "Iteration 6786, Loss: 0.054172150790691376\n",
      "Iteration 6787, Loss: 0.05398968607187271\n",
      "Iteration 6788, Loss: 0.054171957075595856\n",
      "Iteration 6789, Loss: 0.05398987978696823\n",
      "Iteration 6790, Loss: 0.05417194962501526\n",
      "Iteration 6791, Loss: 0.05398987978696823\n",
      "Iteration 6792, Loss: 0.0541718527674675\n",
      "Iteration 6793, Loss: 0.0539897195994854\n",
      "Iteration 6794, Loss: 0.054172009229660034\n",
      "Iteration 6795, Loss: 0.05398960039019585\n",
      "Iteration 6796, Loss: 0.054172273725271225\n",
      "Iteration 6797, Loss: 0.05398944020271301\n",
      "Iteration 6798, Loss: 0.05417228490114212\n",
      "Iteration 6799, Loss: 0.053989291191101074\n",
      "Iteration 6800, Loss: 0.05417224019765854\n",
      "Iteration 6801, Loss: 0.05398949235677719\n",
      "Iteration 6802, Loss: 0.05417219549417496\n",
      "Iteration 6803, Loss: 0.053989604115486145\n",
      "Iteration 6804, Loss: 0.054172076284885406\n",
      "Iteration 6805, Loss: 0.05398968607187271\n",
      "Iteration 6806, Loss: 0.05417203903198242\n",
      "Iteration 6807, Loss: 0.05398968607187271\n",
      "Iteration 6808, Loss: 0.054172154515981674\n",
      "Iteration 6809, Loss: 0.05398960039019585\n",
      "Iteration 6810, Loss: 0.05417224019765854\n",
      "Iteration 6811, Loss: 0.053989410400390625\n",
      "Iteration 6812, Loss: 0.054172467440366745\n",
      "Iteration 6813, Loss: 0.05398936569690704\n",
      "Iteration 6814, Loss: 0.05417243763804436\n",
      "Iteration 6815, Loss: 0.053989291191101074\n",
      "Iteration 6816, Loss: 0.05417246371507645\n",
      "Iteration 6817, Loss: 0.05398944765329361\n",
      "Iteration 6818, Loss: 0.05417230725288391\n",
      "Iteration 6819, Loss: 0.053989529609680176\n",
      "Iteration 6820, Loss: 0.05417212098836899\n",
      "Iteration 6821, Loss: 0.05398957058787346\n",
      "Iteration 6822, Loss: 0.05417203903198242\n",
      "Iteration 6823, Loss: 0.05398968979716301\n",
      "Iteration 6824, Loss: 0.054172031581401825\n",
      "Iteration 6825, Loss: 0.05398976057767868\n",
      "Iteration 6826, Loss: 0.05417191982269287\n",
      "Iteration 6827, Loss: 0.053989917039871216\n",
      "Iteration 6828, Loss: 0.054171882569789886\n",
      "Iteration 6829, Loss: 0.05398976802825928\n",
      "Iteration 6830, Loss: 0.05417206883430481\n",
      "Iteration 6831, Loss: 0.05398968607187271\n",
      "Iteration 6832, Loss: 0.054172199219465256\n",
      "Iteration 6833, Loss: 0.05398945137858391\n",
      "Iteration 6834, Loss: 0.054172348231077194\n",
      "Iteration 6835, Loss: 0.05398933216929436\n",
      "Iteration 6836, Loss: 0.054172348231077194\n",
      "Iteration 6837, Loss: 0.053989484906196594\n",
      "Iteration 6838, Loss: 0.054172150790691376\n",
      "Iteration 6839, Loss: 0.053989604115486145\n",
      "Iteration 6840, Loss: 0.05417206883430481\n",
      "Iteration 6841, Loss: 0.05398987978696823\n",
      "Iteration 6842, Loss: 0.054171882569789886\n",
      "Iteration 6843, Loss: 0.05398980528116226\n",
      "Iteration 6844, Loss: 0.05417203530669212\n",
      "Iteration 6845, Loss: 0.053989678621292114\n",
      "Iteration 6846, Loss: 0.054172199219465256\n",
      "Iteration 6847, Loss: 0.05398959666490555\n",
      "Iteration 6848, Loss: 0.05417227745056152\n",
      "Iteration 6849, Loss: 0.053989477455616\n",
      "Iteration 6850, Loss: 0.05417238920927048\n",
      "Iteration 6851, Loss: 0.053989481180906296\n",
      "Iteration 6852, Loss: 0.05417227745056152\n",
      "Iteration 6853, Loss: 0.05398952215909958\n",
      "Iteration 6854, Loss: 0.05417206883430481\n",
      "Iteration 6855, Loss: 0.053989723324775696\n",
      "Iteration 6856, Loss: 0.05417194962501526\n",
      "Iteration 6857, Loss: 0.05398987978696823\n",
      "Iteration 6858, Loss: 0.05417183041572571\n",
      "Iteration 6859, Loss: 0.05398999899625778\n",
      "Iteration 6860, Loss: 0.054171763360500336\n",
      "Iteration 6861, Loss: 0.05398999899625778\n",
      "Iteration 6862, Loss: 0.054171763360500336\n",
      "Iteration 6863, Loss: 0.05398987978696823\n",
      "Iteration 6864, Loss: 0.054171882569789886\n",
      "Iteration 6865, Loss: 0.053989797830581665\n",
      "Iteration 6866, Loss: 0.05417204648256302\n",
      "Iteration 6867, Loss: 0.05398949235677719\n",
      "Iteration 6868, Loss: 0.054172348231077194\n",
      "Iteration 6869, Loss: 0.053989481180906296\n",
      "Iteration 6870, Loss: 0.05417231470346451\n",
      "Iteration 6871, Loss: 0.05398952215909958\n",
      "Iteration 6872, Loss: 0.054172269999980927\n",
      "Iteration 6873, Loss: 0.05398964136838913\n",
      "Iteration 6874, Loss: 0.05417230352759361\n",
      "Iteration 6875, Loss: 0.05398968607187271\n",
      "Iteration 6876, Loss: 0.054172031581401825\n",
      "Iteration 6877, Loss: 0.05398983880877495\n",
      "Iteration 6878, Loss: 0.054171960800886154\n",
      "Iteration 6879, Loss: 0.053989723324775696\n",
      "Iteration 6880, Loss: 0.054171960800886154\n",
      "Iteration 6881, Loss: 0.053989723324775696\n",
      "Iteration 6882, Loss: 0.054171960800886154\n",
      "Iteration 6883, Loss: 0.05398964881896973\n",
      "Iteration 6884, Loss: 0.054172009229660034\n",
      "Iteration 6885, Loss: 0.05398957058787346\n",
      "Iteration 6886, Loss: 0.054172080010175705\n",
      "Iteration 6887, Loss: 0.053989604115486145\n",
      "Iteration 6888, Loss: 0.054172080010175705\n",
      "Iteration 6889, Loss: 0.05398954078555107\n",
      "Iteration 6890, Loss: 0.05417203903198242\n",
      "Iteration 6891, Loss: 0.05398964881896973\n",
      "Iteration 6892, Loss: 0.054172080010175705\n",
      "Iteration 6893, Loss: 0.05398964881896973\n",
      "Iteration 6894, Loss: 0.054171960800886154\n",
      "Iteration 6895, Loss: 0.053989723324775696\n",
      "Iteration 6896, Loss: 0.054172031581401825\n",
      "Iteration 6897, Loss: 0.05398984253406525\n",
      "Iteration 6898, Loss: 0.0541718453168869\n",
      "Iteration 6899, Loss: 0.053989723324775696\n",
      "Iteration 6900, Loss: 0.05417200177907944\n",
      "Iteration 6901, Loss: 0.05398968607187271\n",
      "Iteration 6902, Loss: 0.05417206883430481\n",
      "Iteration 6903, Loss: 0.0539897195994854\n",
      "Iteration 6904, Loss: 0.054171930998563766\n",
      "Iteration 6905, Loss: 0.053989678621292114\n",
      "Iteration 6906, Loss: 0.05417203903198242\n",
      "Iteration 6907, Loss: 0.053989678621292114\n",
      "Iteration 6908, Loss: 0.05417212098836899\n",
      "Iteration 6909, Loss: 0.053989678621292114\n",
      "Iteration 6910, Loss: 0.05417212098836899\n",
      "Iteration 6911, Loss: 0.05398960039019585\n",
      "Iteration 6912, Loss: 0.054172199219465256\n",
      "Iteration 6913, Loss: 0.053989559412002563\n",
      "Iteration 6914, Loss: 0.05417224019765854\n",
      "Iteration 6915, Loss: 0.053989410400390625\n",
      "Iteration 6916, Loss: 0.05417222902178764\n",
      "Iteration 6917, Loss: 0.053989529609680176\n",
      "Iteration 6918, Loss: 0.05417222902178764\n",
      "Iteration 6919, Loss: 0.05398949980735779\n",
      "Iteration 6920, Loss: 0.054172076284885406\n",
      "Iteration 6921, Loss: 0.05398957058787346\n",
      "Iteration 6922, Loss: 0.05417199432849884\n",
      "Iteration 6923, Loss: 0.05398964881896973\n",
      "Iteration 6924, Loss: 0.05417206883430481\n",
      "Iteration 6925, Loss: 0.05398976802825928\n",
      "Iteration 6926, Loss: 0.05417191609740257\n",
      "Iteration 6927, Loss: 0.053989849984645844\n",
      "Iteration 6928, Loss: 0.0541718415915966\n",
      "Iteration 6929, Loss: 0.05398992449045181\n",
      "Iteration 6930, Loss: 0.0541718527674675\n",
      "Iteration 6931, Loss: 0.05398968979716301\n",
      "Iteration 6932, Loss: 0.0541720911860466\n",
      "Iteration 6933, Loss: 0.05398937314748764\n",
      "Iteration 6934, Loss: 0.05417240783572197\n",
      "Iteration 6935, Loss: 0.05398928374052048\n",
      "Iteration 6936, Loss: 0.05417255684733391\n",
      "Iteration 6937, Loss: 0.05398917198181152\n",
      "Iteration 6938, Loss: 0.054172515869140625\n",
      "Iteration 6939, Loss: 0.05398910492658615\n",
      "Iteration 6940, Loss: 0.054172392934560776\n",
      "Iteration 6941, Loss: 0.053989291191101074\n",
      "Iteration 6942, Loss: 0.05417218804359436\n",
      "Iteration 6943, Loss: 0.05398961156606674\n",
      "Iteration 6944, Loss: 0.054171882569789886\n",
      "Iteration 6945, Loss: 0.05398976802825928\n",
      "Iteration 6946, Loss: 0.054171763360500336\n",
      "Iteration 6947, Loss: 0.053989969193935394\n",
      "Iteration 6948, Loss: 0.05417173355817795\n",
      "Iteration 6949, Loss: 0.05398988723754883\n",
      "Iteration 6950, Loss: 0.05417180061340332\n",
      "Iteration 6951, Loss: 0.05398984253406525\n",
      "Iteration 6952, Loss: 0.054171930998563766\n",
      "Iteration 6953, Loss: 0.05398964136838913\n",
      "Iteration 6954, Loss: 0.054172202944755554\n",
      "Iteration 6955, Loss: 0.053989291191101074\n",
      "Iteration 6956, Loss: 0.05417228862643242\n",
      "Iteration 6957, Loss: 0.05398933216929436\n",
      "Iteration 6958, Loss: 0.05417228862643242\n",
      "Iteration 6959, Loss: 0.05398940294981003\n",
      "Iteration 6960, Loss: 0.05417227745056152\n",
      "Iteration 6961, Loss: 0.05398945137858391\n",
      "Iteration 6962, Loss: 0.05417215824127197\n",
      "Iteration 6963, Loss: 0.05398964136838913\n",
      "Iteration 6964, Loss: 0.054171960800886154\n",
      "Iteration 6965, Loss: 0.05398973077535629\n",
      "Iteration 6966, Loss: 0.05417191982269287\n",
      "Iteration 6967, Loss: 0.05398980900645256\n",
      "Iteration 6968, Loss: 0.054171882569789886\n",
      "Iteration 6969, Loss: 0.053989581763744354\n",
      "Iteration 6970, Loss: 0.054172009229660034\n",
      "Iteration 6971, Loss: 0.05398941785097122\n",
      "Iteration 6972, Loss: 0.05417216941714287\n",
      "Iteration 6973, Loss: 0.05398937314748764\n",
      "Iteration 6974, Loss: 0.05417236313223839\n",
      "Iteration 6975, Loss: 0.053989291191101074\n",
      "Iteration 6976, Loss: 0.054172318428754807\n",
      "Iteration 6977, Loss: 0.053989291191101074\n",
      "Iteration 6978, Loss: 0.054172273725271225\n",
      "Iteration 6979, Loss: 0.05398945137858391\n",
      "Iteration 6980, Loss: 0.05417215824127197\n",
      "Iteration 6981, Loss: 0.05398961156606674\n",
      "Iteration 6982, Loss: 0.054172031581401825\n",
      "Iteration 6983, Loss: 0.05398968979716301\n",
      "Iteration 6984, Loss: 0.054171957075595856\n",
      "Iteration 6985, Loss: 0.053989775478839874\n",
      "Iteration 6986, Loss: 0.0541718527674675\n",
      "Iteration 6987, Loss: 0.05398976802825928\n",
      "Iteration 6988, Loss: 0.0541718527674675\n",
      "Iteration 6989, Loss: 0.05398976057767868\n",
      "Iteration 6990, Loss: 0.054171960800886154\n",
      "Iteration 6991, Loss: 0.05398961156606674\n",
      "Iteration 6992, Loss: 0.054172124713659286\n",
      "Iteration 6993, Loss: 0.05398944765329361\n",
      "Iteration 6994, Loss: 0.05417224392294884\n",
      "Iteration 6995, Loss: 0.0539892241358757\n",
      "Iteration 6996, Loss: 0.0541723296046257\n",
      "Iteration 6997, Loss: 0.05398929864168167\n",
      "Iteration 6998, Loss: 0.054172318428754807\n",
      "Iteration 6999, Loss: 0.05398937314748764\n",
      "Iteration 7000, Loss: 0.05417216941714287\n",
      "Iteration 7001, Loss: 0.05398934334516525\n",
      "Iteration 7002, Loss: 0.05417211353778839\n",
      "Iteration 7003, Loss: 0.05398961156606674\n",
      "Iteration 7004, Loss: 0.05417200177907944\n",
      "Iteration 7005, Loss: 0.05398968979716301\n",
      "Iteration 7006, Loss: 0.05417189002037048\n",
      "Iteration 7007, Loss: 0.05398964881896973\n",
      "Iteration 7008, Loss: 0.05417203903198242\n",
      "Iteration 7009, Loss: 0.05398964881896973\n",
      "Iteration 7010, Loss: 0.05417215824127197\n",
      "Iteration 7011, Loss: 0.05398949235677719\n",
      "Iteration 7012, Loss: 0.054172199219465256\n",
      "Iteration 7013, Loss: 0.053989410400390625\n",
      "Iteration 7014, Loss: 0.05417216569185257\n",
      "Iteration 7015, Loss: 0.053989410400390625\n",
      "Iteration 7016, Loss: 0.054172199219465256\n",
      "Iteration 7017, Loss: 0.05398957058787346\n",
      "Iteration 7018, Loss: 0.054171960800886154\n",
      "Iteration 7019, Loss: 0.05398973077535629\n",
      "Iteration 7020, Loss: 0.0541718527674675\n",
      "Iteration 7021, Loss: 0.05398973077535629\n",
      "Iteration 7022, Loss: 0.05417189002037048\n",
      "Iteration 7023, Loss: 0.05398957058787346\n",
      "Iteration 7024, Loss: 0.054172080010175705\n",
      "Iteration 7025, Loss: 0.05398941785097122\n",
      "Iteration 7026, Loss: 0.05417215824127197\n",
      "Iteration 7027, Loss: 0.05398949235677719\n",
      "Iteration 7028, Loss: 0.054172199219465256\n",
      "Iteration 7029, Loss: 0.053989410400390625\n",
      "Iteration 7030, Loss: 0.05417222902178764\n",
      "Iteration 7031, Loss: 0.05398957058787346\n",
      "Iteration 7032, Loss: 0.05417203903198242\n",
      "Iteration 7033, Loss: 0.05398968979716301\n",
      "Iteration 7034, Loss: 0.054172076284885406\n",
      "Iteration 7035, Loss: 0.05398964881896973\n",
      "Iteration 7036, Loss: 0.05417200177907944\n",
      "Iteration 7037, Loss: 0.05398961156606674\n",
      "Iteration 7038, Loss: 0.05417203903198242\n",
      "Iteration 7039, Loss: 0.05398961156606674\n",
      "Iteration 7040, Loss: 0.054172080010175705\n",
      "Iteration 7041, Loss: 0.05398957058787346\n",
      "Iteration 7042, Loss: 0.054172009229660034\n",
      "Iteration 7043, Loss: 0.05398945137858391\n",
      "Iteration 7044, Loss: 0.054172199219465256\n",
      "Iteration 7045, Loss: 0.05398949235677719\n",
      "Iteration 7046, Loss: 0.05417212098836899\n",
      "Iteration 7047, Loss: 0.05398953706026077\n",
      "Iteration 7048, Loss: 0.05417204648256302\n",
      "Iteration 7049, Loss: 0.053989604115486145\n",
      "Iteration 7050, Loss: 0.054172080010175705\n",
      "Iteration 7051, Loss: 0.053989604115486145\n",
      "Iteration 7052, Loss: 0.05417203530669212\n",
      "Iteration 7053, Loss: 0.05398961156606674\n",
      "Iteration 7054, Loss: 0.05417203530669212\n",
      "Iteration 7055, Loss: 0.05398968979716301\n",
      "Iteration 7056, Loss: 0.054172080010175705\n",
      "Iteration 7057, Loss: 0.05398949235677719\n",
      "Iteration 7058, Loss: 0.05417216569185257\n",
      "Iteration 7059, Loss: 0.05398949235677719\n",
      "Iteration 7060, Loss: 0.054172318428754807\n",
      "Iteration 7061, Loss: 0.053989410400390625\n",
      "Iteration 7062, Loss: 0.054172396659851074\n",
      "Iteration 7063, Loss: 0.05398917943239212\n",
      "Iteration 7064, Loss: 0.054172396659851074\n",
      "Iteration 7065, Loss: 0.05398918315768242\n",
      "Iteration 7066, Loss: 0.05417243763804436\n",
      "Iteration 7067, Loss: 0.05398933216929436\n",
      "Iteration 7068, Loss: 0.05417224019765854\n",
      "Iteration 7069, Loss: 0.05398956686258316\n",
      "Iteration 7070, Loss: 0.05417203530669212\n",
      "Iteration 7071, Loss: 0.05398964881896973\n",
      "Iteration 7072, Loss: 0.05417200177907944\n",
      "Iteration 7073, Loss: 0.05398928374052048\n",
      "Iteration 7074, Loss: 0.05417192727327347\n",
      "Iteration 7075, Loss: 0.05398900434374809\n",
      "Iteration 7076, Loss: 0.05417276546359062\n",
      "Iteration 7077, Loss: 0.053988777101039886\n",
      "Iteration 7078, Loss: 0.05417283624410629\n",
      "Iteration 7079, Loss: 0.05398881435394287\n",
      "Iteration 7080, Loss: 0.054172664880752563\n",
      "Iteration 7081, Loss: 0.05398905277252197\n",
      "Iteration 7082, Loss: 0.054172348231077194\n",
      "Iteration 7083, Loss: 0.05398937314748764\n",
      "Iteration 7084, Loss: 0.05417199432849884\n",
      "Iteration 7085, Loss: 0.05398960039019585\n",
      "Iteration 7086, Loss: 0.0541718415915966\n",
      "Iteration 7087, Loss: 0.05398976802825928\n",
      "Iteration 7088, Loss: 0.05417163670063019\n",
      "Iteration 7089, Loss: 0.05399011820554733\n",
      "Iteration 7090, Loss: 0.05417141318321228\n",
      "Iteration 7091, Loss: 0.05399015173316002\n",
      "Iteration 7092, Loss: 0.05417138338088989\n",
      "Iteration 7093, Loss: 0.05398999899625778\n",
      "Iteration 7094, Loss: 0.054171882569789886\n",
      "Iteration 7095, Loss: 0.0539897195994854\n",
      "Iteration 7096, Loss: 0.05417223274707794\n",
      "Iteration 7097, Loss: 0.053989291191101074\n",
      "Iteration 7098, Loss: 0.05417247861623764\n",
      "Iteration 7099, Loss: 0.05398901551961899\n",
      "Iteration 7100, Loss: 0.05417244881391525\n",
      "Iteration 7101, Loss: 0.05398905277252197\n",
      "Iteration 7102, Loss: 0.05417240783572197\n",
      "Iteration 7103, Loss: 0.05398917198181152\n",
      "Iteration 7104, Loss: 0.05417215824127197\n",
      "Iteration 7105, Loss: 0.053989481180906296\n",
      "Iteration 7106, Loss: 0.054172080010175705\n",
      "Iteration 7107, Loss: 0.05398964136838913\n",
      "Iteration 7108, Loss: 0.054171767085790634\n",
      "Iteration 7109, Loss: 0.05398964881896973\n",
      "Iteration 7110, Loss: 0.054171692579984665\n",
      "Iteration 7111, Loss: 0.05398973077535629\n",
      "Iteration 7112, Loss: 0.0541718527674675\n",
      "Iteration 7113, Loss: 0.053989529609680176\n",
      "Iteration 7114, Loss: 0.05417197197675705\n",
      "Iteration 7115, Loss: 0.05398945137858391\n",
      "Iteration 7116, Loss: 0.05417212098836899\n",
      "Iteration 7117, Loss: 0.05398937314748764\n",
      "Iteration 7118, Loss: 0.054172199219465256\n",
      "Iteration 7119, Loss: 0.05398936569690704\n",
      "Iteration 7120, Loss: 0.054172199219465256\n",
      "Iteration 7121, Loss: 0.05398937314748764\n",
      "Iteration 7122, Loss: 0.05417219549417496\n",
      "Iteration 7123, Loss: 0.053989529609680176\n",
      "Iteration 7124, Loss: 0.054171960800886154\n",
      "Iteration 7125, Loss: 0.05398964881896973\n",
      "Iteration 7126, Loss: 0.054171811789274216\n",
      "Iteration 7127, Loss: 0.05398957058787346\n",
      "Iteration 7128, Loss: 0.054172009229660034\n",
      "Iteration 7129, Loss: 0.053989484906196594\n",
      "Iteration 7130, Loss: 0.05417216941714287\n",
      "Iteration 7131, Loss: 0.053989212960004807\n",
      "Iteration 7132, Loss: 0.05417228862643242\n",
      "Iteration 7133, Loss: 0.05398932099342346\n",
      "Iteration 7134, Loss: 0.05417216941714287\n",
      "Iteration 7135, Loss: 0.053989291191101074\n",
      "Iteration 7136, Loss: 0.05417203903198242\n",
      "Iteration 7137, Loss: 0.05398964136838913\n",
      "Iteration 7138, Loss: 0.05417191982269287\n",
      "Iteration 7139, Loss: 0.053989797830581665\n",
      "Iteration 7140, Loss: 0.054171767085790634\n",
      "Iteration 7141, Loss: 0.053989797830581665\n",
      "Iteration 7142, Loss: 0.054171882569789886\n",
      "Iteration 7143, Loss: 0.05398976430296898\n",
      "Iteration 7144, Loss: 0.054171811789274216\n",
      "Iteration 7145, Loss: 0.053989604115486145\n",
      "Iteration 7146, Loss: 0.054172009229660034\n",
      "Iteration 7147, Loss: 0.053989484906196594\n",
      "Iteration 7148, Loss: 0.05417216941714287\n",
      "Iteration 7149, Loss: 0.05398933216929436\n",
      "Iteration 7150, Loss: 0.05417227745056152\n",
      "Iteration 7151, Loss: 0.053989361971616745\n",
      "Iteration 7152, Loss: 0.054172128438949585\n",
      "Iteration 7153, Loss: 0.053989212960004807\n",
      "Iteration 7154, Loss: 0.05417205020785332\n",
      "Iteration 7155, Loss: 0.05398945137858391\n",
      "Iteration 7156, Loss: 0.0541718527674675\n",
      "Iteration 7157, Loss: 0.05398957058787346\n",
      "Iteration 7158, Loss: 0.0541718527674675\n",
      "Iteration 7159, Loss: 0.05398964881896973\n",
      "Iteration 7160, Loss: 0.054171741008758545\n",
      "Iteration 7161, Loss: 0.05398976430296898\n",
      "Iteration 7162, Loss: 0.054171737283468246\n",
      "Iteration 7163, Loss: 0.05398976057767868\n",
      "Iteration 7164, Loss: 0.0541718453168869\n",
      "Iteration 7165, Loss: 0.05399007722735405\n",
      "Iteration 7166, Loss: 0.054172009229660034\n",
      "Iteration 7167, Loss: 0.05398987978696823\n",
      "Iteration 7168, Loss: 0.05417260527610779\n",
      "Iteration 7169, Loss: 0.05398964881896973\n",
      "Iteration 7170, Loss: 0.054172687232494354\n",
      "Iteration 7171, Loss: 0.05398987978696823\n",
      "Iteration 7172, Loss: 0.05417264252901077\n",
      "Iteration 7173, Loss: 0.05398991331458092\n",
      "Iteration 7174, Loss: 0.05417260527610779\n",
      "Iteration 7175, Loss: 0.05398983880877495\n",
      "Iteration 7176, Loss: 0.0541725680232048\n",
      "Iteration 7177, Loss: 0.05398976802825928\n",
      "Iteration 7178, Loss: 0.05417264625430107\n",
      "Iteration 7179, Loss: 0.05398987978696823\n",
      "Iteration 7180, Loss: 0.05417248234152794\n",
      "Iteration 7181, Loss: 0.05399003624916077\n",
      "Iteration 7182, Loss: 0.05417228862643242\n",
      "Iteration 7183, Loss: 0.05399012565612793\n",
      "Iteration 7184, Loss: 0.054172322154045105\n",
      "Iteration 7185, Loss: 0.05399015545845032\n",
      "Iteration 7186, Loss: 0.0541723296046257\n",
      "Iteration 7187, Loss: 0.05399000272154808\n",
      "Iteration 7188, Loss: 0.05417252331972122\n",
      "Iteration 7189, Loss: 0.053989849984645844\n",
      "Iteration 7190, Loss: 0.05417260527610779\n",
      "Iteration 7191, Loss: 0.05398983880877495\n",
      "Iteration 7192, Loss: 0.054172761738300323\n",
      "Iteration 7193, Loss: 0.05398968607187271\n",
      "Iteration 7194, Loss: 0.05417272448539734\n",
      "Iteration 7195, Loss: 0.05398964509367943\n",
      "Iteration 7196, Loss: 0.05417272448539734\n",
      "Iteration 7197, Loss: 0.05398961529135704\n",
      "Iteration 7198, Loss: 0.05417267978191376\n",
      "Iteration 7199, Loss: 0.053989700973033905\n",
      "Iteration 7200, Loss: 0.05417255684733391\n",
      "Iteration 7201, Loss: 0.05399007350206375\n",
      "Iteration 7202, Loss: 0.05417247861623764\n",
      "Iteration 7203, Loss: 0.053990043699741364\n",
      "Iteration 7204, Loss: 0.05417228862643242\n",
      "Iteration 7205, Loss: 0.05399011820554733\n",
      "Iteration 7206, Loss: 0.05417243763804436\n",
      "Iteration 7207, Loss: 0.05399007722735405\n",
      "Iteration 7208, Loss: 0.05417240411043167\n",
      "Iteration 7209, Loss: 0.05399015173316002\n",
      "Iteration 7210, Loss: 0.054172366857528687\n",
      "Iteration 7211, Loss: 0.053990110754966736\n",
      "Iteration 7212, Loss: 0.054172366857528687\n",
      "Iteration 7213, Loss: 0.05399007722735405\n",
      "Iteration 7214, Loss: 0.05417252704501152\n",
      "Iteration 7215, Loss: 0.053989849984645844\n",
      "Iteration 7216, Loss: 0.05417260527610779\n",
      "Iteration 7217, Loss: 0.05398983880877495\n",
      "Iteration 7218, Loss: 0.0541725717484951\n",
      "Iteration 7219, Loss: 0.05398976430296898\n",
      "Iteration 7220, Loss: 0.05417264625430107\n",
      "Iteration 7221, Loss: 0.05398976802825928\n",
      "Iteration 7222, Loss: 0.054172635078430176\n",
      "Iteration 7223, Loss: 0.053989849984645844\n",
      "Iteration 7224, Loss: 0.05417236313223839\n",
      "Iteration 7225, Loss: 0.0539901964366436\n",
      "Iteration 7226, Loss: 0.05417224392294884\n",
      "Iteration 7227, Loss: 0.05399024114012718\n",
      "Iteration 7228, Loss: 0.05417227745056152\n",
      "Iteration 7229, Loss: 0.05399027466773987\n",
      "Iteration 7230, Loss: 0.05417228862643242\n",
      "Iteration 7231, Loss: 0.05398992821574211\n",
      "Iteration 7232, Loss: 0.05417255684733391\n",
      "Iteration 7233, Loss: 0.05398987978696823\n",
      "Iteration 7234, Loss: 0.054172687232494354\n",
      "Iteration 7235, Loss: 0.053989797830581665\n",
      "Iteration 7236, Loss: 0.054172806441783905\n",
      "Iteration 7237, Loss: 0.05398961156606674\n",
      "Iteration 7238, Loss: 0.05417276546359062\n",
      "Iteration 7239, Loss: 0.05398964509367943\n",
      "Iteration 7240, Loss: 0.05417275428771973\n",
      "Iteration 7241, Loss: 0.05398983880877495\n",
      "Iteration 7242, Loss: 0.054172515869140625\n",
      "Iteration 7243, Loss: 0.053989969193935394\n",
      "Iteration 7244, Loss: 0.054172247648239136\n",
      "Iteration 7245, Loss: 0.05399027466773987\n",
      "Iteration 7246, Loss: 0.054172128438949585\n",
      "Iteration 7247, Loss: 0.053990237414836884\n",
      "Iteration 7248, Loss: 0.05417221039533615\n",
      "Iteration 7249, Loss: 0.05399024114012718\n",
      "Iteration 7250, Loss: 0.0541723296046257\n",
      "Iteration 7251, Loss: 0.053990043699741364\n",
      "Iteration 7252, Loss: 0.05417249724268913\n",
      "Iteration 7253, Loss: 0.05398968979716301\n",
      "Iteration 7254, Loss: 0.054172806441783905\n",
      "Iteration 7255, Loss: 0.05398956686258316\n",
      "Iteration 7256, Loss: 0.05417287349700928\n",
      "Iteration 7257, Loss: 0.05398964881896973\n",
      "Iteration 7258, Loss: 0.05417267605662346\n",
      "Iteration 7259, Loss: 0.053989261388778687\n",
      "Iteration 7260, Loss: 0.05417235940694809\n",
      "Iteration 7261, Loss: 0.05398938059806824\n",
      "Iteration 7262, Loss: 0.05417158454656601\n",
      "Iteration 7263, Loss: 0.05398935079574585\n",
      "Iteration 7264, Loss: 0.05417143553495407\n",
      "Iteration 7265, Loss: 0.05398949980735779\n",
      "Iteration 7266, Loss: 0.054171472787857056\n",
      "Iteration 7267, Loss: 0.05398954078555107\n",
      "Iteration 7268, Loss: 0.054171472787857056\n",
      "Iteration 7269, Loss: 0.05398949608206749\n",
      "Iteration 7270, Loss: 0.05417163297533989\n",
      "Iteration 7271, Loss: 0.05398930236697197\n",
      "Iteration 7272, Loss: 0.05417182669043541\n",
      "Iteration 7273, Loss: 0.05398911237716675\n",
      "Iteration 7274, Loss: 0.05417194217443466\n",
      "Iteration 7275, Loss: 0.05398903042078018\n",
      "Iteration 7276, Loss: 0.054171979427337646\n",
      "Iteration 7277, Loss: 0.053989022970199585\n",
      "Iteration 7278, Loss: 0.05417205020785332\n",
      "Iteration 7279, Loss: 0.05398915335536003\n",
      "Iteration 7280, Loss: 0.05417182669043541\n",
      "Iteration 7281, Loss: 0.05398930236697197\n",
      "Iteration 7282, Loss: 0.05417182296514511\n",
      "Iteration 7283, Loss: 0.0539892241358757\n",
      "Iteration 7284, Loss: 0.05417171120643616\n",
      "Iteration 7285, Loss: 0.05398914963006973\n",
      "Iteration 7286, Loss: 0.05417179316282272\n",
      "Iteration 7287, Loss: 0.05398925393819809\n",
      "Iteration 7288, Loss: 0.05417190492153168\n",
      "Iteration 7289, Loss: 0.05398910492658615\n",
      "Iteration 7290, Loss: 0.05417202040553093\n",
      "Iteration 7291, Loss: 0.05398903414607048\n",
      "Iteration 7292, Loss: 0.054171912372112274\n",
      "Iteration 7293, Loss: 0.05398910492658615\n",
      "Iteration 7294, Loss: 0.054171860218048096\n",
      "Iteration 7295, Loss: 0.0539892241358757\n",
      "Iteration 7296, Loss: 0.05417166277766228\n",
      "Iteration 7297, Loss: 0.05398938059806824\n",
      "Iteration 7298, Loss: 0.054171543568372726\n",
      "Iteration 7299, Loss: 0.0539894625544548\n",
      "Iteration 7300, Loss: 0.05417151004076004\n",
      "Iteration 7301, Loss: 0.0539894700050354\n",
      "Iteration 7302, Loss: 0.054171547293663025\n",
      "Iteration 7303, Loss: 0.05398941785097122\n",
      "Iteration 7304, Loss: 0.05417182296514511\n",
      "Iteration 7305, Loss: 0.053989142179489136\n",
      "Iteration 7306, Loss: 0.05417199060320854\n",
      "Iteration 7307, Loss: 0.053989022970199585\n",
      "Iteration 7308, Loss: 0.0541720986366272\n",
      "Iteration 7309, Loss: 0.053988903760910034\n",
      "Iteration 7310, Loss: 0.0541720949113369\n",
      "Iteration 7311, Loss: 0.0539889931678772\n",
      "Iteration 7312, Loss: 0.05417182296514511\n",
      "Iteration 7313, Loss: 0.053989142179489136\n",
      "Iteration 7314, Loss: 0.05417177826166153\n",
      "Iteration 7315, Loss: 0.053989410400390625\n",
      "Iteration 7316, Loss: 0.054171737283468246\n",
      "Iteration 7317, Loss: 0.05398942157626152\n",
      "Iteration 7318, Loss: 0.054171472787857056\n",
      "Iteration 7319, Loss: 0.0539894625544548\n",
      "Iteration 7320, Loss: 0.05417155474424362\n",
      "Iteration 7321, Loss: 0.05398945137858391\n",
      "Iteration 7322, Loss: 0.05417163297533989\n",
      "Iteration 7323, Loss: 0.05398934334516525\n",
      "Iteration 7324, Loss: 0.05417175218462944\n",
      "Iteration 7325, Loss: 0.05398918315768242\n",
      "Iteration 7326, Loss: 0.05417206138372421\n",
      "Iteration 7327, Loss: 0.05398891493678093\n",
      "Iteration 7328, Loss: 0.05417218059301376\n",
      "Iteration 7329, Loss: 0.05398879572749138\n",
      "Iteration 7330, Loss: 0.05417213961482048\n",
      "Iteration 7331, Loss: 0.05398879572749138\n",
      "Iteration 7332, Loss: 0.05417206138372421\n",
      "Iteration 7333, Loss: 0.0539889857172966\n",
      "Iteration 7334, Loss: 0.05417182296514511\n",
      "Iteration 7335, Loss: 0.05398930236697197\n",
      "Iteration 7336, Loss: 0.05417169630527496\n",
      "Iteration 7337, Loss: 0.05398949980735779\n",
      "Iteration 7338, Loss: 0.05417150259017944\n",
      "Iteration 7339, Loss: 0.05398954078555107\n",
      "Iteration 7340, Loss: 0.054171498864889145\n",
      "Iteration 7341, Loss: 0.053989581763744354\n",
      "Iteration 7342, Loss: 0.05417150259017944\n",
      "Iteration 7343, Loss: 0.05398949980735779\n",
      "Iteration 7344, Loss: 0.054171543568372726\n",
      "Iteration 7345, Loss: 0.05398949608206749\n",
      "Iteration 7346, Loss: 0.054171741008758545\n",
      "Iteration 7347, Loss: 0.053989291191101074\n",
      "Iteration 7348, Loss: 0.05417183041572571\n",
      "Iteration 7349, Loss: 0.05398903042078018\n",
      "Iteration 7350, Loss: 0.05417190492153168\n",
      "Iteration 7351, Loss: 0.053989022970199585\n",
      "Iteration 7352, Loss: 0.054171860218048096\n",
      "Iteration 7353, Loss: 0.05398910492658615\n",
      "Iteration 7354, Loss: 0.05417182669043541\n",
      "Iteration 7355, Loss: 0.05398910865187645\n",
      "Iteration 7356, Loss: 0.05417167395353317\n",
      "Iteration 7357, Loss: 0.05398938059806824\n",
      "Iteration 7358, Loss: 0.05417163297533989\n",
      "Iteration 7359, Loss: 0.053989388048648834\n",
      "Iteration 7360, Loss: 0.05417170748114586\n",
      "Iteration 7361, Loss: 0.05398930236697197\n",
      "Iteration 7362, Loss: 0.054171860218048096\n",
      "Iteration 7363, Loss: 0.0539892241358757\n",
      "Iteration 7364, Loss: 0.054171860218048096\n",
      "Iteration 7365, Loss: 0.053989142179489136\n",
      "Iteration 7366, Loss: 0.05417197197675705\n",
      "Iteration 7367, Loss: 0.0539892241358757\n",
      "Iteration 7368, Loss: 0.054171737283468246\n",
      "Iteration 7369, Loss: 0.05398938059806824\n",
      "Iteration 7370, Loss: 0.05417146533727646\n",
      "Iteration 7371, Loss: 0.05398939177393913\n",
      "Iteration 7372, Loss: 0.05417134612798691\n",
      "Iteration 7373, Loss: 0.05398973822593689\n",
      "Iteration 7374, Loss: 0.054171111434698105\n",
      "Iteration 7375, Loss: 0.05398977920413017\n",
      "Iteration 7376, Loss: 0.05417107790708542\n",
      "Iteration 7377, Loss: 0.05398973822593689\n",
      "Iteration 7378, Loss: 0.05417139455676079\n",
      "Iteration 7379, Loss: 0.05398938059806824\n",
      "Iteration 7380, Loss: 0.05417158827185631\n",
      "Iteration 7381, Loss: 0.05398930236697197\n",
      "Iteration 7382, Loss: 0.05417170375585556\n",
      "Iteration 7383, Loss: 0.053989261388778687\n",
      "Iteration 7384, Loss: 0.05417158827185631\n",
      "Iteration 7385, Loss: 0.053989388048648834\n",
      "Iteration 7386, Loss: 0.054171498864889145\n",
      "Iteration 7387, Loss: 0.0539894700050354\n",
      "Iteration 7388, Loss: 0.05417114496231079\n",
      "Iteration 7389, Loss: 0.05398974567651749\n",
      "Iteration 7390, Loss: 0.05417114496231079\n",
      "Iteration 7391, Loss: 0.05398997291922569\n",
      "Iteration 7392, Loss: 0.05417107790708542\n",
      "Iteration 7393, Loss: 0.05398965999484062\n",
      "Iteration 7394, Loss: 0.05417143553495407\n",
      "Iteration 7395, Loss: 0.05398926883935928\n",
      "Iteration 7396, Loss: 0.05417171120643616\n",
      "Iteration 7397, Loss: 0.053989142179489136\n",
      "Iteration 7398, Loss: 0.05417194962501526\n",
      "Iteration 7399, Loss: 0.05398891493678093\n",
      "Iteration 7400, Loss: 0.05417198687791824\n",
      "Iteration 7401, Loss: 0.053988873958587646\n",
      "Iteration 7402, Loss: 0.05417179316282272\n",
      "Iteration 7403, Loss: 0.05398907512426376\n",
      "Iteration 7404, Loss: 0.05417170375585556\n",
      "Iteration 7405, Loss: 0.0539894625544548\n",
      "Iteration 7406, Loss: 0.054171543568372726\n",
      "Iteration 7407, Loss: 0.05398949980735779\n",
      "Iteration 7408, Loss: 0.054171353578567505\n",
      "Iteration 7409, Loss: 0.05398954078555107\n",
      "Iteration 7410, Loss: 0.054171472787857056\n",
      "Iteration 7411, Loss: 0.05398954078555107\n",
      "Iteration 7412, Loss: 0.054171472787857056\n",
      "Iteration 7413, Loss: 0.05398930609226227\n",
      "Iteration 7414, Loss: 0.05417170375585556\n",
      "Iteration 7415, Loss: 0.053989194333553314\n",
      "Iteration 7416, Loss: 0.05417163297533989\n",
      "Iteration 7417, Loss: 0.05398911237716675\n",
      "Iteration 7418, Loss: 0.05417158827185631\n",
      "Iteration 7419, Loss: 0.053989261388778687\n",
      "Iteration 7420, Loss: 0.054171543568372726\n",
      "Iteration 7421, Loss: 0.05398942157626152\n",
      "Iteration 7422, Loss: 0.05417134612798691\n",
      "Iteration 7423, Loss: 0.0539894700050354\n",
      "Iteration 7424, Loss: 0.054171185940504074\n",
      "Iteration 7425, Loss: 0.05398985743522644\n",
      "Iteration 7426, Loss: 0.05417114123702049\n",
      "Iteration 7427, Loss: 0.05398992821574211\n",
      "Iteration 7428, Loss: 0.054171111434698105\n",
      "Iteration 7429, Loss: 0.053989849984645844\n",
      "Iteration 7430, Loss: 0.054171353578567505\n",
      "Iteration 7431, Loss: 0.05398928374052048\n",
      "Iteration 7432, Loss: 0.05417148023843765\n",
      "Iteration 7433, Loss: 0.053989164531230927\n",
      "Iteration 7434, Loss: 0.05417104810476303\n",
      "Iteration 7435, Loss: 0.053989045321941376\n",
      "Iteration 7436, Loss: 0.054171524941921234\n",
      "Iteration 7437, Loss: 0.05398908257484436\n",
      "Iteration 7438, Loss: 0.05417145416140556\n",
      "Iteration 7439, Loss: 0.053989093750715256\n",
      "Iteration 7440, Loss: 0.0541713610291481\n",
      "Iteration 7441, Loss: 0.05398933216929436\n",
      "Iteration 7442, Loss: 0.0541711263358593\n",
      "Iteration 7443, Loss: 0.053989559412002563\n",
      "Iteration 7444, Loss: 0.054170966148376465\n",
      "Iteration 7445, Loss: 0.053989559412002563\n",
      "Iteration 7446, Loss: 0.05417089909315109\n",
      "Iteration 7447, Loss: 0.053989484906196594\n",
      "Iteration 7448, Loss: 0.05417117103934288\n",
      "Iteration 7449, Loss: 0.05398940294981003\n",
      "Iteration 7450, Loss: 0.0541713684797287\n",
      "Iteration 7451, Loss: 0.05398912727832794\n",
      "Iteration 7452, Loss: 0.05417148768901825\n",
      "Iteration 7453, Loss: 0.05398900806903839\n",
      "Iteration 7454, Loss: 0.054171569645404816\n",
      "Iteration 7455, Loss: 0.05398900806903839\n",
      "Iteration 7456, Loss: 0.05417172238230705\n",
      "Iteration 7457, Loss: 0.053988970816135406\n",
      "Iteration 7458, Loss: 0.054171573370695114\n",
      "Iteration 7459, Loss: 0.05398889631032944\n",
      "Iteration 7460, Loss: 0.054171573370695114\n",
      "Iteration 7461, Loss: 0.05398900434374809\n",
      "Iteration 7462, Loss: 0.05417145416140556\n",
      "Iteration 7463, Loss: 0.05398912727832794\n",
      "Iteration 7464, Loss: 0.054171256721019745\n",
      "Iteration 7465, Loss: 0.05398920178413391\n",
      "Iteration 7466, Loss: 0.05417129397392273\n",
      "Iteration 7467, Loss: 0.05398917198181152\n",
      "Iteration 7468, Loss: 0.05417129397392273\n",
      "Iteration 7469, Loss: 0.05398932099342346\n",
      "Iteration 7470, Loss: 0.05417121201753616\n",
      "Iteration 7471, Loss: 0.05398932844400406\n",
      "Iteration 7472, Loss: 0.05417128652334213\n",
      "Iteration 7473, Loss: 0.05398925393819809\n",
      "Iteration 7474, Loss: 0.05417121201753616\n",
      "Iteration 7475, Loss: 0.05398932844400406\n",
      "Iteration 7476, Loss: 0.05417121201753616\n",
      "Iteration 7477, Loss: 0.05398932844400406\n",
      "Iteration 7478, Loss: 0.054171137511730194\n",
      "Iteration 7479, Loss: 0.05398932844400406\n",
      "Iteration 7480, Loss: 0.05417132377624512\n",
      "Iteration 7481, Loss: 0.05398932099342346\n",
      "Iteration 7482, Loss: 0.05417132377624512\n",
      "Iteration 7483, Loss: 0.05398925393819809\n",
      "Iteration 7484, Loss: 0.05417124554514885\n",
      "Iteration 7485, Loss: 0.05398932099342346\n",
      "Iteration 7486, Loss: 0.05417116731405258\n",
      "Iteration 7487, Loss: 0.05398936569690704\n",
      "Iteration 7488, Loss: 0.05417105555534363\n",
      "Iteration 7489, Loss: 0.053989481180906296\n",
      "Iteration 7490, Loss: 0.05417124554514885\n",
      "Iteration 7491, Loss: 0.053989510983228683\n",
      "Iteration 7492, Loss: 0.05417117476463318\n",
      "Iteration 7493, Loss: 0.05398932099342346\n",
      "Iteration 7494, Loss: 0.05417129024863243\n",
      "Iteration 7495, Loss: 0.05398925393819809\n",
      "Iteration 7496, Loss: 0.05417129397392273\n",
      "Iteration 7497, Loss: 0.053989242762327194\n",
      "Iteration 7498, Loss: 0.05417156219482422\n",
      "Iteration 7499, Loss: 0.05398905277252197\n",
      "Iteration 7500, Loss: 0.054171767085790634\n",
      "Iteration 7501, Loss: 0.0539887361228466\n",
      "Iteration 7502, Loss: 0.05417203903198242\n",
      "Iteration 7503, Loss: 0.0539885014295578\n",
      "Iteration 7504, Loss: 0.05417200177907944\n",
      "Iteration 7505, Loss: 0.053988583385944366\n",
      "Iteration 7506, Loss: 0.05417187511920929\n",
      "Iteration 7507, Loss: 0.05398882180452347\n",
      "Iteration 7508, Loss: 0.05417148396372795\n",
      "Iteration 7509, Loss: 0.05398913472890854\n",
      "Iteration 7510, Loss: 0.05417127534747124\n",
      "Iteration 7511, Loss: 0.05398952215909958\n",
      "Iteration 7512, Loss: 0.05417104810476303\n",
      "Iteration 7513, Loss: 0.053989630192518234\n",
      "Iteration 7514, Loss: 0.054171010851860046\n",
      "Iteration 7515, Loss: 0.053989630192518234\n",
      "Iteration 7516, Loss: 0.05417124927043915\n",
      "Iteration 7517, Loss: 0.05398928374052048\n",
      "Iteration 7518, Loss: 0.05417141318321228\n",
      "Iteration 7519, Loss: 0.05398900806903839\n",
      "Iteration 7520, Loss: 0.05417173355817795\n",
      "Iteration 7521, Loss: 0.05398884415626526\n",
      "Iteration 7522, Loss: 0.05417191982269287\n",
      "Iteration 7523, Loss: 0.05398869514465332\n",
      "Iteration 7524, Loss: 0.05417191982269287\n",
      "Iteration 7525, Loss: 0.053988732397556305\n",
      "Iteration 7526, Loss: 0.054171882569789886\n",
      "Iteration 7527, Loss: 0.05398884415626526\n",
      "Iteration 7528, Loss: 0.054171644151210785\n",
      "Iteration 7529, Loss: 0.05398901551961899\n",
      "Iteration 7530, Loss: 0.054171524941921234\n",
      "Iteration 7531, Loss: 0.05398920923471451\n",
      "Iteration 7532, Loss: 0.05417129397392273\n",
      "Iteration 7533, Loss: 0.05398928374052048\n",
      "Iteration 7534, Loss: 0.05417133495211601\n",
      "Iteration 7535, Loss: 0.053989361971616745\n",
      "Iteration 7536, Loss: 0.054171524941921234\n",
      "Iteration 7537, Loss: 0.053989045321941376\n",
      "Iteration 7538, Loss: 0.054171763360500336\n",
      "Iteration 7539, Loss: 0.05398881435394287\n",
      "Iteration 7540, Loss: 0.05417200177907944\n",
      "Iteration 7541, Loss: 0.05398869514465332\n",
      "Iteration 7542, Loss: 0.05417200177907944\n",
      "Iteration 7543, Loss: 0.05398865044116974\n",
      "Iteration 7544, Loss: 0.05417189002037048\n",
      "Iteration 7545, Loss: 0.053988657891750336\n",
      "Iteration 7546, Loss: 0.054171692579984665\n",
      "Iteration 7547, Loss: 0.05398892983794212\n",
      "Iteration 7548, Loss: 0.054171495139598846\n",
      "Iteration 7549, Loss: 0.05398912727832794\n",
      "Iteration 7550, Loss: 0.05417141318321228\n",
      "Iteration 7551, Loss: 0.05398920178413391\n",
      "Iteration 7552, Loss: 0.054171256721019745\n",
      "Iteration 7553, Loss: 0.053989287465810776\n",
      "Iteration 7554, Loss: 0.054171331226825714\n",
      "Iteration 7555, Loss: 0.053989164531230927\n",
      "Iteration 7556, Loss: 0.054171524941921234\n",
      "Iteration 7557, Loss: 0.053989049047231674\n",
      "Iteration 7558, Loss: 0.0541716068983078\n",
      "Iteration 7559, Loss: 0.05398900434374809\n",
      "Iteration 7560, Loss: 0.05417168140411377\n",
      "Iteration 7561, Loss: 0.05398893356323242\n",
      "Iteration 7562, Loss: 0.054171644151210785\n",
      "Iteration 7563, Loss: 0.05398919805884361\n",
      "Iteration 7564, Loss: 0.054171591997146606\n",
      "Iteration 7565, Loss: 0.05398912727832794\n",
      "Iteration 7566, Loss: 0.05417148396372795\n",
      "Iteration 7567, Loss: 0.053989164531230927\n",
      "Iteration 7568, Loss: 0.05417140945792198\n",
      "Iteration 7569, Loss: 0.053989242762327194\n",
      "Iteration 7570, Loss: 0.05417133495211601\n",
      "Iteration 7571, Loss: 0.053989168256521225\n",
      "Iteration 7572, Loss: 0.05417148396372795\n",
      "Iteration 7573, Loss: 0.053989093750715256\n",
      "Iteration 7574, Loss: 0.054171375930309296\n",
      "Iteration 7575, Loss: 0.05398912727832794\n",
      "Iteration 7576, Loss: 0.054171331226825714\n",
      "Iteration 7577, Loss: 0.05398913472890854\n",
      "Iteration 7578, Loss: 0.05417132377624512\n",
      "Iteration 7579, Loss: 0.053989212960004807\n",
      "Iteration 7580, Loss: 0.0541713647544384\n",
      "Iteration 7581, Loss: 0.053989242762327194\n",
      "Iteration 7582, Loss: 0.05417132377624512\n",
      "Iteration 7583, Loss: 0.05398932099342346\n",
      "Iteration 7584, Loss: 0.0541713684797287\n",
      "Iteration 7585, Loss: 0.05398928374052048\n",
      "Iteration 7586, Loss: 0.05417153239250183\n",
      "Iteration 7587, Loss: 0.053989045321941376\n",
      "Iteration 7588, Loss: 0.054171763360500336\n",
      "Iteration 7589, Loss: 0.05398881062865257\n",
      "Iteration 7590, Loss: 0.05417177081108093\n",
      "Iteration 7591, Loss: 0.05398869514465332\n",
      "Iteration 7592, Loss: 0.05417187511920929\n",
      "Iteration 7593, Loss: 0.05398881435394287\n",
      "Iteration 7594, Loss: 0.05417168140411377\n",
      "Iteration 7595, Loss: 0.05398905277252197\n",
      "Iteration 7596, Loss: 0.05417133495211601\n",
      "Iteration 7597, Loss: 0.053989168256521225\n",
      "Iteration 7598, Loss: 0.054171256721019745\n",
      "Iteration 7599, Loss: 0.053989361971616745\n",
      "Iteration 7600, Loss: 0.05417121574282646\n",
      "Iteration 7601, Loss: 0.053989481180906296\n",
      "Iteration 7602, Loss: 0.05417116731405258\n",
      "Iteration 7603, Loss: 0.053989481180906296\n",
      "Iteration 7604, Loss: 0.05417117476463318\n",
      "Iteration 7605, Loss: 0.05398944020271301\n",
      "Iteration 7606, Loss: 0.05417126044631004\n",
      "Iteration 7607, Loss: 0.053989361971616745\n",
      "Iteration 7608, Loss: 0.054171495139598846\n",
      "Iteration 7609, Loss: 0.05398900434374809\n",
      "Iteration 7610, Loss: 0.05417172238230705\n",
      "Iteration 7611, Loss: 0.05398881435394287\n",
      "Iteration 7612, Loss: 0.054171882569789886\n",
      "Iteration 7613, Loss: 0.05398876965045929\n",
      "Iteration 7614, Loss: 0.05417180061340332\n",
      "Iteration 7615, Loss: 0.05398888885974884\n",
      "Iteration 7616, Loss: 0.054171573370695114\n",
      "Iteration 7617, Loss: 0.053988974541425705\n",
      "Iteration 7618, Loss: 0.05417145416140556\n",
      "Iteration 7619, Loss: 0.05398909002542496\n",
      "Iteration 7620, Loss: 0.05417140945792198\n",
      "Iteration 7621, Loss: 0.053989164531230927\n",
      "Iteration 7622, Loss: 0.05417141318321228\n",
      "Iteration 7623, Loss: 0.05398920178413391\n",
      "Iteration 7624, Loss: 0.05417148396372795\n",
      "Iteration 7625, Loss: 0.053989164531230927\n",
      "Iteration 7626, Loss: 0.054171495139598846\n",
      "Iteration 7627, Loss: 0.05398908257484436\n",
      "Iteration 7628, Loss: 0.054171644151210785\n",
      "Iteration 7629, Loss: 0.053988974541425705\n",
      "Iteration 7630, Loss: 0.054171644151210785\n",
      "Iteration 7631, Loss: 0.053989049047231674\n",
      "Iteration 7632, Loss: 0.05417168140411377\n",
      "Iteration 7633, Loss: 0.05398889631032944\n",
      "Iteration 7634, Loss: 0.05417168140411377\n",
      "Iteration 7635, Loss: 0.05398889631032944\n",
      "Iteration 7636, Loss: 0.054171569645404816\n",
      "Iteration 7637, Loss: 0.05398901551961899\n",
      "Iteration 7638, Loss: 0.054171524941921234\n",
      "Iteration 7639, Loss: 0.053989093750715256\n",
      "Iteration 7640, Loss: 0.05417140573263168\n",
      "Iteration 7641, Loss: 0.05398917198181152\n",
      "Iteration 7642, Loss: 0.05417148396372795\n",
      "Iteration 7643, Loss: 0.053989093750715256\n",
      "Iteration 7644, Loss: 0.05417156219482422\n",
      "Iteration 7645, Loss: 0.05398901551961899\n",
      "Iteration 7646, Loss: 0.05417148768901825\n",
      "Iteration 7647, Loss: 0.053989164531230927\n",
      "Iteration 7648, Loss: 0.05417156219482422\n",
      "Iteration 7649, Loss: 0.05398920178413391\n",
      "Iteration 7650, Loss: 0.05417129397392273\n",
      "Iteration 7651, Loss: 0.053989510983228683\n",
      "Iteration 7652, Loss: 0.05417132377624512\n",
      "Iteration 7653, Loss: 0.05398940294981003\n",
      "Iteration 7654, Loss: 0.054171256721019745\n",
      "Iteration 7655, Loss: 0.05398932099342346\n",
      "Iteration 7656, Loss: 0.05417133495211601\n",
      "Iteration 7657, Loss: 0.05398932099342346\n",
      "Iteration 7658, Loss: 0.0541715994477272\n",
      "Iteration 7659, Loss: 0.05398909002542496\n",
      "Iteration 7660, Loss: 0.05417168140411377\n",
      "Iteration 7661, Loss: 0.05398888885974884\n",
      "Iteration 7662, Loss: 0.054171763360500336\n",
      "Iteration 7663, Loss: 0.05398881062865257\n",
      "Iteration 7664, Loss: 0.05417180061340332\n",
      "Iteration 7665, Loss: 0.05398884415626526\n",
      "Iteration 7666, Loss: 0.05417172238230705\n",
      "Iteration 7667, Loss: 0.053988926112651825\n",
      "Iteration 7668, Loss: 0.05417171120643616\n",
      "Iteration 7669, Loss: 0.05398912355303764\n",
      "Iteration 7670, Loss: 0.054171591997146606\n",
      "Iteration 7671, Loss: 0.05398932099342346\n",
      "Iteration 7672, Loss: 0.05417139455676079\n",
      "Iteration 7673, Loss: 0.05398944020271301\n",
      "Iteration 7674, Loss: 0.05417128652334213\n",
      "Iteration 7675, Loss: 0.05398940294981003\n",
      "Iteration 7676, Loss: 0.05417148396372795\n",
      "Iteration 7677, Loss: 0.05398912355303764\n",
      "Iteration 7678, Loss: 0.05417168140411377\n",
      "Iteration 7679, Loss: 0.053988855332136154\n",
      "Iteration 7680, Loss: 0.05417199060320854\n",
      "Iteration 7681, Loss: 0.05398884415626526\n",
      "Iteration 7682, Loss: 0.05417191982269287\n",
      "Iteration 7683, Loss: 0.05398888140916824\n",
      "Iteration 7684, Loss: 0.05417175590991974\n",
      "Iteration 7685, Loss: 0.053988970816135406\n",
      "Iteration 7686, Loss: 0.054171591997146606\n",
      "Iteration 7687, Loss: 0.053989242762327194\n",
      "Iteration 7688, Loss: 0.0541713647544384\n",
      "Iteration 7689, Loss: 0.05398932844400406\n",
      "Iteration 7690, Loss: 0.0541711263358593\n",
      "Iteration 7691, Loss: 0.05398955196142197\n",
      "Iteration 7692, Loss: 0.05417124554514885\n",
      "Iteration 7693, Loss: 0.053989481180906296\n",
      "Iteration 7694, Loss: 0.05417124554514885\n",
      "Iteration 7695, Loss: 0.053989361971616745\n",
      "Iteration 7696, Loss: 0.05417152866721153\n",
      "Iteration 7697, Loss: 0.053989045321941376\n",
      "Iteration 7698, Loss: 0.05417180061340332\n",
      "Iteration 7699, Loss: 0.05398881062865257\n",
      "Iteration 7700, Loss: 0.05417191982269287\n",
      "Iteration 7701, Loss: 0.0539887361228466\n",
      "Iteration 7702, Loss: 0.05417194962501526\n",
      "Iteration 7703, Loss: 0.05398888513445854\n",
      "Iteration 7704, Loss: 0.054171763360500336\n",
      "Iteration 7705, Loss: 0.05398900434374809\n",
      "Iteration 7706, Loss: 0.0541716031730175\n",
      "Iteration 7707, Loss: 0.05398908257484436\n",
      "Iteration 7708, Loss: 0.05417152866721153\n",
      "Iteration 7709, Loss: 0.053989164531230927\n",
      "Iteration 7710, Loss: 0.054171524941921234\n",
      "Iteration 7711, Loss: 0.05398905277252197\n",
      "Iteration 7712, Loss: 0.054171524941921234\n",
      "Iteration 7713, Loss: 0.05398912355303764\n",
      "Iteration 7714, Loss: 0.0541716068983078\n",
      "Iteration 7715, Loss: 0.05398900806903839\n",
      "Iteration 7716, Loss: 0.05417172238230705\n",
      "Iteration 7717, Loss: 0.053988974541425705\n",
      "Iteration 7718, Loss: 0.05417179316282272\n",
      "Iteration 7719, Loss: 0.05398889631032944\n",
      "Iteration 7720, Loss: 0.05417168140411377\n",
      "Iteration 7721, Loss: 0.05398900434374809\n",
      "Iteration 7722, Loss: 0.0541716031730175\n",
      "Iteration 7723, Loss: 0.053989049047231674\n",
      "Iteration 7724, Loss: 0.05417155846953392\n",
      "Iteration 7725, Loss: 0.053989164531230927\n",
      "Iteration 7726, Loss: 0.054171524941921234\n",
      "Iteration 7727, Loss: 0.053989242762327194\n",
      "Iteration 7728, Loss: 0.0541716031730175\n",
      "Iteration 7729, Loss: 0.053989164531230927\n",
      "Iteration 7730, Loss: 0.054171718657016754\n",
      "Iteration 7731, Loss: 0.05398900434374809\n",
      "Iteration 7732, Loss: 0.05417187511920929\n",
      "Iteration 7733, Loss: 0.053988851606845856\n",
      "Iteration 7734, Loss: 0.05417183041572571\n",
      "Iteration 7735, Loss: 0.05398889631032944\n",
      "Iteration 7736, Loss: 0.05417171120643616\n",
      "Iteration 7737, Loss: 0.05398905277252197\n",
      "Iteration 7738, Loss: 0.054171524941921234\n",
      "Iteration 7739, Loss: 0.053989361971616745\n",
      "Iteration 7740, Loss: 0.05417140573263168\n",
      "Iteration 7741, Loss: 0.05398939177393913\n",
      "Iteration 7742, Loss: 0.054171591997146606\n",
      "Iteration 7743, Loss: 0.05398920178413391\n",
      "Iteration 7744, Loss: 0.054171718657016754\n",
      "Iteration 7745, Loss: 0.053988974541425705\n",
      "Iteration 7746, Loss: 0.05417187139391899\n",
      "Iteration 7747, Loss: 0.053988855332136154\n",
      "Iteration 7748, Loss: 0.054171960800886154\n",
      "Iteration 7749, Loss: 0.053988732397556305\n",
      "Iteration 7750, Loss: 0.054171957075595856\n",
      "Iteration 7751, Loss: 0.05398869141936302\n",
      "Iteration 7752, Loss: 0.054171882569789886\n",
      "Iteration 7753, Loss: 0.05398884415626526\n",
      "Iteration 7754, Loss: 0.05417179688811302\n",
      "Iteration 7755, Loss: 0.05398896336555481\n",
      "Iteration 7756, Loss: 0.05417163297533989\n",
      "Iteration 7757, Loss: 0.053989242762327194\n",
      "Iteration 7758, Loss: 0.054171472787857056\n",
      "Iteration 7759, Loss: 0.05398944020271301\n",
      "Iteration 7760, Loss: 0.05417116731405258\n",
      "Iteration 7761, Loss: 0.05398959666490555\n",
      "Iteration 7762, Loss: 0.05417132005095482\n",
      "Iteration 7763, Loss: 0.05398944020271301\n",
      "Iteration 7764, Loss: 0.05417155846953392\n",
      "Iteration 7765, Loss: 0.053989045321941376\n",
      "Iteration 7766, Loss: 0.054171811789274216\n",
      "Iteration 7767, Loss: 0.05398884415626526\n",
      "Iteration 7768, Loss: 0.054172080010175705\n",
      "Iteration 7769, Loss: 0.05398861691355705\n",
      "Iteration 7770, Loss: 0.05417218804359436\n",
      "Iteration 7771, Loss: 0.05398857593536377\n",
      "Iteration 7772, Loss: 0.05417206883430481\n",
      "Iteration 7773, Loss: 0.05398869514465332\n",
      "Iteration 7774, Loss: 0.05417187139391899\n",
      "Iteration 7775, Loss: 0.05398889631032944\n",
      "Iteration 7776, Loss: 0.05417171120643616\n",
      "Iteration 7777, Loss: 0.053989164531230927\n",
      "Iteration 7778, Loss: 0.054171331226825714\n",
      "Iteration 7779, Loss: 0.05398939549922943\n",
      "Iteration 7780, Loss: 0.0541713610291481\n",
      "Iteration 7781, Loss: 0.053989477455616\n",
      "Iteration 7782, Loss: 0.05417132377624512\n",
      "Iteration 7783, Loss: 0.05398955196142197\n",
      "Iteration 7784, Loss: 0.0541715994477272\n",
      "Iteration 7785, Loss: 0.053989164531230927\n",
      "Iteration 7786, Loss: 0.05417187511920929\n",
      "Iteration 7787, Loss: 0.05398884043097496\n",
      "Iteration 7788, Loss: 0.05417215824127197\n",
      "Iteration 7789, Loss: 0.05398856848478317\n",
      "Iteration 7790, Loss: 0.05417215824127197\n",
      "Iteration 7791, Loss: 0.05398860573768616\n",
      "Iteration 7792, Loss: 0.054171960800886154\n",
      "Iteration 7793, Loss: 0.053988806903362274\n",
      "Iteration 7794, Loss: 0.05417175218462944\n",
      "Iteration 7795, Loss: 0.05398957058787346\n",
      "Iteration 7796, Loss: 0.05417124554514885\n",
      "Iteration 7797, Loss: 0.0539899580180645\n",
      "Iteration 7798, Loss: 0.05417068302631378\n",
      "Iteration 7799, Loss: 0.05399022623896599\n",
      "Iteration 7800, Loss: 0.05417054146528244\n",
      "Iteration 7801, Loss: 0.053990066051483154\n",
      "Iteration 7802, Loss: 0.05417104810476303\n",
      "Iteration 7803, Loss: 0.053989559412002563\n",
      "Iteration 7804, Loss: 0.05417144298553467\n",
      "Iteration 7805, Loss: 0.05398928374052048\n",
      "Iteration 7806, Loss: 0.05417194962501526\n",
      "Iteration 7807, Loss: 0.05398884415626526\n",
      "Iteration 7808, Loss: 0.05417215824127197\n",
      "Iteration 7809, Loss: 0.05398857593536377\n",
      "Iteration 7810, Loss: 0.05417224019765854\n",
      "Iteration 7811, Loss: 0.05398864671587944\n",
      "Iteration 7812, Loss: 0.054172150790691376\n",
      "Iteration 7813, Loss: 0.05398881062865257\n",
      "Iteration 7814, Loss: 0.05417187139391899\n",
      "Iteration 7815, Loss: 0.05398908257484436\n",
      "Iteration 7816, Loss: 0.0541716031730175\n",
      "Iteration 7817, Loss: 0.05398917198181152\n",
      "Iteration 7818, Loss: 0.054171591997146606\n",
      "Iteration 7819, Loss: 0.053989287465810776\n",
      "Iteration 7820, Loss: 0.05417143926024437\n",
      "Iteration 7821, Loss: 0.053989481180906296\n",
      "Iteration 7822, Loss: 0.05417132005095482\n",
      "Iteration 7823, Loss: 0.05398967117071152\n",
      "Iteration 7824, Loss: 0.05417132377624512\n",
      "Iteration 7825, Loss: 0.05398955196142197\n",
      "Iteration 7826, Loss: 0.054171398282051086\n",
      "Iteration 7827, Loss: 0.05398932099342346\n",
      "Iteration 7828, Loss: 0.054171524941921234\n",
      "Iteration 7829, Loss: 0.05398932099342346\n",
      "Iteration 7830, Loss: 0.05417175218462944\n",
      "Iteration 7831, Loss: 0.05398908257484436\n",
      "Iteration 7832, Loss: 0.05417187511920929\n",
      "Iteration 7833, Loss: 0.05398896336555481\n",
      "Iteration 7834, Loss: 0.0541718415915966\n",
      "Iteration 7835, Loss: 0.053988851606845856\n",
      "Iteration 7836, Loss: 0.054171763360500336\n",
      "Iteration 7837, Loss: 0.053988970816135406\n",
      "Iteration 7838, Loss: 0.054171763360500336\n",
      "Iteration 7839, Loss: 0.05398905277252197\n",
      "Iteration 7840, Loss: 0.0541715994477272\n",
      "Iteration 7841, Loss: 0.05398924648761749\n",
      "Iteration 7842, Loss: 0.05417144298553467\n",
      "Iteration 7843, Loss: 0.053989361971616745\n",
      "Iteration 7844, Loss: 0.05417143553495407\n",
      "Iteration 7845, Loss: 0.05398967117071152\n",
      "Iteration 7846, Loss: 0.05417124181985855\n",
      "Iteration 7847, Loss: 0.05398967117071152\n",
      "Iteration 7848, Loss: 0.05417132005095482\n",
      "Iteration 7849, Loss: 0.05398952215909958\n",
      "Iteration 7850, Loss: 0.05417140573263168\n",
      "Iteration 7851, Loss: 0.05398940294981003\n",
      "Iteration 7852, Loss: 0.054171644151210785\n",
      "Iteration 7853, Loss: 0.05398920178413391\n",
      "Iteration 7854, Loss: 0.05417191982269287\n",
      "Iteration 7855, Loss: 0.053988926112651825\n",
      "Iteration 7856, Loss: 0.054172083735466\n",
      "Iteration 7857, Loss: 0.05398872494697571\n",
      "Iteration 7858, Loss: 0.05417215824127197\n",
      "Iteration 7859, Loss: 0.053988657891750336\n",
      "Iteration 7860, Loss: 0.054172076284885406\n",
      "Iteration 7861, Loss: 0.05398870259523392\n",
      "Iteration 7862, Loss: 0.05417183041572571\n",
      "Iteration 7863, Loss: 0.05398912355303764\n",
      "Iteration 7864, Loss: 0.05417155474424362\n",
      "Iteration 7865, Loss: 0.05398940294981003\n",
      "Iteration 7866, Loss: 0.054171234369277954\n",
      "Iteration 7867, Loss: 0.05398979038000107\n",
      "Iteration 7868, Loss: 0.05417107790708542\n",
      "Iteration 7869, Loss: 0.05398990958929062\n",
      "Iteration 7870, Loss: 0.05417100712656975\n",
      "Iteration 7871, Loss: 0.05398982763290405\n",
      "Iteration 7872, Loss: 0.05417129024863243\n",
      "Iteration 7873, Loss: 0.05398932099342346\n",
      "Iteration 7874, Loss: 0.054171718657016754\n",
      "Iteration 7875, Loss: 0.05398896336555481\n",
      "Iteration 7876, Loss: 0.054172080010175705\n",
      "Iteration 7877, Loss: 0.05398876965045929\n",
      "Iteration 7878, Loss: 0.054172273725271225\n",
      "Iteration 7879, Loss: 0.05398865044116974\n",
      "Iteration 7880, Loss: 0.05417230725288391\n",
      "Iteration 7881, Loss: 0.05398857221007347\n",
      "Iteration 7882, Loss: 0.05417203903198242\n",
      "Iteration 7883, Loss: 0.0539887361228466\n",
      "Iteration 7884, Loss: 0.05417191609740257\n",
      "Iteration 7885, Loss: 0.053988974541425705\n",
      "Iteration 7886, Loss: 0.05417175218462944\n",
      "Iteration 7887, Loss: 0.05398913472890854\n",
      "Iteration 7888, Loss: 0.05417156219482422\n",
      "Iteration 7889, Loss: 0.05398925393819809\n",
      "Iteration 7890, Loss: 0.05417148396372795\n",
      "Iteration 7891, Loss: 0.05398940294981003\n",
      "Iteration 7892, Loss: 0.05417143926024437\n",
      "Iteration 7893, Loss: 0.053989481180906296\n",
      "Iteration 7894, Loss: 0.05417148396372795\n",
      "Iteration 7895, Loss: 0.05398932099342346\n",
      "Iteration 7896, Loss: 0.05417155846953392\n",
      "Iteration 7897, Loss: 0.053989242762327194\n",
      "Iteration 7898, Loss: 0.05417179316282272\n",
      "Iteration 7899, Loss: 0.053989119827747345\n",
      "Iteration 7900, Loss: 0.05417199060320854\n",
      "Iteration 7901, Loss: 0.05398884415626526\n",
      "Iteration 7902, Loss: 0.0541718415915966\n",
      "Iteration 7903, Loss: 0.05398889631032944\n",
      "Iteration 7904, Loss: 0.054171763360500336\n",
      "Iteration 7905, Loss: 0.05398905277252197\n",
      "Iteration 7906, Loss: 0.05417156219482422\n",
      "Iteration 7907, Loss: 0.05398920923471451\n",
      "Iteration 7908, Loss: 0.05417140573263168\n",
      "Iteration 7909, Loss: 0.05398932099342346\n",
      "Iteration 7910, Loss: 0.05417140573263168\n",
      "Iteration 7911, Loss: 0.053989287465810776\n",
      "Iteration 7912, Loss: 0.054171524941921234\n",
      "Iteration 7913, Loss: 0.053989287465810776\n",
      "Iteration 7914, Loss: 0.05417148396372795\n",
      "Iteration 7915, Loss: 0.05398932844400406\n",
      "Iteration 7916, Loss: 0.054171524941921234\n",
      "Iteration 7917, Loss: 0.05398935079574585\n",
      "Iteration 7918, Loss: 0.0541715994477272\n",
      "Iteration 7919, Loss: 0.053989212960004807\n",
      "Iteration 7920, Loss: 0.05417148396372795\n",
      "Iteration 7921, Loss: 0.053989242762327194\n",
      "Iteration 7922, Loss: 0.05417175590991974\n",
      "Iteration 7923, Loss: 0.053988974541425705\n",
      "Iteration 7924, Loss: 0.05417187511920929\n",
      "Iteration 7925, Loss: 0.05398892983794212\n",
      "Iteration 7926, Loss: 0.054171882569789886\n",
      "Iteration 7927, Loss: 0.05398892983794212\n",
      "Iteration 7928, Loss: 0.05417187139391899\n",
      "Iteration 7929, Loss: 0.05398888885974884\n",
      "Iteration 7930, Loss: 0.054171763360500336\n",
      "Iteration 7931, Loss: 0.05398901551961899\n",
      "Iteration 7932, Loss: 0.05417167395353317\n",
      "Iteration 7933, Loss: 0.053989242762327194\n",
      "Iteration 7934, Loss: 0.05417148396372795\n",
      "Iteration 7935, Loss: 0.053989242762327194\n",
      "Iteration 7936, Loss: 0.05417148396372795\n",
      "Iteration 7937, Loss: 0.05398940294981003\n",
      "Iteration 7938, Loss: 0.054171524941921234\n",
      "Iteration 7939, Loss: 0.05398928374052048\n",
      "Iteration 7940, Loss: 0.05417152866721153\n",
      "Iteration 7941, Loss: 0.053989164531230927\n",
      "Iteration 7942, Loss: 0.0541718453168869\n",
      "Iteration 7943, Loss: 0.05398888885974884\n",
      "Iteration 7944, Loss: 0.05417203903198242\n",
      "Iteration 7945, Loss: 0.05398868769407272\n",
      "Iteration 7946, Loss: 0.05417200177907944\n",
      "Iteration 7947, Loss: 0.0539887361228466\n",
      "Iteration 7948, Loss: 0.05417194962501526\n",
      "Iteration 7949, Loss: 0.05398892983794212\n",
      "Iteration 7950, Loss: 0.05417167395353317\n",
      "Iteration 7951, Loss: 0.05398924648761749\n",
      "Iteration 7952, Loss: 0.05417143553495407\n",
      "Iteration 7953, Loss: 0.053989630192518234\n",
      "Iteration 7954, Loss: 0.05417116731405258\n",
      "Iteration 7955, Loss: 0.0539897084236145\n",
      "Iteration 7956, Loss: 0.05417127534747124\n",
      "Iteration 7957, Loss: 0.05398958921432495\n",
      "Iteration 7958, Loss: 0.05417144298553467\n",
      "Iteration 7959, Loss: 0.053989361971616745\n",
      "Iteration 7960, Loss: 0.05417152866721153\n",
      "Iteration 7961, Loss: 0.053989049047231674\n",
      "Iteration 7962, Loss: 0.054171912372112274\n",
      "Iteration 7963, Loss: 0.05398895964026451\n",
      "Iteration 7964, Loss: 0.05417199432849884\n",
      "Iteration 7965, Loss: 0.053988777101039886\n",
      "Iteration 7966, Loss: 0.05417200177907944\n",
      "Iteration 7967, Loss: 0.053988657891750336\n",
      "Iteration 7968, Loss: 0.054171960800886154\n",
      "Iteration 7969, Loss: 0.053988926112651825\n",
      "Iteration 7970, Loss: 0.05417187139391899\n",
      "Iteration 7971, Loss: 0.053989093750715256\n",
      "Iteration 7972, Loss: 0.05417156219482422\n",
      "Iteration 7973, Loss: 0.053989242762327194\n",
      "Iteration 7974, Loss: 0.05417143926024437\n",
      "Iteration 7975, Loss: 0.05398944020271301\n",
      "Iteration 7976, Loss: 0.054171256721019745\n",
      "Iteration 7977, Loss: 0.05398955196142197\n",
      "Iteration 7978, Loss: 0.05417129397392273\n",
      "Iteration 7979, Loss: 0.05398940294981003\n",
      "Iteration 7980, Loss: 0.05417155474424362\n",
      "Iteration 7981, Loss: 0.05398928374052048\n",
      "Iteration 7982, Loss: 0.054171718657016754\n",
      "Iteration 7983, Loss: 0.05398900434374809\n",
      "Iteration 7984, Loss: 0.05417180061340332\n",
      "Iteration 7985, Loss: 0.05398896336555481\n",
      "Iteration 7986, Loss: 0.05417183041572571\n",
      "Iteration 7987, Loss: 0.05398912355303764\n",
      "Iteration 7988, Loss: 0.05417163670063019\n",
      "Iteration 7989, Loss: 0.05398913472890854\n",
      "Iteration 7990, Loss: 0.05417148396372795\n",
      "Iteration 7991, Loss: 0.053989287465810776\n",
      "Iteration 7992, Loss: 0.05417144298553467\n",
      "Iteration 7993, Loss: 0.05398932844400406\n",
      "Iteration 7994, Loss: 0.054171524941921234\n",
      "Iteration 7995, Loss: 0.05398928374052048\n",
      "Iteration 7996, Loss: 0.05417153239250183\n",
      "Iteration 7997, Loss: 0.05398909002542496\n",
      "Iteration 7998, Loss: 0.05417172238230705\n",
      "Iteration 7999, Loss: 0.05398903787136078\n",
      "Iteration 8000, Loss: 0.05417194962501526\n",
      "Iteration 8001, Loss: 0.05398896336555481\n",
      "Iteration 8002, Loss: 0.05417191982269287\n",
      "Iteration 8003, Loss: 0.05398888513445854\n",
      "Iteration 8004, Loss: 0.054171957075595856\n",
      "Iteration 8005, Loss: 0.053988851606845856\n",
      "Iteration 8006, Loss: 0.05417187511920929\n",
      "Iteration 8007, Loss: 0.05398892983794212\n",
      "Iteration 8008, Loss: 0.05417183041572571\n",
      "Iteration 8009, Loss: 0.05398912355303764\n",
      "Iteration 8010, Loss: 0.05417155846953392\n",
      "Iteration 8011, Loss: 0.05398932844400406\n",
      "Iteration 8012, Loss: 0.05417140573263168\n",
      "Iteration 8013, Loss: 0.053989481180906296\n",
      "Iteration 8014, Loss: 0.05417129397392273\n",
      "Iteration 8015, Loss: 0.0539894700050354\n",
      "Iteration 8016, Loss: 0.05417148396372795\n",
      "Iteration 8017, Loss: 0.053989242762327194\n",
      "Iteration 8018, Loss: 0.05417168140411377\n",
      "Iteration 8019, Loss: 0.053988974541425705\n",
      "Iteration 8020, Loss: 0.0541718415915966\n",
      "Iteration 8021, Loss: 0.05398888885974884\n",
      "Iteration 8022, Loss: 0.05417191982269287\n",
      "Iteration 8023, Loss: 0.05398896336555481\n",
      "Iteration 8024, Loss: 0.05417191982269287\n",
      "Iteration 8025, Loss: 0.053988851606845856\n",
      "Iteration 8026, Loss: 0.05417187511920929\n",
      "Iteration 8027, Loss: 0.05398896336555481\n",
      "Iteration 8028, Loss: 0.05417172238230705\n",
      "Iteration 8029, Loss: 0.05398909002542496\n",
      "Iteration 8030, Loss: 0.05417163670063019\n",
      "Iteration 8031, Loss: 0.053989242762327194\n",
      "Iteration 8032, Loss: 0.05417148023843765\n",
      "Iteration 8033, Loss: 0.05398933216929436\n",
      "Iteration 8034, Loss: 0.05417143553495407\n",
      "Iteration 8035, Loss: 0.05398933216929436\n",
      "Iteration 8036, Loss: 0.05417133495211601\n",
      "Iteration 8037, Loss: 0.05398928374052048\n",
      "Iteration 8038, Loss: 0.05417163670063019\n",
      "Iteration 8039, Loss: 0.05398912727832794\n",
      "Iteration 8040, Loss: 0.05417172238230705\n",
      "Iteration 8041, Loss: 0.05398900434374809\n",
      "Iteration 8042, Loss: 0.0541718415915966\n",
      "Iteration 8043, Loss: 0.05398895591497421\n",
      "Iteration 8044, Loss: 0.0541718415915966\n",
      "Iteration 8045, Loss: 0.05398907884955406\n",
      "Iteration 8046, Loss: 0.054171960800886154\n",
      "Iteration 8047, Loss: 0.05398896336555481\n",
      "Iteration 8048, Loss: 0.054171957075595856\n",
      "Iteration 8049, Loss: 0.05398881062865257\n",
      "Iteration 8050, Loss: 0.054171960800886154\n",
      "Iteration 8051, Loss: 0.053988806903362274\n",
      "Iteration 8052, Loss: 0.054171960800886154\n",
      "Iteration 8053, Loss: 0.05398884415626526\n",
      "Iteration 8054, Loss: 0.0541718415915966\n",
      "Iteration 8055, Loss: 0.053989045321941376\n",
      "Iteration 8056, Loss: 0.05417163297533989\n",
      "Iteration 8057, Loss: 0.053989168256521225\n",
      "Iteration 8058, Loss: 0.0541713647544384\n",
      "Iteration 8059, Loss: 0.05398936569690704\n",
      "Iteration 8060, Loss: 0.05417121574282646\n",
      "Iteration 8061, Loss: 0.05398944020271301\n",
      "Iteration 8062, Loss: 0.05417132377624512\n",
      "Iteration 8063, Loss: 0.05398951470851898\n",
      "Iteration 8064, Loss: 0.05417129024863243\n",
      "Iteration 8065, Loss: 0.05398935824632645\n",
      "Iteration 8066, Loss: 0.05417155846953392\n",
      "Iteration 8067, Loss: 0.05398927628993988\n",
      "Iteration 8068, Loss: 0.05417172238230705\n",
      "Iteration 8069, Loss: 0.053989164531230927\n",
      "Iteration 8070, Loss: 0.05417183041572571\n",
      "Iteration 8071, Loss: 0.053988926112651825\n",
      "Iteration 8072, Loss: 0.054171837866306305\n",
      "Iteration 8073, Loss: 0.05398896336555481\n",
      "Iteration 8074, Loss: 0.054171644151210785\n",
      "Iteration 8075, Loss: 0.05398927628993988\n",
      "Iteration 8076, Loss: 0.05417148396372795\n",
      "Iteration 8077, Loss: 0.05398940294981003\n",
      "Iteration 8078, Loss: 0.0541713610291481\n",
      "Iteration 8079, Loss: 0.053989481180906296\n",
      "Iteration 8080, Loss: 0.05417124554514885\n",
      "Iteration 8081, Loss: 0.05398952215909958\n",
      "Iteration 8082, Loss: 0.05417124554514885\n",
      "Iteration 8083, Loss: 0.053989481180906296\n",
      "Iteration 8084, Loss: 0.05417124554514885\n",
      "Iteration 8085, Loss: 0.05398940294981003\n",
      "Iteration 8086, Loss: 0.05417148396372795\n",
      "Iteration 8087, Loss: 0.053989242762327194\n",
      "Iteration 8088, Loss: 0.05417175218462944\n",
      "Iteration 8089, Loss: 0.05398912355303764\n",
      "Iteration 8090, Loss: 0.05417194962501526\n",
      "Iteration 8091, Loss: 0.05398896336555481\n",
      "Iteration 8092, Loss: 0.05417191609740257\n",
      "Iteration 8093, Loss: 0.05398896336555481\n",
      "Iteration 8094, Loss: 0.0541718415915966\n",
      "Iteration 8095, Loss: 0.05398893356323242\n",
      "Iteration 8096, Loss: 0.054171644151210785\n",
      "Iteration 8097, Loss: 0.053989093750715256\n",
      "Iteration 8098, Loss: 0.05417156219482422\n",
      "Iteration 8099, Loss: 0.05398913472890854\n",
      "Iteration 8100, Loss: 0.05417148396372795\n",
      "Iteration 8101, Loss: 0.05398935079574585\n",
      "Iteration 8102, Loss: 0.054171450436115265\n",
      "Iteration 8103, Loss: 0.05398927256464958\n",
      "Iteration 8104, Loss: 0.054171569645404816\n",
      "Iteration 8105, Loss: 0.05398901551961899\n",
      "Iteration 8106, Loss: 0.0541716031730175\n",
      "Iteration 8107, Loss: 0.053989093750715256\n",
      "Iteration 8108, Loss: 0.05417141318321228\n",
      "Iteration 8109, Loss: 0.05398925393819809\n",
      "Iteration 8110, Loss: 0.05417124927043915\n",
      "Iteration 8111, Loss: 0.05398937314748764\n",
      "Iteration 8112, Loss: 0.05417128652334213\n",
      "Iteration 8113, Loss: 0.05398944020271301\n",
      "Iteration 8114, Loss: 0.05417132377624512\n",
      "Iteration 8115, Loss: 0.053989291191101074\n",
      "Iteration 8116, Loss: 0.05417148396372795\n",
      "Iteration 8117, Loss: 0.05398928374052048\n",
      "Iteration 8118, Loss: 0.054171644151210785\n",
      "Iteration 8119, Loss: 0.05398908257484436\n",
      "Iteration 8120, Loss: 0.05417183041572571\n",
      "Iteration 8121, Loss: 0.05398889631032944\n",
      "Iteration 8122, Loss: 0.05417191982269287\n",
      "Iteration 8123, Loss: 0.0539887361228466\n",
      "Iteration 8124, Loss: 0.054171960800886154\n",
      "Iteration 8125, Loss: 0.05398881435394287\n",
      "Iteration 8126, Loss: 0.054171763360500336\n",
      "Iteration 8127, Loss: 0.05398901551961899\n",
      "Iteration 8128, Loss: 0.05417156219482422\n",
      "Iteration 8129, Loss: 0.053989242762327194\n",
      "Iteration 8130, Loss: 0.05417140573263168\n",
      "Iteration 8131, Loss: 0.053989481180906296\n",
      "Iteration 8132, Loss: 0.05417124181985855\n",
      "Iteration 8133, Loss: 0.0539897084236145\n",
      "Iteration 8134, Loss: 0.05417109280824661\n",
      "Iteration 8135, Loss: 0.053989481180906296\n",
      "Iteration 8136, Loss: 0.05417122691869736\n",
      "Iteration 8137, Loss: 0.05398932844400406\n",
      "Iteration 8138, Loss: 0.05417152866721153\n",
      "Iteration 8139, Loss: 0.05398901551961899\n",
      "Iteration 8140, Loss: 0.05417180061340332\n",
      "Iteration 8141, Loss: 0.05398888885974884\n",
      "Iteration 8142, Loss: 0.05417203530669212\n",
      "Iteration 8143, Loss: 0.05398888513445854\n",
      "Iteration 8144, Loss: 0.05417191982269287\n",
      "Iteration 8145, Loss: 0.05398889631032944\n",
      "Iteration 8146, Loss: 0.054171763360500336\n",
      "Iteration 8147, Loss: 0.053988978266716\n",
      "Iteration 8148, Loss: 0.05417168140411377\n",
      "Iteration 8149, Loss: 0.053989168256521225\n",
      "Iteration 8150, Loss: 0.05417155846953392\n",
      "Iteration 8151, Loss: 0.05398925393819809\n",
      "Iteration 8152, Loss: 0.054171256721019745\n",
      "Iteration 8153, Loss: 0.053989361971616745\n",
      "Iteration 8154, Loss: 0.054171375930309296\n",
      "Iteration 8155, Loss: 0.05398928374052048\n",
      "Iteration 8156, Loss: 0.054171569645404816\n",
      "Iteration 8157, Loss: 0.05398905277252197\n",
      "Iteration 8158, Loss: 0.05417168140411377\n",
      "Iteration 8159, Loss: 0.05398892983794212\n",
      "Iteration 8160, Loss: 0.054171882569789886\n",
      "Iteration 8161, Loss: 0.053988855332136154\n",
      "Iteration 8162, Loss: 0.054171837866306305\n",
      "Iteration 8163, Loss: 0.053988855332136154\n",
      "Iteration 8164, Loss: 0.054171644151210785\n",
      "Iteration 8165, Loss: 0.05398913472890854\n",
      "Iteration 8166, Loss: 0.0541715994477272\n",
      "Iteration 8167, Loss: 0.053989291191101074\n",
      "Iteration 8168, Loss: 0.05417144298553467\n",
      "Iteration 8169, Loss: 0.053989287465810776\n",
      "Iteration 8170, Loss: 0.05417133495211601\n",
      "Iteration 8171, Loss: 0.05398928374052048\n",
      "Iteration 8172, Loss: 0.05417148768901825\n",
      "Iteration 8173, Loss: 0.053989242762327194\n",
      "Iteration 8174, Loss: 0.05417148768901825\n",
      "Iteration 8175, Loss: 0.05398920178413391\n",
      "Iteration 8176, Loss: 0.05417156219482422\n",
      "Iteration 8177, Loss: 0.05398920178413391\n",
      "Iteration 8178, Loss: 0.054171644151210785\n",
      "Iteration 8179, Loss: 0.053989164531230927\n",
      "Iteration 8180, Loss: 0.05417156219482422\n",
      "Iteration 8181, Loss: 0.053989093750715256\n",
      "Iteration 8182, Loss: 0.054171375930309296\n",
      "Iteration 8183, Loss: 0.053989291191101074\n",
      "Iteration 8184, Loss: 0.05417141318321228\n",
      "Iteration 8185, Loss: 0.053989361971616745\n",
      "Iteration 8186, Loss: 0.05417148768901825\n",
      "Iteration 8187, Loss: 0.05398912727832794\n",
      "Iteration 8188, Loss: 0.05417175218462944\n",
      "Iteration 8189, Loss: 0.053989049047231674\n",
      "Iteration 8190, Loss: 0.054171882569789886\n",
      "Iteration 8191, Loss: 0.05398888885974884\n",
      "Iteration 8192, Loss: 0.05417197197675705\n",
      "Iteration 8193, Loss: 0.05398857593536377\n",
      "Iteration 8194, Loss: 0.054171960800886154\n",
      "Iteration 8195, Loss: 0.053988657891750336\n",
      "Iteration 8196, Loss: 0.054171957075595856\n",
      "Iteration 8197, Loss: 0.053988855332136154\n",
      "Iteration 8198, Loss: 0.054171718657016754\n",
      "Iteration 8199, Loss: 0.05398928374052048\n",
      "Iteration 8200, Loss: 0.054171450436115265\n",
      "Iteration 8201, Loss: 0.05398932844400406\n",
      "Iteration 8202, Loss: 0.05417140573263168\n",
      "Iteration 8203, Loss: 0.05398932099342346\n",
      "Iteration 8204, Loss: 0.05417148396372795\n",
      "Iteration 8205, Loss: 0.05398928374052048\n",
      "Iteration 8206, Loss: 0.054171569645404816\n",
      "Iteration 8207, Loss: 0.053989093750715256\n",
      "Iteration 8208, Loss: 0.05417164787650108\n",
      "Iteration 8209, Loss: 0.05398900434374809\n",
      "Iteration 8210, Loss: 0.054171767085790634\n",
      "Iteration 8211, Loss: 0.05398892983794212\n",
      "Iteration 8212, Loss: 0.05417172238230705\n",
      "Iteration 8213, Loss: 0.05398893356323242\n",
      "Iteration 8214, Loss: 0.054171644151210785\n",
      "Iteration 8215, Loss: 0.05398909002542496\n",
      "Iteration 8216, Loss: 0.05417141318321228\n",
      "Iteration 8217, Loss: 0.053989291191101074\n",
      "Iteration 8218, Loss: 0.054171331226825714\n",
      "Iteration 8219, Loss: 0.053989216685295105\n",
      "Iteration 8220, Loss: 0.05417129397392273\n",
      "Iteration 8221, Loss: 0.05398933216929436\n",
      "Iteration 8222, Loss: 0.05417141318321228\n",
      "Iteration 8223, Loss: 0.05398920923471451\n",
      "Iteration 8224, Loss: 0.054171644151210785\n",
      "Iteration 8225, Loss: 0.05398909002542496\n",
      "Iteration 8226, Loss: 0.05417180061340332\n",
      "Iteration 8227, Loss: 0.05398881435394287\n",
      "Iteration 8228, Loss: 0.05417191982269287\n",
      "Iteration 8229, Loss: 0.0539887361228466\n",
      "Iteration 8230, Loss: 0.05417191982269287\n",
      "Iteration 8231, Loss: 0.053988855332136154\n",
      "Iteration 8232, Loss: 0.05417172238230705\n",
      "Iteration 8233, Loss: 0.05398901551961899\n",
      "Iteration 8234, Loss: 0.05417148396372795\n",
      "Iteration 8235, Loss: 0.053989168256521225\n",
      "Iteration 8236, Loss: 0.0541713647544384\n",
      "Iteration 8237, Loss: 0.053989410400390625\n",
      "Iteration 8238, Loss: 0.05417105555534363\n",
      "Iteration 8239, Loss: 0.05398952215909958\n",
      "Iteration 8240, Loss: 0.054171107709407806\n",
      "Iteration 8241, Loss: 0.05398940294981003\n",
      "Iteration 8242, Loss: 0.054171256721019745\n",
      "Iteration 8243, Loss: 0.05398924648761749\n",
      "Iteration 8244, Loss: 0.054171424359083176\n",
      "Iteration 8245, Loss: 0.05398909002542496\n",
      "Iteration 8246, Loss: 0.05417173355817795\n",
      "Iteration 8247, Loss: 0.053988777101039886\n",
      "Iteration 8248, Loss: 0.05417191982269287\n",
      "Iteration 8249, Loss: 0.05398881435394287\n",
      "Iteration 8250, Loss: 0.0541718415915966\n",
      "Iteration 8251, Loss: 0.053988780826330185\n",
      "Iteration 8252, Loss: 0.054171763360500336\n",
      "Iteration 8253, Loss: 0.05398908257484436\n",
      "Iteration 8254, Loss: 0.05417145416140556\n",
      "Iteration 8255, Loss: 0.05398924648761749\n",
      "Iteration 8256, Loss: 0.05417129397392273\n",
      "Iteration 8257, Loss: 0.05398936569690704\n",
      "Iteration 8258, Loss: 0.05417124927043915\n",
      "Iteration 8259, Loss: 0.05398933216929436\n",
      "Iteration 8260, Loss: 0.05417122691869736\n",
      "Iteration 8261, Loss: 0.053989361971616745\n",
      "Iteration 8262, Loss: 0.05417126417160034\n",
      "Iteration 8263, Loss: 0.05398913472890854\n",
      "Iteration 8264, Loss: 0.054171495139598846\n",
      "Iteration 8265, Loss: 0.053989093750715256\n",
      "Iteration 8266, Loss: 0.0541716143488884\n",
      "Iteration 8267, Loss: 0.05398892983794212\n",
      "Iteration 8268, Loss: 0.05417173355817795\n",
      "Iteration 8269, Loss: 0.053988926112651825\n",
      "Iteration 8270, Loss: 0.05417172238230705\n",
      "Iteration 8271, Loss: 0.05398893356323242\n",
      "Iteration 8272, Loss: 0.054171573370695114\n",
      "Iteration 8273, Loss: 0.05398908257484436\n",
      "Iteration 8274, Loss: 0.054171450436115265\n",
      "Iteration 8275, Loss: 0.053989287465810776\n",
      "Iteration 8276, Loss: 0.0541713684797287\n",
      "Iteration 8277, Loss: 0.05398925393819809\n",
      "Iteration 8278, Loss: 0.054171375930309296\n",
      "Iteration 8279, Loss: 0.05398917198181152\n",
      "Iteration 8280, Loss: 0.054171379655599594\n",
      "Iteration 8281, Loss: 0.05398909002542496\n",
      "Iteration 8282, Loss: 0.054171450436115265\n",
      "Iteration 8283, Loss: 0.05398913472890854\n",
      "Iteration 8284, Loss: 0.054171450436115265\n",
      "Iteration 8285, Loss: 0.053989093750715256\n",
      "Iteration 8286, Loss: 0.05417144298553467\n",
      "Iteration 8287, Loss: 0.05398920178413391\n",
      "Iteration 8288, Loss: 0.05417152866721153\n",
      "Iteration 8289, Loss: 0.05398912355303764\n",
      "Iteration 8290, Loss: 0.05417152866721153\n",
      "Iteration 8291, Loss: 0.053989239037036896\n",
      "Iteration 8292, Loss: 0.054171495139598846\n",
      "Iteration 8293, Loss: 0.05398919805884361\n",
      "Iteration 8294, Loss: 0.054171495139598846\n",
      "Iteration 8295, Loss: 0.053989164531230927\n",
      "Iteration 8296, Loss: 0.054171573370695114\n",
      "Iteration 8297, Loss: 0.053989045321941376\n",
      "Iteration 8298, Loss: 0.05417165160179138\n",
      "Iteration 8299, Loss: 0.05398889631032944\n",
      "Iteration 8300, Loss: 0.0541716143488884\n",
      "Iteration 8301, Loss: 0.053988900035619736\n",
      "Iteration 8302, Loss: 0.0541716143488884\n",
      "Iteration 8303, Loss: 0.053988978266716\n",
      "Iteration 8304, Loss: 0.05417156219482422\n",
      "Iteration 8305, Loss: 0.05398912355303764\n",
      "Iteration 8306, Loss: 0.05417141318321228\n",
      "Iteration 8307, Loss: 0.05398905277252197\n",
      "Iteration 8308, Loss: 0.05417141318321228\n",
      "Iteration 8309, Loss: 0.05398924648761749\n",
      "Iteration 8310, Loss: 0.05417133867740631\n",
      "Iteration 8311, Loss: 0.05398928374052048\n",
      "Iteration 8312, Loss: 0.054171375930309296\n",
      "Iteration 8313, Loss: 0.05398920178413391\n",
      "Iteration 8314, Loss: 0.05417152866721153\n",
      "Iteration 8315, Loss: 0.05398912727832794\n",
      "Iteration 8316, Loss: 0.0541716068983078\n",
      "Iteration 8317, Loss: 0.05398908257484436\n",
      "Iteration 8318, Loss: 0.054171573370695114\n",
      "Iteration 8319, Loss: 0.05398900806903839\n",
      "Iteration 8320, Loss: 0.054171573370695114\n",
      "Iteration 8321, Loss: 0.05398900806903839\n",
      "Iteration 8322, Loss: 0.0541716143488884\n",
      "Iteration 8323, Loss: 0.05398912355303764\n",
      "Iteration 8324, Loss: 0.0541716143488884\n",
      "Iteration 8325, Loss: 0.05398909002542496\n",
      "Iteration 8326, Loss: 0.054171573370695114\n",
      "Iteration 8327, Loss: 0.05398909002542496\n",
      "Iteration 8328, Loss: 0.054171573370695114\n",
      "Iteration 8329, Loss: 0.05398900806903839\n",
      "Iteration 8330, Loss: 0.0541716143488884\n",
      "Iteration 8331, Loss: 0.05398900806903839\n",
      "Iteration 8332, Loss: 0.05417165160179138\n",
      "Iteration 8333, Loss: 0.05398908257484436\n",
      "Iteration 8334, Loss: 0.054171882569789886\n",
      "Iteration 8335, Loss: 0.0539887361228466\n",
      "Iteration 8336, Loss: 0.054171882569789886\n",
      "Iteration 8337, Loss: 0.05398870259523392\n",
      "Iteration 8338, Loss: 0.054171763360500336\n",
      "Iteration 8339, Loss: 0.05398889631032944\n",
      "Iteration 8340, Loss: 0.0541716031730175\n",
      "Iteration 8341, Loss: 0.05398913472890854\n",
      "Iteration 8342, Loss: 0.05417133495211601\n",
      "Iteration 8343, Loss: 0.05398932099342346\n",
      "Iteration 8344, Loss: 0.05417117476463318\n",
      "Iteration 8345, Loss: 0.05398925766348839\n",
      "Iteration 8346, Loss: 0.05417122691869736\n",
      "Iteration 8347, Loss: 0.053989361971616745\n",
      "Iteration 8348, Loss: 0.05417122691869736\n",
      "Iteration 8349, Loss: 0.05398917198181152\n",
      "Iteration 8350, Loss: 0.05417133495211601\n",
      "Iteration 8351, Loss: 0.05398913472890854\n",
      "Iteration 8352, Loss: 0.054171644151210785\n",
      "Iteration 8353, Loss: 0.053988970816135406\n",
      "Iteration 8354, Loss: 0.05417173355817795\n",
      "Iteration 8355, Loss: 0.05398881435394287\n",
      "Iteration 8356, Loss: 0.05417177081108093\n",
      "Iteration 8357, Loss: 0.05398882180452347\n",
      "Iteration 8358, Loss: 0.054171882569789886\n",
      "Iteration 8359, Loss: 0.053988855332136154\n",
      "Iteration 8360, Loss: 0.054171811789274216\n",
      "Iteration 8361, Loss: 0.05398881435394287\n",
      "Iteration 8362, Loss: 0.054171811789274216\n",
      "Iteration 8363, Loss: 0.053988780826330185\n",
      "Iteration 8364, Loss: 0.054171886295080185\n",
      "Iteration 8365, Loss: 0.05398881435394287\n",
      "Iteration 8366, Loss: 0.05417177081108093\n",
      "Iteration 8367, Loss: 0.05398881435394287\n",
      "Iteration 8368, Loss: 0.05417173355817795\n",
      "Iteration 8369, Loss: 0.05398882180452347\n",
      "Iteration 8370, Loss: 0.054171763360500336\n",
      "Iteration 8371, Loss: 0.05398901551961899\n",
      "Iteration 8372, Loss: 0.054171524941921234\n",
      "Iteration 8373, Loss: 0.05398913472890854\n",
      "Iteration 8374, Loss: 0.05417133495211601\n",
      "Iteration 8375, Loss: 0.05398937314748764\n",
      "Iteration 8376, Loss: 0.05417129397392273\n",
      "Iteration 8377, Loss: 0.053989212960004807\n",
      "Iteration 8378, Loss: 0.05417152866721153\n",
      "Iteration 8379, Loss: 0.053989093750715256\n",
      "Iteration 8380, Loss: 0.05417164787650108\n",
      "Iteration 8381, Loss: 0.053989045321941376\n",
      "Iteration 8382, Loss: 0.0541716143488884\n",
      "Iteration 8383, Loss: 0.053988900035619736\n",
      "Iteration 8384, Loss: 0.05417153984308243\n",
      "Iteration 8385, Loss: 0.053988978266716\n",
      "Iteration 8386, Loss: 0.05417145788669586\n",
      "Iteration 8387, Loss: 0.053988978266716\n",
      "Iteration 8388, Loss: 0.0541716031730175\n",
      "Iteration 8389, Loss: 0.053989093750715256\n",
      "Iteration 8390, Loss: 0.05417148396372795\n",
      "Iteration 8391, Loss: 0.053989216685295105\n",
      "Iteration 8392, Loss: 0.05417121574282646\n",
      "Iteration 8393, Loss: 0.053989142179489136\n",
      "Iteration 8394, Loss: 0.054171256721019745\n",
      "Iteration 8395, Loss: 0.05398933216929436\n",
      "Iteration 8396, Loss: 0.054171331226825714\n",
      "Iteration 8397, Loss: 0.053989291191101074\n",
      "Iteration 8398, Loss: 0.054171331226825714\n",
      "Iteration 8399, Loss: 0.05398933216929436\n",
      "Iteration 8400, Loss: 0.05417133495211601\n",
      "Iteration 8401, Loss: 0.05398933216929436\n",
      "Iteration 8402, Loss: 0.05417129397392273\n",
      "Iteration 8403, Loss: 0.053989242762327194\n",
      "Iteration 8404, Loss: 0.05417133867740631\n",
      "Iteration 8405, Loss: 0.05398912727832794\n",
      "Iteration 8406, Loss: 0.05417148768901825\n",
      "Iteration 8407, Loss: 0.053989093750715256\n",
      "Iteration 8408, Loss: 0.05417148768901825\n",
      "Iteration 8409, Loss: 0.053989093750715256\n",
      "Iteration 8410, Loss: 0.054171450436115265\n",
      "Iteration 8411, Loss: 0.053989022970199585\n",
      "Iteration 8412, Loss: 0.05417144298553467\n",
      "Iteration 8413, Loss: 0.053989212960004807\n",
      "Iteration 8414, Loss: 0.054171256721019745\n",
      "Iteration 8415, Loss: 0.05398925393819809\n",
      "Iteration 8416, Loss: 0.05417133495211601\n",
      "Iteration 8417, Loss: 0.05398925766348839\n",
      "Iteration 8418, Loss: 0.05417129397392273\n",
      "Iteration 8419, Loss: 0.05398925393819809\n",
      "Iteration 8420, Loss: 0.05417133495211601\n",
      "Iteration 8421, Loss: 0.053989168256521225\n",
      "Iteration 8422, Loss: 0.054171495139598846\n",
      "Iteration 8423, Loss: 0.05398905277252197\n",
      "Iteration 8424, Loss: 0.05417172238230705\n",
      "Iteration 8425, Loss: 0.05398889631032944\n",
      "Iteration 8426, Loss: 0.054171692579984665\n",
      "Iteration 8427, Loss: 0.053988825529813766\n",
      "Iteration 8428, Loss: 0.054171644151210785\n",
      "Iteration 8429, Loss: 0.053989093750715256\n",
      "Iteration 8430, Loss: 0.05417141318321228\n",
      "Iteration 8431, Loss: 0.05398925393819809\n",
      "Iteration 8432, Loss: 0.054171256721019745\n",
      "Iteration 8433, Loss: 0.053989361971616745\n",
      "Iteration 8434, Loss: 0.05417117476463318\n",
      "Iteration 8435, Loss: 0.05398937314748764\n",
      "Iteration 8436, Loss: 0.05417121574282646\n",
      "Iteration 8437, Loss: 0.053989291191101074\n",
      "Iteration 8438, Loss: 0.05417133495211601\n",
      "Iteration 8439, Loss: 0.05398976430296898\n",
      "Iteration 8440, Loss: 0.054171573370695114\n",
      "Iteration 8441, Loss: 0.05398945137858391\n",
      "Iteration 8442, Loss: 0.054172247648239136\n",
      "Iteration 8443, Loss: 0.05398913845419884\n",
      "Iteration 8444, Loss: 0.05417243763804436\n",
      "Iteration 8445, Loss: 0.05398913472890854\n",
      "Iteration 8446, Loss: 0.054172366857528687\n",
      "Iteration 8447, Loss: 0.05398917198181152\n",
      "Iteration 8448, Loss: 0.05417228490114212\n",
      "Iteration 8449, Loss: 0.053989291191101074\n",
      "Iteration 8450, Loss: 0.05417204648256302\n",
      "Iteration 8451, Loss: 0.05398949235677719\n",
      "Iteration 8452, Loss: 0.054171960800886154\n",
      "Iteration 8453, Loss: 0.05398968979716301\n",
      "Iteration 8454, Loss: 0.054171644151210785\n",
      "Iteration 8455, Loss: 0.05398992821574211\n",
      "Iteration 8456, Loss: 0.05417153239250183\n",
      "Iteration 8457, Loss: 0.05398992821574211\n",
      "Iteration 8458, Loss: 0.05417148768901825\n",
      "Iteration 8459, Loss: 0.05398988723754883\n",
      "Iteration 8460, Loss: 0.05417148768901825\n",
      "Iteration 8461, Loss: 0.05398992821574211\n",
      "Iteration 8462, Loss: 0.054171573370695114\n",
      "Iteration 8463, Loss: 0.05398992821574211\n",
      "Iteration 8464, Loss: 0.054171495139598846\n",
      "Iteration 8465, Loss: 0.05398980900645256\n",
      "Iteration 8466, Loss: 0.054171495139598846\n",
      "Iteration 8467, Loss: 0.0539899617433548\n",
      "Iteration 8468, Loss: 0.05417145416140556\n",
      "Iteration 8469, Loss: 0.05398999899625778\n",
      "Iteration 8470, Loss: 0.05417145416140556\n",
      "Iteration 8471, Loss: 0.05398992821574211\n",
      "Iteration 8472, Loss: 0.054171573370695114\n",
      "Iteration 8473, Loss: 0.05398987978696823\n",
      "Iteration 8474, Loss: 0.0541718415915966\n",
      "Iteration 8475, Loss: 0.053989678621292114\n",
      "Iteration 8476, Loss: 0.05417200177907944\n",
      "Iteration 8477, Loss: 0.05398952588438988\n",
      "Iteration 8478, Loss: 0.054172080010175705\n",
      "Iteration 8479, Loss: 0.05398949235677719\n",
      "Iteration 8480, Loss: 0.054171960800886154\n",
      "Iteration 8481, Loss: 0.05398957058787346\n",
      "Iteration 8482, Loss: 0.05417175590991974\n",
      "Iteration 8483, Loss: 0.05398973822593689\n",
      "Iteration 8484, Loss: 0.054171375930309296\n",
      "Iteration 8485, Loss: 0.0539901927113533\n",
      "Iteration 8486, Loss: 0.054171256721019745\n",
      "Iteration 8487, Loss: 0.05399015545845032\n",
      "Iteration 8488, Loss: 0.05417133867740631\n",
      "Iteration 8489, Loss: 0.05398999899625778\n",
      "Iteration 8490, Loss: 0.05417164787650108\n",
      "Iteration 8491, Loss: 0.05398987978696823\n",
      "Iteration 8492, Loss: 0.05417173355817795\n",
      "Iteration 8493, Loss: 0.05398964509367943\n",
      "Iteration 8494, Loss: 0.05417191982269287\n",
      "Iteration 8495, Loss: 0.05398945510387421\n",
      "Iteration 8496, Loss: 0.05417177081108093\n",
      "Iteration 8497, Loss: 0.05398964881896973\n",
      "Iteration 8498, Loss: 0.05417172610759735\n",
      "Iteration 8499, Loss: 0.05398973077535629\n",
      "Iteration 8500, Loss: 0.054171573370695114\n",
      "Iteration 8501, Loss: 0.05398988723754883\n",
      "Iteration 8502, Loss: 0.05417153239250183\n",
      "Iteration 8503, Loss: 0.0539899580180645\n",
      "Iteration 8504, Loss: 0.054171692579984665\n",
      "Iteration 8505, Loss: 0.05398968979716301\n",
      "Iteration 8506, Loss: 0.054171960800886154\n",
      "Iteration 8507, Loss: 0.053989529609680176\n",
      "Iteration 8508, Loss: 0.054172083735466\n",
      "Iteration 8509, Loss: 0.05398940667510033\n",
      "Iteration 8510, Loss: 0.0541720911860466\n",
      "Iteration 8511, Loss: 0.05398937314748764\n",
      "Iteration 8512, Loss: 0.05417203903198242\n",
      "Iteration 8513, Loss: 0.053989410400390625\n",
      "Iteration 8514, Loss: 0.05417180806398392\n",
      "Iteration 8515, Loss: 0.05398973077535629\n",
      "Iteration 8516, Loss: 0.054171573370695114\n",
      "Iteration 8517, Loss: 0.05398988351225853\n",
      "Iteration 8518, Loss: 0.054171375930309296\n",
      "Iteration 8519, Loss: 0.05399003624916077\n",
      "Iteration 8520, Loss: 0.05417141318321228\n",
      "Iteration 8521, Loss: 0.05399004742503166\n",
      "Iteration 8522, Loss: 0.05417152866721153\n",
      "Iteration 8523, Loss: 0.0539899617433548\n",
      "Iteration 8524, Loss: 0.054171543568372726\n",
      "Iteration 8525, Loss: 0.05398964881896973\n",
      "Iteration 8526, Loss: 0.054171886295080185\n",
      "Iteration 8527, Loss: 0.05398957058787346\n",
      "Iteration 8528, Loss: 0.054172083735466\n",
      "Iteration 8529, Loss: 0.05398925766348839\n",
      "Iteration 8530, Loss: 0.05417224392294884\n",
      "Iteration 8531, Loss: 0.053989291191101074\n",
      "Iteration 8532, Loss: 0.05417216941714287\n",
      "Iteration 8533, Loss: 0.053989287465810776\n",
      "Iteration 8534, Loss: 0.05417215824127197\n",
      "Iteration 8535, Loss: 0.053989484906196594\n",
      "Iteration 8536, Loss: 0.05417191982269287\n",
      "Iteration 8537, Loss: 0.05398961156606674\n",
      "Iteration 8538, Loss: 0.054171763360500336\n",
      "Iteration 8539, Loss: 0.05398980900645256\n",
      "Iteration 8540, Loss: 0.05417153239250183\n",
      "Iteration 8541, Loss: 0.0539899580180645\n",
      "Iteration 8542, Loss: 0.054171573370695114\n",
      "Iteration 8543, Loss: 0.05398988723754883\n",
      "Iteration 8544, Loss: 0.05417138338088989\n",
      "Iteration 8545, Loss: 0.0539899580180645\n",
      "Iteration 8546, Loss: 0.05417165160179138\n",
      "Iteration 8547, Loss: 0.05398983880877495\n",
      "Iteration 8548, Loss: 0.05417177081108093\n",
      "Iteration 8549, Loss: 0.05398961901664734\n",
      "Iteration 8550, Loss: 0.05417166277766228\n",
      "Iteration 8551, Loss: 0.05398965999484062\n",
      "Iteration 8552, Loss: 0.05417177826166153\n",
      "Iteration 8553, Loss: 0.05398964509367943\n",
      "Iteration 8554, Loss: 0.054171811789274216\n",
      "Iteration 8555, Loss: 0.05398968979716301\n",
      "Iteration 8556, Loss: 0.054171882569789886\n",
      "Iteration 8557, Loss: 0.05398968979716301\n",
      "Iteration 8558, Loss: 0.05417191982269287\n",
      "Iteration 8559, Loss: 0.05398964881896973\n",
      "Iteration 8560, Loss: 0.054171811789274216\n",
      "Iteration 8561, Loss: 0.05398957058787346\n",
      "Iteration 8562, Loss: 0.0541718453168869\n",
      "Iteration 8563, Loss: 0.05398961156606674\n",
      "Iteration 8564, Loss: 0.054171882569789886\n",
      "Iteration 8565, Loss: 0.05398961156606674\n",
      "Iteration 8566, Loss: 0.05417180061340332\n",
      "Iteration 8567, Loss: 0.05398968979716301\n",
      "Iteration 8568, Loss: 0.054171692579984665\n",
      "Iteration 8569, Loss: 0.05398988351225853\n",
      "Iteration 8570, Loss: 0.054171573370695114\n",
      "Iteration 8571, Loss: 0.05398988723754883\n",
      "Iteration 8572, Loss: 0.05417173355817795\n",
      "Iteration 8573, Loss: 0.0539897195994854\n",
      "Iteration 8574, Loss: 0.0541718527674675\n",
      "Iteration 8575, Loss: 0.053989604115486145\n",
      "Iteration 8576, Loss: 0.054172009229660034\n",
      "Iteration 8577, Loss: 0.05398940667510033\n",
      "Iteration 8578, Loss: 0.054172247648239136\n",
      "Iteration 8579, Loss: 0.053989291191101074\n",
      "Iteration 8580, Loss: 0.054172322154045105\n",
      "Iteration 8581, Loss: 0.05398917198181152\n",
      "Iteration 8582, Loss: 0.05417228490114212\n",
      "Iteration 8583, Loss: 0.0539892241358757\n",
      "Iteration 8584, Loss: 0.054172083735466\n",
      "Iteration 8585, Loss: 0.05398941785097122\n",
      "Iteration 8586, Loss: 0.0541718527674675\n",
      "Iteration 8587, Loss: 0.05398965999484062\n",
      "Iteration 8588, Loss: 0.05417172610759735\n",
      "Iteration 8589, Loss: 0.05398987978696823\n",
      "Iteration 8590, Loss: 0.05417168140411377\n",
      "Iteration 8591, Loss: 0.05398988723754883\n",
      "Iteration 8592, Loss: 0.05417172238230705\n",
      "Iteration 8593, Loss: 0.05398988351225853\n",
      "Iteration 8594, Loss: 0.0541718527674675\n",
      "Iteration 8595, Loss: 0.05398957058787346\n",
      "Iteration 8596, Loss: 0.05417205020785332\n",
      "Iteration 8597, Loss: 0.053989361971616745\n",
      "Iteration 8598, Loss: 0.054172366857528687\n",
      "Iteration 8599, Loss: 0.05398912727832794\n",
      "Iteration 8600, Loss: 0.05417248606681824\n",
      "Iteration 8601, Loss: 0.053988978266716\n",
      "Iteration 8602, Loss: 0.05417252331972122\n",
      "Iteration 8603, Loss: 0.05398906022310257\n",
      "Iteration 8604, Loss: 0.05417224019765854\n",
      "Iteration 8605, Loss: 0.05398930236697197\n",
      "Iteration 8606, Loss: 0.05417191982269287\n",
      "Iteration 8607, Loss: 0.05398965999484062\n",
      "Iteration 8608, Loss: 0.0541716143488884\n",
      "Iteration 8609, Loss: 0.05398987978696823\n",
      "Iteration 8610, Loss: 0.054171573370695114\n",
      "Iteration 8611, Loss: 0.05398988723754883\n",
      "Iteration 8612, Loss: 0.054171767085790634\n",
      "Iteration 8613, Loss: 0.05398973077535629\n",
      "Iteration 8614, Loss: 0.054171886295080185\n",
      "Iteration 8615, Loss: 0.053989529609680176\n",
      "Iteration 8616, Loss: 0.05417201668024063\n",
      "Iteration 8617, Loss: 0.05398925393819809\n",
      "Iteration 8618, Loss: 0.054172441363334656\n",
      "Iteration 8619, Loss: 0.053989022970199585\n",
      "Iteration 8620, Loss: 0.05417255684733391\n",
      "Iteration 8621, Loss: 0.053988974541425705\n",
      "Iteration 8622, Loss: 0.05417247861623764\n",
      "Iteration 8623, Loss: 0.05398910492658615\n",
      "Iteration 8624, Loss: 0.05417221039533615\n",
      "Iteration 8625, Loss: 0.05398929864168167\n",
      "Iteration 8626, Loss: 0.054172005504369736\n",
      "Iteration 8627, Loss: 0.0539894625544548\n",
      "Iteration 8628, Loss: 0.054171811789274216\n",
      "Iteration 8629, Loss: 0.05398980528116226\n",
      "Iteration 8630, Loss: 0.054171692579984665\n",
      "Iteration 8631, Loss: 0.05398968979716301\n",
      "Iteration 8632, Loss: 0.05417189002037048\n",
      "Iteration 8633, Loss: 0.05398945510387421\n",
      "Iteration 8634, Loss: 0.0541720911860466\n",
      "Iteration 8635, Loss: 0.053989291191101074\n",
      "Iteration 8636, Loss: 0.05417224392294884\n",
      "Iteration 8637, Loss: 0.053989216685295105\n",
      "Iteration 8638, Loss: 0.05417235940694809\n",
      "Iteration 8639, Loss: 0.053989216685295105\n",
      "Iteration 8640, Loss: 0.05417221039533615\n",
      "Iteration 8641, Loss: 0.053989142179489136\n",
      "Iteration 8642, Loss: 0.05417224019765854\n",
      "Iteration 8643, Loss: 0.05398929864168167\n",
      "Iteration 8644, Loss: 0.05417205020785332\n",
      "Iteration 8645, Loss: 0.0539894625544548\n",
      "Iteration 8646, Loss: 0.05417192727327347\n",
      "Iteration 8647, Loss: 0.05398964881896973\n",
      "Iteration 8648, Loss: 0.05417189002037048\n",
      "Iteration 8649, Loss: 0.053989604115486145\n",
      "Iteration 8650, Loss: 0.05417189002037048\n",
      "Iteration 8651, Loss: 0.05398953706026077\n",
      "Iteration 8652, Loss: 0.05417189002037048\n",
      "Iteration 8653, Loss: 0.05398942157626152\n",
      "Iteration 8654, Loss: 0.05417200177907944\n",
      "Iteration 8655, Loss: 0.053989604115486145\n",
      "Iteration 8656, Loss: 0.054171811789274216\n",
      "Iteration 8657, Loss: 0.05398964881896973\n",
      "Iteration 8658, Loss: 0.05417173355817795\n",
      "Iteration 8659, Loss: 0.05398965999484062\n",
      "Iteration 8660, Loss: 0.05417166277766228\n",
      "Iteration 8661, Loss: 0.053989700973033905\n",
      "Iteration 8662, Loss: 0.054171741008758545\n",
      "Iteration 8663, Loss: 0.053989700973033905\n",
      "Iteration 8664, Loss: 0.0541718527674675\n",
      "Iteration 8665, Loss: 0.05398954078555107\n",
      "Iteration 8666, Loss: 0.05417189002037048\n",
      "Iteration 8667, Loss: 0.053989529609680176\n",
      "Iteration 8668, Loss: 0.05417189002037048\n",
      "Iteration 8669, Loss: 0.053989529609680176\n",
      "Iteration 8670, Loss: 0.05417197197675705\n",
      "Iteration 8671, Loss: 0.05398949235677719\n",
      "Iteration 8672, Loss: 0.054172005504369736\n",
      "Iteration 8673, Loss: 0.05398949608206749\n",
      "Iteration 8674, Loss: 0.05417189002037048\n",
      "Iteration 8675, Loss: 0.05398957058787346\n",
      "Iteration 8676, Loss: 0.054171882569789886\n",
      "Iteration 8677, Loss: 0.05398968979716301\n",
      "Iteration 8678, Loss: 0.054171811789274216\n",
      "Iteration 8679, Loss: 0.053989529609680176\n",
      "Iteration 8680, Loss: 0.054171811789274216\n",
      "Iteration 8681, Loss: 0.05398953706026077\n",
      "Iteration 8682, Loss: 0.05417190119624138\n",
      "Iteration 8683, Loss: 0.05398949235677719\n",
      "Iteration 8684, Loss: 0.05417205020785332\n",
      "Iteration 8685, Loss: 0.053989410400390625\n",
      "Iteration 8686, Loss: 0.05417224392294884\n",
      "Iteration 8687, Loss: 0.053989142179489136\n",
      "Iteration 8688, Loss: 0.054172318428754807\n",
      "Iteration 8689, Loss: 0.05398906394839287\n",
      "Iteration 8690, Loss: 0.05417228862643242\n",
      "Iteration 8691, Loss: 0.05398918315768242\n",
      "Iteration 8692, Loss: 0.054172083735466\n",
      "Iteration 8693, Loss: 0.05398930236697197\n",
      "Iteration 8694, Loss: 0.05417203903198242\n",
      "Iteration 8695, Loss: 0.053989529609680176\n",
      "Iteration 8696, Loss: 0.05417192727327347\n",
      "Iteration 8697, Loss: 0.05398957058787346\n",
      "Iteration 8698, Loss: 0.05417180061340332\n",
      "Iteration 8699, Loss: 0.053989581763744354\n",
      "Iteration 8700, Loss: 0.05417177081108093\n",
      "Iteration 8701, Loss: 0.05398973450064659\n",
      "Iteration 8702, Loss: 0.054171621799468994\n",
      "Iteration 8703, Loss: 0.053989849984645844\n",
      "Iteration 8704, Loss: 0.05417180806398392\n",
      "Iteration 8705, Loss: 0.05398961156606674\n",
      "Iteration 8706, Loss: 0.054171930998563766\n",
      "Iteration 8707, Loss: 0.05398945137858391\n",
      "Iteration 8708, Loss: 0.05417205020785332\n",
      "Iteration 8709, Loss: 0.05398933216929436\n",
      "Iteration 8710, Loss: 0.054172124713659286\n",
      "Iteration 8711, Loss: 0.05398937314748764\n",
      "Iteration 8712, Loss: 0.05417203903198242\n",
      "Iteration 8713, Loss: 0.05398949235677719\n",
      "Iteration 8714, Loss: 0.054171930998563766\n",
      "Iteration 8715, Loss: 0.05398949608206749\n",
      "Iteration 8716, Loss: 0.05417189002037048\n",
      "Iteration 8717, Loss: 0.05398957058787346\n",
      "Iteration 8718, Loss: 0.05417189002037048\n",
      "Iteration 8719, Loss: 0.05398964881896973\n",
      "Iteration 8720, Loss: 0.05417192727327347\n",
      "Iteration 8721, Loss: 0.05398894101381302\n",
      "Iteration 8722, Loss: 0.0541720911860466\n",
      "Iteration 8723, Loss: 0.053988587111234665\n",
      "Iteration 8724, Loss: 0.05417259782552719\n",
      "Iteration 8725, Loss: 0.05398818850517273\n",
      "Iteration 8726, Loss: 0.054173510521650314\n",
      "Iteration 8727, Loss: 0.05398784205317497\n",
      "Iteration 8728, Loss: 0.05417346954345703\n",
      "Iteration 8729, Loss: 0.053988002240657806\n",
      "Iteration 8730, Loss: 0.05417303368449211\n",
      "Iteration 8731, Loss: 0.0539885088801384\n",
      "Iteration 8732, Loss: 0.05417262762784958\n",
      "Iteration 8733, Loss: 0.05398906022310257\n",
      "Iteration 8734, Loss: 0.054171960800886154\n",
      "Iteration 8735, Loss: 0.05398949608206749\n",
      "Iteration 8736, Loss: 0.05417163670063019\n",
      "Iteration 8737, Loss: 0.053989849984645844\n",
      "Iteration 8738, Loss: 0.05417124927043915\n",
      "Iteration 8739, Loss: 0.053990043699741364\n",
      "Iteration 8740, Loss: 0.0541713684797287\n",
      "Iteration 8741, Loss: 0.05398999899625778\n",
      "Iteration 8742, Loss: 0.05417150259017944\n",
      "Iteration 8743, Loss: 0.05398961156606674\n",
      "Iteration 8744, Loss: 0.05417200177907944\n",
      "Iteration 8745, Loss: 0.053989291191101074\n",
      "Iteration 8746, Loss: 0.05417228862643242\n",
      "Iteration 8747, Loss: 0.053988903760910034\n",
      "Iteration 8748, Loss: 0.05417252704501152\n",
      "Iteration 8749, Loss: 0.05398886650800705\n",
      "Iteration 8750, Loss: 0.05417256057262421\n",
      "Iteration 8751, Loss: 0.053988710045814514\n",
      "Iteration 8752, Loss: 0.054172515869140625\n",
      "Iteration 8753, Loss: 0.053989019244909286\n",
      "Iteration 8754, Loss: 0.054172318428754807\n",
      "Iteration 8755, Loss: 0.05398913472890854\n",
      "Iteration 8756, Loss: 0.05417215824127197\n",
      "Iteration 8757, Loss: 0.053989142179489136\n",
      "Iteration 8758, Loss: 0.05417212098836899\n",
      "Iteration 8759, Loss: 0.053989291191101074\n",
      "Iteration 8760, Loss: 0.05417215824127197\n",
      "Iteration 8761, Loss: 0.0539892241358757\n",
      "Iteration 8762, Loss: 0.054172080010175705\n",
      "Iteration 8763, Loss: 0.053989261388778687\n",
      "Iteration 8764, Loss: 0.054171930998563766\n",
      "Iteration 8765, Loss: 0.053989410400390625\n",
      "Iteration 8766, Loss: 0.0541718527674675\n",
      "Iteration 8767, Loss: 0.05398957058787346\n",
      "Iteration 8768, Loss: 0.05417165160179138\n",
      "Iteration 8769, Loss: 0.05398968979716301\n",
      "Iteration 8770, Loss: 0.0541718527674675\n",
      "Iteration 8771, Loss: 0.053989529609680176\n",
      "Iteration 8772, Loss: 0.05417189002037048\n",
      "Iteration 8773, Loss: 0.05398937314748764\n",
      "Iteration 8774, Loss: 0.05417216941714287\n",
      "Iteration 8775, Loss: 0.05398913472890854\n",
      "Iteration 8776, Loss: 0.054172173142433167\n",
      "Iteration 8777, Loss: 0.053989022970199585\n",
      "Iteration 8778, Loss: 0.05417231470346451\n",
      "Iteration 8779, Loss: 0.05398910492658615\n",
      "Iteration 8780, Loss: 0.05417215824127197\n",
      "Iteration 8781, Loss: 0.053989212960004807\n",
      "Iteration 8782, Loss: 0.054171960800886154\n",
      "Iteration 8783, Loss: 0.05398945137858391\n",
      "Iteration 8784, Loss: 0.05417180061340332\n",
      "Iteration 8785, Loss: 0.05398968979716301\n",
      "Iteration 8786, Loss: 0.05417164787650108\n",
      "Iteration 8787, Loss: 0.05398968979716301\n",
      "Iteration 8788, Loss: 0.05417164787650108\n",
      "Iteration 8789, Loss: 0.05398973077535629\n",
      "Iteration 8790, Loss: 0.05417177081108093\n",
      "Iteration 8791, Loss: 0.053989529609680176\n",
      "Iteration 8792, Loss: 0.054171960800886154\n",
      "Iteration 8793, Loss: 0.05398952215909958\n",
      "Iteration 8794, Loss: 0.054172124713659286\n",
      "Iteration 8795, Loss: 0.05398924648761749\n",
      "Iteration 8796, Loss: 0.054172247648239136\n",
      "Iteration 8797, Loss: 0.05398905277252197\n",
      "Iteration 8798, Loss: 0.05417228490114212\n",
      "Iteration 8799, Loss: 0.05398894473910332\n",
      "Iteration 8800, Loss: 0.0541720911860466\n",
      "Iteration 8801, Loss: 0.053989261388778687\n",
      "Iteration 8802, Loss: 0.054171767085790634\n",
      "Iteration 8803, Loss: 0.05398949608206749\n",
      "Iteration 8804, Loss: 0.05417168140411377\n",
      "Iteration 8805, Loss: 0.05398976802825928\n",
      "Iteration 8806, Loss: 0.054171524941921234\n",
      "Iteration 8807, Loss: 0.05398984253406525\n",
      "Iteration 8808, Loss: 0.05417141318321228\n",
      "Iteration 8809, Loss: 0.053989775478839874\n",
      "Iteration 8810, Loss: 0.05417153239250183\n",
      "Iteration 8811, Loss: 0.05398965999484062\n",
      "Iteration 8812, Loss: 0.05417180806398392\n",
      "Iteration 8813, Loss: 0.05398956686258316\n",
      "Iteration 8814, Loss: 0.05417203903198242\n",
      "Iteration 8815, Loss: 0.05398937687277794\n",
      "Iteration 8816, Loss: 0.054172124713659286\n",
      "Iteration 8817, Loss: 0.05398917943239212\n",
      "Iteration 8818, Loss: 0.054172322154045105\n",
      "Iteration 8819, Loss: 0.05398906022310257\n",
      "Iteration 8820, Loss: 0.05417225882411003\n",
      "Iteration 8821, Loss: 0.05398901551961899\n",
      "Iteration 8822, Loss: 0.05417228490114212\n",
      "Iteration 8823, Loss: 0.05398910492658615\n",
      "Iteration 8824, Loss: 0.05417203903198242\n",
      "Iteration 8825, Loss: 0.053989410400390625\n",
      "Iteration 8826, Loss: 0.05417180806398392\n",
      "Iteration 8827, Loss: 0.05398954078555107\n",
      "Iteration 8828, Loss: 0.054171692579984665\n",
      "Iteration 8829, Loss: 0.05398949980735779\n",
      "Iteration 8830, Loss: 0.05417173355817795\n",
      "Iteration 8831, Loss: 0.053989507257938385\n",
      "Iteration 8832, Loss: 0.05417180061340332\n",
      "Iteration 8833, Loss: 0.05398961156606674\n",
      "Iteration 8834, Loss: 0.05417191982269287\n",
      "Iteration 8835, Loss: 0.05398945137858391\n",
      "Iteration 8836, Loss: 0.05417196452617645\n",
      "Iteration 8837, Loss: 0.05398933216929436\n",
      "Iteration 8838, Loss: 0.05417204648256302\n",
      "Iteration 8839, Loss: 0.05398925766348839\n",
      "Iteration 8840, Loss: 0.05417203903198242\n",
      "Iteration 8841, Loss: 0.053989410400390625\n",
      "Iteration 8842, Loss: 0.05417197197675705\n",
      "Iteration 8843, Loss: 0.05398956686258316\n",
      "Iteration 8844, Loss: 0.05417200177907944\n",
      "Iteration 8845, Loss: 0.05398934334516525\n",
      "Iteration 8846, Loss: 0.054172031581401825\n",
      "Iteration 8847, Loss: 0.05398941785097122\n",
      "Iteration 8848, Loss: 0.05417191982269287\n",
      "Iteration 8849, Loss: 0.05398949235677719\n",
      "Iteration 8850, Loss: 0.054171882569789886\n",
      "Iteration 8851, Loss: 0.053989529609680176\n",
      "Iteration 8852, Loss: 0.05417191982269287\n",
      "Iteration 8853, Loss: 0.05398953706026077\n",
      "Iteration 8854, Loss: 0.054171882569789886\n",
      "Iteration 8855, Loss: 0.05398945510387421\n",
      "Iteration 8856, Loss: 0.0541718453168869\n",
      "Iteration 8857, Loss: 0.0539894625544548\n",
      "Iteration 8858, Loss: 0.054171811789274216\n",
      "Iteration 8859, Loss: 0.05398949235677719\n",
      "Iteration 8860, Loss: 0.054172009229660034\n",
      "Iteration 8861, Loss: 0.053989410400390625\n",
      "Iteration 8862, Loss: 0.054172273725271225\n",
      "Iteration 8863, Loss: 0.053989291191101074\n",
      "Iteration 8864, Loss: 0.05417235940694809\n",
      "Iteration 8865, Loss: 0.05398913472890854\n",
      "Iteration 8866, Loss: 0.05417255312204361\n",
      "Iteration 8867, Loss: 0.053988900035619736\n",
      "Iteration 8868, Loss: 0.054172586649656296\n",
      "Iteration 8869, Loss: 0.053989093750715256\n",
      "Iteration 8870, Loss: 0.054172318428754807\n",
      "Iteration 8871, Loss: 0.053989212960004807\n",
      "Iteration 8872, Loss: 0.054172080010175705\n",
      "Iteration 8873, Loss: 0.05398944020271301\n",
      "Iteration 8874, Loss: 0.05417192727327347\n",
      "Iteration 8875, Loss: 0.05398960039019585\n",
      "Iteration 8876, Loss: 0.0541718453168869\n",
      "Iteration 8877, Loss: 0.05398964509367943\n",
      "Iteration 8878, Loss: 0.054171886295080185\n",
      "Iteration 8879, Loss: 0.05398957058787346\n",
      "Iteration 8880, Loss: 0.05417196452617645\n",
      "Iteration 8881, Loss: 0.05398937314748764\n",
      "Iteration 8882, Loss: 0.054172080010175705\n",
      "Iteration 8883, Loss: 0.053989261388778687\n",
      "Iteration 8884, Loss: 0.054171960800886154\n",
      "Iteration 8885, Loss: 0.05398945137858391\n",
      "Iteration 8886, Loss: 0.05417177826166153\n",
      "Iteration 8887, Loss: 0.05398964509367943\n",
      "Iteration 8888, Loss: 0.05417180061340332\n",
      "Iteration 8889, Loss: 0.05398976430296898\n",
      "Iteration 8890, Loss: 0.05417191609740257\n",
      "Iteration 8891, Loss: 0.053989723324775696\n",
      "Iteration 8892, Loss: 0.054171957075595856\n",
      "Iteration 8893, Loss: 0.05398960039019585\n",
      "Iteration 8894, Loss: 0.05417215824127197\n",
      "Iteration 8895, Loss: 0.053989287465810776\n",
      "Iteration 8896, Loss: 0.05417247861623764\n",
      "Iteration 8897, Loss: 0.053988974541425705\n",
      "Iteration 8898, Loss: 0.05417244881391525\n",
      "Iteration 8899, Loss: 0.053988903760910034\n",
      "Iteration 8900, Loss: 0.054172299802303314\n",
      "Iteration 8901, Loss: 0.05398913472890854\n",
      "Iteration 8902, Loss: 0.05417216941714287\n",
      "Iteration 8903, Loss: 0.05398924648761749\n",
      "Iteration 8904, Loss: 0.054172005504369736\n",
      "Iteration 8905, Loss: 0.05398940294981003\n",
      "Iteration 8906, Loss: 0.0541718527674675\n",
      "Iteration 8907, Loss: 0.05398960039019585\n",
      "Iteration 8908, Loss: 0.05417169630527496\n",
      "Iteration 8909, Loss: 0.053989559412002563\n",
      "Iteration 8910, Loss: 0.054171811789274216\n",
      "Iteration 8911, Loss: 0.05398964881896973\n",
      "Iteration 8912, Loss: 0.05417172610759735\n",
      "Iteration 8913, Loss: 0.05398961156606674\n",
      "Iteration 8914, Loss: 0.05417189002037048\n",
      "Iteration 8915, Loss: 0.05398960039019585\n",
      "Iteration 8916, Loss: 0.05417205020785332\n",
      "Iteration 8917, Loss: 0.05398917198181152\n",
      "Iteration 8918, Loss: 0.054172396659851074\n",
      "Iteration 8919, Loss: 0.05398893356323242\n",
      "Iteration 8920, Loss: 0.0541725680232048\n",
      "Iteration 8921, Loss: 0.05398894101381302\n",
      "Iteration 8922, Loss: 0.05417259782552719\n",
      "Iteration 8923, Loss: 0.053988974541425705\n",
      "Iteration 8924, Loss: 0.05417243763804436\n",
      "Iteration 8925, Loss: 0.05398905277252197\n",
      "Iteration 8926, Loss: 0.054172318428754807\n",
      "Iteration 8927, Loss: 0.053989097476005554\n",
      "Iteration 8928, Loss: 0.054172053933143616\n",
      "Iteration 8929, Loss: 0.053989216685295105\n",
      "Iteration 8930, Loss: 0.05417192727327347\n",
      "Iteration 8931, Loss: 0.05398960039019585\n",
      "Iteration 8932, Loss: 0.054171767085790634\n",
      "Iteration 8933, Loss: 0.05398968979716301\n",
      "Iteration 8934, Loss: 0.054171692579984665\n",
      "Iteration 8935, Loss: 0.0539897195994854\n",
      "Iteration 8936, Loss: 0.054171882569789886\n",
      "Iteration 8937, Loss: 0.05398949235677719\n",
      "Iteration 8938, Loss: 0.0541718527674675\n",
      "Iteration 8939, Loss: 0.05398945137858391\n",
      "Iteration 8940, Loss: 0.054171886295080185\n",
      "Iteration 8941, Loss: 0.05398949608206749\n",
      "Iteration 8942, Loss: 0.0541718453168869\n",
      "Iteration 8943, Loss: 0.05398964881896973\n",
      "Iteration 8944, Loss: 0.054171811789274216\n",
      "Iteration 8945, Loss: 0.053989529609680176\n",
      "Iteration 8946, Loss: 0.054172005504369736\n",
      "Iteration 8947, Loss: 0.05398944765329361\n",
      "Iteration 8948, Loss: 0.0541720911860466\n",
      "Iteration 8949, Loss: 0.05398925393819809\n",
      "Iteration 8950, Loss: 0.054172247648239136\n",
      "Iteration 8951, Loss: 0.05398906394839287\n",
      "Iteration 8952, Loss: 0.05417224019765854\n",
      "Iteration 8953, Loss: 0.05398917943239212\n",
      "Iteration 8954, Loss: 0.05417224019765854\n",
      "Iteration 8955, Loss: 0.053989212960004807\n",
      "Iteration 8956, Loss: 0.05417205020785332\n",
      "Iteration 8957, Loss: 0.05398940294981003\n",
      "Iteration 8958, Loss: 0.054172005504369736\n",
      "Iteration 8959, Loss: 0.05398944765329361\n",
      "Iteration 8960, Loss: 0.054171811789274216\n",
      "Iteration 8961, Loss: 0.05398952588438988\n",
      "Iteration 8962, Loss: 0.054171811789274216\n",
      "Iteration 8963, Loss: 0.053989604115486145\n",
      "Iteration 8964, Loss: 0.05417165160179138\n",
      "Iteration 8965, Loss: 0.05398973077535629\n",
      "Iteration 8966, Loss: 0.054171692579984665\n",
      "Iteration 8967, Loss: 0.05398976430296898\n",
      "Iteration 8968, Loss: 0.054171882569789886\n",
      "Iteration 8969, Loss: 0.05398949235677719\n",
      "Iteration 8970, Loss: 0.0541720911860466\n",
      "Iteration 8971, Loss: 0.05398925393819809\n",
      "Iteration 8972, Loss: 0.05417243391275406\n",
      "Iteration 8973, Loss: 0.05398905277252197\n",
      "Iteration 8974, Loss: 0.054172322154045105\n",
      "Iteration 8975, Loss: 0.05398906022310257\n",
      "Iteration 8976, Loss: 0.05417216569185257\n",
      "Iteration 8977, Loss: 0.0539892241358757\n",
      "Iteration 8978, Loss: 0.05417192727327347\n",
      "Iteration 8979, Loss: 0.05398957058787346\n",
      "Iteration 8980, Loss: 0.05417172238230705\n",
      "Iteration 8981, Loss: 0.05398964881896973\n",
      "Iteration 8982, Loss: 0.05417153239250183\n",
      "Iteration 8983, Loss: 0.05398988351225853\n",
      "Iteration 8984, Loss: 0.054171573370695114\n",
      "Iteration 8985, Loss: 0.05398983880877495\n",
      "Iteration 8986, Loss: 0.05417165160179138\n",
      "Iteration 8987, Loss: 0.0539901964366436\n",
      "Iteration 8988, Loss: 0.054171930998563766\n",
      "Iteration 8989, Loss: 0.053989849984645844\n",
      "Iteration 8990, Loss: 0.054172687232494354\n",
      "Iteration 8991, Loss: 0.0539897195994854\n",
      "Iteration 8992, Loss: 0.054172955453395844\n",
      "Iteration 8993, Loss: 0.05398937687277794\n",
      "Iteration 8994, Loss: 0.05417291447520256\n",
      "Iteration 8995, Loss: 0.05398957431316376\n",
      "Iteration 8996, Loss: 0.05417260527610779\n",
      "Iteration 8997, Loss: 0.05398976802825928\n",
      "Iteration 8998, Loss: 0.05417248234152794\n",
      "Iteration 8999, Loss: 0.05399029701948166\n",
      "Iteration 9000, Loss: 0.05417224392294884\n",
      "Iteration 9001, Loss: 0.05399087071418762\n",
      "Iteration 9002, Loss: 0.054171882569789886\n",
      "Iteration 9003, Loss: 0.053991034626960754\n",
      "Iteration 9004, Loss: 0.054171256721019745\n",
      "Iteration 9005, Loss: 0.05399115011096001\n",
      "Iteration 9006, Loss: 0.05417153239250183\n",
      "Iteration 9007, Loss: 0.053990673273801804\n",
      "Iteration 9008, Loss: 0.054172009229660034\n",
      "Iteration 9009, Loss: 0.053990285843610764\n",
      "Iteration 9010, Loss: 0.05417252704501152\n",
      "Iteration 9011, Loss: 0.053989849984645844\n",
      "Iteration 9012, Loss: 0.05417291820049286\n",
      "Iteration 9013, Loss: 0.05398960039019585\n",
      "Iteration 9014, Loss: 0.054172929376363754\n",
      "Iteration 9015, Loss: 0.05398957058787346\n",
      "Iteration 9016, Loss: 0.05417285114526749\n",
      "Iteration 9017, Loss: 0.05398954078555107\n",
      "Iteration 9018, Loss: 0.05417260527610779\n",
      "Iteration 9019, Loss: 0.053989820182323456\n",
      "Iteration 9020, Loss: 0.05417244881391525\n",
      "Iteration 9021, Loss: 0.05399012565612793\n",
      "Iteration 9022, Loss: 0.054172318428754807\n",
      "Iteration 9023, Loss: 0.05399013310670853\n",
      "Iteration 9024, Loss: 0.0541723296046257\n",
      "Iteration 9025, Loss: 0.053990207612514496\n",
      "Iteration 9026, Loss: 0.05417235940694809\n",
      "Iteration 9027, Loss: 0.053990017622709274\n",
      "Iteration 9028, Loss: 0.05417254567146301\n",
      "Iteration 9029, Loss: 0.05399000644683838\n",
      "Iteration 9030, Loss: 0.05417248234152794\n",
      "Iteration 9031, Loss: 0.05399011820554733\n",
      "Iteration 9032, Loss: 0.05417240783572197\n",
      "Iteration 9033, Loss: 0.05399004742503166\n",
      "Iteration 9034, Loss: 0.05417252331972122\n",
      "Iteration 9035, Loss: 0.05398999899625778\n",
      "Iteration 9036, Loss: 0.05417267605662346\n",
      "Iteration 9037, Loss: 0.05398999899625778\n",
      "Iteration 9038, Loss: 0.0541725680232048\n",
      "Iteration 9039, Loss: 0.05399003624916077\n",
      "Iteration 9040, Loss: 0.054172635078430176\n",
      "Iteration 9041, Loss: 0.05398992821574211\n",
      "Iteration 9042, Loss: 0.05417260155081749\n",
      "Iteration 9043, Loss: 0.05398992449045181\n",
      "Iteration 9044, Loss: 0.05417252704501152\n",
      "Iteration 9045, Loss: 0.053989969193935394\n",
      "Iteration 9046, Loss: 0.05417240783572197\n",
      "Iteration 9047, Loss: 0.053989969193935394\n",
      "Iteration 9048, Loss: 0.054172441363334656\n",
      "Iteration 9049, Loss: 0.05399000644683838\n",
      "Iteration 9050, Loss: 0.05417235940694809\n",
      "Iteration 9051, Loss: 0.05399013310670853\n",
      "Iteration 9052, Loss: 0.054172247648239136\n",
      "Iteration 9053, Loss: 0.053990136831998825\n",
      "Iteration 9054, Loss: 0.05417213961482048\n",
      "Iteration 9055, Loss: 0.05399024486541748\n",
      "Iteration 9056, Loss: 0.05417228862643242\n",
      "Iteration 9057, Loss: 0.05399012565612793\n",
      "Iteration 9058, Loss: 0.05417244881391525\n",
      "Iteration 9059, Loss: 0.053989917039871216\n",
      "Iteration 9060, Loss: 0.05417260527610779\n",
      "Iteration 9061, Loss: 0.05398968979716301\n",
      "Iteration 9062, Loss: 0.05417276546359062\n",
      "Iteration 9063, Loss: 0.05398968979716301\n",
      "Iteration 9064, Loss: 0.05417275428771973\n",
      "Iteration 9065, Loss: 0.05398973822593689\n",
      "Iteration 9066, Loss: 0.05417267605662346\n",
      "Iteration 9067, Loss: 0.053989969193935394\n",
      "Iteration 9068, Loss: 0.05417252704501152\n",
      "Iteration 9069, Loss: 0.053989969193935394\n",
      "Iteration 9070, Loss: 0.054172366857528687\n",
      "Iteration 9071, Loss: 0.053990088403224945\n",
      "Iteration 9072, Loss: 0.0541723296046257\n",
      "Iteration 9073, Loss: 0.05399015545845032\n",
      "Iteration 9074, Loss: 0.0541723296046257\n",
      "Iteration 9075, Loss: 0.05399000644683838\n",
      "Iteration 9076, Loss: 0.05417237430810928\n",
      "Iteration 9077, Loss: 0.05399004742503166\n",
      "Iteration 9078, Loss: 0.05417248606681824\n",
      "Iteration 9079, Loss: 0.053989969193935394\n",
      "Iteration 9080, Loss: 0.05417252331972122\n",
      "Iteration 9081, Loss: 0.05398992821574211\n",
      "Iteration 9082, Loss: 0.054172441363334656\n",
      "Iteration 9083, Loss: 0.05398982763290405\n",
      "Iteration 9084, Loss: 0.05417228862643242\n",
      "Iteration 9085, Loss: 0.05399013310670853\n",
      "Iteration 9086, Loss: 0.0541720986366272\n",
      "Iteration 9087, Loss: 0.053990285843610764\n",
      "Iteration 9088, Loss: 0.05417227745056152\n",
      "Iteration 9089, Loss: 0.053990285843610764\n",
      "Iteration 9090, Loss: 0.05417218059301376\n",
      "Iteration 9091, Loss: 0.05399016663432121\n",
      "Iteration 9092, Loss: 0.05417244881391525\n",
      "Iteration 9093, Loss: 0.053989969193935394\n",
      "Iteration 9094, Loss: 0.054172635078430176\n",
      "Iteration 9095, Loss: 0.05398988723754883\n",
      "Iteration 9096, Loss: 0.054172761738300323\n",
      "Iteration 9097, Loss: 0.05398976802825928\n",
      "Iteration 9098, Loss: 0.05417264625430107\n",
      "Iteration 9099, Loss: 0.05398976802825928\n",
      "Iteration 9100, Loss: 0.05417264252901077\n",
      "Iteration 9101, Loss: 0.053989700973033905\n",
      "Iteration 9102, Loss: 0.05417248606681824\n",
      "Iteration 9103, Loss: 0.05398993194103241\n",
      "Iteration 9104, Loss: 0.0541723296046257\n",
      "Iteration 9105, Loss: 0.05398997664451599\n",
      "Iteration 9106, Loss: 0.05417221039533615\n",
      "Iteration 9107, Loss: 0.05399024486541748\n",
      "Iteration 9108, Loss: 0.05417235940694809\n",
      "Iteration 9109, Loss: 0.05399031564593315\n",
      "Iteration 9110, Loss: 0.05417221784591675\n",
      "Iteration 9111, Loss: 0.0539901964366436\n",
      "Iteration 9112, Loss: 0.05417248606681824\n",
      "Iteration 9113, Loss: 0.05399008095264435\n",
      "Iteration 9114, Loss: 0.05417255684733391\n",
      "Iteration 9115, Loss: 0.05399000644683838\n",
      "Iteration 9116, Loss: 0.05417252704501152\n",
      "Iteration 9117, Loss: 0.05398992821574211\n",
      "Iteration 9118, Loss: 0.05417248606681824\n",
      "Iteration 9119, Loss: 0.053989969193935394\n",
      "Iteration 9120, Loss: 0.05417255684733391\n",
      "Iteration 9121, Loss: 0.05399003624916077\n",
      "Iteration 9122, Loss: 0.05417244881391525\n",
      "Iteration 9123, Loss: 0.05399007722735405\n",
      "Iteration 9124, Loss: 0.05417248606681824\n",
      "Iteration 9125, Loss: 0.05398999899625778\n",
      "Iteration 9126, Loss: 0.05417260527610779\n",
      "Iteration 9127, Loss: 0.05398976802825928\n",
      "Iteration 9128, Loss: 0.054172635078430176\n",
      "Iteration 9129, Loss: 0.05398988723754883\n",
      "Iteration 9130, Loss: 0.05417248606681824\n",
      "Iteration 9131, Loss: 0.05398978292942047\n",
      "Iteration 9132, Loss: 0.054172366857528687\n",
      "Iteration 9133, Loss: 0.05399012565612793\n",
      "Iteration 9134, Loss: 0.054172247648239136\n",
      "Iteration 9135, Loss: 0.0539902001619339\n",
      "Iteration 9136, Loss: 0.05417236313223839\n",
      "Iteration 9137, Loss: 0.0539901964366436\n",
      "Iteration 9138, Loss: 0.054172366857528687\n",
      "Iteration 9139, Loss: 0.05399012193083763\n",
      "Iteration 9140, Loss: 0.05417236313223839\n",
      "Iteration 9141, Loss: 0.05398993939161301\n",
      "Iteration 9142, Loss: 0.054172366857528687\n",
      "Iteration 9143, Loss: 0.05399012193083763\n",
      "Iteration 9144, Loss: 0.054172366857528687\n",
      "Iteration 9145, Loss: 0.05399008095264435\n",
      "Iteration 9146, Loss: 0.05417244881391525\n",
      "Iteration 9147, Loss: 0.05399000644683838\n",
      "Iteration 9148, Loss: 0.05417252704501152\n",
      "Iteration 9149, Loss: 0.05398985370993614\n",
      "Iteration 9150, Loss: 0.05417260527610779\n",
      "Iteration 9151, Loss: 0.05398977920413017\n",
      "Iteration 9152, Loss: 0.05417267605662346\n",
      "Iteration 9153, Loss: 0.053989849984645844\n",
      "Iteration 9154, Loss: 0.05417240783572197\n",
      "Iteration 9155, Loss: 0.05398993194103241\n",
      "Iteration 9156, Loss: 0.05417236313223839\n",
      "Iteration 9157, Loss: 0.053990088403224945\n",
      "Iteration 9158, Loss: 0.054172128438949585\n",
      "Iteration 9159, Loss: 0.053990211337804794\n",
      "Iteration 9160, Loss: 0.054172128438949585\n",
      "Iteration 9161, Loss: 0.05399036407470703\n",
      "Iteration 9162, Loss: 0.054172173142433167\n",
      "Iteration 9163, Loss: 0.053990285843610764\n",
      "Iteration 9164, Loss: 0.054172333329916\n",
      "Iteration 9165, Loss: 0.053989969193935394\n",
      "Iteration 9166, Loss: 0.0541725680232048\n",
      "Iteration 9167, Loss: 0.05398976802825928\n",
      "Iteration 9168, Loss: 0.05417260527610779\n",
      "Iteration 9169, Loss: 0.05398961901664734\n",
      "Iteration 9170, Loss: 0.054172635078430176\n",
      "Iteration 9171, Loss: 0.053989704698324203\n",
      "Iteration 9172, Loss: 0.05417240783572197\n",
      "Iteration 9173, Loss: 0.05399005115032196\n",
      "Iteration 9174, Loss: 0.054172247648239136\n",
      "Iteration 9175, Loss: 0.05399016663432121\n",
      "Iteration 9176, Loss: 0.05417221039533615\n",
      "Iteration 9177, Loss: 0.05399031937122345\n",
      "Iteration 9178, Loss: 0.054172128438949585\n",
      "Iteration 9179, Loss: 0.05399031564593315\n",
      "Iteration 9180, Loss: 0.05417221412062645\n",
      "Iteration 9181, Loss: 0.05399012193083763\n",
      "Iteration 9182, Loss: 0.054172299802303314\n",
      "Iteration 9183, Loss: 0.05399012193083763\n",
      "Iteration 9184, Loss: 0.054172441363334656\n",
      "Iteration 9185, Loss: 0.05399004742503166\n",
      "Iteration 9186, Loss: 0.05417259782552719\n",
      "Iteration 9187, Loss: 0.053989164531230927\n",
      "Iteration 9188, Loss: 0.05417267605662346\n",
      "Iteration 9189, Loss: 0.05398915335536003\n",
      "Iteration 9190, Loss: 0.054171912372112274\n",
      "Iteration 9191, Loss: 0.053989045321941376\n",
      "Iteration 9192, Loss: 0.05417190492153168\n",
      "Iteration 9193, Loss: 0.053989239037036896\n",
      "Iteration 9194, Loss: 0.05417182296514511\n",
      "Iteration 9195, Loss: 0.05398939177393913\n",
      "Iteration 9196, Loss: 0.05417182296514511\n",
      "Iteration 9197, Loss: 0.05398942530155182\n",
      "Iteration 9198, Loss: 0.05417175218462944\n",
      "Iteration 9199, Loss: 0.053989313542842865\n",
      "Iteration 9200, Loss: 0.054171860218048096\n",
      "Iteration 9201, Loss: 0.05398927256464958\n",
      "Iteration 9202, Loss: 0.054171860218048096\n",
      "Iteration 9203, Loss: 0.05398935079574585\n",
      "Iteration 9204, Loss: 0.05417170748114586\n",
      "Iteration 9205, Loss: 0.0539894700050354\n",
      "Iteration 9206, Loss: 0.054171666502952576\n",
      "Iteration 9207, Loss: 0.0539894700050354\n",
      "Iteration 9208, Loss: 0.05417158827185631\n",
      "Iteration 9209, Loss: 0.05398961901664734\n",
      "Iteration 9210, Loss: 0.05417158827185631\n",
      "Iteration 9211, Loss: 0.0539894662797451\n",
      "Iteration 9212, Loss: 0.0541716031730175\n",
      "Iteration 9213, Loss: 0.05398939177393913\n",
      "Iteration 9214, Loss: 0.05417175218462944\n",
      "Iteration 9215, Loss: 0.05398935079574585\n",
      "Iteration 9216, Loss: 0.05417182669043541\n",
      "Iteration 9217, Loss: 0.053989313542842865\n",
      "Iteration 9218, Loss: 0.054171718657016754\n",
      "Iteration 9219, Loss: 0.05398927256464958\n",
      "Iteration 9220, Loss: 0.05417190119624138\n",
      "Iteration 9221, Loss: 0.05398938059806824\n",
      "Iteration 9222, Loss: 0.054171860218048096\n",
      "Iteration 9223, Loss: 0.053989313542842865\n",
      "Iteration 9224, Loss: 0.054171741008758545\n",
      "Iteration 9225, Loss: 0.053989432752132416\n",
      "Iteration 9226, Loss: 0.054171666502952576\n",
      "Iteration 9227, Loss: 0.05398954451084137\n",
      "Iteration 9228, Loss: 0.05417166277766228\n",
      "Iteration 9229, Loss: 0.05398955196142197\n",
      "Iteration 9230, Loss: 0.05417166277766228\n",
      "Iteration 9231, Loss: 0.05398967117071152\n",
      "Iteration 9232, Loss: 0.054171621799468994\n",
      "Iteration 9233, Loss: 0.05398954451084137\n",
      "Iteration 9234, Loss: 0.05417182296514511\n",
      "Iteration 9235, Loss: 0.05398927256464958\n",
      "Iteration 9236, Loss: 0.05417191609740257\n",
      "Iteration 9237, Loss: 0.05398903414607048\n",
      "Iteration 9238, Loss: 0.05417221784591675\n",
      "Iteration 9239, Loss: 0.05398891493678093\n",
      "Iteration 9240, Loss: 0.05417218059301376\n",
      "Iteration 9241, Loss: 0.05398907512426376\n",
      "Iteration 9242, Loss: 0.054171979427337646\n",
      "Iteration 9243, Loss: 0.05398908257484436\n",
      "Iteration 9244, Loss: 0.05417178198695183\n",
      "Iteration 9245, Loss: 0.05398940294981003\n",
      "Iteration 9246, Loss: 0.054171621799468994\n",
      "Iteration 9247, Loss: 0.05398955196142197\n",
      "Iteration 9248, Loss: 0.054171543568372726\n",
      "Iteration 9249, Loss: 0.05398965999484062\n",
      "Iteration 9250, Loss: 0.05417162925004959\n",
      "Iteration 9251, Loss: 0.05398955196142197\n",
      "Iteration 9252, Loss: 0.05417178198695183\n",
      "Iteration 9253, Loss: 0.0539892315864563\n",
      "Iteration 9254, Loss: 0.0541720986366272\n",
      "Iteration 9255, Loss: 0.05398891866207123\n",
      "Iteration 9256, Loss: 0.05417225882411003\n",
      "Iteration 9257, Loss: 0.053988873958587646\n",
      "Iteration 9258, Loss: 0.05417218059301376\n",
      "Iteration 9259, Loss: 0.05398895964026451\n",
      "Iteration 9260, Loss: 0.05417194217443466\n",
      "Iteration 9261, Loss: 0.05398927256464958\n",
      "Iteration 9262, Loss: 0.05417151376605034\n",
      "Iteration 9263, Loss: 0.053989626467227936\n",
      "Iteration 9264, Loss: 0.054171353578567505\n",
      "Iteration 9265, Loss: 0.0539897084236145\n",
      "Iteration 9266, Loss: 0.05417119711637497\n",
      "Iteration 9267, Loss: 0.053989898413419724\n",
      "Iteration 9268, Loss: 0.05417127534747124\n",
      "Iteration 9269, Loss: 0.053989749401807785\n",
      "Iteration 9270, Loss: 0.054171543568372726\n",
      "Iteration 9271, Loss: 0.053989507257938385\n",
      "Iteration 9272, Loss: 0.05417182296514511\n",
      "Iteration 9273, Loss: 0.053989164531230927\n",
      "Iteration 9274, Loss: 0.05417194962501526\n",
      "Iteration 9275, Loss: 0.053989000618457794\n",
      "Iteration 9276, Loss: 0.05417199060320854\n",
      "Iteration 9277, Loss: 0.05398896336555481\n",
      "Iteration 9278, Loss: 0.05417190492153168\n",
      "Iteration 9279, Loss: 0.053989045321941376\n",
      "Iteration 9280, Loss: 0.054171741008758545\n",
      "Iteration 9281, Loss: 0.05398924648761749\n",
      "Iteration 9282, Loss: 0.05417155474424362\n",
      "Iteration 9283, Loss: 0.05398940294981003\n",
      "Iteration 9284, Loss: 0.05417155474424362\n",
      "Iteration 9285, Loss: 0.05398951470851898\n",
      "Iteration 9286, Loss: 0.05417162925004959\n",
      "Iteration 9287, Loss: 0.05398955196142197\n",
      "Iteration 9288, Loss: 0.05417140573263168\n",
      "Iteration 9289, Loss: 0.053989510983228683\n",
      "Iteration 9290, Loss: 0.05417163297533989\n",
      "Iteration 9291, Loss: 0.053989432752132416\n",
      "Iteration 9292, Loss: 0.054171718657016754\n",
      "Iteration 9293, Loss: 0.05398934707045555\n",
      "Iteration 9294, Loss: 0.05417171120643616\n",
      "Iteration 9295, Loss: 0.05398928374052048\n",
      "Iteration 9296, Loss: 0.05417182296514511\n",
      "Iteration 9297, Loss: 0.05398920178413391\n",
      "Iteration 9298, Loss: 0.05417183041572571\n",
      "Iteration 9299, Loss: 0.05398919805884361\n",
      "Iteration 9300, Loss: 0.05417187139391899\n",
      "Iteration 9301, Loss: 0.05398912355303764\n",
      "Iteration 9302, Loss: 0.05417187139391899\n",
      "Iteration 9303, Loss: 0.05398920178413391\n",
      "Iteration 9304, Loss: 0.05417183041572571\n",
      "Iteration 9305, Loss: 0.05398928374052048\n",
      "Iteration 9306, Loss: 0.0541715994477272\n",
      "Iteration 9307, Loss: 0.053989432752132416\n",
      "Iteration 9308, Loss: 0.05417170375585556\n",
      "Iteration 9309, Loss: 0.0539894700050354\n",
      "Iteration 9310, Loss: 0.05417166277766228\n",
      "Iteration 9311, Loss: 0.0539894700050354\n",
      "Iteration 9312, Loss: 0.05417158827185631\n",
      "Iteration 9313, Loss: 0.05398944020271301\n",
      "Iteration 9314, Loss: 0.05417162925004959\n",
      "Iteration 9315, Loss: 0.053989510983228683\n",
      "Iteration 9316, Loss: 0.05417151376605034\n",
      "Iteration 9317, Loss: 0.0539894700050354\n",
      "Iteration 9318, Loss: 0.054171524941921234\n",
      "Iteration 9319, Loss: 0.053989510983228683\n",
      "Iteration 9320, Loss: 0.054171524941921234\n",
      "Iteration 9321, Loss: 0.0539894662797451\n",
      "Iteration 9322, Loss: 0.0541716031730175\n",
      "Iteration 9323, Loss: 0.053989242762327194\n",
      "Iteration 9324, Loss: 0.054171763360500336\n",
      "Iteration 9325, Loss: 0.05398908257484436\n",
      "Iteration 9326, Loss: 0.05417202413082123\n",
      "Iteration 9327, Loss: 0.053989000618457794\n",
      "Iteration 9328, Loss: 0.054171957075595856\n",
      "Iteration 9329, Loss: 0.05398892983794212\n",
      "Iteration 9330, Loss: 0.05417179316282272\n",
      "Iteration 9331, Loss: 0.05398912355303764\n",
      "Iteration 9332, Loss: 0.05417151376605034\n",
      "Iteration 9333, Loss: 0.05398952215909958\n",
      "Iteration 9334, Loss: 0.054171279072761536\n",
      "Iteration 9335, Loss: 0.053989559412002563\n",
      "Iteration 9336, Loss: 0.05417127534747124\n",
      "Iteration 9337, Loss: 0.05398990958929062\n",
      "Iteration 9338, Loss: 0.05417128652334213\n",
      "Iteration 9339, Loss: 0.05398973822593689\n",
      "Iteration 9340, Loss: 0.05417148396372795\n",
      "Iteration 9341, Loss: 0.053989313542842865\n",
      "Iteration 9342, Loss: 0.05417187511920929\n",
      "Iteration 9343, Loss: 0.053988926112651825\n",
      "Iteration 9344, Loss: 0.05417210981249809\n",
      "Iteration 9345, Loss: 0.05398888513445854\n",
      "Iteration 9346, Loss: 0.05417218804359436\n",
      "Iteration 9347, Loss: 0.05398876592516899\n",
      "Iteration 9348, Loss: 0.05417206883430481\n",
      "Iteration 9349, Loss: 0.05398888513445854\n",
      "Iteration 9350, Loss: 0.05417199060320854\n",
      "Iteration 9351, Loss: 0.053989045321941376\n",
      "Iteration 9352, Loss: 0.05417171120643616\n",
      "Iteration 9353, Loss: 0.05398935079574585\n",
      "Iteration 9354, Loss: 0.05417155474424362\n",
      "Iteration 9355, Loss: 0.05398958921432495\n",
      "Iteration 9356, Loss: 0.0541713647544384\n",
      "Iteration 9357, Loss: 0.053989484906196594\n",
      "Iteration 9358, Loss: 0.054171353578567505\n",
      "Iteration 9359, Loss: 0.053989630192518234\n",
      "Iteration 9360, Loss: 0.05417148396372795\n",
      "Iteration 9361, Loss: 0.053989510983228683\n",
      "Iteration 9362, Loss: 0.05417167395353317\n",
      "Iteration 9363, Loss: 0.053989239037036896\n",
      "Iteration 9364, Loss: 0.05417194962501526\n",
      "Iteration 9365, Loss: 0.05398915335536003\n",
      "Iteration 9366, Loss: 0.05417172238230705\n",
      "Iteration 9367, Loss: 0.05398912355303764\n",
      "Iteration 9368, Loss: 0.05417179316282272\n",
      "Iteration 9369, Loss: 0.05398927256464958\n",
      "Iteration 9370, Loss: 0.05417182669043541\n",
      "Iteration 9371, Loss: 0.053989313542842865\n",
      "Iteration 9372, Loss: 0.05417179316282272\n",
      "Iteration 9373, Loss: 0.053989239037036896\n",
      "Iteration 9374, Loss: 0.054171763360500336\n",
      "Iteration 9375, Loss: 0.053989194333553314\n",
      "Iteration 9376, Loss: 0.05417180061340332\n",
      "Iteration 9377, Loss: 0.0539892315864563\n",
      "Iteration 9378, Loss: 0.0541718415915966\n",
      "Iteration 9379, Loss: 0.05398919805884361\n",
      "Iteration 9380, Loss: 0.054171837866306305\n",
      "Iteration 9381, Loss: 0.053989194333553314\n",
      "Iteration 9382, Loss: 0.054171912372112274\n",
      "Iteration 9383, Loss: 0.05398907512426376\n",
      "Iteration 9384, Loss: 0.0541718415915966\n",
      "Iteration 9385, Loss: 0.05398908257484436\n",
      "Iteration 9386, Loss: 0.05417179316282272\n",
      "Iteration 9387, Loss: 0.05398900806903839\n",
      "Iteration 9388, Loss: 0.0541715994477272\n",
      "Iteration 9389, Loss: 0.05398932099342346\n",
      "Iteration 9390, Loss: 0.05417155846953392\n",
      "Iteration 9391, Loss: 0.05398940294981003\n",
      "Iteration 9392, Loss: 0.05417155474424362\n",
      "Iteration 9393, Loss: 0.053989510983228683\n",
      "Iteration 9394, Loss: 0.054171591997146606\n",
      "Iteration 9395, Loss: 0.053989432752132416\n",
      "Iteration 9396, Loss: 0.05417179316282272\n",
      "Iteration 9397, Loss: 0.053989313542842865\n",
      "Iteration 9398, Loss: 0.05417183041572571\n",
      "Iteration 9399, Loss: 0.053989194333553314\n",
      "Iteration 9400, Loss: 0.05417179316282272\n",
      "Iteration 9401, Loss: 0.053989239037036896\n",
      "Iteration 9402, Loss: 0.05417163670063019\n",
      "Iteration 9403, Loss: 0.05398939177393913\n",
      "Iteration 9404, Loss: 0.05417163297533989\n",
      "Iteration 9405, Loss: 0.0539894700050354\n",
      "Iteration 9406, Loss: 0.05417151749134064\n",
      "Iteration 9407, Loss: 0.053989477455616\n",
      "Iteration 9408, Loss: 0.054171524941921234\n",
      "Iteration 9409, Loss: 0.05398955196142197\n",
      "Iteration 9410, Loss: 0.054171524941921234\n",
      "Iteration 9411, Loss: 0.05398935079574585\n",
      "Iteration 9412, Loss: 0.05417175218462944\n",
      "Iteration 9413, Loss: 0.0539892315864563\n",
      "Iteration 9414, Loss: 0.05417175218462944\n",
      "Iteration 9415, Loss: 0.053989239037036896\n",
      "Iteration 9416, Loss: 0.05417171120643616\n",
      "Iteration 9417, Loss: 0.05398927628993988\n",
      "Iteration 9418, Loss: 0.054171591997146606\n",
      "Iteration 9419, Loss: 0.05398932844400406\n",
      "Iteration 9420, Loss: 0.05417143926024437\n",
      "Iteration 9421, Loss: 0.05398952215909958\n",
      "Iteration 9422, Loss: 0.05417146906256676\n",
      "Iteration 9423, Loss: 0.05398951470851898\n",
      "Iteration 9424, Loss: 0.05417148023843765\n",
      "Iteration 9425, Loss: 0.05398958921432495\n",
      "Iteration 9426, Loss: 0.054171591997146606\n",
      "Iteration 9427, Loss: 0.05398927256464958\n",
      "Iteration 9428, Loss: 0.05417175590991974\n",
      "Iteration 9429, Loss: 0.05398908257484436\n",
      "Iteration 9430, Loss: 0.05417191982269287\n",
      "Iteration 9431, Loss: 0.053988926112651825\n",
      "Iteration 9432, Loss: 0.054172031581401825\n",
      "Iteration 9433, Loss: 0.05398896336555481\n",
      "Iteration 9434, Loss: 0.05417200177907944\n",
      "Iteration 9435, Loss: 0.05398888885974884\n",
      "Iteration 9436, Loss: 0.05417199060320854\n",
      "Iteration 9437, Loss: 0.05398893356323242\n",
      "Iteration 9438, Loss: 0.054171644151210785\n",
      "Iteration 9439, Loss: 0.053989242762327194\n",
      "Iteration 9440, Loss: 0.05417139455676079\n",
      "Iteration 9441, Loss: 0.05398959666490555\n",
      "Iteration 9442, Loss: 0.054171279072761536\n",
      "Iteration 9443, Loss: 0.053989529609680176\n",
      "Iteration 9444, Loss: 0.05417139083147049\n",
      "Iteration 9445, Loss: 0.053989630192518234\n",
      "Iteration 9446, Loss: 0.054171472787857056\n",
      "Iteration 9447, Loss: 0.05398944765329361\n",
      "Iteration 9448, Loss: 0.05417175218462944\n",
      "Iteration 9449, Loss: 0.05398912355303764\n",
      "Iteration 9450, Loss: 0.05417198687791824\n",
      "Iteration 9451, Loss: 0.05398896336555481\n",
      "Iteration 9452, Loss: 0.054172031581401825\n",
      "Iteration 9453, Loss: 0.05398884415626526\n",
      "Iteration 9454, Loss: 0.05417206883430481\n",
      "Iteration 9455, Loss: 0.053988855332136154\n",
      "Iteration 9456, Loss: 0.05417179688811302\n",
      "Iteration 9457, Loss: 0.053989242762327194\n",
      "Iteration 9458, Loss: 0.05417148396372795\n",
      "Iteration 9459, Loss: 0.05398940294981003\n",
      "Iteration 9460, Loss: 0.05417148023843765\n",
      "Iteration 9461, Loss: 0.05398944020271301\n",
      "Iteration 9462, Loss: 0.05417143926024437\n",
      "Iteration 9463, Loss: 0.05398952215909958\n",
      "Iteration 9464, Loss: 0.054171472787857056\n",
      "Iteration 9465, Loss: 0.053989481180906296\n",
      "Iteration 9466, Loss: 0.05417148023843765\n",
      "Iteration 9467, Loss: 0.05398940294981003\n",
      "Iteration 9468, Loss: 0.05417155846953392\n",
      "Iteration 9469, Loss: 0.05398935824632645\n",
      "Iteration 9470, Loss: 0.054171591997146606\n",
      "Iteration 9471, Loss: 0.05398928374052048\n",
      "Iteration 9472, Loss: 0.05417167395353317\n",
      "Iteration 9473, Loss: 0.05398931726813316\n",
      "Iteration 9474, Loss: 0.054171591997146606\n",
      "Iteration 9475, Loss: 0.053989242762327194\n",
      "Iteration 9476, Loss: 0.05417155846953392\n",
      "Iteration 9477, Loss: 0.05398928374052048\n",
      "Iteration 9478, Loss: 0.05417163670063019\n",
      "Iteration 9479, Loss: 0.05398927256464958\n",
      "Iteration 9480, Loss: 0.054171644151210785\n",
      "Iteration 9481, Loss: 0.053989239037036896\n",
      "Iteration 9482, Loss: 0.05417183041572571\n",
      "Iteration 9483, Loss: 0.0539892315864563\n",
      "Iteration 9484, Loss: 0.054171763360500336\n",
      "Iteration 9485, Loss: 0.05398908257484436\n",
      "Iteration 9486, Loss: 0.05417187139391899\n",
      "Iteration 9487, Loss: 0.05398919805884361\n",
      "Iteration 9488, Loss: 0.05417167395353317\n",
      "Iteration 9489, Loss: 0.05398906022310257\n",
      "Iteration 9490, Loss: 0.054171450436115265\n",
      "Iteration 9491, Loss: 0.05398940667510033\n",
      "Iteration 9492, Loss: 0.05417144298553467\n",
      "Iteration 9493, Loss: 0.05398952215909958\n",
      "Iteration 9494, Loss: 0.05417140573263168\n",
      "Iteration 9495, Loss: 0.05398944020271301\n",
      "Iteration 9496, Loss: 0.05417151749134064\n",
      "Iteration 9497, Loss: 0.05398940294981003\n",
      "Iteration 9498, Loss: 0.05417148396372795\n",
      "Iteration 9499, Loss: 0.05398928374052048\n",
      "Iteration 9500, Loss: 0.05417180061340332\n",
      "Iteration 9501, Loss: 0.05398900434374809\n",
      "Iteration 9502, Loss: 0.054172031581401825\n",
      "Iteration 9503, Loss: 0.053988851606845856\n",
      "Iteration 9504, Loss: 0.054172076284885406\n",
      "Iteration 9505, Loss: 0.053988851606845856\n",
      "Iteration 9506, Loss: 0.05417191609740257\n",
      "Iteration 9507, Loss: 0.05398900806903839\n",
      "Iteration 9508, Loss: 0.05417179316282272\n",
      "Iteration 9509, Loss: 0.05398909002542496\n",
      "Iteration 9510, Loss: 0.054171524941921234\n",
      "Iteration 9511, Loss: 0.05398944020271301\n",
      "Iteration 9512, Loss: 0.05417144298553467\n",
      "Iteration 9513, Loss: 0.053989559412002563\n",
      "Iteration 9514, Loss: 0.054171524941921234\n",
      "Iteration 9515, Loss: 0.05398944020271301\n",
      "Iteration 9516, Loss: 0.05417163670063019\n",
      "Iteration 9517, Loss: 0.05398912355303764\n",
      "Iteration 9518, Loss: 0.054171763360500336\n",
      "Iteration 9519, Loss: 0.05398893356323242\n",
      "Iteration 9520, Loss: 0.05417194962501526\n",
      "Iteration 9521, Loss: 0.05398896336555481\n",
      "Iteration 9522, Loss: 0.05417194962501526\n",
      "Iteration 9523, Loss: 0.05398905277252197\n",
      "Iteration 9524, Loss: 0.05417179316282272\n",
      "Iteration 9525, Loss: 0.05398912727832794\n",
      "Iteration 9526, Loss: 0.05417163297533989\n",
      "Iteration 9527, Loss: 0.05398920178413391\n",
      "Iteration 9528, Loss: 0.05417151749134064\n",
      "Iteration 9529, Loss: 0.053989287465810776\n",
      "Iteration 9530, Loss: 0.05417151376605034\n",
      "Iteration 9531, Loss: 0.053989410400390625\n",
      "Iteration 9532, Loss: 0.05417139455676079\n",
      "Iteration 9533, Loss: 0.053989630192518234\n",
      "Iteration 9534, Loss: 0.054171472787857056\n",
      "Iteration 9535, Loss: 0.05398967117071152\n",
      "Iteration 9536, Loss: 0.05417140573263168\n",
      "Iteration 9537, Loss: 0.05398928374052048\n",
      "Iteration 9538, Loss: 0.054171524941921234\n",
      "Iteration 9539, Loss: 0.053989164531230927\n",
      "Iteration 9540, Loss: 0.05417179316282272\n",
      "Iteration 9541, Loss: 0.05398911237716675\n",
      "Iteration 9542, Loss: 0.054171763360500336\n",
      "Iteration 9543, Loss: 0.05398830771446228\n",
      "Iteration 9544, Loss: 0.05417163670063019\n",
      "Iteration 9545, Loss: 0.05398857593536377\n",
      "Iteration 9546, Loss: 0.05417144298553467\n",
      "Iteration 9547, Loss: 0.053988706320524216\n",
      "Iteration 9548, Loss: 0.05417066067457199\n",
      "Iteration 9549, Loss: 0.05398878455162048\n",
      "Iteration 9550, Loss: 0.054170623421669006\n",
      "Iteration 9551, Loss: 0.05398882180452347\n",
      "Iteration 9552, Loss: 0.05417077988386154\n",
      "Iteration 9553, Loss: 0.053988706320524216\n",
      "Iteration 9554, Loss: 0.05417077988386154\n",
      "Iteration 9555, Loss: 0.053988657891750336\n",
      "Iteration 9556, Loss: 0.05417077988386154\n",
      "Iteration 9557, Loss: 0.0539887361228466\n",
      "Iteration 9558, Loss: 0.05417073890566826\n",
      "Iteration 9559, Loss: 0.053988710045814514\n",
      "Iteration 9560, Loss: 0.054170623421669006\n",
      "Iteration 9561, Loss: 0.05398867279291153\n",
      "Iteration 9562, Loss: 0.05417066067457199\n",
      "Iteration 9563, Loss: 0.05398878455162048\n",
      "Iteration 9564, Loss: 0.05417069047689438\n",
      "Iteration 9565, Loss: 0.0539887472987175\n",
      "Iteration 9566, Loss: 0.05417054519057274\n",
      "Iteration 9567, Loss: 0.05398878455162048\n",
      "Iteration 9568, Loss: 0.05417061969637871\n",
      "Iteration 9569, Loss: 0.05398878455162048\n",
      "Iteration 9570, Loss: 0.05417058616876602\n",
      "Iteration 9571, Loss: 0.05398878455162048\n",
      "Iteration 9572, Loss: 0.05417058616876602\n",
      "Iteration 9573, Loss: 0.053988706320524216\n",
      "Iteration 9574, Loss: 0.05417061969637871\n",
      "Iteration 9575, Loss: 0.05398881435394287\n",
      "Iteration 9576, Loss: 0.05417054146528244\n",
      "Iteration 9577, Loss: 0.05398894101381302\n",
      "Iteration 9578, Loss: 0.05417050048708916\n",
      "Iteration 9579, Loss: 0.05398905277252197\n",
      "Iteration 9580, Loss: 0.054170653223991394\n",
      "Iteration 9581, Loss: 0.053988855332136154\n",
      "Iteration 9582, Loss: 0.05417077988386154\n",
      "Iteration 9583, Loss: 0.053988661617040634\n",
      "Iteration 9584, Loss: 0.054170817136764526\n",
      "Iteration 9585, Loss: 0.05398857593536377\n",
      "Iteration 9586, Loss: 0.05417089909315109\n",
      "Iteration 9587, Loss: 0.053988587111234665\n",
      "Iteration 9588, Loss: 0.05417082458734512\n",
      "Iteration 9589, Loss: 0.053988587111234665\n",
      "Iteration 9590, Loss: 0.05417078360915184\n",
      "Iteration 9591, Loss: 0.05398862808942795\n",
      "Iteration 9592, Loss: 0.05417077988386154\n",
      "Iteration 9593, Loss: 0.05398862808942795\n",
      "Iteration 9594, Loss: 0.05417066439986229\n",
      "Iteration 9595, Loss: 0.053988777101039886\n",
      "Iteration 9596, Loss: 0.054170578718185425\n",
      "Iteration 9597, Loss: 0.053988855332136154\n",
      "Iteration 9598, Loss: 0.05417054146528244\n",
      "Iteration 9599, Loss: 0.05398905277252197\n",
      "Iteration 9600, Loss: 0.054170381277799606\n",
      "Iteration 9601, Loss: 0.053989164531230927\n",
      "Iteration 9602, Loss: 0.05417042225599289\n",
      "Iteration 9603, Loss: 0.05398912355303764\n",
      "Iteration 9604, Loss: 0.05417061969637871\n",
      "Iteration 9605, Loss: 0.0539887361228466\n",
      "Iteration 9606, Loss: 0.05417093262076378\n",
      "Iteration 9607, Loss: 0.05398845672607422\n",
      "Iteration 9608, Loss: 0.05417101830244064\n",
      "Iteration 9609, Loss: 0.05398834869265556\n",
      "Iteration 9610, Loss: 0.05417109280824661\n",
      "Iteration 9611, Loss: 0.05398842319846153\n",
      "Iteration 9612, Loss: 0.054171010851860046\n",
      "Iteration 9613, Loss: 0.05398854240775108\n",
      "Iteration 9614, Loss: 0.05417070537805557\n",
      "Iteration 9615, Loss: 0.0539887361228466\n",
      "Iteration 9616, Loss: 0.05417054146528244\n",
      "Iteration 9617, Loss: 0.053988855332136154\n",
      "Iteration 9618, Loss: 0.05417050048708916\n",
      "Iteration 9619, Loss: 0.05398905277252197\n",
      "Iteration 9620, Loss: 0.054170459508895874\n",
      "Iteration 9621, Loss: 0.05398901551961899\n",
      "Iteration 9622, Loss: 0.05417050048708916\n",
      "Iteration 9623, Loss: 0.053989045321941376\n",
      "Iteration 9624, Loss: 0.0541706308722496\n",
      "Iteration 9625, Loss: 0.053988777101039886\n",
      "Iteration 9626, Loss: 0.05417089909315109\n",
      "Iteration 9627, Loss: 0.053988538682460785\n",
      "Iteration 9628, Loss: 0.05417105555534363\n",
      "Iteration 9629, Loss: 0.05398838222026825\n",
      "Iteration 9630, Loss: 0.05417085811495781\n",
      "Iteration 9631, Loss: 0.05398843437433243\n",
      "Iteration 9632, Loss: 0.054170817136764526\n",
      "Iteration 9633, Loss: 0.05398854613304138\n",
      "Iteration 9634, Loss: 0.05417066812515259\n",
      "Iteration 9635, Loss: 0.053988780826330185\n",
      "Iteration 9636, Loss: 0.05417047068476677\n",
      "Iteration 9637, Loss: 0.05398889631032944\n",
      "Iteration 9638, Loss: 0.05417047068476677\n",
      "Iteration 9639, Loss: 0.05398894101381302\n",
      "Iteration 9640, Loss: 0.054170578718185425\n",
      "Iteration 9641, Loss: 0.05398901551961899\n",
      "Iteration 9642, Loss: 0.054170578718185425\n",
      "Iteration 9643, Loss: 0.053988855332136154\n",
      "Iteration 9644, Loss: 0.054170697927474976\n",
      "Iteration 9645, Loss: 0.05398885905742645\n",
      "Iteration 9646, Loss: 0.05417077988386154\n",
      "Iteration 9647, Loss: 0.053988777101039886\n",
      "Iteration 9648, Loss: 0.054170697927474976\n",
      "Iteration 9649, Loss: 0.05398869514465332\n",
      "Iteration 9650, Loss: 0.054170817136764526\n",
      "Iteration 9651, Loss: 0.053988706320524216\n",
      "Iteration 9652, Loss: 0.0541706308722496\n",
      "Iteration 9653, Loss: 0.05398881435394287\n",
      "Iteration 9654, Loss: 0.05417070537805557\n",
      "Iteration 9655, Loss: 0.053988855332136154\n",
      "Iteration 9656, Loss: 0.054170750081539154\n",
      "Iteration 9657, Loss: 0.053988732397556305\n",
      "Iteration 9658, Loss: 0.05417093634605408\n",
      "Iteration 9659, Loss: 0.05398842319846153\n",
      "Iteration 9660, Loss: 0.054171137511730194\n",
      "Iteration 9661, Loss: 0.053988270461559296\n",
      "Iteration 9662, Loss: 0.05417121201753616\n",
      "Iteration 9663, Loss: 0.05398830771446228\n",
      "Iteration 9664, Loss: 0.0541711263358593\n",
      "Iteration 9665, Loss: 0.053988389670848846\n",
      "Iteration 9666, Loss: 0.05417093634605408\n",
      "Iteration 9667, Loss: 0.053988587111234665\n",
      "Iteration 9668, Loss: 0.05417097359895706\n",
      "Iteration 9669, Loss: 0.05398861691355705\n",
      "Iteration 9670, Loss: 0.05417085811495781\n",
      "Iteration 9671, Loss: 0.053988657891750336\n",
      "Iteration 9672, Loss: 0.05417089909315109\n",
      "Iteration 9673, Loss: 0.05398854613304138\n",
      "Iteration 9674, Loss: 0.05417093262076378\n",
      "Iteration 9675, Loss: 0.0539885088801384\n",
      "Iteration 9676, Loss: 0.05417093262076378\n",
      "Iteration 9677, Loss: 0.0539885088801384\n",
      "Iteration 9678, Loss: 0.054170750081539154\n",
      "Iteration 9679, Loss: 0.0539885088801384\n",
      "Iteration 9680, Loss: 0.05417089909315109\n",
      "Iteration 9681, Loss: 0.053988587111234665\n",
      "Iteration 9682, Loss: 0.054170817136764526\n",
      "Iteration 9683, Loss: 0.053988657891750336\n",
      "Iteration 9684, Loss: 0.05417093634605408\n",
      "Iteration 9685, Loss: 0.0539885088801384\n",
      "Iteration 9686, Loss: 0.05417089909315109\n",
      "Iteration 9687, Loss: 0.0539885088801384\n",
      "Iteration 9688, Loss: 0.05417093262076378\n",
      "Iteration 9689, Loss: 0.05398854613304138\n",
      "Iteration 9690, Loss: 0.05417093262076378\n",
      "Iteration 9691, Loss: 0.05398854613304138\n",
      "Iteration 9692, Loss: 0.05417097732424736\n",
      "Iteration 9693, Loss: 0.053988538682460785\n",
      "Iteration 9694, Loss: 0.05417117476463318\n",
      "Iteration 9695, Loss: 0.05398830398917198\n",
      "Iteration 9696, Loss: 0.05417133495211601\n",
      "Iteration 9697, Loss: 0.0539882592856884\n",
      "Iteration 9698, Loss: 0.05417126044631004\n",
      "Iteration 9699, Loss: 0.05398815497756004\n",
      "Iteration 9700, Loss: 0.05417132377624512\n",
      "Iteration 9701, Loss: 0.053988270461559296\n",
      "Iteration 9702, Loss: 0.05417117103934288\n",
      "Iteration 9703, Loss: 0.053988389670848846\n",
      "Iteration 9704, Loss: 0.05417097732424736\n",
      "Iteration 9705, Loss: 0.053988389670848846\n",
      "Iteration 9706, Loss: 0.05417090281844139\n",
      "Iteration 9707, Loss: 0.0539885088801384\n",
      "Iteration 9708, Loss: 0.05417086184024811\n",
      "Iteration 9709, Loss: 0.05398862808942795\n",
      "Iteration 9710, Loss: 0.05417089909315109\n",
      "Iteration 9711, Loss: 0.053988583385944366\n",
      "Iteration 9712, Loss: 0.05417101830244064\n",
      "Iteration 9713, Loss: 0.05398900806903839\n",
      "Iteration 9714, Loss: 0.05417121574282646\n",
      "Iteration 9715, Loss: 0.05398888885974884\n",
      "Iteration 9716, Loss: 0.05417173355817795\n",
      "Iteration 9717, Loss: 0.053988706320524216\n",
      "Iteration 9718, Loss: 0.05417172238230705\n",
      "Iteration 9719, Loss: 0.053988710045814514\n",
      "Iteration 9720, Loss: 0.05417126417160034\n",
      "Iteration 9721, Loss: 0.053988978266716\n",
      "Iteration 9722, Loss: 0.05417129397392273\n",
      "Iteration 9723, Loss: 0.05398906394839287\n",
      "Iteration 9724, Loss: 0.05417133495211601\n",
      "Iteration 9725, Loss: 0.05398906022310257\n",
      "Iteration 9726, Loss: 0.05417140945792198\n",
      "Iteration 9727, Loss: 0.05398917198181152\n",
      "Iteration 9728, Loss: 0.05417140945792198\n",
      "Iteration 9729, Loss: 0.053989097476005554\n",
      "Iteration 9730, Loss: 0.0541713684797287\n",
      "Iteration 9731, Loss: 0.05398917198181152\n",
      "Iteration 9732, Loss: 0.05417141318321228\n",
      "Iteration 9733, Loss: 0.053988974541425705\n",
      "Iteration 9734, Loss: 0.05417153239250183\n",
      "Iteration 9735, Loss: 0.053988970816135406\n",
      "Iteration 9736, Loss: 0.054171573370695114\n",
      "Iteration 9737, Loss: 0.05398889631032944\n",
      "Iteration 9738, Loss: 0.05417148396372795\n",
      "Iteration 9739, Loss: 0.05398893356323242\n",
      "Iteration 9740, Loss: 0.05417133495211601\n",
      "Iteration 9741, Loss: 0.05398913472890854\n",
      "Iteration 9742, Loss: 0.054171256721019745\n",
      "Iteration 9743, Loss: 0.05398913845419884\n",
      "Iteration 9744, Loss: 0.05417129397392273\n",
      "Iteration 9745, Loss: 0.05398906394839287\n",
      "Iteration 9746, Loss: 0.0541713684797287\n",
      "Iteration 9747, Loss: 0.053989093750715256\n",
      "Iteration 9748, Loss: 0.05417145416140556\n",
      "Iteration 9749, Loss: 0.05398889631032944\n",
      "Iteration 9750, Loss: 0.0541716143488884\n",
      "Iteration 9751, Loss: 0.05398893356323242\n",
      "Iteration 9752, Loss: 0.05417156219482422\n",
      "Iteration 9753, Loss: 0.05398894101381302\n",
      "Iteration 9754, Loss: 0.05417140945792198\n",
      "Iteration 9755, Loss: 0.0539889857172966\n",
      "Iteration 9756, Loss: 0.05417141318321228\n",
      "Iteration 9757, Loss: 0.05398905277252197\n",
      "Iteration 9758, Loss: 0.054171450436115265\n",
      "Iteration 9759, Loss: 0.05398905277252197\n",
      "Iteration 9760, Loss: 0.054171375930309296\n",
      "Iteration 9761, Loss: 0.05398905277252197\n",
      "Iteration 9762, Loss: 0.05417133495211601\n",
      "Iteration 9763, Loss: 0.053989022970199585\n",
      "Iteration 9764, Loss: 0.05417129397392273\n",
      "Iteration 9765, Loss: 0.05398906394839287\n",
      "Iteration 9766, Loss: 0.05417126044631004\n",
      "Iteration 9767, Loss: 0.0539889857172966\n",
      "Iteration 9768, Loss: 0.05417141318321228\n",
      "Iteration 9769, Loss: 0.053989022970199585\n",
      "Iteration 9770, Loss: 0.05417129397392273\n",
      "Iteration 9771, Loss: 0.053989022970199585\n",
      "Iteration 9772, Loss: 0.054171375930309296\n",
      "Iteration 9773, Loss: 0.0539889857172966\n",
      "Iteration 9774, Loss: 0.05417144298553467\n",
      "Iteration 9775, Loss: 0.053988978266716\n",
      "Iteration 9776, Loss: 0.05417126044631004\n",
      "Iteration 9777, Loss: 0.053989097476005554\n",
      "Iteration 9778, Loss: 0.05417133495211601\n",
      "Iteration 9779, Loss: 0.05398906394839287\n",
      "Iteration 9780, Loss: 0.05417133495211601\n",
      "Iteration 9781, Loss: 0.053989022970199585\n",
      "Iteration 9782, Loss: 0.05417133495211601\n",
      "Iteration 9783, Loss: 0.053989022970199585\n",
      "Iteration 9784, Loss: 0.05417144298553467\n",
      "Iteration 9785, Loss: 0.053988978266716\n",
      "Iteration 9786, Loss: 0.05417140573263168\n",
      "Iteration 9787, Loss: 0.0539889857172966\n",
      "Iteration 9788, Loss: 0.05417129397392273\n",
      "Iteration 9789, Loss: 0.05398906022310257\n",
      "Iteration 9790, Loss: 0.054171301424503326\n",
      "Iteration 9791, Loss: 0.05398891121149063\n",
      "Iteration 9792, Loss: 0.05417129397392273\n",
      "Iteration 9793, Loss: 0.05398913472890854\n",
      "Iteration 9794, Loss: 0.054171301424503326\n",
      "Iteration 9795, Loss: 0.05398913472890854\n",
      "Iteration 9796, Loss: 0.05417141318321228\n",
      "Iteration 9797, Loss: 0.05398905277252197\n",
      "Iteration 9798, Loss: 0.05417153239250183\n",
      "Iteration 9799, Loss: 0.053988855332136154\n",
      "Iteration 9800, Loss: 0.0541716143488884\n",
      "Iteration 9801, Loss: 0.05398866534233093\n",
      "Iteration 9802, Loss: 0.0541718453168869\n",
      "Iteration 9803, Loss: 0.053988657891750336\n",
      "Iteration 9804, Loss: 0.05417180061340332\n",
      "Iteration 9805, Loss: 0.05398866534233093\n",
      "Iteration 9806, Loss: 0.054171573370695114\n",
      "Iteration 9807, Loss: 0.05398889631032944\n",
      "Iteration 9808, Loss: 0.054171375930309296\n",
      "Iteration 9809, Loss: 0.05398913472890854\n",
      "Iteration 9810, Loss: 0.05417133495211601\n",
      "Iteration 9811, Loss: 0.053989022970199585\n",
      "Iteration 9812, Loss: 0.05417129397392273\n",
      "Iteration 9813, Loss: 0.05398906394839287\n",
      "Iteration 9814, Loss: 0.05417140573263168\n",
      "Iteration 9815, Loss: 0.0539889857172966\n",
      "Iteration 9816, Loss: 0.054171375930309296\n",
      "Iteration 9817, Loss: 0.053989022970199585\n",
      "Iteration 9818, Loss: 0.054171375930309296\n",
      "Iteration 9819, Loss: 0.05398913472890854\n",
      "Iteration 9820, Loss: 0.0541716143488884\n",
      "Iteration 9821, Loss: 0.05398893356323242\n",
      "Iteration 9822, Loss: 0.05417165160179138\n",
      "Iteration 9823, Loss: 0.05398869514465332\n",
      "Iteration 9824, Loss: 0.05417173355817795\n",
      "Iteration 9825, Loss: 0.05398866534233093\n",
      "Iteration 9826, Loss: 0.0541716143488884\n",
      "Iteration 9827, Loss: 0.05398885905742645\n",
      "Iteration 9828, Loss: 0.05417140573263168\n",
      "Iteration 9829, Loss: 0.05398906394839287\n",
      "Iteration 9830, Loss: 0.05417121574282646\n",
      "Iteration 9831, Loss: 0.05398925766348839\n",
      "Iteration 9832, Loss: 0.05417121574282646\n",
      "Iteration 9833, Loss: 0.05398925393819809\n",
      "Iteration 9834, Loss: 0.0541713647544384\n",
      "Iteration 9835, Loss: 0.05398913472890854\n",
      "Iteration 9836, Loss: 0.054171573370695114\n",
      "Iteration 9837, Loss: 0.05398878455162048\n",
      "Iteration 9838, Loss: 0.05417180061340332\n",
      "Iteration 9839, Loss: 0.0539887361228466\n",
      "Iteration 9840, Loss: 0.05417191982269287\n",
      "Iteration 9841, Loss: 0.05398862063884735\n",
      "Iteration 9842, Loss: 0.054171960800886154\n",
      "Iteration 9843, Loss: 0.053988587111234665\n",
      "Iteration 9844, Loss: 0.054171763360500336\n",
      "Iteration 9845, Loss: 0.0539887398481369\n",
      "Iteration 9846, Loss: 0.05417156219482422\n",
      "Iteration 9847, Loss: 0.053988900035619736\n",
      "Iteration 9848, Loss: 0.0541713684797287\n",
      "Iteration 9849, Loss: 0.05398910492658615\n",
      "Iteration 9850, Loss: 0.054171137511730194\n",
      "Iteration 9851, Loss: 0.053989216685295105\n",
      "Iteration 9852, Loss: 0.05417117476463318\n",
      "Iteration 9853, Loss: 0.05398925766348839\n",
      "Iteration 9854, Loss: 0.05417129397392273\n",
      "Iteration 9855, Loss: 0.053988974541425705\n",
      "Iteration 9856, Loss: 0.054171569645404816\n",
      "Iteration 9857, Loss: 0.05398889631032944\n",
      "Iteration 9858, Loss: 0.05417168140411377\n",
      "Iteration 9859, Loss: 0.05398878455162048\n",
      "Iteration 9860, Loss: 0.0541716068983078\n",
      "Iteration 9861, Loss: 0.05398885905742645\n",
      "Iteration 9862, Loss: 0.054171524941921234\n",
      "Iteration 9863, Loss: 0.05398917198181152\n",
      "Iteration 9864, Loss: 0.05417129024863243\n",
      "Iteration 9865, Loss: 0.053989291191101074\n",
      "Iteration 9866, Loss: 0.054171204566955566\n",
      "Iteration 9867, Loss: 0.05398933216929436\n",
      "Iteration 9868, Loss: 0.054171331226825714\n",
      "Iteration 9869, Loss: 0.05398917198181152\n",
      "Iteration 9870, Loss: 0.054171524941921234\n",
      "Iteration 9871, Loss: 0.05398901551961899\n",
      "Iteration 9872, Loss: 0.05417165160179138\n",
      "Iteration 9873, Loss: 0.053988851606845856\n",
      "Iteration 9874, Loss: 0.05417180806398392\n",
      "Iteration 9875, Loss: 0.053988587111234665\n",
      "Iteration 9876, Loss: 0.05417177081108093\n",
      "Iteration 9877, Loss: 0.053988661617040634\n",
      "Iteration 9878, Loss: 0.0541716143488884\n",
      "Iteration 9879, Loss: 0.0539887472987175\n",
      "Iteration 9880, Loss: 0.054171375930309296\n",
      "Iteration 9881, Loss: 0.053989022970199585\n",
      "Iteration 9882, Loss: 0.05417117476463318\n",
      "Iteration 9883, Loss: 0.053989291191101074\n",
      "Iteration 9884, Loss: 0.054171137511730194\n",
      "Iteration 9885, Loss: 0.05398937314748764\n",
      "Iteration 9886, Loss: 0.05417128652334213\n",
      "Iteration 9887, Loss: 0.05398925393819809\n",
      "Iteration 9888, Loss: 0.05417141318321228\n",
      "Iteration 9889, Loss: 0.05398901551961899\n",
      "Iteration 9890, Loss: 0.05417164787650108\n",
      "Iteration 9891, Loss: 0.0539887398481369\n",
      "Iteration 9892, Loss: 0.05417173355817795\n",
      "Iteration 9893, Loss: 0.053988780826330185\n",
      "Iteration 9894, Loss: 0.05417172238230705\n",
      "Iteration 9895, Loss: 0.05398878455162048\n",
      "Iteration 9896, Loss: 0.05417156219482422\n",
      "Iteration 9897, Loss: 0.05398885905742645\n",
      "Iteration 9898, Loss: 0.05417145416140556\n",
      "Iteration 9899, Loss: 0.05398901551961899\n",
      "Iteration 9900, Loss: 0.054171256721019745\n",
      "Iteration 9901, Loss: 0.053989022970199585\n",
      "Iteration 9902, Loss: 0.05417133495211601\n",
      "Iteration 9903, Loss: 0.053989093750715256\n",
      "Iteration 9904, Loss: 0.054171495139598846\n",
      "Iteration 9905, Loss: 0.053988974541425705\n",
      "Iteration 9906, Loss: 0.0541716143488884\n",
      "Iteration 9907, Loss: 0.05398889631032944\n",
      "Iteration 9908, Loss: 0.054171811789274216\n",
      "Iteration 9909, Loss: 0.053988657891750336\n",
      "Iteration 9910, Loss: 0.05417191982269287\n",
      "Iteration 9911, Loss: 0.05398854613304138\n",
      "Iteration 9912, Loss: 0.0541718415915966\n",
      "Iteration 9913, Loss: 0.05398870259523392\n",
      "Iteration 9914, Loss: 0.054171763360500336\n",
      "Iteration 9915, Loss: 0.053988855332136154\n",
      "Iteration 9916, Loss: 0.054171573370695114\n",
      "Iteration 9917, Loss: 0.05398886650800705\n",
      "Iteration 9918, Loss: 0.05417132377624512\n",
      "Iteration 9919, Loss: 0.05398910492658615\n",
      "Iteration 9920, Loss: 0.05417124927043915\n",
      "Iteration 9921, Loss: 0.05398918315768242\n",
      "Iteration 9922, Loss: 0.05417109653353691\n",
      "Iteration 9923, Loss: 0.05398925393819809\n",
      "Iteration 9924, Loss: 0.05417121574282646\n",
      "Iteration 9925, Loss: 0.053989216685295105\n",
      "Iteration 9926, Loss: 0.05417133495211601\n",
      "Iteration 9927, Loss: 0.05398915708065033\n",
      "Iteration 9928, Loss: 0.054171573370695114\n",
      "Iteration 9929, Loss: 0.053988777101039886\n",
      "Iteration 9930, Loss: 0.05417173355817795\n",
      "Iteration 9931, Loss: 0.05398878455162048\n",
      "Iteration 9932, Loss: 0.0541716143488884\n",
      "Iteration 9933, Loss: 0.05398889631032944\n",
      "Iteration 9934, Loss: 0.054171569645404816\n",
      "Iteration 9935, Loss: 0.05398901551961899\n",
      "Iteration 9936, Loss: 0.05417141318321228\n",
      "Iteration 9937, Loss: 0.05398901551961899\n",
      "Iteration 9938, Loss: 0.05417133495211601\n",
      "Iteration 9939, Loss: 0.053989168256521225\n",
      "Iteration 9940, Loss: 0.0541713684797287\n",
      "Iteration 9941, Loss: 0.05398906394839287\n",
      "Iteration 9942, Loss: 0.05417140945792198\n",
      "Iteration 9943, Loss: 0.05398906022310257\n",
      "Iteration 9944, Loss: 0.05417133495211601\n",
      "Iteration 9945, Loss: 0.0539889857172966\n",
      "Iteration 9946, Loss: 0.05417148023843765\n",
      "Iteration 9947, Loss: 0.053988948464393616\n",
      "Iteration 9948, Loss: 0.05417148396372795\n",
      "Iteration 9949, Loss: 0.05398906022310257\n",
      "Iteration 9950, Loss: 0.054171450436115265\n",
      "Iteration 9951, Loss: 0.05398906022310257\n",
      "Iteration 9952, Loss: 0.05417141318321228\n",
      "Iteration 9953, Loss: 0.05398905277252197\n",
      "Iteration 9954, Loss: 0.05417133867740631\n",
      "Iteration 9955, Loss: 0.05398909002542496\n",
      "Iteration 9956, Loss: 0.05417153239250183\n",
      "Iteration 9957, Loss: 0.05398889631032944\n",
      "Iteration 9958, Loss: 0.054171692579984665\n",
      "Iteration 9959, Loss: 0.05398870259523392\n",
      "Iteration 9960, Loss: 0.0541718527674675\n",
      "Iteration 9961, Loss: 0.05398862063884735\n",
      "Iteration 9962, Loss: 0.054171886295080185\n",
      "Iteration 9963, Loss: 0.05398862808942795\n",
      "Iteration 9964, Loss: 0.054171763360500336\n",
      "Iteration 9965, Loss: 0.053988780826330185\n",
      "Iteration 9966, Loss: 0.05417148768901825\n",
      "Iteration 9967, Loss: 0.053989019244909286\n",
      "Iteration 9968, Loss: 0.054171256721019745\n",
      "Iteration 9969, Loss: 0.053989216685295105\n",
      "Iteration 9970, Loss: 0.05417109653353691\n",
      "Iteration 9971, Loss: 0.05398936569690704\n",
      "Iteration 9972, Loss: 0.05417105555534363\n",
      "Iteration 9973, Loss: 0.05398937314748764\n",
      "Iteration 9974, Loss: 0.054171256721019745\n",
      "Iteration 9975, Loss: 0.053989242762327194\n",
      "Iteration 9976, Loss: 0.05417153239250183\n",
      "Iteration 9977, Loss: 0.053989045321941376\n",
      "Iteration 9978, Loss: 0.054171763360500336\n",
      "Iteration 9979, Loss: 0.0539887361228466\n",
      "Iteration 9980, Loss: 0.054171811789274216\n",
      "Iteration 9981, Loss: 0.05398862808942795\n",
      "Iteration 9982, Loss: 0.05417169630527496\n",
      "Iteration 9983, Loss: 0.05398881435394287\n",
      "Iteration 9984, Loss: 0.054171688854694366\n",
      "Iteration 9985, Loss: 0.05398893356323242\n",
      "Iteration 9986, Loss: 0.05417145416140556\n",
      "Iteration 9987, Loss: 0.053988974541425705\n",
      "Iteration 9988, Loss: 0.05417133495211601\n",
      "Iteration 9989, Loss: 0.05398913472890854\n",
      "Iteration 9990, Loss: 0.05417117476463318\n",
      "Iteration 9991, Loss: 0.05398925393819809\n",
      "Iteration 9992, Loss: 0.05417121201753616\n",
      "Iteration 9993, Loss: 0.05398925393819809\n",
      "Iteration 9994, Loss: 0.054171256721019745\n",
      "Iteration 9995, Loss: 0.05398932099342346\n",
      "Iteration 9996, Loss: 0.054171569645404816\n",
      "Iteration 9997, Loss: 0.05398892983794212\n",
      "Iteration 9998, Loss: 0.0541718415915966\n",
      "Iteration 9999, Loss: 0.053988732397556305\n",
      "Iteration 10000, Loss: 0.05417189002037048\n",
      "Iteration 10001, Loss: 0.0539885014295578\n",
      "Iteration 10002, Loss: 0.05417197197675705\n",
      "Iteration 10003, Loss: 0.053988583385944366\n",
      "Iteration 10004, Loss: 0.054171882569789886\n",
      "Iteration 10005, Loss: 0.0539887361228466\n",
      "Iteration 10006, Loss: 0.05417153239250183\n",
      "Iteration 10007, Loss: 0.05398893356323242\n",
      "Iteration 10008, Loss: 0.05417144298553467\n",
      "Iteration 10009, Loss: 0.05398928374052048\n",
      "Iteration 10010, Loss: 0.054171256721019745\n",
      "Iteration 10011, Loss: 0.05398925393819809\n",
      "Iteration 10012, Loss: 0.05417129397392273\n",
      "Iteration 10013, Loss: 0.05398913845419884\n",
      "Iteration 10014, Loss: 0.05417126044631004\n",
      "Iteration 10015, Loss: 0.05398913472890854\n",
      "Iteration 10016, Loss: 0.05417133495211601\n",
      "Iteration 10017, Loss: 0.05398920178413391\n",
      "Iteration 10018, Loss: 0.05417145416140556\n",
      "Iteration 10019, Loss: 0.053989022970199585\n",
      "Iteration 10020, Loss: 0.054171450436115265\n",
      "Iteration 10021, Loss: 0.053989093750715256\n",
      "Iteration 10022, Loss: 0.05417148396372795\n",
      "Iteration 10023, Loss: 0.05398909002542496\n",
      "Iteration 10024, Loss: 0.054171524941921234\n",
      "Iteration 10025, Loss: 0.05398901551961899\n",
      "Iteration 10026, Loss: 0.054171524941921234\n",
      "Iteration 10027, Loss: 0.053989093750715256\n",
      "Iteration 10028, Loss: 0.054171524941921234\n",
      "Iteration 10029, Loss: 0.053989019244909286\n",
      "Iteration 10030, Loss: 0.05417156219482422\n",
      "Iteration 10031, Loss: 0.05398894101381302\n",
      "Iteration 10032, Loss: 0.05417164787650108\n",
      "Iteration 10033, Loss: 0.053988974541425705\n",
      "Iteration 10034, Loss: 0.054171763360500336\n",
      "Iteration 10035, Loss: 0.053988855332136154\n",
      "Iteration 10036, Loss: 0.05417173355817795\n",
      "Iteration 10037, Loss: 0.05398881435394287\n",
      "Iteration 10038, Loss: 0.05417164787650108\n",
      "Iteration 10039, Loss: 0.053988974541425705\n",
      "Iteration 10040, Loss: 0.0541716068983078\n",
      "Iteration 10041, Loss: 0.05398901551961899\n",
      "Iteration 10042, Loss: 0.054171644151210785\n",
      "Iteration 10043, Loss: 0.053988974541425705\n",
      "Iteration 10044, Loss: 0.054171644151210785\n",
      "Iteration 10045, Loss: 0.053988974541425705\n",
      "Iteration 10046, Loss: 0.054171644151210785\n",
      "Iteration 10047, Loss: 0.053988974541425705\n",
      "Iteration 10048, Loss: 0.05417153239250183\n",
      "Iteration 10049, Loss: 0.05398893356323242\n",
      "Iteration 10050, Loss: 0.05417172610759735\n",
      "Iteration 10051, Loss: 0.053988777101039886\n",
      "Iteration 10052, Loss: 0.054171767085790634\n",
      "Iteration 10053, Loss: 0.0539887472987175\n",
      "Iteration 10054, Loss: 0.05417168140411377\n",
      "Iteration 10055, Loss: 0.053988855332136154\n",
      "Iteration 10056, Loss: 0.05417156219482422\n",
      "Iteration 10057, Loss: 0.053988974541425705\n",
      "Iteration 10058, Loss: 0.05417144298553467\n",
      "Iteration 10059, Loss: 0.05398924648761749\n",
      "Iteration 10060, Loss: 0.05417128652334213\n",
      "Iteration 10061, Loss: 0.053989481180906296\n",
      "Iteration 10062, Loss: 0.05417132377624512\n",
      "Iteration 10063, Loss: 0.05398940294981003\n",
      "Iteration 10064, Loss: 0.05417148396372795\n",
      "Iteration 10065, Loss: 0.05398912727832794\n",
      "Iteration 10066, Loss: 0.054171573370695114\n",
      "Iteration 10067, Loss: 0.05398888513445854\n",
      "Iteration 10068, Loss: 0.054171886295080185\n",
      "Iteration 10069, Loss: 0.053988657891750336\n",
      "Iteration 10070, Loss: 0.05417203530669212\n",
      "Iteration 10071, Loss: 0.05398857593536377\n",
      "Iteration 10072, Loss: 0.054171957075595856\n",
      "Iteration 10073, Loss: 0.05398869514465332\n",
      "Iteration 10074, Loss: 0.05417168140411377\n",
      "Iteration 10075, Loss: 0.053988855332136154\n",
      "Iteration 10076, Loss: 0.0541715994477272\n",
      "Iteration 10077, Loss: 0.053988978266716\n",
      "Iteration 10078, Loss: 0.05417132377624512\n",
      "Iteration 10079, Loss: 0.05398925766348839\n",
      "Iteration 10080, Loss: 0.05417121201753616\n",
      "Iteration 10081, Loss: 0.05398933216929436\n",
      "Iteration 10082, Loss: 0.05417124554514885\n",
      "Iteration 10083, Loss: 0.053989361971616745\n",
      "Iteration 10084, Loss: 0.05417156219482422\n",
      "Iteration 10085, Loss: 0.053989045321941376\n",
      "Iteration 10086, Loss: 0.0541718415915966\n",
      "Iteration 10087, Loss: 0.053988806903362274\n",
      "Iteration 10088, Loss: 0.05417200177907944\n",
      "Iteration 10089, Loss: 0.05398861691355705\n",
      "Iteration 10090, Loss: 0.054172005504369736\n",
      "Iteration 10091, Loss: 0.05398861691355705\n",
      "Iteration 10092, Loss: 0.05417194962501526\n",
      "Iteration 10093, Loss: 0.0539887398481369\n",
      "Iteration 10094, Loss: 0.05417152866721153\n",
      "Iteration 10095, Loss: 0.05398901551961899\n",
      "Iteration 10096, Loss: 0.054171375930309296\n",
      "Iteration 10097, Loss: 0.05398913472890854\n",
      "Iteration 10098, Loss: 0.05417129397392273\n",
      "Iteration 10099, Loss: 0.05398917198181152\n",
      "Iteration 10100, Loss: 0.05417148396372795\n",
      "Iteration 10101, Loss: 0.053989168256521225\n",
      "Iteration 10102, Loss: 0.05417153239250183\n",
      "Iteration 10103, Loss: 0.05398893356323242\n",
      "Iteration 10104, Loss: 0.054171573370695114\n",
      "Iteration 10105, Loss: 0.05398894101381302\n",
      "Iteration 10106, Loss: 0.05417168140411377\n",
      "Iteration 10107, Loss: 0.05398889631032944\n",
      "Iteration 10108, Loss: 0.05417152866721153\n",
      "Iteration 10109, Loss: 0.053988974541425705\n",
      "Iteration 10110, Loss: 0.05417148768901825\n",
      "Iteration 10111, Loss: 0.053989045321941376\n",
      "Iteration 10112, Loss: 0.05417148396372795\n",
      "Iteration 10113, Loss: 0.05398905277252197\n",
      "Iteration 10114, Loss: 0.05417133495211601\n",
      "Iteration 10115, Loss: 0.05398913472890854\n",
      "Iteration 10116, Loss: 0.054171524941921234\n",
      "Iteration 10117, Loss: 0.05398920178413391\n",
      "Iteration 10118, Loss: 0.05417168140411377\n",
      "Iteration 10119, Loss: 0.05398892983794212\n",
      "Iteration 10120, Loss: 0.05417172610759735\n",
      "Iteration 10121, Loss: 0.0539887361228466\n",
      "Iteration 10122, Loss: 0.054171688854694366\n",
      "Iteration 10123, Loss: 0.05398893356323242\n",
      "Iteration 10124, Loss: 0.0541716031730175\n",
      "Iteration 10125, Loss: 0.053988974541425705\n",
      "Iteration 10126, Loss: 0.054171375930309296\n",
      "Iteration 10127, Loss: 0.053989168256521225\n",
      "Iteration 10128, Loss: 0.05417133495211601\n",
      "Iteration 10129, Loss: 0.05398920178413391\n",
      "Iteration 10130, Loss: 0.05417129024863243\n",
      "Iteration 10131, Loss: 0.053989212960004807\n",
      "Iteration 10132, Loss: 0.05417133495211601\n",
      "Iteration 10133, Loss: 0.05398920923471451\n",
      "Iteration 10134, Loss: 0.05417144298553467\n",
      "Iteration 10135, Loss: 0.053989093750715256\n",
      "Iteration 10136, Loss: 0.0541716031730175\n",
      "Iteration 10137, Loss: 0.053989164531230927\n",
      "Iteration 10138, Loss: 0.05417164787650108\n",
      "Iteration 10139, Loss: 0.053988855332136154\n",
      "Iteration 10140, Loss: 0.05417180061340332\n",
      "Iteration 10141, Loss: 0.05398870259523392\n",
      "Iteration 10142, Loss: 0.054171763360500336\n",
      "Iteration 10143, Loss: 0.053988851606845856\n",
      "Iteration 10144, Loss: 0.05417168140411377\n",
      "Iteration 10145, Loss: 0.053988900035619736\n",
      "Iteration 10146, Loss: 0.05417133495211601\n",
      "Iteration 10147, Loss: 0.05398920923471451\n",
      "Iteration 10148, Loss: 0.054171256721019745\n",
      "Iteration 10149, Loss: 0.05398932099342346\n",
      "Iteration 10150, Loss: 0.054171256721019745\n",
      "Iteration 10151, Loss: 0.05398932844400406\n",
      "Iteration 10152, Loss: 0.05417133495211601\n",
      "Iteration 10153, Loss: 0.053989164531230927\n",
      "Iteration 10154, Loss: 0.05417156219482422\n",
      "Iteration 10155, Loss: 0.05398900806903839\n",
      "Iteration 10156, Loss: 0.054171688854694366\n",
      "Iteration 10157, Loss: 0.053988777101039886\n",
      "Iteration 10158, Loss: 0.05417180061340332\n",
      "Iteration 10159, Loss: 0.05398866534233093\n",
      "Iteration 10160, Loss: 0.05417177081108093\n",
      "Iteration 10161, Loss: 0.05398881062865257\n",
      "Iteration 10162, Loss: 0.05417168140411377\n",
      "Iteration 10163, Loss: 0.05398893356323242\n",
      "Iteration 10164, Loss: 0.05417148768901825\n",
      "Iteration 10165, Loss: 0.05398905277252197\n",
      "Iteration 10166, Loss: 0.05417129397392273\n",
      "Iteration 10167, Loss: 0.053989212960004807\n",
      "Iteration 10168, Loss: 0.054171256721019745\n",
      "Iteration 10169, Loss: 0.053989216685295105\n",
      "Iteration 10170, Loss: 0.0541713684797287\n",
      "Iteration 10171, Loss: 0.05398924648761749\n",
      "Iteration 10172, Loss: 0.054171573370695114\n",
      "Iteration 10173, Loss: 0.05398907512426376\n",
      "Iteration 10174, Loss: 0.0541718415915966\n",
      "Iteration 10175, Loss: 0.053988806903362274\n",
      "Iteration 10176, Loss: 0.0541718527674675\n",
      "Iteration 10177, Loss: 0.05398861691355705\n",
      "Iteration 10178, Loss: 0.054171960800886154\n",
      "Iteration 10179, Loss: 0.053988780826330185\n",
      "Iteration 10180, Loss: 0.05417172610759735\n",
      "Iteration 10181, Loss: 0.05398893356323242\n",
      "Iteration 10182, Loss: 0.05417156219482422\n",
      "Iteration 10183, Loss: 0.053988974541425705\n",
      "Iteration 10184, Loss: 0.05417140945792198\n",
      "Iteration 10185, Loss: 0.053989093750715256\n",
      "Iteration 10186, Loss: 0.05417129397392273\n",
      "Iteration 10187, Loss: 0.053989291191101074\n",
      "Iteration 10188, Loss: 0.054171256721019745\n",
      "Iteration 10189, Loss: 0.05398925393819809\n",
      "Iteration 10190, Loss: 0.054171375930309296\n",
      "Iteration 10191, Loss: 0.053989097476005554\n",
      "Iteration 10192, Loss: 0.054171375930309296\n",
      "Iteration 10193, Loss: 0.053989093750715256\n",
      "Iteration 10194, Loss: 0.05417141318321228\n",
      "Iteration 10195, Loss: 0.05398912355303764\n",
      "Iteration 10196, Loss: 0.054171569645404816\n",
      "Iteration 10197, Loss: 0.05398894101381302\n",
      "Iteration 10198, Loss: 0.054171644151210785\n",
      "Iteration 10199, Loss: 0.05398900806903839\n",
      "Iteration 10200, Loss: 0.05417168140411377\n",
      "Iteration 10201, Loss: 0.05398893356323242\n",
      "Iteration 10202, Loss: 0.0541715994477272\n",
      "Iteration 10203, Loss: 0.05398901551961899\n",
      "Iteration 10204, Loss: 0.05417148396372795\n",
      "Iteration 10205, Loss: 0.05398957058787346\n",
      "Iteration 10206, Loss: 0.054171375930309296\n",
      "Iteration 10207, Loss: 0.053989604115486145\n",
      "Iteration 10208, Loss: 0.05417081341147423\n",
      "Iteration 10209, Loss: 0.05398983880877495\n",
      "Iteration 10210, Loss: 0.05417085811495781\n",
      "Iteration 10211, Loss: 0.05398968607187271\n",
      "Iteration 10212, Loss: 0.05417109653353691\n",
      "Iteration 10213, Loss: 0.053989291191101074\n",
      "Iteration 10214, Loss: 0.05417140945792198\n",
      "Iteration 10215, Loss: 0.053989164531230927\n",
      "Iteration 10216, Loss: 0.054171692579984665\n",
      "Iteration 10217, Loss: 0.05398881435394287\n",
      "Iteration 10218, Loss: 0.054171960800886154\n",
      "Iteration 10219, Loss: 0.05398869141936302\n",
      "Iteration 10220, Loss: 0.054172009229660034\n",
      "Iteration 10221, Loss: 0.053988657891750336\n",
      "Iteration 10222, Loss: 0.05417191982269287\n",
      "Iteration 10223, Loss: 0.05398866534233093\n",
      "Iteration 10224, Loss: 0.05417180806398392\n",
      "Iteration 10225, Loss: 0.05398886650800705\n",
      "Iteration 10226, Loss: 0.054171644151210785\n",
      "Iteration 10227, Loss: 0.053988978266716\n",
      "Iteration 10228, Loss: 0.054171375930309296\n",
      "Iteration 10229, Loss: 0.053989142179489136\n",
      "Iteration 10230, Loss: 0.0541713647544384\n",
      "Iteration 10231, Loss: 0.05398945137858391\n",
      "Iteration 10232, Loss: 0.054171398282051086\n",
      "Iteration 10233, Loss: 0.05398936569690704\n",
      "Iteration 10234, Loss: 0.054171331226825714\n",
      "Iteration 10235, Loss: 0.053989287465810776\n",
      "Iteration 10236, Loss: 0.05417153239250183\n",
      "Iteration 10237, Loss: 0.053989049047231674\n",
      "Iteration 10238, Loss: 0.054171882569789886\n",
      "Iteration 10239, Loss: 0.053988806903362274\n",
      "Iteration 10240, Loss: 0.05417212098836899\n",
      "Iteration 10241, Loss: 0.053988657891750336\n",
      "Iteration 10242, Loss: 0.054172080010175705\n",
      "Iteration 10243, Loss: 0.05398854240775108\n",
      "Iteration 10244, Loss: 0.05417196452617645\n",
      "Iteration 10245, Loss: 0.053988587111234665\n",
      "Iteration 10246, Loss: 0.05417180061340332\n",
      "Iteration 10247, Loss: 0.05398893356323242\n",
      "Iteration 10248, Loss: 0.05417152866721153\n",
      "Iteration 10249, Loss: 0.05398906394839287\n",
      "Iteration 10250, Loss: 0.05417140573263168\n",
      "Iteration 10251, Loss: 0.05398918315768242\n",
      "Iteration 10252, Loss: 0.05417124927043915\n",
      "Iteration 10253, Loss: 0.05398933216929436\n",
      "Iteration 10254, Loss: 0.054171256721019745\n",
      "Iteration 10255, Loss: 0.05398940667510033\n",
      "Iteration 10256, Loss: 0.054171256721019745\n",
      "Iteration 10257, Loss: 0.053989361971616745\n",
      "Iteration 10258, Loss: 0.05417148768901825\n",
      "Iteration 10259, Loss: 0.05398920178413391\n",
      "Iteration 10260, Loss: 0.0541716143488884\n",
      "Iteration 10261, Loss: 0.05398896336555481\n",
      "Iteration 10262, Loss: 0.0541718453168869\n",
      "Iteration 10263, Loss: 0.053988732397556305\n",
      "Iteration 10264, Loss: 0.054171960800886154\n",
      "Iteration 10265, Loss: 0.0539887361228466\n",
      "Iteration 10266, Loss: 0.05417199060320854\n",
      "Iteration 10267, Loss: 0.05398881435394287\n",
      "Iteration 10268, Loss: 0.05417168140411377\n",
      "Iteration 10269, Loss: 0.05398894101381302\n",
      "Iteration 10270, Loss: 0.054171450436115265\n",
      "Iteration 10271, Loss: 0.05398925393819809\n",
      "Iteration 10272, Loss: 0.05417117476463318\n",
      "Iteration 10273, Loss: 0.05398937314748764\n",
      "Iteration 10274, Loss: 0.05417117476463318\n",
      "Iteration 10275, Loss: 0.05398937314748764\n",
      "Iteration 10276, Loss: 0.05417121574282646\n",
      "Iteration 10277, Loss: 0.05398924648761749\n",
      "Iteration 10278, Loss: 0.05417145416140556\n",
      "Iteration 10279, Loss: 0.05398910492658615\n",
      "Iteration 10280, Loss: 0.05417173355817795\n",
      "Iteration 10281, Loss: 0.05398888885974884\n",
      "Iteration 10282, Loss: 0.05417200177907944\n",
      "Iteration 10283, Loss: 0.05398861691355705\n",
      "Iteration 10284, Loss: 0.054172080010175705\n",
      "Iteration 10285, Loss: 0.053988661617040634\n",
      "Iteration 10286, Loss: 0.0541718415915966\n",
      "Iteration 10287, Loss: 0.05398889631032944\n",
      "Iteration 10288, Loss: 0.05417144298553467\n",
      "Iteration 10289, Loss: 0.05398917198181152\n",
      "Iteration 10290, Loss: 0.0541713647544384\n",
      "Iteration 10291, Loss: 0.053989361971616745\n",
      "Iteration 10292, Loss: 0.05417124554514885\n",
      "Iteration 10293, Loss: 0.053989410400390625\n",
      "Iteration 10294, Loss: 0.05417121574282646\n",
      "Iteration 10295, Loss: 0.05398937314748764\n",
      "Iteration 10296, Loss: 0.054171256721019745\n",
      "Iteration 10297, Loss: 0.05398933216929436\n",
      "Iteration 10298, Loss: 0.05417144298553467\n",
      "Iteration 10299, Loss: 0.05398906394839287\n",
      "Iteration 10300, Loss: 0.0541716068983078\n",
      "Iteration 10301, Loss: 0.05398909002542496\n",
      "Iteration 10302, Loss: 0.0541716143488884\n",
      "Iteration 10303, Loss: 0.05398901551961899\n",
      "Iteration 10304, Loss: 0.05417153239250183\n",
      "Iteration 10305, Loss: 0.05398909002542496\n",
      "Iteration 10306, Loss: 0.054171573370695114\n",
      "Iteration 10307, Loss: 0.05398901551961899\n",
      "Iteration 10308, Loss: 0.05417163297533989\n",
      "Iteration 10309, Loss: 0.053989093750715256\n",
      "Iteration 10310, Loss: 0.0541716031730175\n",
      "Iteration 10311, Loss: 0.05398912727832794\n",
      "Iteration 10312, Loss: 0.05417168140411377\n",
      "Iteration 10313, Loss: 0.05398901551961899\n",
      "Iteration 10314, Loss: 0.054171692579984665\n",
      "Iteration 10315, Loss: 0.05398889631032944\n",
      "Iteration 10316, Loss: 0.05417172238230705\n",
      "Iteration 10317, Loss: 0.05398881435394287\n",
      "Iteration 10318, Loss: 0.05417172238230705\n",
      "Iteration 10319, Loss: 0.05398900806903839\n",
      "Iteration 10320, Loss: 0.054171573370695114\n",
      "Iteration 10321, Loss: 0.053988903760910034\n",
      "Iteration 10322, Loss: 0.054171495139598846\n",
      "Iteration 10323, Loss: 0.05398913845419884\n",
      "Iteration 10324, Loss: 0.054171375930309296\n",
      "Iteration 10325, Loss: 0.053989212960004807\n",
      "Iteration 10326, Loss: 0.054171375930309296\n",
      "Iteration 10327, Loss: 0.05398913845419884\n",
      "Iteration 10328, Loss: 0.054171524941921234\n",
      "Iteration 10329, Loss: 0.05398909002542496\n",
      "Iteration 10330, Loss: 0.05417168140411377\n",
      "Iteration 10331, Loss: 0.05398908257484436\n",
      "Iteration 10332, Loss: 0.05417180061340332\n",
      "Iteration 10333, Loss: 0.053988855332136154\n",
      "Iteration 10334, Loss: 0.05417180061340332\n",
      "Iteration 10335, Loss: 0.05398889631032944\n",
      "Iteration 10336, Loss: 0.05417172238230705\n",
      "Iteration 10337, Loss: 0.05398886650800705\n",
      "Iteration 10338, Loss: 0.054171524941921234\n",
      "Iteration 10339, Loss: 0.05398906022310257\n",
      "Iteration 10340, Loss: 0.054171375930309296\n",
      "Iteration 10341, Loss: 0.053989212960004807\n",
      "Iteration 10342, Loss: 0.054171331226825714\n",
      "Iteration 10343, Loss: 0.05398925393819809\n",
      "Iteration 10344, Loss: 0.05417140573263168\n",
      "Iteration 10345, Loss: 0.05398925393819809\n",
      "Iteration 10346, Loss: 0.05417144298553467\n",
      "Iteration 10347, Loss: 0.053989242762327194\n",
      "Iteration 10348, Loss: 0.05417156219482422\n",
      "Iteration 10349, Loss: 0.053989049047231674\n",
      "Iteration 10350, Loss: 0.054171763360500336\n",
      "Iteration 10351, Loss: 0.053988855332136154\n",
      "Iteration 10352, Loss: 0.054171763360500336\n",
      "Iteration 10353, Loss: 0.05398885905742645\n",
      "Iteration 10354, Loss: 0.05417168140411377\n",
      "Iteration 10355, Loss: 0.053988903760910034\n",
      "Iteration 10356, Loss: 0.054171524941921234\n",
      "Iteration 10357, Loss: 0.05398906022310257\n",
      "Iteration 10358, Loss: 0.054171375930309296\n",
      "Iteration 10359, Loss: 0.05398925393819809\n",
      "Iteration 10360, Loss: 0.0541713647544384\n",
      "Iteration 10361, Loss: 0.053989291191101074\n",
      "Iteration 10362, Loss: 0.05417133495211601\n",
      "Iteration 10363, Loss: 0.05398928374052048\n",
      "Iteration 10364, Loss: 0.05417156219482422\n",
      "Iteration 10365, Loss: 0.053989093750715256\n",
      "Iteration 10366, Loss: 0.0541716031730175\n",
      "Iteration 10367, Loss: 0.05398912355303764\n",
      "Iteration 10368, Loss: 0.054171692579984665\n",
      "Iteration 10369, Loss: 0.053989045321941376\n",
      "Iteration 10370, Loss: 0.05417173355817795\n",
      "Iteration 10371, Loss: 0.05398830026388168\n",
      "Iteration 10372, Loss: 0.054171811789274216\n",
      "Iteration 10373, Loss: 0.05398818105459213\n",
      "Iteration 10374, Loss: 0.05417255684733391\n",
      "Iteration 10375, Loss: 0.05398791283369064\n",
      "Iteration 10376, Loss: 0.05417255684733391\n",
      "Iteration 10377, Loss: 0.05398821830749512\n",
      "Iteration 10378, Loss: 0.05417215824127197\n",
      "Iteration 10379, Loss: 0.0539885088801384\n",
      "Iteration 10380, Loss: 0.05417172238230705\n",
      "Iteration 10381, Loss: 0.053988974541425705\n",
      "Iteration 10382, Loss: 0.05417129397392273\n",
      "Iteration 10383, Loss: 0.05398925393819809\n",
      "Iteration 10384, Loss: 0.054171256721019745\n",
      "Iteration 10385, Loss: 0.05398925393819809\n",
      "Iteration 10386, Loss: 0.054171256721019745\n",
      "Iteration 10387, Loss: 0.053989093750715256\n",
      "Iteration 10388, Loss: 0.05417148768901825\n",
      "Iteration 10389, Loss: 0.05398901551961899\n",
      "Iteration 10390, Loss: 0.054171688854694366\n",
      "Iteration 10391, Loss: 0.05398881435394287\n",
      "Iteration 10392, Loss: 0.0541718527674675\n",
      "Iteration 10393, Loss: 0.053988657891750336\n",
      "Iteration 10394, Loss: 0.054171882569789886\n",
      "Iteration 10395, Loss: 0.05398861691355705\n",
      "Iteration 10396, Loss: 0.05417173355817795\n",
      "Iteration 10397, Loss: 0.05398881435394287\n",
      "Iteration 10398, Loss: 0.054171644151210785\n",
      "Iteration 10399, Loss: 0.05398909002542496\n",
      "Iteration 10400, Loss: 0.05417141318321228\n",
      "Iteration 10401, Loss: 0.053989019244909286\n",
      "Iteration 10402, Loss: 0.05417132377624512\n",
      "Iteration 10403, Loss: 0.05398917943239212\n",
      "Iteration 10404, Loss: 0.05417132377624512\n",
      "Iteration 10405, Loss: 0.05398928374052048\n",
      "Iteration 10406, Loss: 0.05417129024863243\n",
      "Iteration 10407, Loss: 0.053988974541425705\n",
      "Iteration 10408, Loss: 0.05417140945792198\n",
      "Iteration 10409, Loss: 0.05398901551961899\n",
      "Iteration 10410, Loss: 0.05417156219482422\n",
      "Iteration 10411, Loss: 0.05398912727832794\n",
      "Iteration 10412, Loss: 0.0541716068983078\n",
      "Iteration 10413, Loss: 0.05398885905742645\n",
      "Iteration 10414, Loss: 0.05417172238230705\n",
      "Iteration 10415, Loss: 0.0539887361228466\n",
      "Iteration 10416, Loss: 0.054171882569789886\n",
      "Iteration 10417, Loss: 0.05398881435394287\n",
      "Iteration 10418, Loss: 0.054171882569789886\n",
      "Iteration 10419, Loss: 0.05398870259523392\n",
      "Iteration 10420, Loss: 0.05417180806398392\n",
      "Iteration 10421, Loss: 0.053988732397556305\n",
      "Iteration 10422, Loss: 0.05417180806398392\n",
      "Iteration 10423, Loss: 0.05398861691355705\n",
      "Iteration 10424, Loss: 0.054171692579984665\n",
      "Iteration 10425, Loss: 0.05398881435394287\n",
      "Iteration 10426, Loss: 0.05417153239250183\n",
      "Iteration 10427, Loss: 0.05398900434374809\n",
      "Iteration 10428, Loss: 0.054171375930309296\n",
      "Iteration 10429, Loss: 0.05398893356323242\n",
      "Iteration 10430, Loss: 0.05417133495211601\n",
      "Iteration 10431, Loss: 0.05398913472890854\n",
      "Iteration 10432, Loss: 0.054171256721019745\n",
      "Iteration 10433, Loss: 0.053989287465810776\n",
      "Iteration 10434, Loss: 0.05417122691869736\n",
      "Iteration 10435, Loss: 0.053989093750715256\n",
      "Iteration 10436, Loss: 0.054171495139598846\n",
      "Iteration 10437, Loss: 0.053988970816135406\n",
      "Iteration 10438, Loss: 0.0541716143488884\n",
      "Iteration 10439, Loss: 0.053988777101039886\n",
      "Iteration 10440, Loss: 0.054171688854694366\n",
      "Iteration 10441, Loss: 0.0539887361228466\n",
      "Iteration 10442, Loss: 0.0541718415915966\n",
      "Iteration 10443, Loss: 0.05398869514465332\n",
      "Iteration 10444, Loss: 0.054171573370695114\n",
      "Iteration 10445, Loss: 0.05398889631032944\n",
      "Iteration 10446, Loss: 0.054171450436115265\n",
      "Iteration 10447, Loss: 0.05398920178413391\n",
      "Iteration 10448, Loss: 0.054171256721019745\n",
      "Iteration 10449, Loss: 0.05398925393819809\n",
      "Iteration 10450, Loss: 0.05417117476463318\n",
      "Iteration 10451, Loss: 0.053989212960004807\n",
      "Iteration 10452, Loss: 0.05417124554514885\n",
      "Iteration 10453, Loss: 0.053989212960004807\n",
      "Iteration 10454, Loss: 0.05417114496231079\n",
      "Iteration 10455, Loss: 0.05398906394839287\n",
      "Iteration 10456, Loss: 0.05417138338088989\n",
      "Iteration 10457, Loss: 0.05398885905742645\n",
      "Iteration 10458, Loss: 0.05417169630527496\n",
      "Iteration 10459, Loss: 0.05398861691355705\n",
      "Iteration 10460, Loss: 0.05417189002037048\n",
      "Iteration 10461, Loss: 0.05398845672607422\n",
      "Iteration 10462, Loss: 0.05417189002037048\n",
      "Iteration 10463, Loss: 0.05398854613304138\n",
      "Iteration 10464, Loss: 0.05417165160179138\n",
      "Iteration 10465, Loss: 0.05398889631032944\n",
      "Iteration 10466, Loss: 0.05417121946811676\n",
      "Iteration 10467, Loss: 0.05398917943239212\n",
      "Iteration 10468, Loss: 0.054170869290828705\n",
      "Iteration 10469, Loss: 0.05398937687277794\n",
      "Iteration 10470, Loss: 0.05417078733444214\n",
      "Iteration 10471, Loss: 0.05398952588438988\n",
      "Iteration 10472, Loss: 0.05417094752192497\n",
      "Iteration 10473, Loss: 0.05398933216929436\n",
      "Iteration 10474, Loss: 0.05417126417160034\n",
      "Iteration 10475, Loss: 0.053988974541425705\n",
      "Iteration 10476, Loss: 0.05417166277766228\n",
      "Iteration 10477, Loss: 0.053988587111234665\n",
      "Iteration 10478, Loss: 0.05417190119624138\n",
      "Iteration 10479, Loss: 0.053988389670848846\n",
      "Iteration 10480, Loss: 0.05417205020785332\n",
      "Iteration 10481, Loss: 0.053988270461559296\n",
      "Iteration 10482, Loss: 0.054171785712242126\n",
      "Iteration 10483, Loss: 0.05398857593536377\n",
      "Iteration 10484, Loss: 0.0541716143488884\n",
      "Iteration 10485, Loss: 0.053988825529813766\n",
      "Iteration 10486, Loss: 0.054171305149793625\n",
      "Iteration 10487, Loss: 0.05398905277252197\n",
      "Iteration 10488, Loss: 0.05417114496231079\n",
      "Iteration 10489, Loss: 0.053989216685295105\n",
      "Iteration 10490, Loss: 0.05417121574282646\n",
      "Iteration 10491, Loss: 0.05398913472890854\n",
      "Iteration 10492, Loss: 0.05417133867740631\n",
      "Iteration 10493, Loss: 0.05398901551961899\n",
      "Iteration 10494, Loss: 0.054171569645404816\n",
      "Iteration 10495, Loss: 0.053988780826330185\n",
      "Iteration 10496, Loss: 0.05417153239250183\n",
      "Iteration 10497, Loss: 0.0539887472987175\n",
      "Iteration 10498, Loss: 0.05417145788669586\n",
      "Iteration 10499, Loss: 0.053988903760910034\n",
      "Iteration 10500, Loss: 0.05417145416140556\n",
      "Iteration 10501, Loss: 0.05398886650800705\n",
      "Iteration 10502, Loss: 0.054171495139598846\n",
      "Iteration 10503, Loss: 0.053988780826330185\n",
      "Iteration 10504, Loss: 0.05417138338088989\n",
      "Iteration 10505, Loss: 0.05398901551961899\n",
      "Iteration 10506, Loss: 0.05417141318321228\n",
      "Iteration 10507, Loss: 0.05398901551961899\n",
      "Iteration 10508, Loss: 0.054171301424503326\n",
      "Iteration 10509, Loss: 0.05398906022310257\n",
      "Iteration 10510, Loss: 0.05417129397392273\n",
      "Iteration 10511, Loss: 0.053989022970199585\n",
      "Iteration 10512, Loss: 0.05417126044631004\n",
      "Iteration 10513, Loss: 0.05398894473910332\n",
      "Iteration 10514, Loss: 0.05417126044631004\n",
      "Iteration 10515, Loss: 0.0539889857172966\n",
      "Iteration 10516, Loss: 0.054171185940504074\n",
      "Iteration 10517, Loss: 0.05398917198181152\n",
      "Iteration 10518, Loss: 0.05417114496231079\n",
      "Iteration 10519, Loss: 0.05398906394839287\n",
      "Iteration 10520, Loss: 0.05417129397392273\n",
      "Iteration 10521, Loss: 0.05398910492658615\n",
      "Iteration 10522, Loss: 0.054171379655599594\n",
      "Iteration 10523, Loss: 0.05398886650800705\n",
      "Iteration 10524, Loss: 0.05417153239250183\n",
      "Iteration 10525, Loss: 0.05398881435394287\n",
      "Iteration 10526, Loss: 0.054171618074178696\n",
      "Iteration 10527, Loss: 0.05398862808942795\n",
      "Iteration 10528, Loss: 0.05417170375585556\n",
      "Iteration 10529, Loss: 0.05398862808942795\n",
      "Iteration 10530, Loss: 0.05417165160179138\n",
      "Iteration 10531, Loss: 0.05398870259523392\n",
      "Iteration 10532, Loss: 0.054171573370695114\n",
      "Iteration 10533, Loss: 0.05398878455162048\n",
      "Iteration 10534, Loss: 0.054171375930309296\n",
      "Iteration 10535, Loss: 0.05398894101381302\n",
      "Iteration 10536, Loss: 0.05417129397392273\n",
      "Iteration 10537, Loss: 0.05398906022310257\n",
      "Iteration 10538, Loss: 0.054171182215213776\n",
      "Iteration 10539, Loss: 0.05398917943239212\n",
      "Iteration 10540, Loss: 0.05417110025882721\n",
      "Iteration 10541, Loss: 0.05398966372013092\n",
      "Iteration 10542, Loss: 0.054170988500118256\n",
      "Iteration 10543, Loss: 0.05398969352245331\n",
      "Iteration 10544, Loss: 0.05417114496231079\n",
      "Iteration 10545, Loss: 0.05398938059806824\n",
      "Iteration 10546, Loss: 0.0541718564927578\n",
      "Iteration 10547, Loss: 0.05398926883935928\n",
      "Iteration 10548, Loss: 0.05417190119624138\n",
      "Iteration 10549, Loss: 0.05398934334516525\n",
      "Iteration 10550, Loss: 0.05417178198695183\n",
      "Iteration 10551, Loss: 0.05398939177393913\n",
      "Iteration 10552, Loss: 0.054171737283468246\n",
      "Iteration 10553, Loss: 0.05398954078555107\n",
      "Iteration 10554, Loss: 0.05417165905237198\n",
      "Iteration 10555, Loss: 0.05398954078555107\n",
      "Iteration 10556, Loss: 0.054171692579984665\n",
      "Iteration 10557, Loss: 0.05398961529135704\n",
      "Iteration 10558, Loss: 0.05417177081108093\n",
      "Iteration 10559, Loss: 0.053989388048648834\n",
      "Iteration 10560, Loss: 0.054172009229660034\n",
      "Iteration 10561, Loss: 0.05398930236697197\n",
      "Iteration 10562, Loss: 0.05417216941714287\n",
      "Iteration 10563, Loss: 0.05398906394839287\n",
      "Iteration 10564, Loss: 0.0541720986366272\n",
      "Iteration 10565, Loss: 0.05398910492658615\n",
      "Iteration 10566, Loss: 0.05417213588953018\n",
      "Iteration 10567, Loss: 0.05398906394839287\n",
      "Iteration 10568, Loss: 0.05417206138372421\n",
      "Iteration 10569, Loss: 0.05398903042078018\n",
      "Iteration 10570, Loss: 0.05417190492153168\n",
      "Iteration 10571, Loss: 0.05398937314748764\n",
      "Iteration 10572, Loss: 0.0541718527674675\n",
      "Iteration 10573, Loss: 0.0539894625544548\n",
      "Iteration 10574, Loss: 0.05417166277766228\n",
      "Iteration 10575, Loss: 0.0539894700050354\n",
      "Iteration 10576, Loss: 0.05417165905237198\n",
      "Iteration 10577, Loss: 0.053989581763744354\n",
      "Iteration 10578, Loss: 0.05417177081108093\n",
      "Iteration 10579, Loss: 0.05398957431316376\n",
      "Iteration 10580, Loss: 0.05417170375585556\n",
      "Iteration 10581, Loss: 0.05398942157626152\n",
      "Iteration 10582, Loss: 0.0541718527674675\n",
      "Iteration 10583, Loss: 0.05398934707045555\n",
      "Iteration 10584, Loss: 0.054171815514564514\n",
      "Iteration 10585, Loss: 0.053989313542842865\n",
      "Iteration 10586, Loss: 0.05417189747095108\n",
      "Iteration 10587, Loss: 0.05398930609226227\n",
      "Iteration 10588, Loss: 0.054171860218048096\n",
      "Iteration 10589, Loss: 0.0539892241358757\n",
      "Iteration 10590, Loss: 0.05417197570204735\n",
      "Iteration 10591, Loss: 0.05398929864168167\n",
      "Iteration 10592, Loss: 0.05417194217443466\n",
      "Iteration 10593, Loss: 0.05398937314748764\n",
      "Iteration 10594, Loss: 0.054172009229660034\n",
      "Iteration 10595, Loss: 0.05398918315768242\n",
      "Iteration 10596, Loss: 0.05417201668024063\n",
      "Iteration 10597, Loss: 0.05398914963006973\n",
      "Iteration 10598, Loss: 0.05417205020785332\n",
      "Iteration 10599, Loss: 0.05398914963006973\n",
      "Iteration 10600, Loss: 0.054172128438949585\n",
      "Iteration 10601, Loss: 0.053989216685295105\n",
      "Iteration 10602, Loss: 0.0541720949113369\n",
      "Iteration 10603, Loss: 0.05398910492658615\n",
      "Iteration 10604, Loss: 0.05417216941714287\n",
      "Iteration 10605, Loss: 0.05398911237716675\n",
      "Iteration 10606, Loss: 0.054172128438949585\n",
      "Iteration 10607, Loss: 0.05398914963006973\n",
      "Iteration 10608, Loss: 0.05417178198695183\n",
      "Iteration 10609, Loss: 0.05398942157626152\n",
      "Iteration 10610, Loss: 0.054171737283468246\n",
      "Iteration 10611, Loss: 0.053989507257938385\n",
      "Iteration 10612, Loss: 0.05417170375585556\n",
      "Iteration 10613, Loss: 0.0539894625544548\n",
      "Iteration 10614, Loss: 0.05417170375585556\n",
      "Iteration 10615, Loss: 0.0539894625544548\n",
      "Iteration 10616, Loss: 0.054171860218048096\n",
      "Iteration 10617, Loss: 0.05398937687277794\n",
      "Iteration 10618, Loss: 0.054172083735466\n",
      "Iteration 10619, Loss: 0.0539892241358757\n",
      "Iteration 10620, Loss: 0.05417215824127197\n",
      "Iteration 10621, Loss: 0.05398918315768242\n",
      "Iteration 10622, Loss: 0.054171934723854065\n",
      "Iteration 10623, Loss: 0.05398942157626152\n",
      "Iteration 10624, Loss: 0.054171741008758545\n",
      "Iteration 10625, Loss: 0.05398961901664734\n",
      "Iteration 10626, Loss: 0.05417158454656601\n",
      "Iteration 10627, Loss: 0.05398965999484062\n",
      "Iteration 10628, Loss: 0.05417165905237198\n",
      "Iteration 10629, Loss: 0.05398961901664734\n",
      "Iteration 10630, Loss: 0.054171591997146606\n",
      "Iteration 10631, Loss: 0.05398949980735779\n",
      "Iteration 10632, Loss: 0.054171860218048096\n",
      "Iteration 10633, Loss: 0.0539892315864563\n",
      "Iteration 10634, Loss: 0.0541720986366272\n",
      "Iteration 10635, Loss: 0.05398895591497421\n",
      "Iteration 10636, Loss: 0.05417221784591675\n",
      "Iteration 10637, Loss: 0.05398891493678093\n",
      "Iteration 10638, Loss: 0.054172173142433167\n",
      "Iteration 10639, Loss: 0.05398911237716675\n",
      "Iteration 10640, Loss: 0.05417189002037048\n",
      "Iteration 10641, Loss: 0.05398938059806824\n",
      "Iteration 10642, Loss: 0.054171692579984665\n",
      "Iteration 10643, Loss: 0.05398961901664734\n",
      "Iteration 10644, Loss: 0.05417152866721153\n",
      "Iteration 10645, Loss: 0.05398988723754883\n",
      "Iteration 10646, Loss: 0.054171375930309296\n",
      "Iteration 10647, Loss: 0.053989820182323456\n",
      "Iteration 10648, Loss: 0.05417145788669586\n",
      "Iteration 10649, Loss: 0.053989775478839874\n",
      "Iteration 10650, Loss: 0.054171621799468994\n",
      "Iteration 10651, Loss: 0.05398957431316376\n",
      "Iteration 10652, Loss: 0.05417189747095108\n",
      "Iteration 10653, Loss: 0.053989194333553314\n",
      "Iteration 10654, Loss: 0.05417190119624138\n",
      "Iteration 10655, Loss: 0.05398914963006973\n",
      "Iteration 10656, Loss: 0.054172009229660034\n",
      "Iteration 10657, Loss: 0.05398911237716675\n",
      "Iteration 10658, Loss: 0.054172009229660034\n",
      "Iteration 10659, Loss: 0.05398934334516525\n",
      "Iteration 10660, Loss: 0.054171666502952576\n",
      "Iteration 10661, Loss: 0.05398949980735779\n",
      "Iteration 10662, Loss: 0.0541716143488884\n",
      "Iteration 10663, Loss: 0.05398961901664734\n",
      "Iteration 10664, Loss: 0.054171424359083176\n",
      "Iteration 10665, Loss: 0.053989969193935394\n",
      "Iteration 10666, Loss: 0.054171230643987656\n",
      "Iteration 10667, Loss: 0.05398993939161301\n",
      "Iteration 10668, Loss: 0.054171305149793625\n",
      "Iteration 10669, Loss: 0.053989849984645844\n",
      "Iteration 10670, Loss: 0.054171495139598846\n",
      "Iteration 10671, Loss: 0.053989656269550323\n",
      "Iteration 10672, Loss: 0.054171543568372726\n",
      "Iteration 10673, Loss: 0.05398961901664734\n",
      "Iteration 10674, Loss: 0.05417169630527496\n",
      "Iteration 10675, Loss: 0.053989581763744354\n",
      "Iteration 10676, Loss: 0.05417162925004959\n",
      "Iteration 10677, Loss: 0.05398953706026077\n",
      "Iteration 10678, Loss: 0.05417178198695183\n",
      "Iteration 10679, Loss: 0.05398934334516525\n",
      "Iteration 10680, Loss: 0.05417182296514511\n",
      "Iteration 10681, Loss: 0.05398937687277794\n",
      "Iteration 10682, Loss: 0.054172009229660034\n",
      "Iteration 10683, Loss: 0.053989194333553314\n",
      "Iteration 10684, Loss: 0.05417197197675705\n",
      "Iteration 10685, Loss: 0.05398918688297272\n",
      "Iteration 10686, Loss: 0.05417189002037048\n",
      "Iteration 10687, Loss: 0.0539894625544548\n",
      "Iteration 10688, Loss: 0.05417177081108093\n",
      "Iteration 10689, Loss: 0.0539894625544548\n",
      "Iteration 10690, Loss: 0.05417165905237198\n",
      "Iteration 10691, Loss: 0.053989700973033905\n",
      "Iteration 10692, Loss: 0.054171424359083176\n",
      "Iteration 10693, Loss: 0.05398974567651749\n",
      "Iteration 10694, Loss: 0.05417134612798691\n",
      "Iteration 10695, Loss: 0.053989820182323456\n",
      "Iteration 10696, Loss: 0.054171424359083176\n",
      "Iteration 10697, Loss: 0.053989700973033905\n",
      "Iteration 10698, Loss: 0.054171543568372726\n",
      "Iteration 10699, Loss: 0.05398954078555107\n",
      "Iteration 10700, Loss: 0.05417170375585556\n",
      "Iteration 10701, Loss: 0.05398949980735779\n",
      "Iteration 10702, Loss: 0.05417178198695183\n",
      "Iteration 10703, Loss: 0.05398945137858391\n",
      "Iteration 10704, Loss: 0.05417182296514511\n",
      "Iteration 10705, Loss: 0.05398930236697197\n",
      "Iteration 10706, Loss: 0.05417197197675705\n",
      "Iteration 10707, Loss: 0.053989227861166\n",
      "Iteration 10708, Loss: 0.054171930998563766\n",
      "Iteration 10709, Loss: 0.05398938059806824\n",
      "Iteration 10710, Loss: 0.05417170375585556\n",
      "Iteration 10711, Loss: 0.05398949980735779\n",
      "Iteration 10712, Loss: 0.05417146533727646\n",
      "Iteration 10713, Loss: 0.05398977920413017\n",
      "Iteration 10714, Loss: 0.054171256721019745\n",
      "Iteration 10715, Loss: 0.05398993194103241\n",
      "Iteration 10716, Loss: 0.05417122691869736\n",
      "Iteration 10717, Loss: 0.05398993194103241\n",
      "Iteration 10718, Loss: 0.05417134612798691\n",
      "Iteration 10719, Loss: 0.053989704698324203\n",
      "Iteration 10720, Loss: 0.054171737283468246\n",
      "Iteration 10721, Loss: 0.05398949980735779\n",
      "Iteration 10722, Loss: 0.05417194217443466\n",
      "Iteration 10723, Loss: 0.053989142179489136\n",
      "Iteration 10724, Loss: 0.05417218059301376\n",
      "Iteration 10725, Loss: 0.0539889857172966\n",
      "Iteration 10726, Loss: 0.05417213961482048\n",
      "Iteration 10727, Loss: 0.053989142179489136\n",
      "Iteration 10728, Loss: 0.0541718527674675\n",
      "Iteration 10729, Loss: 0.05398930609226227\n",
      "Iteration 10730, Loss: 0.054171692579984665\n",
      "Iteration 10731, Loss: 0.05398965999484062\n",
      "Iteration 10732, Loss: 0.05417141318321228\n",
      "Iteration 10733, Loss: 0.05398977920413017\n",
      "Iteration 10734, Loss: 0.054171379655599594\n",
      "Iteration 10735, Loss: 0.05398992821574211\n",
      "Iteration 10736, Loss: 0.054171379655599594\n",
      "Iteration 10737, Loss: 0.053989849984645844\n",
      "Iteration 10738, Loss: 0.05417165160179138\n",
      "Iteration 10739, Loss: 0.053989510983228683\n",
      "Iteration 10740, Loss: 0.05417182296514511\n",
      "Iteration 10741, Loss: 0.05398927256464958\n",
      "Iteration 10742, Loss: 0.05417206510901451\n",
      "Iteration 10743, Loss: 0.0539889931678772\n",
      "Iteration 10744, Loss: 0.05417218431830406\n",
      "Iteration 10745, Loss: 0.05398891493678093\n",
      "Iteration 10746, Loss: 0.05417221039533615\n",
      "Iteration 10747, Loss: 0.05398910492658615\n",
      "Iteration 10748, Loss: 0.0541720911860466\n",
      "Iteration 10749, Loss: 0.05398914963006973\n",
      "Iteration 10750, Loss: 0.05417202040553093\n",
      "Iteration 10751, Loss: 0.053989261388778687\n",
      "Iteration 10752, Loss: 0.054172009229660034\n",
      "Iteration 10753, Loss: 0.05398930236697197\n",
      "Iteration 10754, Loss: 0.05417189747095108\n",
      "Iteration 10755, Loss: 0.05398930236697197\n",
      "Iteration 10756, Loss: 0.054171930998563766\n",
      "Iteration 10757, Loss: 0.05398930236697197\n",
      "Iteration 10758, Loss: 0.054171860218048096\n",
      "Iteration 10759, Loss: 0.05398930236697197\n",
      "Iteration 10760, Loss: 0.05417202040553093\n",
      "Iteration 10761, Loss: 0.05398925766348839\n",
      "Iteration 10762, Loss: 0.0541720949113369\n",
      "Iteration 10763, Loss: 0.053989142179489136\n",
      "Iteration 10764, Loss: 0.05417202040553093\n",
      "Iteration 10765, Loss: 0.05398911237716675\n",
      "Iteration 10766, Loss: 0.054171979427337646\n",
      "Iteration 10767, Loss: 0.0539892241358757\n",
      "Iteration 10768, Loss: 0.054171860218048096\n",
      "Iteration 10769, Loss: 0.05398938059806824\n",
      "Iteration 10770, Loss: 0.054171741008758545\n",
      "Iteration 10771, Loss: 0.0539894625544548\n",
      "Iteration 10772, Loss: 0.054171666502952576\n",
      "Iteration 10773, Loss: 0.05398942157626152\n",
      "Iteration 10774, Loss: 0.05417189747095108\n",
      "Iteration 10775, Loss: 0.05398930609226227\n",
      "Iteration 10776, Loss: 0.05417205020785332\n",
      "Iteration 10777, Loss: 0.05398930236697197\n",
      "Iteration 10778, Loss: 0.054172128438949585\n",
      "Iteration 10779, Loss: 0.053989142179489136\n",
      "Iteration 10780, Loss: 0.054172128438949585\n",
      "Iteration 10781, Loss: 0.05398903787136078\n",
      "Iteration 10782, Loss: 0.054172009229660034\n",
      "Iteration 10783, Loss: 0.05398930236697197\n",
      "Iteration 10784, Loss: 0.05417189747095108\n",
      "Iteration 10785, Loss: 0.05398938059806824\n",
      "Iteration 10786, Loss: 0.054171741008758545\n",
      "Iteration 10787, Loss: 0.0539894625544548\n",
      "Iteration 10788, Loss: 0.05417166277766228\n",
      "Iteration 10789, Loss: 0.05398957058787346\n",
      "Iteration 10790, Loss: 0.05417158827185631\n",
      "Iteration 10791, Loss: 0.0539894625544548\n",
      "Iteration 10792, Loss: 0.054171666502952576\n",
      "Iteration 10793, Loss: 0.05398949235677719\n",
      "Iteration 10794, Loss: 0.054171930998563766\n",
      "Iteration 10795, Loss: 0.05398937687277794\n",
      "Iteration 10796, Loss: 0.054171934723854065\n",
      "Iteration 10797, Loss: 0.05398930236697197\n",
      "Iteration 10798, Loss: 0.05417197570204735\n",
      "Iteration 10799, Loss: 0.05398911237716675\n",
      "Iteration 10800, Loss: 0.05417197197675705\n",
      "Iteration 10801, Loss: 0.05398934334516525\n",
      "Iteration 10802, Loss: 0.05417189002037048\n",
      "Iteration 10803, Loss: 0.0539894625544548\n",
      "Iteration 10804, Loss: 0.054171741008758545\n",
      "Iteration 10805, Loss: 0.053989388048648834\n",
      "Iteration 10806, Loss: 0.054171979427337646\n",
      "Iteration 10807, Loss: 0.053989261388778687\n",
      "Iteration 10808, Loss: 0.0541720986366272\n",
      "Iteration 10809, Loss: 0.053989067673683167\n",
      "Iteration 10810, Loss: 0.05417228862643242\n",
      "Iteration 10811, Loss: 0.05398883670568466\n",
      "Iteration 10812, Loss: 0.05417228862643242\n",
      "Iteration 10813, Loss: 0.053989142179489136\n",
      "Iteration 10814, Loss: 0.05417190119624138\n",
      "Iteration 10815, Loss: 0.0539892315864563\n",
      "Iteration 10816, Loss: 0.05417178198695183\n",
      "Iteration 10817, Loss: 0.05398949608206749\n",
      "Iteration 10818, Loss: 0.05417150259017944\n",
      "Iteration 10819, Loss: 0.053989700973033905\n",
      "Iteration 10820, Loss: 0.054171305149793625\n",
      "Iteration 10821, Loss: 0.053989820182323456\n",
      "Iteration 10822, Loss: 0.054171305149793625\n",
      "Iteration 10823, Loss: 0.05398985743522644\n",
      "Iteration 10824, Loss: 0.05417150259017944\n",
      "Iteration 10825, Loss: 0.05398977920413017\n",
      "Iteration 10826, Loss: 0.054171737283468246\n",
      "Iteration 10827, Loss: 0.0539894625544548\n",
      "Iteration 10828, Loss: 0.05417202040553093\n",
      "Iteration 10829, Loss: 0.05398914963006973\n",
      "Iteration 10830, Loss: 0.05417221784591675\n",
      "Iteration 10831, Loss: 0.053988903760910034\n",
      "Iteration 10832, Loss: 0.0541723296046257\n",
      "Iteration 10833, Loss: 0.053988873958587646\n",
      "Iteration 10834, Loss: 0.054172173142433167\n",
      "Iteration 10835, Loss: 0.05398906394839287\n",
      "Iteration 10836, Loss: 0.054172128438949585\n",
      "Iteration 10837, Loss: 0.05398918688297272\n",
      "Iteration 10838, Loss: 0.054171811789274216\n",
      "Iteration 10839, Loss: 0.053989388048648834\n",
      "Iteration 10840, Loss: 0.054171621799468994\n",
      "Iteration 10841, Loss: 0.05398965999484062\n",
      "Iteration 10842, Loss: 0.05417166277766228\n",
      "Iteration 10843, Loss: 0.05398954451084137\n",
      "Iteration 10844, Loss: 0.05417151376605034\n",
      "Iteration 10845, Loss: 0.05398964881896973\n",
      "Iteration 10846, Loss: 0.054171666502952576\n",
      "Iteration 10847, Loss: 0.05398938059806824\n",
      "Iteration 10848, Loss: 0.054172009229660034\n",
      "Iteration 10849, Loss: 0.05398929864168167\n",
      "Iteration 10850, Loss: 0.0541720986366272\n",
      "Iteration 10851, Loss: 0.05398911237716675\n",
      "Iteration 10852, Loss: 0.054172173142433167\n",
      "Iteration 10853, Loss: 0.05398895591497421\n",
      "Iteration 10854, Loss: 0.05417218059301376\n",
      "Iteration 10855, Loss: 0.05398903042078018\n",
      "Iteration 10856, Loss: 0.0541720949113369\n",
      "Iteration 10857, Loss: 0.053989142179489136\n",
      "Iteration 10858, Loss: 0.05417194217443466\n",
      "Iteration 10859, Loss: 0.05398930236697197\n",
      "Iteration 10860, Loss: 0.054171930998563766\n",
      "Iteration 10861, Loss: 0.05398938059806824\n",
      "Iteration 10862, Loss: 0.0541718527674675\n",
      "Iteration 10863, Loss: 0.05398942530155182\n",
      "Iteration 10864, Loss: 0.05417170375585556\n",
      "Iteration 10865, Loss: 0.05398942157626152\n",
      "Iteration 10866, Loss: 0.054171737283468246\n",
      "Iteration 10867, Loss: 0.0539894625544548\n",
      "Iteration 10868, Loss: 0.05417170375585556\n",
      "Iteration 10869, Loss: 0.0539894625544548\n",
      "Iteration 10870, Loss: 0.05417182296514511\n",
      "Iteration 10871, Loss: 0.05398934707045555\n",
      "Iteration 10872, Loss: 0.05417190119624138\n",
      "Iteration 10873, Loss: 0.05398937687277794\n",
      "Iteration 10874, Loss: 0.054171979427337646\n",
      "Iteration 10875, Loss: 0.053989142179489136\n",
      "Iteration 10876, Loss: 0.0541720986366272\n",
      "Iteration 10877, Loss: 0.05398911237716675\n",
      "Iteration 10878, Loss: 0.05417202040553093\n",
      "Iteration 10879, Loss: 0.05398907512426376\n",
      "Iteration 10880, Loss: 0.054171979427337646\n",
      "Iteration 10881, Loss: 0.053989261388778687\n",
      "Iteration 10882, Loss: 0.05417194217443466\n",
      "Iteration 10883, Loss: 0.05398934707045555\n",
      "Iteration 10884, Loss: 0.054171886295080185\n",
      "Iteration 10885, Loss: 0.0539894662797451\n",
      "Iteration 10886, Loss: 0.05417173355817795\n",
      "Iteration 10887, Loss: 0.05398954078555107\n",
      "Iteration 10888, Loss: 0.054171692579984665\n",
      "Iteration 10889, Loss: 0.05398954451084137\n",
      "Iteration 10890, Loss: 0.05417173355817795\n",
      "Iteration 10891, Loss: 0.05398957058787346\n",
      "Iteration 10892, Loss: 0.0541718527674675\n",
      "Iteration 10893, Loss: 0.05398942157626152\n",
      "Iteration 10894, Loss: 0.054171979427337646\n",
      "Iteration 10895, Loss: 0.05398918315768242\n",
      "Iteration 10896, Loss: 0.05417225509881973\n",
      "Iteration 10897, Loss: 0.05398891493678093\n",
      "Iteration 10898, Loss: 0.05417241156101227\n",
      "Iteration 10899, Loss: 0.05398879200220108\n",
      "Iteration 10900, Loss: 0.0541723370552063\n",
      "Iteration 10901, Loss: 0.053988873958587646\n",
      "Iteration 10902, Loss: 0.0541720986366272\n",
      "Iteration 10903, Loss: 0.05398918315768242\n",
      "Iteration 10904, Loss: 0.054171934723854065\n",
      "Iteration 10905, Loss: 0.053989313542842865\n",
      "Iteration 10906, Loss: 0.05417177081108093\n",
      "Iteration 10907, Loss: 0.05398954078555107\n",
      "Iteration 10908, Loss: 0.0541716143488884\n",
      "Iteration 10909, Loss: 0.053989700973033905\n",
      "Iteration 10910, Loss: 0.05417145788669586\n",
      "Iteration 10911, Loss: 0.05398974567651749\n",
      "Iteration 10912, Loss: 0.0541716143488884\n",
      "Iteration 10913, Loss: 0.05398968979716301\n",
      "Iteration 10914, Loss: 0.05417166277766228\n",
      "Iteration 10915, Loss: 0.053989432752132416\n",
      "Iteration 10916, Loss: 0.05417201668024063\n",
      "Iteration 10917, Loss: 0.0539892241358757\n",
      "Iteration 10918, Loss: 0.05417213961482048\n",
      "Iteration 10919, Loss: 0.0539889894425869\n",
      "Iteration 10920, Loss: 0.05417228862643242\n",
      "Iteration 10921, Loss: 0.05398891493678093\n",
      "Iteration 10922, Loss: 0.05417221039533615\n",
      "Iteration 10923, Loss: 0.05398907512426376\n",
      "Iteration 10924, Loss: 0.05417190119624138\n",
      "Iteration 10925, Loss: 0.053989313542842865\n",
      "Iteration 10926, Loss: 0.05417170375585556\n",
      "Iteration 10927, Loss: 0.05398961529135704\n",
      "Iteration 10928, Loss: 0.054171498864889145\n",
      "Iteration 10929, Loss: 0.05398977920413017\n",
      "Iteration 10930, Loss: 0.05417138338088989\n",
      "Iteration 10931, Loss: 0.053989700973033905\n",
      "Iteration 10932, Loss: 0.054171621799468994\n",
      "Iteration 10933, Loss: 0.053989700973033905\n",
      "Iteration 10934, Loss: 0.05417177081108093\n",
      "Iteration 10935, Loss: 0.05398934334516525\n",
      "Iteration 10936, Loss: 0.0541720911860466\n",
      "Iteration 10937, Loss: 0.05398910865187645\n",
      "Iteration 10938, Loss: 0.05417216941714287\n",
      "Iteration 10939, Loss: 0.05398907512426376\n",
      "Iteration 10940, Loss: 0.0541720986366272\n",
      "Iteration 10941, Loss: 0.053989000618457794\n",
      "Iteration 10942, Loss: 0.05417205020785332\n",
      "Iteration 10943, Loss: 0.05398856848478317\n",
      "Iteration 10944, Loss: 0.05417196452617645\n",
      "Iteration 10945, Loss: 0.05398861691355705\n",
      "Iteration 10946, Loss: 0.0541711151599884\n",
      "Iteration 10947, Loss: 0.05398884415626526\n",
      "Iteration 10948, Loss: 0.05417114496231079\n",
      "Iteration 10949, Loss: 0.05398896336555481\n",
      "Iteration 10950, Loss: 0.054171040654182434\n",
      "Iteration 10951, Loss: 0.053988806903362274\n",
      "Iteration 10952, Loss: 0.05417127162218094\n",
      "Iteration 10953, Loss: 0.05398868769407272\n",
      "Iteration 10954, Loss: 0.054171543568372726\n",
      "Iteration 10955, Loss: 0.053988486528396606\n",
      "Iteration 10956, Loss: 0.05417143553495407\n",
      "Iteration 10957, Loss: 0.05398845672607422\n",
      "Iteration 10958, Loss: 0.05417139455676079\n",
      "Iteration 10959, Loss: 0.053988538682460785\n",
      "Iteration 10960, Loss: 0.054171353578567505\n",
      "Iteration 10961, Loss: 0.05398865044116974\n",
      "Iteration 10962, Loss: 0.054171353578567505\n",
      "Iteration 10963, Loss: 0.05398865044116974\n",
      "Iteration 10964, Loss: 0.05417139455676079\n",
      "Iteration 10965, Loss: 0.05398864671587944\n",
      "Iteration 10966, Loss: 0.0541713610291481\n",
      "Iteration 10967, Loss: 0.0539884939789772\n",
      "Iteration 10968, Loss: 0.054171472787857056\n",
      "Iteration 10969, Loss: 0.05398841202259064\n",
      "Iteration 10970, Loss: 0.05417143926024437\n",
      "Iteration 10971, Loss: 0.053988486528396606\n",
      "Iteration 10972, Loss: 0.05417139455676079\n",
      "Iteration 10973, Loss: 0.05398845672607422\n",
      "Iteration 10974, Loss: 0.05417131632566452\n",
      "Iteration 10975, Loss: 0.05398845672607422\n",
      "Iteration 10976, Loss: 0.054171349853277206\n",
      "Iteration 10977, Loss: 0.05398865044116974\n",
      "Iteration 10978, Loss: 0.05417134612798691\n",
      "Iteration 10979, Loss: 0.05398868769407272\n",
      "Iteration 10980, Loss: 0.054171234369277954\n",
      "Iteration 10981, Loss: 0.05398876592516899\n",
      "Iteration 10982, Loss: 0.05417134612798691\n",
      "Iteration 10983, Loss: 0.05398879572749138\n",
      "Iteration 10984, Loss: 0.05417127534747124\n",
      "Iteration 10985, Loss: 0.05398867651820183\n",
      "Iteration 10986, Loss: 0.054171398282051086\n",
      "Iteration 10987, Loss: 0.05398900434374809\n",
      "Iteration 10988, Loss: 0.05417155474424362\n",
      "Iteration 10989, Loss: 0.053988777101039886\n",
      "Iteration 10990, Loss: 0.054171472787857056\n",
      "Iteration 10991, Loss: 0.053988926112651825\n",
      "Iteration 10992, Loss: 0.05417186766862869\n",
      "Iteration 10993, Loss: 0.05398901551961899\n",
      "Iteration 10994, Loss: 0.05417151749134064\n",
      "Iteration 10995, Loss: 0.053989361971616745\n",
      "Iteration 10996, Loss: 0.05417150259017944\n",
      "Iteration 10997, Loss: 0.053989477455616\n",
      "Iteration 10998, Loss: 0.05417127534747124\n",
      "Iteration 10999, Loss: 0.05398959666490555\n",
      "Iteration 11000, Loss: 0.05417127534747124\n",
      "Iteration 11001, Loss: 0.05398958921432495\n",
      "Iteration 11002, Loss: 0.05417155474424362\n",
      "Iteration 11003, Loss: 0.05398939177393913\n",
      "Iteration 11004, Loss: 0.05417179316282272\n",
      "Iteration 11005, Loss: 0.05398900434374809\n",
      "Iteration 11006, Loss: 0.054172225296497345\n",
      "Iteration 11007, Loss: 0.05398868769407272\n",
      "Iteration 11008, Loss: 0.05417222902178764\n",
      "Iteration 11009, Loss: 0.05398860573768616\n",
      "Iteration 11010, Loss: 0.05417222902178764\n",
      "Iteration 11011, Loss: 0.05398872494697571\n",
      "Iteration 11012, Loss: 0.054172031581401825\n",
      "Iteration 11013, Loss: 0.05398903414607048\n",
      "Iteration 11014, Loss: 0.05417167395353317\n",
      "Iteration 11015, Loss: 0.05398942530155182\n",
      "Iteration 11016, Loss: 0.05417143553495407\n",
      "Iteration 11017, Loss: 0.05398955196142197\n",
      "Iteration 11018, Loss: 0.05417132377624512\n",
      "Iteration 11019, Loss: 0.05398940294981003\n",
      "Iteration 11020, Loss: 0.05417151376605034\n",
      "Iteration 11021, Loss: 0.05398935079574585\n",
      "Iteration 11022, Loss: 0.05417163670063019\n",
      "Iteration 11023, Loss: 0.0539892315864563\n",
      "Iteration 11024, Loss: 0.05417182296514511\n",
      "Iteration 11025, Loss: 0.05398915708065033\n",
      "Iteration 11026, Loss: 0.054171912372112274\n",
      "Iteration 11027, Loss: 0.05398900434374809\n",
      "Iteration 11028, Loss: 0.05417191609740257\n",
      "Iteration 11029, Loss: 0.05398888513445854\n",
      "Iteration 11030, Loss: 0.05417206883430481\n",
      "Iteration 11031, Loss: 0.05398888140916824\n",
      "Iteration 11032, Loss: 0.05417206883430481\n",
      "Iteration 11033, Loss: 0.05398888513445854\n",
      "Iteration 11034, Loss: 0.05417199060320854\n",
      "Iteration 11035, Loss: 0.053989000618457794\n",
      "Iteration 11036, Loss: 0.05417187139391899\n",
      "Iteration 11037, Loss: 0.0539892315864563\n",
      "Iteration 11038, Loss: 0.05417151749134064\n",
      "Iteration 11039, Loss: 0.053989432752132416\n",
      "Iteration 11040, Loss: 0.05417151376605034\n",
      "Iteration 11041, Loss: 0.0539894700050354\n",
      "Iteration 11042, Loss: 0.05417151004076004\n",
      "Iteration 11043, Loss: 0.05398955196142197\n",
      "Iteration 11044, Loss: 0.05417146906256676\n",
      "Iteration 11045, Loss: 0.05398951470851898\n",
      "Iteration 11046, Loss: 0.05417139455676079\n",
      "Iteration 11047, Loss: 0.05398940294981003\n",
      "Iteration 11048, Loss: 0.05417151376605034\n",
      "Iteration 11049, Loss: 0.05398935079574585\n",
      "Iteration 11050, Loss: 0.05417163297533989\n",
      "Iteration 11051, Loss: 0.053989242762327194\n",
      "Iteration 11052, Loss: 0.05417171120643616\n",
      "Iteration 11053, Loss: 0.05398920178413391\n",
      "Iteration 11054, Loss: 0.05417175218462944\n",
      "Iteration 11055, Loss: 0.05398919805884361\n",
      "Iteration 11056, Loss: 0.05417190119624138\n",
      "Iteration 11057, Loss: 0.053989194333553314\n",
      "Iteration 11058, Loss: 0.05417179316282272\n",
      "Iteration 11059, Loss: 0.05398900434374809\n",
      "Iteration 11060, Loss: 0.05417190492153168\n",
      "Iteration 11061, Loss: 0.053989045321941376\n",
      "Iteration 11062, Loss: 0.05417190119624138\n",
      "Iteration 11063, Loss: 0.05398908257484436\n",
      "Iteration 11064, Loss: 0.05417182296514511\n",
      "Iteration 11065, Loss: 0.053989194333553314\n",
      "Iteration 11066, Loss: 0.05417171120643616\n",
      "Iteration 11067, Loss: 0.05398927256464958\n",
      "Iteration 11068, Loss: 0.05417151376605034\n",
      "Iteration 11069, Loss: 0.0539894700050354\n",
      "Iteration 11070, Loss: 0.054171472787857056\n",
      "Iteration 11071, Loss: 0.053989481180906296\n",
      "Iteration 11072, Loss: 0.054171472787857056\n",
      "Iteration 11073, Loss: 0.05398939177393913\n",
      "Iteration 11074, Loss: 0.05417155846953392\n",
      "Iteration 11075, Loss: 0.0539892315864563\n",
      "Iteration 11076, Loss: 0.054171785712242126\n",
      "Iteration 11077, Loss: 0.053989045321941376\n",
      "Iteration 11078, Loss: 0.05417182296514511\n",
      "Iteration 11079, Loss: 0.053989164531230927\n",
      "Iteration 11080, Loss: 0.05417171120643616\n",
      "Iteration 11081, Loss: 0.05398927628993988\n",
      "Iteration 11082, Loss: 0.05417170375585556\n",
      "Iteration 11083, Loss: 0.05398935824632645\n",
      "Iteration 11084, Loss: 0.05417151376605034\n",
      "Iteration 11085, Loss: 0.053989510983228683\n",
      "Iteration 11086, Loss: 0.05417143553495407\n",
      "Iteration 11087, Loss: 0.053989510983228683\n",
      "Iteration 11088, Loss: 0.05417155474424362\n",
      "Iteration 11089, Loss: 0.05398928374052048\n",
      "Iteration 11090, Loss: 0.05417167767882347\n",
      "Iteration 11091, Loss: 0.05398919805884361\n",
      "Iteration 11092, Loss: 0.05417194962501526\n",
      "Iteration 11093, Loss: 0.05398903414607048\n",
      "Iteration 11094, Loss: 0.054172150790691376\n",
      "Iteration 11095, Loss: 0.05398864671587944\n",
      "Iteration 11096, Loss: 0.05417222902178764\n",
      "Iteration 11097, Loss: 0.05398871749639511\n",
      "Iteration 11098, Loss: 0.054172031581401825\n",
      "Iteration 11099, Loss: 0.053989000618457794\n",
      "Iteration 11100, Loss: 0.054171785712242126\n",
      "Iteration 11101, Loss: 0.05398927628993988\n",
      "Iteration 11102, Loss: 0.05417151004076004\n",
      "Iteration 11103, Loss: 0.05398955196142197\n",
      "Iteration 11104, Loss: 0.05417127534747124\n",
      "Iteration 11105, Loss: 0.0539897084236145\n",
      "Iteration 11106, Loss: 0.05417127534747124\n",
      "Iteration 11107, Loss: 0.05398952215909958\n",
      "Iteration 11108, Loss: 0.05417139455676079\n",
      "Iteration 11109, Loss: 0.053989510983228683\n",
      "Iteration 11110, Loss: 0.054171591997146606\n",
      "Iteration 11111, Loss: 0.05398927256464958\n",
      "Iteration 11112, Loss: 0.05417179316282272\n",
      "Iteration 11113, Loss: 0.05398907512426376\n",
      "Iteration 11114, Loss: 0.054171912372112274\n",
      "Iteration 11115, Loss: 0.05398881435394287\n",
      "Iteration 11116, Loss: 0.05417194217443466\n",
      "Iteration 11117, Loss: 0.053989045321941376\n",
      "Iteration 11118, Loss: 0.05417190119624138\n",
      "Iteration 11119, Loss: 0.0539892315864563\n",
      "Iteration 11120, Loss: 0.054171741008758545\n",
      "Iteration 11121, Loss: 0.05398931726813316\n",
      "Iteration 11122, Loss: 0.05417148023843765\n",
      "Iteration 11123, Loss: 0.05398942530155182\n",
      "Iteration 11124, Loss: 0.054171472787857056\n",
      "Iteration 11125, Loss: 0.0539894700050354\n",
      "Iteration 11126, Loss: 0.05417143553495407\n",
      "Iteration 11127, Loss: 0.053989510983228683\n",
      "Iteration 11128, Loss: 0.054171472787857056\n",
      "Iteration 11129, Loss: 0.0539894700050354\n",
      "Iteration 11130, Loss: 0.05417155474424362\n",
      "Iteration 11131, Loss: 0.05398940294981003\n",
      "Iteration 11132, Loss: 0.05417155474424362\n",
      "Iteration 11133, Loss: 0.05398928374052048\n",
      "Iteration 11134, Loss: 0.054171591997146606\n",
      "Iteration 11135, Loss: 0.053989164531230927\n",
      "Iteration 11136, Loss: 0.05417167395353317\n",
      "Iteration 11137, Loss: 0.05398915335536003\n",
      "Iteration 11138, Loss: 0.05417179688811302\n",
      "Iteration 11139, Loss: 0.053989000618457794\n",
      "Iteration 11140, Loss: 0.05417194962501526\n",
      "Iteration 11141, Loss: 0.05398896336555481\n",
      "Iteration 11142, Loss: 0.05417199060320854\n",
      "Iteration 11143, Loss: 0.053988926112651825\n",
      "Iteration 11144, Loss: 0.054171912372112274\n",
      "Iteration 11145, Loss: 0.053989045321941376\n",
      "Iteration 11146, Loss: 0.05417187139391899\n",
      "Iteration 11147, Loss: 0.05398903787136078\n",
      "Iteration 11148, Loss: 0.054171912372112274\n",
      "Iteration 11149, Loss: 0.05398896336555481\n",
      "Iteration 11150, Loss: 0.05417179316282272\n",
      "Iteration 11151, Loss: 0.053989164531230927\n",
      "Iteration 11152, Loss: 0.05417171120643616\n",
      "Iteration 11153, Loss: 0.05398927628993988\n",
      "Iteration 11154, Loss: 0.05417163297533989\n",
      "Iteration 11155, Loss: 0.05398935079574585\n",
      "Iteration 11156, Loss: 0.05417155474424362\n",
      "Iteration 11157, Loss: 0.053989361971616745\n",
      "Iteration 11158, Loss: 0.05417163297533989\n",
      "Iteration 11159, Loss: 0.05398927256464958\n",
      "Iteration 11160, Loss: 0.05417175218462944\n",
      "Iteration 11161, Loss: 0.05398907884955406\n",
      "Iteration 11162, Loss: 0.05417187511920929\n",
      "Iteration 11163, Loss: 0.05398888513445854\n",
      "Iteration 11164, Loss: 0.054172106087207794\n",
      "Iteration 11165, Loss: 0.05398881435394287\n",
      "Iteration 11166, Loss: 0.05417199060320854\n",
      "Iteration 11167, Loss: 0.053989000618457794\n",
      "Iteration 11168, Loss: 0.05417190492153168\n",
      "Iteration 11169, Loss: 0.053989194333553314\n",
      "Iteration 11170, Loss: 0.05417170748114586\n",
      "Iteration 11171, Loss: 0.053989242762327194\n",
      "Iteration 11172, Loss: 0.05417158454656601\n",
      "Iteration 11173, Loss: 0.053989432752132416\n",
      "Iteration 11174, Loss: 0.0541713610291481\n",
      "Iteration 11175, Loss: 0.053989510983228683\n",
      "Iteration 11176, Loss: 0.054171472787857056\n",
      "Iteration 11177, Loss: 0.053989555686712265\n",
      "Iteration 11178, Loss: 0.054171472787857056\n",
      "Iteration 11179, Loss: 0.0539894700050354\n",
      "Iteration 11180, Loss: 0.05417155474424362\n",
      "Iteration 11181, Loss: 0.053989313542842865\n",
      "Iteration 11182, Loss: 0.05417168140411377\n",
      "Iteration 11183, Loss: 0.05398903414607048\n",
      "Iteration 11184, Loss: 0.05417194962501526\n",
      "Iteration 11185, Loss: 0.05398884415626526\n",
      "Iteration 11186, Loss: 0.054172031581401825\n",
      "Iteration 11187, Loss: 0.053988855332136154\n",
      "Iteration 11188, Loss: 0.054171912372112274\n",
      "Iteration 11189, Loss: 0.053988855332136154\n",
      "Iteration 11190, Loss: 0.054171837866306305\n",
      "Iteration 11191, Loss: 0.05398907512426376\n",
      "Iteration 11192, Loss: 0.05417190119624138\n",
      "Iteration 11193, Loss: 0.053989045321941376\n",
      "Iteration 11194, Loss: 0.05417167395353317\n",
      "Iteration 11195, Loss: 0.05398939177393913\n",
      "Iteration 11196, Loss: 0.05417155474424362\n",
      "Iteration 11197, Loss: 0.0539894700050354\n",
      "Iteration 11198, Loss: 0.05417158827185631\n",
      "Iteration 11199, Loss: 0.053989510983228683\n",
      "Iteration 11200, Loss: 0.054171591997146606\n",
      "Iteration 11201, Loss: 0.05398927256464958\n",
      "Iteration 11202, Loss: 0.05417183041572571\n",
      "Iteration 11203, Loss: 0.05398903414607048\n",
      "Iteration 11204, Loss: 0.05417183041572571\n",
      "Iteration 11205, Loss: 0.05398900434374809\n",
      "Iteration 11206, Loss: 0.054171860218048096\n",
      "Iteration 11207, Loss: 0.0539892315864563\n",
      "Iteration 11208, Loss: 0.05417167395353317\n",
      "Iteration 11209, Loss: 0.053989388048648834\n",
      "Iteration 11210, Loss: 0.054171591997146606\n",
      "Iteration 11211, Loss: 0.05398931726813316\n",
      "Iteration 11212, Loss: 0.05417163297533989\n",
      "Iteration 11213, Loss: 0.05398927628993988\n",
      "Iteration 11214, Loss: 0.05417174845933914\n",
      "Iteration 11215, Loss: 0.053989164531230927\n",
      "Iteration 11216, Loss: 0.05417179316282272\n",
      "Iteration 11217, Loss: 0.0539892315864563\n",
      "Iteration 11218, Loss: 0.05417175218462944\n",
      "Iteration 11219, Loss: 0.05398908257484436\n",
      "Iteration 11220, Loss: 0.054171860218048096\n",
      "Iteration 11221, Loss: 0.053989164531230927\n",
      "Iteration 11222, Loss: 0.05417190119624138\n",
      "Iteration 11223, Loss: 0.0539892315864563\n",
      "Iteration 11224, Loss: 0.054171860218048096\n",
      "Iteration 11225, Loss: 0.053989313542842865\n",
      "Iteration 11226, Loss: 0.05417178198695183\n",
      "Iteration 11227, Loss: 0.05398931726813316\n",
      "Iteration 11228, Loss: 0.05417171120643616\n",
      "Iteration 11229, Loss: 0.05398927256464958\n",
      "Iteration 11230, Loss: 0.05417179316282272\n",
      "Iteration 11231, Loss: 0.05398907512426376\n",
      "Iteration 11232, Loss: 0.05417183041572571\n",
      "Iteration 11233, Loss: 0.053989119827747345\n",
      "Iteration 11234, Loss: 0.05417194962501526\n",
      "Iteration 11235, Loss: 0.05398903787136078\n",
      "Iteration 11236, Loss: 0.05417206138372421\n",
      "Iteration 11237, Loss: 0.053989119827747345\n",
      "Iteration 11238, Loss: 0.05417190492153168\n",
      "Iteration 11239, Loss: 0.053989194333553314\n",
      "Iteration 11240, Loss: 0.05417190119624138\n",
      "Iteration 11241, Loss: 0.0539892315864563\n",
      "Iteration 11242, Loss: 0.054171860218048096\n",
      "Iteration 11243, Loss: 0.05398908257484436\n",
      "Iteration 11244, Loss: 0.05417186766862869\n",
      "Iteration 11245, Loss: 0.053989119827747345\n",
      "Iteration 11246, Loss: 0.05417182669043541\n",
      "Iteration 11247, Loss: 0.053989164531230927\n",
      "Iteration 11248, Loss: 0.05417179316282272\n",
      "Iteration 11249, Loss: 0.053989164531230927\n",
      "Iteration 11250, Loss: 0.05417170375585556\n",
      "Iteration 11251, Loss: 0.05398928374052048\n",
      "Iteration 11252, Loss: 0.05417170375585556\n",
      "Iteration 11253, Loss: 0.053989313542842865\n",
      "Iteration 11254, Loss: 0.054171741008758545\n",
      "Iteration 11255, Loss: 0.05398927628993988\n",
      "Iteration 11256, Loss: 0.05417174845933914\n",
      "Iteration 11257, Loss: 0.053989239037036896\n",
      "Iteration 11258, Loss: 0.05417163297533989\n",
      "Iteration 11259, Loss: 0.05398927256464958\n",
      "Iteration 11260, Loss: 0.054171666502952576\n",
      "Iteration 11261, Loss: 0.053989432752132416\n",
      "Iteration 11262, Loss: 0.05417151376605034\n",
      "Iteration 11263, Loss: 0.05398935079574585\n",
      "Iteration 11264, Loss: 0.05417151376605034\n",
      "Iteration 11265, Loss: 0.05398967117071152\n",
      "Iteration 11266, Loss: 0.05417151004076004\n",
      "Iteration 11267, Loss: 0.053989477455616\n",
      "Iteration 11268, Loss: 0.05417155474424362\n",
      "Iteration 11269, Loss: 0.05398930609226227\n",
      "Iteration 11270, Loss: 0.054171718657016754\n",
      "Iteration 11271, Loss: 0.05398907512426376\n",
      "Iteration 11272, Loss: 0.05417202413082123\n",
      "Iteration 11273, Loss: 0.05398896336555481\n",
      "Iteration 11274, Loss: 0.05417187139391899\n",
      "Iteration 11275, Loss: 0.053989045321941376\n",
      "Iteration 11276, Loss: 0.05417183041572571\n",
      "Iteration 11277, Loss: 0.05398908257484436\n",
      "Iteration 11278, Loss: 0.05417170748114586\n",
      "Iteration 11279, Loss: 0.05398927256464958\n",
      "Iteration 11280, Loss: 0.05417170375585556\n",
      "Iteration 11281, Loss: 0.0539894700050354\n",
      "Iteration 11282, Loss: 0.05417166277766228\n",
      "Iteration 11283, Loss: 0.053989432752132416\n",
      "Iteration 11284, Loss: 0.05417170375585556\n",
      "Iteration 11285, Loss: 0.05398935079574585\n",
      "Iteration 11286, Loss: 0.05417167395353317\n",
      "Iteration 11287, Loss: 0.053989313542842865\n",
      "Iteration 11288, Loss: 0.05417183041572571\n",
      "Iteration 11289, Loss: 0.05398907884955406\n",
      "Iteration 11290, Loss: 0.05417187139391899\n",
      "Iteration 11291, Loss: 0.05398911237716675\n",
      "Iteration 11292, Loss: 0.05417183041572571\n",
      "Iteration 11293, Loss: 0.053989045321941376\n",
      "Iteration 11294, Loss: 0.05417190492153168\n",
      "Iteration 11295, Loss: 0.05398912355303764\n",
      "Iteration 11296, Loss: 0.05417178198695183\n",
      "Iteration 11297, Loss: 0.05398927256464958\n",
      "Iteration 11298, Loss: 0.05417163297533989\n",
      "Iteration 11299, Loss: 0.05398928374052048\n",
      "Iteration 11300, Loss: 0.05417167395353317\n",
      "Iteration 11301, Loss: 0.05398927628993988\n",
      "Iteration 11302, Loss: 0.05417171120643616\n",
      "Iteration 11303, Loss: 0.053989239037036896\n",
      "Iteration 11304, Loss: 0.05417182669043541\n",
      "Iteration 11305, Loss: 0.053989164531230927\n",
      "Iteration 11306, Loss: 0.05417186766862869\n",
      "Iteration 11307, Loss: 0.05398911237716675\n",
      "Iteration 11308, Loss: 0.054171979427337646\n",
      "Iteration 11309, Loss: 0.05398907884955406\n",
      "Iteration 11310, Loss: 0.05417194589972496\n",
      "Iteration 11311, Loss: 0.05398918688297272\n",
      "Iteration 11312, Loss: 0.05417187139391899\n",
      "Iteration 11313, Loss: 0.05398908257484436\n",
      "Iteration 11314, Loss: 0.05417179316282272\n",
      "Iteration 11315, Loss: 0.05398934334516525\n",
      "Iteration 11316, Loss: 0.054171785712242126\n",
      "Iteration 11317, Loss: 0.05398927628993988\n",
      "Iteration 11318, Loss: 0.0541715994477272\n",
      "Iteration 11319, Loss: 0.05398935079574585\n",
      "Iteration 11320, Loss: 0.05417155846953392\n",
      "Iteration 11321, Loss: 0.0539894662797451\n",
      "Iteration 11322, Loss: 0.054171591997146606\n",
      "Iteration 11323, Loss: 0.05398928374052048\n",
      "Iteration 11324, Loss: 0.054171666502952576\n",
      "Iteration 11325, Loss: 0.053989313542842865\n",
      "Iteration 11326, Loss: 0.05417170748114586\n",
      "Iteration 11327, Loss: 0.053989242762327194\n",
      "Iteration 11328, Loss: 0.05417182296514511\n",
      "Iteration 11329, Loss: 0.05398919805884361\n",
      "Iteration 11330, Loss: 0.05417170748114586\n",
      "Iteration 11331, Loss: 0.053989313542842865\n",
      "Iteration 11332, Loss: 0.05417183041572571\n",
      "Iteration 11333, Loss: 0.05398915335536003\n",
      "Iteration 11334, Loss: 0.054171979427337646\n",
      "Iteration 11335, Loss: 0.05398907512426376\n",
      "Iteration 11336, Loss: 0.05417187139391899\n",
      "Iteration 11337, Loss: 0.05398907512426376\n",
      "Iteration 11338, Loss: 0.05417187139391899\n",
      "Iteration 11339, Loss: 0.05398907512426376\n",
      "Iteration 11340, Loss: 0.05417194589972496\n",
      "Iteration 11341, Loss: 0.05398911237716675\n",
      "Iteration 11342, Loss: 0.05417187139391899\n",
      "Iteration 11343, Loss: 0.05398896336555481\n",
      "Iteration 11344, Loss: 0.05417183041572571\n",
      "Iteration 11345, Loss: 0.05398915708065033\n",
      "Iteration 11346, Loss: 0.054171860218048096\n",
      "Iteration 11347, Loss: 0.05398915708065033\n",
      "Iteration 11348, Loss: 0.05417166277766228\n",
      "Iteration 11349, Loss: 0.053989510983228683\n",
      "Iteration 11350, Loss: 0.05417151004076004\n",
      "Iteration 11351, Loss: 0.053989481180906296\n",
      "Iteration 11352, Loss: 0.05417139083147049\n",
      "Iteration 11353, Loss: 0.053989674896001816\n",
      "Iteration 11354, Loss: 0.05417139455676079\n",
      "Iteration 11355, Loss: 0.053989630192518234\n",
      "Iteration 11356, Loss: 0.05417155474424362\n",
      "Iteration 11357, Loss: 0.053989313542842865\n",
      "Iteration 11358, Loss: 0.05417187139391899\n",
      "Iteration 11359, Loss: 0.053989000618457794\n",
      "Iteration 11360, Loss: 0.05417206883430481\n",
      "Iteration 11361, Loss: 0.05398884415626526\n",
      "Iteration 11362, Loss: 0.05417206883430481\n",
      "Iteration 11363, Loss: 0.05398896336555481\n",
      "Iteration 11364, Loss: 0.0541720911860466\n",
      "Iteration 11365, Loss: 0.053989045321941376\n",
      "Iteration 11366, Loss: 0.05417190119624138\n",
      "Iteration 11367, Loss: 0.05398930236697197\n",
      "Iteration 11368, Loss: 0.05417171120643616\n",
      "Iteration 11369, Loss: 0.053989313542842865\n",
      "Iteration 11370, Loss: 0.05417171120643616\n",
      "Iteration 11371, Loss: 0.053989239037036896\n",
      "Iteration 11372, Loss: 0.05417171120643616\n",
      "Iteration 11373, Loss: 0.05398927256464958\n",
      "Iteration 11374, Loss: 0.054171860218048096\n",
      "Iteration 11375, Loss: 0.05398927256464958\n",
      "Iteration 11376, Loss: 0.05417175590991974\n",
      "Iteration 11377, Loss: 0.05398907512426376\n",
      "Iteration 11378, Loss: 0.05417198687791824\n",
      "Iteration 11379, Loss: 0.05398903414607048\n",
      "Iteration 11380, Loss: 0.05417190492153168\n",
      "Iteration 11381, Loss: 0.05398900434374809\n",
      "Iteration 11382, Loss: 0.05417190492153168\n",
      "Iteration 11383, Loss: 0.05398915335536003\n",
      "Iteration 11384, Loss: 0.05417175218462944\n",
      "Iteration 11385, Loss: 0.053989313542842865\n",
      "Iteration 11386, Loss: 0.054171860218048096\n",
      "Iteration 11387, Loss: 0.05398927628993988\n",
      "Iteration 11388, Loss: 0.05417174845933914\n",
      "Iteration 11389, Loss: 0.053989313542842865\n",
      "Iteration 11390, Loss: 0.05417163297533989\n",
      "Iteration 11391, Loss: 0.05398927256464958\n",
      "Iteration 11392, Loss: 0.05417190119624138\n",
      "Iteration 11393, Loss: 0.053989164531230927\n",
      "Iteration 11394, Loss: 0.054171860218048096\n",
      "Iteration 11395, Loss: 0.05398908257484436\n",
      "Iteration 11396, Loss: 0.054171860218048096\n",
      "Iteration 11397, Loss: 0.053989164531230927\n",
      "Iteration 11398, Loss: 0.05417183041572571\n",
      "Iteration 11399, Loss: 0.05398915708065033\n",
      "Iteration 11400, Loss: 0.05417187139391899\n",
      "Iteration 11401, Loss: 0.053989119827747345\n",
      "Iteration 11402, Loss: 0.054171785712242126\n",
      "Iteration 11403, Loss: 0.05398930609226227\n",
      "Iteration 11404, Loss: 0.05417151749134064\n",
      "Iteration 11405, Loss: 0.05398935824632645\n",
      "Iteration 11406, Loss: 0.05417155474424362\n",
      "Iteration 11407, Loss: 0.0539894700050354\n",
      "Iteration 11408, Loss: 0.05417151376605034\n",
      "Iteration 11409, Loss: 0.053989946842193604\n",
      "Iteration 11410, Loss: 0.05417143553495407\n",
      "Iteration 11411, Loss: 0.05399002879858017\n",
      "Iteration 11412, Loss: 0.0541708841919899\n",
      "Iteration 11413, Loss: 0.05399017781019211\n",
      "Iteration 11414, Loss: 0.05417122691869736\n",
      "Iteration 11415, Loss: 0.05398985743522644\n",
      "Iteration 11416, Loss: 0.05417151376605034\n",
      "Iteration 11417, Loss: 0.0539894700050354\n",
      "Iteration 11418, Loss: 0.054171860218048096\n",
      "Iteration 11419, Loss: 0.05398920178413391\n",
      "Iteration 11420, Loss: 0.05417199060320854\n",
      "Iteration 11421, Loss: 0.05398907512426376\n",
      "Iteration 11422, Loss: 0.05417206510901451\n",
      "Iteration 11423, Loss: 0.0539889931678772\n",
      "Iteration 11424, Loss: 0.054172225296497345\n",
      "Iteration 11425, Loss: 0.05398888513445854\n",
      "Iteration 11426, Loss: 0.054172269999980927\n",
      "Iteration 11427, Loss: 0.05398884415626526\n",
      "Iteration 11428, Loss: 0.05417219549417496\n",
      "Iteration 11429, Loss: 0.05398876592516899\n",
      "Iteration 11430, Loss: 0.05417218431830406\n",
      "Iteration 11431, Loss: 0.05398888513445854\n",
      "Iteration 11432, Loss: 0.05417199060320854\n",
      "Iteration 11433, Loss: 0.05398912355303764\n",
      "Iteration 11434, Loss: 0.05417163297533989\n",
      "Iteration 11435, Loss: 0.053989242762327194\n",
      "Iteration 11436, Loss: 0.054171591997146606\n",
      "Iteration 11437, Loss: 0.053989555686712265\n",
      "Iteration 11438, Loss: 0.05417146533727646\n",
      "Iteration 11439, Loss: 0.05398951470851898\n",
      "Iteration 11440, Loss: 0.05417143553495407\n",
      "Iteration 11441, Loss: 0.05398951470851898\n",
      "Iteration 11442, Loss: 0.05417174845933914\n",
      "Iteration 11443, Loss: 0.05398939177393913\n",
      "Iteration 11444, Loss: 0.05417194962501526\n",
      "Iteration 11445, Loss: 0.053989000618457794\n",
      "Iteration 11446, Loss: 0.054172269999980927\n",
      "Iteration 11447, Loss: 0.05398868769407272\n",
      "Iteration 11448, Loss: 0.054172299802303314\n",
      "Iteration 11449, Loss: 0.05398876965045929\n",
      "Iteration 11450, Loss: 0.05417214334011078\n",
      "Iteration 11451, Loss: 0.05398900434374809\n",
      "Iteration 11452, Loss: 0.05417175218462944\n",
      "Iteration 11453, Loss: 0.05398932099342346\n",
      "Iteration 11454, Loss: 0.05417151749134064\n",
      "Iteration 11455, Loss: 0.05398958921432495\n",
      "Iteration 11456, Loss: 0.054171353578567505\n",
      "Iteration 11457, Loss: 0.05398978292942047\n",
      "Iteration 11458, Loss: 0.05417130887508392\n",
      "Iteration 11459, Loss: 0.0539897158741951\n",
      "Iteration 11460, Loss: 0.05417127534747124\n",
      "Iteration 11461, Loss: 0.05398959666490555\n",
      "Iteration 11462, Loss: 0.05417151376605034\n",
      "Iteration 11463, Loss: 0.05398958921432495\n",
      "Iteration 11464, Loss: 0.054171718657016754\n",
      "Iteration 11465, Loss: 0.053989194333553314\n",
      "Iteration 11466, Loss: 0.05417206883430481\n",
      "Iteration 11467, Loss: 0.05398895591497421\n",
      "Iteration 11468, Loss: 0.054172299802303314\n",
      "Iteration 11469, Loss: 0.05398876592516899\n",
      "Iteration 11470, Loss: 0.054172150790691376\n",
      "Iteration 11471, Loss: 0.053988777101039886\n",
      "Iteration 11472, Loss: 0.0541720986366272\n",
      "Iteration 11473, Loss: 0.05398908257484436\n",
      "Iteration 11474, Loss: 0.05417190119624138\n",
      "Iteration 11475, Loss: 0.053989239037036896\n",
      "Iteration 11476, Loss: 0.05417178198695183\n",
      "Iteration 11477, Loss: 0.05398939177393913\n",
      "Iteration 11478, Loss: 0.05417163297533989\n",
      "Iteration 11479, Loss: 0.05398939177393913\n",
      "Iteration 11480, Loss: 0.05417163297533989\n",
      "Iteration 11481, Loss: 0.0539894625544548\n",
      "Iteration 11482, Loss: 0.05417167395353317\n",
      "Iteration 11483, Loss: 0.05398920178413391\n",
      "Iteration 11484, Loss: 0.05417182296514511\n",
      "Iteration 11485, Loss: 0.05398919805884361\n",
      "Iteration 11486, Loss: 0.05417179316282272\n",
      "Iteration 11487, Loss: 0.05398912355303764\n",
      "Iteration 11488, Loss: 0.05417194962501526\n",
      "Iteration 11489, Loss: 0.05398911237716675\n",
      "Iteration 11490, Loss: 0.05417186766862869\n",
      "Iteration 11491, Loss: 0.05398908257484436\n",
      "Iteration 11492, Loss: 0.05417183041572571\n",
      "Iteration 11493, Loss: 0.053989194333553314\n",
      "Iteration 11494, Loss: 0.05417194217443466\n",
      "Iteration 11495, Loss: 0.05398912355303764\n",
      "Iteration 11496, Loss: 0.05417183041572571\n",
      "Iteration 11497, Loss: 0.05398908257484436\n",
      "Iteration 11498, Loss: 0.05417190492153168\n",
      "Iteration 11499, Loss: 0.05398915335536003\n",
      "Iteration 11500, Loss: 0.05417183041572571\n",
      "Iteration 11501, Loss: 0.05398900434374809\n",
      "Iteration 11502, Loss: 0.05417194217443466\n",
      "Iteration 11503, Loss: 0.05398915335536003\n",
      "Iteration 11504, Loss: 0.05417182669043541\n",
      "Iteration 11505, Loss: 0.05398912355303764\n",
      "Iteration 11506, Loss: 0.05417182669043541\n",
      "Iteration 11507, Loss: 0.053989164531230927\n",
      "Iteration 11508, Loss: 0.05417190492153168\n",
      "Iteration 11509, Loss: 0.05398927256464958\n",
      "Iteration 11510, Loss: 0.05417175218462944\n",
      "Iteration 11511, Loss: 0.053989239037036896\n",
      "Iteration 11512, Loss: 0.05417167395353317\n",
      "Iteration 11513, Loss: 0.053989313542842865\n",
      "Iteration 11514, Loss: 0.054171591997146606\n",
      "Iteration 11515, Loss: 0.05398920178413391\n",
      "Iteration 11516, Loss: 0.05417171120643616\n",
      "Iteration 11517, Loss: 0.05398927628993988\n",
      "Iteration 11518, Loss: 0.05417175590991974\n",
      "Iteration 11519, Loss: 0.05398908257484436\n",
      "Iteration 11520, Loss: 0.05417187139391899\n",
      "Iteration 11521, Loss: 0.05398900434374809\n",
      "Iteration 11522, Loss: 0.05417194962501526\n",
      "Iteration 11523, Loss: 0.053988974541425705\n",
      "Iteration 11524, Loss: 0.05417183041572571\n",
      "Iteration 11525, Loss: 0.05398912355303764\n",
      "Iteration 11526, Loss: 0.05417190119624138\n",
      "Iteration 11527, Loss: 0.05398920178413391\n",
      "Iteration 11528, Loss: 0.054171741008758545\n",
      "Iteration 11529, Loss: 0.05398928374052048\n",
      "Iteration 11530, Loss: 0.054171666502952576\n",
      "Iteration 11531, Loss: 0.05398935824632645\n",
      "Iteration 11532, Loss: 0.05417168140411377\n",
      "Iteration 11533, Loss: 0.05398927628993988\n",
      "Iteration 11534, Loss: 0.05417175218462944\n",
      "Iteration 11535, Loss: 0.053989119827747345\n",
      "Iteration 11536, Loss: 0.054171882569789886\n",
      "Iteration 11537, Loss: 0.053988926112651825\n",
      "Iteration 11538, Loss: 0.05417202413082123\n",
      "Iteration 11539, Loss: 0.05398896336555481\n",
      "Iteration 11540, Loss: 0.05417187139391899\n",
      "Iteration 11541, Loss: 0.0539892315864563\n",
      "Iteration 11542, Loss: 0.054171591997146606\n",
      "Iteration 11543, Loss: 0.0539894700050354\n",
      "Iteration 11544, Loss: 0.05417143926024437\n",
      "Iteration 11545, Loss: 0.05398952215909958\n",
      "Iteration 11546, Loss: 0.05417132377624512\n",
      "Iteration 11547, Loss: 0.05398958921432495\n",
      "Iteration 11548, Loss: 0.05417148396372795\n",
      "Iteration 11549, Loss: 0.05398940294981003\n",
      "Iteration 11550, Loss: 0.054171591997146606\n",
      "Iteration 11551, Loss: 0.05398928374052048\n",
      "Iteration 11552, Loss: 0.05417175218462944\n",
      "Iteration 11553, Loss: 0.05398912355303764\n",
      "Iteration 11554, Loss: 0.05417199060320854\n",
      "Iteration 11555, Loss: 0.05398888513445854\n",
      "Iteration 11556, Loss: 0.054172031581401825\n",
      "Iteration 11557, Loss: 0.05398888885974884\n",
      "Iteration 11558, Loss: 0.054171979427337646\n",
      "Iteration 11559, Loss: 0.05398915708065033\n",
      "Iteration 11560, Loss: 0.05417175218462944\n",
      "Iteration 11561, Loss: 0.053989313542842865\n",
      "Iteration 11562, Loss: 0.054171591997146606\n",
      "Iteration 11563, Loss: 0.05398955196142197\n",
      "Iteration 11564, Loss: 0.05417124554514885\n",
      "Iteration 11565, Loss: 0.0539897084236145\n",
      "Iteration 11566, Loss: 0.05417127534747124\n",
      "Iteration 11567, Loss: 0.053989749401807785\n",
      "Iteration 11568, Loss: 0.05417143553495407\n",
      "Iteration 11569, Loss: 0.053989510983228683\n",
      "Iteration 11570, Loss: 0.054171524941921234\n",
      "Iteration 11571, Loss: 0.05398928374052048\n",
      "Iteration 11572, Loss: 0.054171718657016754\n",
      "Iteration 11573, Loss: 0.05398919805884361\n",
      "Iteration 11574, Loss: 0.05417187139391899\n",
      "Iteration 11575, Loss: 0.053989045321941376\n",
      "Iteration 11576, Loss: 0.05417194589972496\n",
      "Iteration 11577, Loss: 0.05398911237716675\n",
      "Iteration 11578, Loss: 0.05417186766862869\n",
      "Iteration 11579, Loss: 0.053989045321941376\n",
      "Iteration 11580, Loss: 0.05417175218462944\n",
      "Iteration 11581, Loss: 0.05398912355303764\n",
      "Iteration 11582, Loss: 0.05417175218462944\n",
      "Iteration 11583, Loss: 0.05398927628993988\n",
      "Iteration 11584, Loss: 0.05417179316282272\n",
      "Iteration 11585, Loss: 0.053989194333553314\n",
      "Iteration 11586, Loss: 0.05417175218462944\n",
      "Iteration 11587, Loss: 0.053989164531230927\n",
      "Iteration 11588, Loss: 0.05417179688811302\n",
      "Iteration 11589, Loss: 0.05398915708065033\n",
      "Iteration 11590, Loss: 0.05417183041572571\n",
      "Iteration 11591, Loss: 0.05398915335536003\n",
      "Iteration 11592, Loss: 0.05417187139391899\n",
      "Iteration 11593, Loss: 0.053989194333553314\n",
      "Iteration 11594, Loss: 0.05417183041572571\n",
      "Iteration 11595, Loss: 0.05398908257484436\n",
      "Iteration 11596, Loss: 0.05417175218462944\n",
      "Iteration 11597, Loss: 0.053989164531230927\n",
      "Iteration 11598, Loss: 0.05417170748114586\n",
      "Iteration 11599, Loss: 0.05398928374052048\n",
      "Iteration 11600, Loss: 0.05417170375585556\n",
      "Iteration 11601, Loss: 0.05398932844400406\n",
      "Iteration 11602, Loss: 0.05417155474424362\n",
      "Iteration 11603, Loss: 0.05398932099342346\n",
      "Iteration 11604, Loss: 0.05417170748114586\n",
      "Iteration 11605, Loss: 0.05398920178413391\n",
      "Iteration 11606, Loss: 0.05417168140411377\n",
      "Iteration 11607, Loss: 0.05398909002542496\n",
      "Iteration 11608, Loss: 0.05417179316282272\n",
      "Iteration 11609, Loss: 0.05398909002542496\n",
      "Iteration 11610, Loss: 0.05417179316282272\n",
      "Iteration 11611, Loss: 0.05398905277252197\n",
      "Iteration 11612, Loss: 0.05417171120643616\n",
      "Iteration 11613, Loss: 0.053989168256521225\n",
      "Iteration 11614, Loss: 0.054171666502952576\n",
      "Iteration 11615, Loss: 0.05398944020271301\n",
      "Iteration 11616, Loss: 0.05417155474424362\n",
      "Iteration 11617, Loss: 0.05398939549922943\n",
      "Iteration 11618, Loss: 0.05417163297533989\n",
      "Iteration 11619, Loss: 0.053989168256521225\n",
      "Iteration 11620, Loss: 0.05417168140411377\n",
      "Iteration 11621, Loss: 0.05398909002542496\n",
      "Iteration 11622, Loss: 0.05417179316282272\n",
      "Iteration 11623, Loss: 0.053989164531230927\n",
      "Iteration 11624, Loss: 0.05417186766862869\n",
      "Iteration 11625, Loss: 0.05398919805884361\n",
      "Iteration 11626, Loss: 0.05417175590991974\n",
      "Iteration 11627, Loss: 0.05398908257484436\n",
      "Iteration 11628, Loss: 0.05417182669043541\n",
      "Iteration 11629, Loss: 0.053988974541425705\n",
      "Iteration 11630, Loss: 0.05417182669043541\n",
      "Iteration 11631, Loss: 0.05398893356323242\n",
      "Iteration 11632, Loss: 0.05417168140411377\n",
      "Iteration 11633, Loss: 0.053988974541425705\n",
      "Iteration 11634, Loss: 0.05417171120643616\n",
      "Iteration 11635, Loss: 0.053989242762327194\n",
      "Iteration 11636, Loss: 0.05417167395353317\n",
      "Iteration 11637, Loss: 0.053989361971616745\n",
      "Iteration 11638, Loss: 0.05417148396372795\n",
      "Iteration 11639, Loss: 0.05398924648761749\n",
      "Iteration 11640, Loss: 0.05417163297533989\n",
      "Iteration 11641, Loss: 0.05398901551961899\n",
      "Iteration 11642, Loss: 0.054171837866306305\n",
      "Iteration 11643, Loss: 0.05398896336555481\n",
      "Iteration 11644, Loss: 0.05417187139391899\n",
      "Iteration 11645, Loss: 0.053989049047231674\n",
      "Iteration 11646, Loss: 0.054171785712242126\n",
      "Iteration 11647, Loss: 0.05398917198181152\n",
      "Iteration 11648, Loss: 0.054171524941921234\n",
      "Iteration 11649, Loss: 0.05398940294981003\n",
      "Iteration 11650, Loss: 0.05417140573263168\n",
      "Iteration 11651, Loss: 0.05398940294981003\n",
      "Iteration 11652, Loss: 0.05417139455676079\n",
      "Iteration 11653, Loss: 0.05398940294981003\n",
      "Iteration 11654, Loss: 0.054171472787857056\n",
      "Iteration 11655, Loss: 0.05398933216929436\n",
      "Iteration 11656, Loss: 0.05417144298553467\n",
      "Iteration 11657, Loss: 0.05398928374052048\n",
      "Iteration 11658, Loss: 0.05417171120643616\n",
      "Iteration 11659, Loss: 0.05398920923471451\n",
      "Iteration 11660, Loss: 0.05417194962501526\n",
      "Iteration 11661, Loss: 0.05398881062865257\n",
      "Iteration 11662, Loss: 0.05417210981249809\n",
      "Iteration 11663, Loss: 0.05398861691355705\n",
      "Iteration 11664, Loss: 0.054171960800886154\n",
      "Iteration 11665, Loss: 0.0539887361228466\n",
      "Iteration 11666, Loss: 0.054171837866306305\n",
      "Iteration 11667, Loss: 0.05398905277252197\n",
      "Iteration 11668, Loss: 0.05417163297533989\n",
      "Iteration 11669, Loss: 0.05398932099342346\n",
      "Iteration 11670, Loss: 0.054171472787857056\n",
      "Iteration 11671, Loss: 0.05398952588438988\n",
      "Iteration 11672, Loss: 0.05417124181985855\n",
      "Iteration 11673, Loss: 0.053989678621292114\n",
      "Iteration 11674, Loss: 0.054171204566955566\n",
      "Iteration 11675, Loss: 0.05398964136838913\n",
      "Iteration 11676, Loss: 0.05417129024863243\n",
      "Iteration 11677, Loss: 0.05398935079574585\n",
      "Iteration 11678, Loss: 0.0541716031730175\n",
      "Iteration 11679, Loss: 0.05398912355303764\n",
      "Iteration 11680, Loss: 0.05417167767882347\n",
      "Iteration 11681, Loss: 0.053989049047231674\n",
      "Iteration 11682, Loss: 0.05417179316282272\n",
      "Iteration 11683, Loss: 0.053989049047231674\n",
      "Iteration 11684, Loss: 0.05417171120643616\n",
      "Iteration 11685, Loss: 0.053988974541425705\n",
      "Iteration 11686, Loss: 0.05417167767882347\n",
      "Iteration 11687, Loss: 0.05398905277252197\n",
      "Iteration 11688, Loss: 0.05417167395353317\n",
      "Iteration 11689, Loss: 0.05398928374052048\n",
      "Iteration 11690, Loss: 0.054171524941921234\n",
      "Iteration 11691, Loss: 0.05398932844400406\n",
      "Iteration 11692, Loss: 0.05417144298553467\n",
      "Iteration 11693, Loss: 0.053989287465810776\n",
      "Iteration 11694, Loss: 0.05417152866721153\n",
      "Iteration 11695, Loss: 0.053989164531230927\n",
      "Iteration 11696, Loss: 0.05417171120643616\n",
      "Iteration 11697, Loss: 0.05398905277252197\n",
      "Iteration 11698, Loss: 0.054171718657016754\n",
      "Iteration 11699, Loss: 0.05398900806903839\n",
      "Iteration 11700, Loss: 0.05417183041572571\n",
      "Iteration 11701, Loss: 0.05398901551961899\n",
      "Iteration 11702, Loss: 0.05417175218462944\n",
      "Iteration 11703, Loss: 0.05398920178413391\n",
      "Iteration 11704, Loss: 0.054171524941921234\n",
      "Iteration 11705, Loss: 0.05398920923471451\n",
      "Iteration 11706, Loss: 0.05417148396372795\n",
      "Iteration 11707, Loss: 0.05398928374052048\n",
      "Iteration 11708, Loss: 0.05417151749134064\n",
      "Iteration 11709, Loss: 0.05398932099342346\n",
      "Iteration 11710, Loss: 0.05417144298553467\n",
      "Iteration 11711, Loss: 0.05398932844400406\n",
      "Iteration 11712, Loss: 0.05417140573263168\n",
      "Iteration 11713, Loss: 0.05398924648761749\n",
      "Iteration 11714, Loss: 0.05417155846953392\n",
      "Iteration 11715, Loss: 0.05398924648761749\n",
      "Iteration 11716, Loss: 0.05417155474424362\n",
      "Iteration 11717, Loss: 0.053989093750715256\n",
      "Iteration 11718, Loss: 0.054171524941921234\n",
      "Iteration 11719, Loss: 0.053989212960004807\n",
      "Iteration 11720, Loss: 0.054171472787857056\n",
      "Iteration 11721, Loss: 0.05398944020271301\n",
      "Iteration 11722, Loss: 0.054171204566955566\n",
      "Iteration 11723, Loss: 0.05398963391780853\n",
      "Iteration 11724, Loss: 0.054171234369277954\n",
      "Iteration 11725, Loss: 0.05398964136838913\n",
      "Iteration 11726, Loss: 0.054171279072761536\n",
      "Iteration 11727, Loss: 0.05398967117071152\n",
      "Iteration 11728, Loss: 0.0541713647544384\n",
      "Iteration 11729, Loss: 0.05398932099342346\n",
      "Iteration 11730, Loss: 0.05417148768901825\n",
      "Iteration 11731, Loss: 0.05398909002542496\n",
      "Iteration 11732, Loss: 0.054171763360500336\n",
      "Iteration 11733, Loss: 0.05398893356323242\n",
      "Iteration 11734, Loss: 0.05417180061340332\n",
      "Iteration 11735, Loss: 0.05398889631032944\n",
      "Iteration 11736, Loss: 0.05417175218462944\n",
      "Iteration 11737, Loss: 0.053989164531230927\n",
      "Iteration 11738, Loss: 0.05417148396372795\n",
      "Iteration 11739, Loss: 0.05398932844400406\n",
      "Iteration 11740, Loss: 0.05417124554514885\n",
      "Iteration 11741, Loss: 0.053989559412002563\n",
      "Iteration 11742, Loss: 0.054171204566955566\n",
      "Iteration 11743, Loss: 0.05398957058787346\n",
      "Iteration 11744, Loss: 0.054171204566955566\n",
      "Iteration 11745, Loss: 0.05398959666490555\n",
      "Iteration 11746, Loss: 0.05417139455676079\n",
      "Iteration 11747, Loss: 0.05398944020271301\n",
      "Iteration 11748, Loss: 0.054171591997146606\n",
      "Iteration 11749, Loss: 0.053989168256521225\n",
      "Iteration 11750, Loss: 0.05417167395353317\n",
      "Iteration 11751, Loss: 0.05398909002542496\n",
      "Iteration 11752, Loss: 0.05417183041572571\n",
      "Iteration 11753, Loss: 0.05398893356323242\n",
      "Iteration 11754, Loss: 0.05417175218462944\n",
      "Iteration 11755, Loss: 0.05398894101381302\n",
      "Iteration 11756, Loss: 0.0541715994477272\n",
      "Iteration 11757, Loss: 0.053989242762327194\n",
      "Iteration 11758, Loss: 0.05417144298553467\n",
      "Iteration 11759, Loss: 0.05398944020271301\n",
      "Iteration 11760, Loss: 0.05417128652334213\n",
      "Iteration 11761, Loss: 0.05398957058787346\n",
      "Iteration 11762, Loss: 0.054171122610569\n",
      "Iteration 11763, Loss: 0.0539897195994854\n",
      "Iteration 11764, Loss: 0.054170891642570496\n",
      "Iteration 11765, Loss: 0.05398979410529137\n",
      "Iteration 11766, Loss: 0.05417100712656975\n",
      "Iteration 11767, Loss: 0.05398963391780853\n",
      "Iteration 11768, Loss: 0.05417116731405258\n",
      "Iteration 11769, Loss: 0.05398963391780853\n",
      "Iteration 11770, Loss: 0.05417124554514885\n",
      "Iteration 11771, Loss: 0.05398944020271301\n",
      "Iteration 11772, Loss: 0.05417148023843765\n",
      "Iteration 11773, Loss: 0.053989287465810776\n",
      "Iteration 11774, Loss: 0.054171524941921234\n",
      "Iteration 11775, Loss: 0.053989242762327194\n",
      "Iteration 11776, Loss: 0.05417167395353317\n",
      "Iteration 11777, Loss: 0.05398905277252197\n",
      "Iteration 11778, Loss: 0.05417163297533989\n",
      "Iteration 11779, Loss: 0.05398920923471451\n",
      "Iteration 11780, Loss: 0.054171524941921234\n",
      "Iteration 11781, Loss: 0.05398917198181152\n",
      "Iteration 11782, Loss: 0.05417148396372795\n",
      "Iteration 11783, Loss: 0.05398913472890854\n",
      "Iteration 11784, Loss: 0.05417151749134064\n",
      "Iteration 11785, Loss: 0.05398920923471451\n",
      "Iteration 11786, Loss: 0.05417143926024437\n",
      "Iteration 11787, Loss: 0.05398924648761749\n",
      "Iteration 11788, Loss: 0.05417143926024437\n",
      "Iteration 11789, Loss: 0.05398936569690704\n",
      "Iteration 11790, Loss: 0.0541713610291481\n",
      "Iteration 11791, Loss: 0.05398960039019585\n",
      "Iteration 11792, Loss: 0.05417128652334213\n",
      "Iteration 11793, Loss: 0.05398952215909958\n",
      "Iteration 11794, Loss: 0.05417132377624512\n",
      "Iteration 11795, Loss: 0.053989291191101074\n",
      "Iteration 11796, Loss: 0.054171644151210785\n",
      "Iteration 11797, Loss: 0.05398920178413391\n",
      "Iteration 11798, Loss: 0.05417171120643616\n",
      "Iteration 11799, Loss: 0.053989168256521225\n",
      "Iteration 11800, Loss: 0.0541715994477272\n",
      "Iteration 11801, Loss: 0.05398905277252197\n",
      "Iteration 11802, Loss: 0.0541715994477272\n",
      "Iteration 11803, Loss: 0.053989093750715256\n",
      "Iteration 11804, Loss: 0.05417148023843765\n",
      "Iteration 11805, Loss: 0.05398937314748764\n",
      "Iteration 11806, Loss: 0.05417143553495407\n",
      "Iteration 11807, Loss: 0.053989410400390625\n",
      "Iteration 11808, Loss: 0.05417127534747124\n",
      "Iteration 11809, Loss: 0.05398960039019585\n",
      "Iteration 11810, Loss: 0.0541711263358593\n",
      "Iteration 11811, Loss: 0.053989678621292114\n",
      "Iteration 11812, Loss: 0.0541711151599884\n",
      "Iteration 11813, Loss: 0.05398976057767868\n",
      "Iteration 11814, Loss: 0.054171085357666016\n",
      "Iteration 11815, Loss: 0.053989674896001816\n",
      "Iteration 11816, Loss: 0.05417121201753616\n",
      "Iteration 11817, Loss: 0.05398940294981003\n",
      "Iteration 11818, Loss: 0.05417148396372795\n",
      "Iteration 11819, Loss: 0.053989164531230927\n",
      "Iteration 11820, Loss: 0.05417179688811302\n",
      "Iteration 11821, Loss: 0.05398900806903839\n",
      "Iteration 11822, Loss: 0.05417187511920929\n",
      "Iteration 11823, Loss: 0.05398892983794212\n",
      "Iteration 11824, Loss: 0.054171912372112274\n",
      "Iteration 11825, Loss: 0.05398885905742645\n",
      "Iteration 11826, Loss: 0.05417172238230705\n",
      "Iteration 11827, Loss: 0.053988974541425705\n",
      "Iteration 11828, Loss: 0.05417156219482422\n",
      "Iteration 11829, Loss: 0.053988587111234665\n",
      "Iteration 11830, Loss: 0.05417155846953392\n",
      "Iteration 11831, Loss: 0.05398866534233093\n",
      "Iteration 11832, Loss: 0.054170817136764526\n",
      "Iteration 11833, Loss: 0.053988706320524216\n",
      "Iteration 11834, Loss: 0.05417073890566826\n",
      "Iteration 11835, Loss: 0.0539887398481369\n",
      "Iteration 11836, Loss: 0.054170817136764526\n",
      "Iteration 11837, Loss: 0.053988583385944366\n",
      "Iteration 11838, Loss: 0.05417093634605408\n",
      "Iteration 11839, Loss: 0.053988467901945114\n",
      "Iteration 11840, Loss: 0.054170869290828705\n",
      "Iteration 11841, Loss: 0.05398854613304138\n",
      "Iteration 11842, Loss: 0.05417082831263542\n",
      "Iteration 11843, Loss: 0.0539885088801384\n",
      "Iteration 11844, Loss: 0.05417082458734512\n",
      "Iteration 11845, Loss: 0.05398862808942795\n",
      "Iteration 11846, Loss: 0.05417078733444214\n",
      "Iteration 11847, Loss: 0.053988587111234665\n",
      "Iteration 11848, Loss: 0.05417082458734512\n",
      "Iteration 11849, Loss: 0.053988516330718994\n",
      "Iteration 11850, Loss: 0.05417078733444214\n",
      "Iteration 11851, Loss: 0.0539885088801384\n",
      "Iteration 11852, Loss: 0.05417090281844139\n",
      "Iteration 11853, Loss: 0.05398842692375183\n",
      "Iteration 11854, Loss: 0.054171137511730194\n",
      "Iteration 11855, Loss: 0.053988199681043625\n",
      "Iteration 11856, Loss: 0.05417121946811676\n",
      "Iteration 11857, Loss: 0.053988151252269745\n",
      "Iteration 11858, Loss: 0.054171375930309296\n",
      "Iteration 11859, Loss: 0.05398810654878616\n",
      "Iteration 11860, Loss: 0.054171256721019745\n",
      "Iteration 11861, Loss: 0.053988151252269745\n",
      "Iteration 11862, Loss: 0.05417117476463318\n",
      "Iteration 11863, Loss: 0.05398834869265556\n",
      "Iteration 11864, Loss: 0.05417093634605408\n",
      "Iteration 11865, Loss: 0.05398861691355705\n",
      "Iteration 11866, Loss: 0.05417073890566826\n",
      "Iteration 11867, Loss: 0.053988706320524216\n",
      "Iteration 11868, Loss: 0.054170578718185425\n",
      "Iteration 11869, Loss: 0.053988825529813766\n",
      "Iteration 11870, Loss: 0.05417051166296005\n",
      "Iteration 11871, Loss: 0.053988825529813766\n",
      "Iteration 11872, Loss: 0.05417070910334587\n",
      "Iteration 11873, Loss: 0.05398869514465332\n",
      "Iteration 11874, Loss: 0.05417090654373169\n",
      "Iteration 11875, Loss: 0.053988467901945114\n",
      "Iteration 11876, Loss: 0.05417101830244064\n",
      "Iteration 11877, Loss: 0.05398835241794586\n",
      "Iteration 11878, Loss: 0.05417105555534363\n",
      "Iteration 11879, Loss: 0.05398835241794586\n",
      "Iteration 11880, Loss: 0.054171137511730194\n",
      "Iteration 11881, Loss: 0.05398835241794586\n",
      "Iteration 11882, Loss: 0.05417102202773094\n",
      "Iteration 11883, Loss: 0.053988464176654816\n",
      "Iteration 11884, Loss: 0.05417109653353691\n",
      "Iteration 11885, Loss: 0.05398835241794586\n",
      "Iteration 11886, Loss: 0.05417109653353691\n",
      "Iteration 11887, Loss: 0.053988389670848846\n",
      "Iteration 11888, Loss: 0.05417109653353691\n",
      "Iteration 11889, Loss: 0.053988199681043625\n",
      "Iteration 11890, Loss: 0.05417105555534363\n",
      "Iteration 11891, Loss: 0.053988318890333176\n",
      "Iteration 11892, Loss: 0.05417105183005333\n",
      "Iteration 11893, Loss: 0.05398854613304138\n",
      "Iteration 11894, Loss: 0.05417089909315109\n",
      "Iteration 11895, Loss: 0.0539885088801384\n",
      "Iteration 11896, Loss: 0.05417093262076378\n",
      "Iteration 11897, Loss: 0.05398866534233093\n",
      "Iteration 11898, Loss: 0.054170750081539154\n",
      "Iteration 11899, Loss: 0.053988661617040634\n",
      "Iteration 11900, Loss: 0.05417070910334587\n",
      "Iteration 11901, Loss: 0.053988657891750336\n",
      "Iteration 11902, Loss: 0.05417089909315109\n",
      "Iteration 11903, Loss: 0.05398861691355705\n",
      "Iteration 11904, Loss: 0.05417101830244064\n",
      "Iteration 11905, Loss: 0.05398834869265556\n",
      "Iteration 11906, Loss: 0.05417106673121452\n",
      "Iteration 11907, Loss: 0.053988151252269745\n",
      "Iteration 11908, Loss: 0.05417114496231079\n",
      "Iteration 11909, Loss: 0.05398818850517273\n",
      "Iteration 11910, Loss: 0.05417117103934288\n",
      "Iteration 11911, Loss: 0.05398834869265556\n",
      "Iteration 11912, Loss: 0.05417097732424736\n",
      "Iteration 11913, Loss: 0.053988512605428696\n",
      "Iteration 11914, Loss: 0.05417073890566826\n",
      "Iteration 11915, Loss: 0.053988706320524216\n",
      "Iteration 11916, Loss: 0.05417066439986229\n",
      "Iteration 11917, Loss: 0.053988706320524216\n",
      "Iteration 11918, Loss: 0.05417070537805557\n",
      "Iteration 11919, Loss: 0.05398862808942795\n",
      "Iteration 11920, Loss: 0.05417085811495781\n",
      "Iteration 11921, Loss: 0.05398866534233093\n",
      "Iteration 11922, Loss: 0.05417097732424736\n",
      "Iteration 11923, Loss: 0.05398854613304138\n",
      "Iteration 11924, Loss: 0.054171063005924225\n",
      "Iteration 11925, Loss: 0.05398830398917198\n",
      "Iteration 11926, Loss: 0.054171185940504074\n",
      "Iteration 11927, Loss: 0.05398811027407646\n",
      "Iteration 11928, Loss: 0.054171256721019745\n",
      "Iteration 11929, Loss: 0.05398822948336601\n",
      "Iteration 11930, Loss: 0.05417109653353691\n",
      "Iteration 11931, Loss: 0.05398834869265556\n",
      "Iteration 11932, Loss: 0.05417089909315109\n",
      "Iteration 11933, Loss: 0.05398866534233093\n",
      "Iteration 11934, Loss: 0.05417066067457199\n",
      "Iteration 11935, Loss: 0.05398882180452347\n",
      "Iteration 11936, Loss: 0.05417051166296005\n",
      "Iteration 11937, Loss: 0.0539887472987175\n",
      "Iteration 11938, Loss: 0.05417066812515259\n",
      "Iteration 11939, Loss: 0.05398854613304138\n",
      "Iteration 11940, Loss: 0.05417082831263542\n",
      "Iteration 11941, Loss: 0.053988464176654816\n",
      "Iteration 11942, Loss: 0.054171063005924225\n",
      "Iteration 11943, Loss: 0.05398830771446228\n",
      "Iteration 11944, Loss: 0.054171137511730194\n",
      "Iteration 11945, Loss: 0.05398834869265556\n",
      "Iteration 11946, Loss: 0.05417117476463318\n",
      "Iteration 11947, Loss: 0.05398830771446228\n",
      "Iteration 11948, Loss: 0.054171137511730194\n",
      "Iteration 11949, Loss: 0.053988389670848846\n",
      "Iteration 11950, Loss: 0.05417101830244064\n",
      "Iteration 11951, Loss: 0.053988389670848846\n",
      "Iteration 11952, Loss: 0.05417086184024811\n",
      "Iteration 11953, Loss: 0.05398854613304138\n",
      "Iteration 11954, Loss: 0.05417093634605408\n",
      "Iteration 11955, Loss: 0.053988467901945114\n",
      "Iteration 11956, Loss: 0.05417101830244064\n",
      "Iteration 11957, Loss: 0.0539885088801384\n",
      "Iteration 11958, Loss: 0.05417097732424736\n",
      "Iteration 11959, Loss: 0.05398845672607422\n",
      "Iteration 11960, Loss: 0.054171063005924225\n",
      "Iteration 11961, Loss: 0.05398830771446228\n",
      "Iteration 11962, Loss: 0.05417109280824661\n",
      "Iteration 11963, Loss: 0.053988318890333176\n",
      "Iteration 11964, Loss: 0.05417097359895706\n",
      "Iteration 11965, Loss: 0.05398839712142944\n",
      "Iteration 11966, Loss: 0.05417077988386154\n",
      "Iteration 11967, Loss: 0.05398867279291153\n",
      "Iteration 11968, Loss: 0.054170697927474976\n",
      "Iteration 11969, Loss: 0.05398886650800705\n",
      "Iteration 11970, Loss: 0.054170697927474976\n",
      "Iteration 11971, Loss: 0.05398881435394287\n",
      "Iteration 11972, Loss: 0.05417073890566826\n",
      "Iteration 11973, Loss: 0.0539887398481369\n",
      "Iteration 11974, Loss: 0.05417085811495781\n",
      "Iteration 11975, Loss: 0.05398861691355705\n",
      "Iteration 11976, Loss: 0.05417105183005333\n",
      "Iteration 11977, Loss: 0.053988389670848846\n",
      "Iteration 11978, Loss: 0.05417124554514885\n",
      "Iteration 11979, Loss: 0.05398815497756004\n",
      "Iteration 11980, Loss: 0.05417121574282646\n",
      "Iteration 11981, Loss: 0.053988270461559296\n",
      "Iteration 11982, Loss: 0.05417117476463318\n",
      "Iteration 11983, Loss: 0.053988274186849594\n",
      "Iteration 11984, Loss: 0.05417104810476303\n",
      "Iteration 11985, Loss: 0.053988512605428696\n",
      "Iteration 11986, Loss: 0.05417077988386154\n",
      "Iteration 11987, Loss: 0.05398881435394287\n",
      "Iteration 11988, Loss: 0.05417058989405632\n",
      "Iteration 11989, Loss: 0.05398886650800705\n",
      "Iteration 11990, Loss: 0.05417054146528244\n",
      "Iteration 11991, Loss: 0.05398878455162048\n",
      "Iteration 11992, Loss: 0.054170697927474976\n",
      "Iteration 11993, Loss: 0.0539887398481369\n",
      "Iteration 11994, Loss: 0.05417089909315109\n",
      "Iteration 11995, Loss: 0.05398854613304138\n",
      "Iteration 11996, Loss: 0.054170988500118256\n",
      "Iteration 11997, Loss: 0.05398842692375183\n",
      "Iteration 11998, Loss: 0.0541713647544384\n",
      "Iteration 11999, Loss: 0.053988151252269745\n",
      "Iteration 12000, Loss: 0.05417141318321228\n",
      "Iteration 12001, Loss: 0.05398806929588318\n",
      "Iteration 12002, Loss: 0.05417133495211601\n",
      "Iteration 12003, Loss: 0.05398822948336601\n",
      "Iteration 12004, Loss: 0.0541711300611496\n",
      "Iteration 12005, Loss: 0.0539885088801384\n",
      "Iteration 12006, Loss: 0.054170817136764526\n",
      "Iteration 12007, Loss: 0.05398859828710556\n",
      "Iteration 12008, Loss: 0.05417066439986229\n",
      "Iteration 12009, Loss: 0.053988825529813766\n",
      "Iteration 12010, Loss: 0.0541706308722496\n",
      "Iteration 12011, Loss: 0.05398866534233093\n",
      "Iteration 12012, Loss: 0.05417089909315109\n",
      "Iteration 12013, Loss: 0.053988538682460785\n",
      "Iteration 12014, Loss: 0.05417102202773094\n",
      "Iteration 12015, Loss: 0.05398830771446228\n",
      "Iteration 12016, Loss: 0.054171256721019745\n",
      "Iteration 12017, Loss: 0.05398811399936676\n",
      "Iteration 12018, Loss: 0.054171256721019745\n",
      "Iteration 12019, Loss: 0.05398812144994736\n",
      "Iteration 12020, Loss: 0.05417121574282646\n",
      "Iteration 12021, Loss: 0.053987711668014526\n",
      "Iteration 12022, Loss: 0.05417124927043915\n",
      "Iteration 12023, Loss: 0.05398748442530632\n",
      "Iteration 12024, Loss: 0.054171930998563766\n",
      "Iteration 12025, Loss: 0.05398762971162796\n",
      "Iteration 12026, Loss: 0.05417173355817795\n",
      "Iteration 12027, Loss: 0.05398771911859512\n",
      "Iteration 12028, Loss: 0.05417140945792198\n",
      "Iteration 12029, Loss: 0.05398811027407646\n",
      "Iteration 12030, Loss: 0.05417105555534363\n",
      "Iteration 12031, Loss: 0.05398842692375183\n",
      "Iteration 12032, Loss: 0.05417066812515259\n",
      "Iteration 12033, Loss: 0.053988583385944366\n",
      "Iteration 12034, Loss: 0.05417073890566826\n",
      "Iteration 12035, Loss: 0.05398869514465332\n",
      "Iteration 12036, Loss: 0.05417073890566826\n",
      "Iteration 12037, Loss: 0.053988732397556305\n",
      "Iteration 12038, Loss: 0.05417073890566826\n",
      "Iteration 12039, Loss: 0.05398869141936302\n",
      "Iteration 12040, Loss: 0.05417074263095856\n",
      "Iteration 12041, Loss: 0.053988538682460785\n",
      "Iteration 12042, Loss: 0.05417078733444214\n",
      "Iteration 12043, Loss: 0.05398834869265556\n",
      "Iteration 12044, Loss: 0.05417105555534363\n",
      "Iteration 12045, Loss: 0.053988270461559296\n",
      "Iteration 12046, Loss: 0.05417109653353691\n",
      "Iteration 12047, Loss: 0.053988225758075714\n",
      "Iteration 12048, Loss: 0.05417117476463318\n",
      "Iteration 12049, Loss: 0.05398806929588318\n",
      "Iteration 12050, Loss: 0.054171256721019745\n",
      "Iteration 12051, Loss: 0.05398806929588318\n",
      "Iteration 12052, Loss: 0.05417101830244064\n",
      "Iteration 12053, Loss: 0.05398833751678467\n",
      "Iteration 12054, Loss: 0.05417097732424736\n",
      "Iteration 12055, Loss: 0.05398845672607422\n",
      "Iteration 12056, Loss: 0.05417093262076378\n",
      "Iteration 12057, Loss: 0.05398845672607422\n",
      "Iteration 12058, Loss: 0.054170817136764526\n",
      "Iteration 12059, Loss: 0.05398857593536377\n",
      "Iteration 12060, Loss: 0.05417077988386154\n",
      "Iteration 12061, Loss: 0.05398857593536377\n",
      "Iteration 12062, Loss: 0.05417070537805557\n",
      "Iteration 12063, Loss: 0.05398854240775108\n",
      "Iteration 12064, Loss: 0.054170817136764526\n",
      "Iteration 12065, Loss: 0.0539884977042675\n",
      "Iteration 12066, Loss: 0.054170869290828705\n",
      "Iteration 12067, Loss: 0.053988344967365265\n",
      "Iteration 12068, Loss: 0.05417105555534363\n",
      "Iteration 12069, Loss: 0.0539884939789772\n",
      "Iteration 12070, Loss: 0.05417093634605408\n",
      "Iteration 12071, Loss: 0.05398834869265556\n",
      "Iteration 12072, Loss: 0.05417093634605408\n",
      "Iteration 12073, Loss: 0.053988467901945114\n",
      "Iteration 12074, Loss: 0.05417093262076378\n",
      "Iteration 12075, Loss: 0.0539884977042675\n",
      "Iteration 12076, Loss: 0.05417101830244064\n",
      "Iteration 12077, Loss: 0.05398842319846153\n",
      "Iteration 12078, Loss: 0.05417109653353691\n",
      "Iteration 12079, Loss: 0.0539882630109787\n",
      "Iteration 12080, Loss: 0.054171137511730194\n",
      "Iteration 12081, Loss: 0.053988270461559296\n",
      "Iteration 12082, Loss: 0.05417121574282646\n",
      "Iteration 12083, Loss: 0.05398830771446228\n",
      "Iteration 12084, Loss: 0.054171137511730194\n",
      "Iteration 12085, Loss: 0.053988151252269745\n",
      "Iteration 12086, Loss: 0.05417105555534363\n",
      "Iteration 12087, Loss: 0.05398830398917198\n",
      "Iteration 12088, Loss: 0.05417097732424736\n",
      "Iteration 12089, Loss: 0.05398845672607422\n",
      "Iteration 12090, Loss: 0.05417085811495781\n",
      "Iteration 12091, Loss: 0.053988538682460785\n",
      "Iteration 12092, Loss: 0.05417070537805557\n",
      "Iteration 12093, Loss: 0.05398861691355705\n",
      "Iteration 12094, Loss: 0.0541706308722496\n",
      "Iteration 12095, Loss: 0.053988777101039886\n",
      "Iteration 12096, Loss: 0.054170697927474976\n",
      "Iteration 12097, Loss: 0.053989212960004807\n",
      "Iteration 12098, Loss: 0.05417070910334587\n",
      "Iteration 12099, Loss: 0.053989093750715256\n",
      "Iteration 12100, Loss: 0.05417126417160034\n",
      "Iteration 12101, Loss: 0.053989019244909286\n",
      "Iteration 12102, Loss: 0.05417153239250183\n",
      "Iteration 12103, Loss: 0.05398886650800705\n",
      "Iteration 12104, Loss: 0.05417153239250183\n",
      "Iteration 12105, Loss: 0.05398878455162048\n",
      "Iteration 12106, Loss: 0.054171573370695114\n",
      "Iteration 12107, Loss: 0.05398893356323242\n",
      "Iteration 12108, Loss: 0.05417141318321228\n",
      "Iteration 12109, Loss: 0.05398905277252197\n",
      "Iteration 12110, Loss: 0.05417129397392273\n",
      "Iteration 12111, Loss: 0.053989093750715256\n",
      "Iteration 12112, Loss: 0.05417114123702049\n",
      "Iteration 12113, Loss: 0.05398925393819809\n",
      "Iteration 12114, Loss: 0.05417121574282646\n",
      "Iteration 12115, Loss: 0.053989212960004807\n",
      "Iteration 12116, Loss: 0.05417126044631004\n",
      "Iteration 12117, Loss: 0.05398913472890854\n",
      "Iteration 12118, Loss: 0.05417126417160034\n",
      "Iteration 12119, Loss: 0.05398894473910332\n",
      "Iteration 12120, Loss: 0.05417145416140556\n",
      "Iteration 12121, Loss: 0.053988974541425705\n",
      "Iteration 12122, Loss: 0.054171495139598846\n",
      "Iteration 12123, Loss: 0.053988825529813766\n",
      "Iteration 12124, Loss: 0.054171495139598846\n",
      "Iteration 12125, Loss: 0.05398885905742645\n",
      "Iteration 12126, Loss: 0.05417153239250183\n",
      "Iteration 12127, Loss: 0.053988903760910034\n",
      "Iteration 12128, Loss: 0.054171379655599594\n",
      "Iteration 12129, Loss: 0.053988974541425705\n",
      "Iteration 12130, Loss: 0.05417122691869736\n",
      "Iteration 12131, Loss: 0.05398901551961899\n",
      "Iteration 12132, Loss: 0.05417133495211601\n",
      "Iteration 12133, Loss: 0.05398917198181152\n",
      "Iteration 12134, Loss: 0.054171256721019745\n",
      "Iteration 12135, Loss: 0.053989093750715256\n",
      "Iteration 12136, Loss: 0.05417145416140556\n",
      "Iteration 12137, Loss: 0.05398886650800705\n",
      "Iteration 12138, Loss: 0.054171495139598846\n",
      "Iteration 12139, Loss: 0.05398859828710556\n",
      "Iteration 12140, Loss: 0.05417165160179138\n",
      "Iteration 12141, Loss: 0.05398885905742645\n",
      "Iteration 12142, Loss: 0.05417153239250183\n",
      "Iteration 12143, Loss: 0.05398882180452347\n",
      "Iteration 12144, Loss: 0.054171375930309296\n",
      "Iteration 12145, Loss: 0.05398909002542496\n",
      "Iteration 12146, Loss: 0.054171185940504074\n",
      "Iteration 12147, Loss: 0.05398913472890854\n",
      "Iteration 12148, Loss: 0.05417121946811676\n",
      "Iteration 12149, Loss: 0.05398910492658615\n",
      "Iteration 12150, Loss: 0.05417126417160034\n",
      "Iteration 12151, Loss: 0.05398905277252197\n",
      "Iteration 12152, Loss: 0.05417138338088989\n",
      "Iteration 12153, Loss: 0.05398905277252197\n",
      "Iteration 12154, Loss: 0.05417146533727646\n",
      "Iteration 12155, Loss: 0.05398867279291153\n",
      "Iteration 12156, Loss: 0.05417146533727646\n",
      "Iteration 12157, Loss: 0.05398882180452347\n",
      "Iteration 12158, Loss: 0.05417153984308243\n",
      "Iteration 12159, Loss: 0.053988780826330185\n",
      "Iteration 12160, Loss: 0.054171379655599594\n",
      "Iteration 12161, Loss: 0.053988978266716\n",
      "Iteration 12162, Loss: 0.05417121946811676\n",
      "Iteration 12163, Loss: 0.053989212960004807\n",
      "Iteration 12164, Loss: 0.05417129397392273\n",
      "Iteration 12165, Loss: 0.053989168256521225\n",
      "Iteration 12166, Loss: 0.054171424359083176\n",
      "Iteration 12167, Loss: 0.05398885905742645\n",
      "Iteration 12168, Loss: 0.054171692579984665\n",
      "Iteration 12169, Loss: 0.05398862808942795\n",
      "Iteration 12170, Loss: 0.05417170375585556\n",
      "Iteration 12171, Loss: 0.053988587111234665\n",
      "Iteration 12172, Loss: 0.05417170375585556\n",
      "Iteration 12173, Loss: 0.053988587111234665\n",
      "Iteration 12174, Loss: 0.05417165160179138\n",
      "Iteration 12175, Loss: 0.05398869514465332\n",
      "Iteration 12176, Loss: 0.05417145416140556\n",
      "Iteration 12177, Loss: 0.05398893356323242\n",
      "Iteration 12178, Loss: 0.05417133495211601\n",
      "Iteration 12179, Loss: 0.053989019244909286\n",
      "Iteration 12180, Loss: 0.054171185940504074\n",
      "Iteration 12181, Loss: 0.053989093750715256\n",
      "Iteration 12182, Loss: 0.05417122691869736\n",
      "Iteration 12183, Loss: 0.05398905277252197\n",
      "Iteration 12184, Loss: 0.05417122691869736\n",
      "Iteration 12185, Loss: 0.05398905277252197\n",
      "Iteration 12186, Loss: 0.05417129397392273\n",
      "Iteration 12187, Loss: 0.053989019244909286\n",
      "Iteration 12188, Loss: 0.05417141318321228\n",
      "Iteration 12189, Loss: 0.05398894473910332\n",
      "Iteration 12190, Loss: 0.05417146533727646\n",
      "Iteration 12191, Loss: 0.05398867279291153\n",
      "Iteration 12192, Loss: 0.05417165160179138\n",
      "Iteration 12193, Loss: 0.05398882180452347\n",
      "Iteration 12194, Loss: 0.05417153239250183\n",
      "Iteration 12195, Loss: 0.05398882180452347\n",
      "Iteration 12196, Loss: 0.054171375930309296\n",
      "Iteration 12197, Loss: 0.053988978266716\n",
      "Iteration 12198, Loss: 0.054171185940504074\n",
      "Iteration 12199, Loss: 0.05398913472890854\n",
      "Iteration 12200, Loss: 0.05417117476463318\n",
      "Iteration 12201, Loss: 0.05398917198181152\n",
      "Iteration 12202, Loss: 0.05417114123702049\n",
      "Iteration 12203, Loss: 0.053989361971616745\n",
      "Iteration 12204, Loss: 0.054171185940504074\n",
      "Iteration 12205, Loss: 0.05398913472890854\n",
      "Iteration 12206, Loss: 0.05417141318321228\n",
      "Iteration 12207, Loss: 0.053988903760910034\n",
      "Iteration 12208, Loss: 0.054171618074178696\n",
      "Iteration 12209, Loss: 0.0539887398481369\n",
      "Iteration 12210, Loss: 0.054171737283468246\n",
      "Iteration 12211, Loss: 0.053988587111234665\n",
      "Iteration 12212, Loss: 0.054171692579984665\n",
      "Iteration 12213, Loss: 0.05398866534233093\n",
      "Iteration 12214, Loss: 0.05417165160179138\n",
      "Iteration 12215, Loss: 0.05398866534233093\n",
      "Iteration 12216, Loss: 0.054171495139598846\n",
      "Iteration 12217, Loss: 0.05398905277252197\n",
      "Iteration 12218, Loss: 0.05417114496231079\n",
      "Iteration 12219, Loss: 0.05398917198181152\n",
      "Iteration 12220, Loss: 0.054171256721019745\n",
      "Iteration 12221, Loss: 0.05398906394839287\n",
      "Iteration 12222, Loss: 0.05417129397392273\n",
      "Iteration 12223, Loss: 0.05398894473910332\n",
      "Iteration 12224, Loss: 0.054171424359083176\n",
      "Iteration 12225, Loss: 0.053988903760910034\n",
      "Iteration 12226, Loss: 0.05417150259017944\n",
      "Iteration 12227, Loss: 0.05398866534233093\n",
      "Iteration 12228, Loss: 0.054171621799468994\n",
      "Iteration 12229, Loss: 0.05398855358362198\n",
      "Iteration 12230, Loss: 0.05417169630527496\n",
      "Iteration 12231, Loss: 0.053988587111234665\n",
      "Iteration 12232, Loss: 0.0541716143488884\n",
      "Iteration 12233, Loss: 0.0539887472987175\n",
      "Iteration 12234, Loss: 0.05417165160179138\n",
      "Iteration 12235, Loss: 0.05398866534233093\n",
      "Iteration 12236, Loss: 0.05417153239250183\n",
      "Iteration 12237, Loss: 0.05398893356323242\n",
      "Iteration 12238, Loss: 0.05417134612798691\n",
      "Iteration 12239, Loss: 0.05398894473910332\n",
      "Iteration 12240, Loss: 0.05417133495211601\n",
      "Iteration 12241, Loss: 0.053989093750715256\n",
      "Iteration 12242, Loss: 0.054171375930309296\n",
      "Iteration 12243, Loss: 0.053988978266716\n",
      "Iteration 12244, Loss: 0.054171498864889145\n",
      "Iteration 12245, Loss: 0.05398886650800705\n",
      "Iteration 12246, Loss: 0.054171573370695114\n",
      "Iteration 12247, Loss: 0.0539887472987175\n",
      "Iteration 12248, Loss: 0.05417165160179138\n",
      "Iteration 12249, Loss: 0.053988706320524216\n",
      "Iteration 12250, Loss: 0.0541716143488884\n",
      "Iteration 12251, Loss: 0.05398878455162048\n",
      "Iteration 12252, Loss: 0.05417172610759735\n",
      "Iteration 12253, Loss: 0.0539887361228466\n",
      "Iteration 12254, Loss: 0.054171618074178696\n",
      "Iteration 12255, Loss: 0.053988587111234665\n",
      "Iteration 12256, Loss: 0.05417166277766228\n",
      "Iteration 12257, Loss: 0.0539887472987175\n",
      "Iteration 12258, Loss: 0.05417158454656601\n",
      "Iteration 12259, Loss: 0.0539887398481369\n",
      "Iteration 12260, Loss: 0.054171573370695114\n",
      "Iteration 12261, Loss: 0.05398937314748764\n",
      "Iteration 12262, Loss: 0.054171495139598846\n",
      "Iteration 12263, Loss: 0.053989410400390625\n",
      "Iteration 12264, Loss: 0.05417141318321228\n",
      "Iteration 12265, Loss: 0.05398937687277794\n",
      "Iteration 12266, Loss: 0.054171811789274216\n",
      "Iteration 12267, Loss: 0.05398945510387421\n",
      "Iteration 12268, Loss: 0.05417165160179138\n",
      "Iteration 12269, Loss: 0.05398964136838913\n",
      "Iteration 12270, Loss: 0.05417166277766228\n",
      "Iteration 12271, Loss: 0.053989678621292114\n",
      "Iteration 12272, Loss: 0.054171692579984665\n",
      "Iteration 12273, Loss: 0.05398957058787346\n",
      "Iteration 12274, Loss: 0.054171860218048096\n",
      "Iteration 12275, Loss: 0.05398949235677719\n",
      "Iteration 12276, Loss: 0.05417216569185257\n",
      "Iteration 12277, Loss: 0.05398917198181152\n",
      "Iteration 12278, Loss: 0.054172322154045105\n",
      "Iteration 12279, Loss: 0.05398886650800705\n",
      "Iteration 12280, Loss: 0.05417240411043167\n",
      "Iteration 12281, Loss: 0.05398905277252197\n",
      "Iteration 12282, Loss: 0.0541723296046257\n",
      "Iteration 12283, Loss: 0.05398905277252197\n",
      "Iteration 12284, Loss: 0.054172009229660034\n",
      "Iteration 12285, Loss: 0.05398925766348839\n",
      "Iteration 12286, Loss: 0.054171811789274216\n",
      "Iteration 12287, Loss: 0.05398945137858391\n",
      "Iteration 12288, Loss: 0.05417173355817795\n",
      "Iteration 12289, Loss: 0.053989604115486145\n",
      "Iteration 12290, Loss: 0.054171692579984665\n",
      "Iteration 12291, Loss: 0.0539897195994854\n",
      "Iteration 12292, Loss: 0.054171737283468246\n",
      "Iteration 12293, Loss: 0.05398953706026077\n",
      "Iteration 12294, Loss: 0.05417177081108093\n",
      "Iteration 12295, Loss: 0.053989529609680176\n",
      "Iteration 12296, Loss: 0.054171737283468246\n",
      "Iteration 12297, Loss: 0.05398949235677719\n",
      "Iteration 12298, Loss: 0.05417197197675705\n",
      "Iteration 12299, Loss: 0.05398933216929436\n",
      "Iteration 12300, Loss: 0.054172124713659286\n",
      "Iteration 12301, Loss: 0.053989212960004807\n",
      "Iteration 12302, Loss: 0.05417216941714287\n",
      "Iteration 12303, Loss: 0.05398925393819809\n",
      "Iteration 12304, Loss: 0.054172053933143616\n",
      "Iteration 12305, Loss: 0.05398936569690704\n",
      "Iteration 12306, Loss: 0.054172009229660034\n",
      "Iteration 12307, Loss: 0.053989335894584656\n",
      "Iteration 12308, Loss: 0.0541718527674675\n",
      "Iteration 12309, Loss: 0.05398956686258316\n",
      "Iteration 12310, Loss: 0.05417177081108093\n",
      "Iteration 12311, Loss: 0.05398957058787346\n",
      "Iteration 12312, Loss: 0.05417166277766228\n",
      "Iteration 12313, Loss: 0.053989529609680176\n",
      "Iteration 12314, Loss: 0.05417166277766228\n",
      "Iteration 12315, Loss: 0.05398942157626152\n",
      "Iteration 12316, Loss: 0.054171934723854065\n",
      "Iteration 12317, Loss: 0.053989410400390625\n",
      "Iteration 12318, Loss: 0.054171934723854065\n",
      "Iteration 12319, Loss: 0.05398936569690704\n",
      "Iteration 12320, Loss: 0.054172128438949585\n",
      "Iteration 12321, Loss: 0.053989291191101074\n",
      "Iteration 12322, Loss: 0.0541720911860466\n",
      "Iteration 12323, Loss: 0.05398932844400406\n",
      "Iteration 12324, Loss: 0.054172124713659286\n",
      "Iteration 12325, Loss: 0.05398933216929436\n",
      "Iteration 12326, Loss: 0.05417203903198242\n",
      "Iteration 12327, Loss: 0.053989410400390625\n",
      "Iteration 12328, Loss: 0.05417189747095108\n",
      "Iteration 12329, Loss: 0.05398937314748764\n",
      "Iteration 12330, Loss: 0.05417190119624138\n",
      "Iteration 12331, Loss: 0.053989410400390625\n",
      "Iteration 12332, Loss: 0.05417197570204735\n",
      "Iteration 12333, Loss: 0.05398937314748764\n",
      "Iteration 12334, Loss: 0.054172009229660034\n",
      "Iteration 12335, Loss: 0.05398933216929436\n",
      "Iteration 12336, Loss: 0.054172083735466\n",
      "Iteration 12337, Loss: 0.05398929864168167\n",
      "Iteration 12338, Loss: 0.05417197197675705\n",
      "Iteration 12339, Loss: 0.05398937687277794\n",
      "Iteration 12340, Loss: 0.054171934723854065\n",
      "Iteration 12341, Loss: 0.05398937314748764\n",
      "Iteration 12342, Loss: 0.054171860218048096\n",
      "Iteration 12343, Loss: 0.05398936569690704\n",
      "Iteration 12344, Loss: 0.05417197197675705\n",
      "Iteration 12345, Loss: 0.053989484906196594\n",
      "Iteration 12346, Loss: 0.054171930998563766\n",
      "Iteration 12347, Loss: 0.053989410400390625\n",
      "Iteration 12348, Loss: 0.054172009229660034\n",
      "Iteration 12349, Loss: 0.05398929864168167\n",
      "Iteration 12350, Loss: 0.05417197197675705\n",
      "Iteration 12351, Loss: 0.053989261388778687\n",
      "Iteration 12352, Loss: 0.054171811789274216\n",
      "Iteration 12353, Loss: 0.0539894625544548\n",
      "Iteration 12354, Loss: 0.05417146533727646\n",
      "Iteration 12355, Loss: 0.053989656269550323\n",
      "Iteration 12356, Loss: 0.054171573370695114\n",
      "Iteration 12357, Loss: 0.05398976802825928\n",
      "Iteration 12358, Loss: 0.054171618074178696\n",
      "Iteration 12359, Loss: 0.05398961156606674\n",
      "Iteration 12360, Loss: 0.05417180806398392\n",
      "Iteration 12361, Loss: 0.05398964881896973\n",
      "Iteration 12362, Loss: 0.0541718527674675\n",
      "Iteration 12363, Loss: 0.05398949235677719\n",
      "Iteration 12364, Loss: 0.054172009229660034\n",
      "Iteration 12365, Loss: 0.05398936569690704\n",
      "Iteration 12366, Loss: 0.05417221039533615\n",
      "Iteration 12367, Loss: 0.053989093750715256\n",
      "Iteration 12368, Loss: 0.054172247648239136\n",
      "Iteration 12369, Loss: 0.053989212960004807\n",
      "Iteration 12370, Loss: 0.05417216941714287\n",
      "Iteration 12371, Loss: 0.053989212960004807\n",
      "Iteration 12372, Loss: 0.05417205020785332\n",
      "Iteration 12373, Loss: 0.053989335894584656\n",
      "Iteration 12374, Loss: 0.05417200177907944\n",
      "Iteration 12375, Loss: 0.053989481180906296\n",
      "Iteration 12376, Loss: 0.05417177081108093\n",
      "Iteration 12377, Loss: 0.05398957058787346\n",
      "Iteration 12378, Loss: 0.054171811789274216\n",
      "Iteration 12379, Loss: 0.05398964509367943\n",
      "Iteration 12380, Loss: 0.054171737283468246\n",
      "Iteration 12381, Loss: 0.05398949235677719\n",
      "Iteration 12382, Loss: 0.05417197197675705\n",
      "Iteration 12383, Loss: 0.053989261388778687\n",
      "Iteration 12384, Loss: 0.05417205020785332\n",
      "Iteration 12385, Loss: 0.05398910492658615\n",
      "Iteration 12386, Loss: 0.05417216941714287\n",
      "Iteration 12387, Loss: 0.053989216685295105\n",
      "Iteration 12388, Loss: 0.054172202944755554\n",
      "Iteration 12389, Loss: 0.05398925393819809\n",
      "Iteration 12390, Loss: 0.054172124713659286\n",
      "Iteration 12391, Loss: 0.05398925393819809\n",
      "Iteration 12392, Loss: 0.05417197197675705\n",
      "Iteration 12393, Loss: 0.05398944020271301\n",
      "Iteration 12394, Loss: 0.0541718527674675\n",
      "Iteration 12395, Loss: 0.053989529609680176\n",
      "Iteration 12396, Loss: 0.0541718415915966\n",
      "Iteration 12397, Loss: 0.053989678621292114\n",
      "Iteration 12398, Loss: 0.054171737283468246\n",
      "Iteration 12399, Loss: 0.05398949235677719\n",
      "Iteration 12400, Loss: 0.05417200177907944\n",
      "Iteration 12401, Loss: 0.053989335894584656\n",
      "Iteration 12402, Loss: 0.0541720911860466\n",
      "Iteration 12403, Loss: 0.05398917943239212\n",
      "Iteration 12404, Loss: 0.054172128438949585\n",
      "Iteration 12405, Loss: 0.053989212960004807\n",
      "Iteration 12406, Loss: 0.054172128438949585\n",
      "Iteration 12407, Loss: 0.05398925393819809\n",
      "Iteration 12408, Loss: 0.054172009229660034\n",
      "Iteration 12409, Loss: 0.05398929864168167\n",
      "Iteration 12410, Loss: 0.054172009229660034\n",
      "Iteration 12411, Loss: 0.053989481180906296\n",
      "Iteration 12412, Loss: 0.05417191982269287\n",
      "Iteration 12413, Loss: 0.053989559412002563\n",
      "Iteration 12414, Loss: 0.054171886295080185\n",
      "Iteration 12415, Loss: 0.053989529609680176\n",
      "Iteration 12416, Loss: 0.0541718527674675\n",
      "Iteration 12417, Loss: 0.05398952588438988\n",
      "Iteration 12418, Loss: 0.054171960800886154\n",
      "Iteration 12419, Loss: 0.053989484906196594\n",
      "Iteration 12420, Loss: 0.05417204648256302\n",
      "Iteration 12421, Loss: 0.05398933216929436\n",
      "Iteration 12422, Loss: 0.054172124713659286\n",
      "Iteration 12423, Loss: 0.05398917198181152\n",
      "Iteration 12424, Loss: 0.054172128438949585\n",
      "Iteration 12425, Loss: 0.053989212960004807\n",
      "Iteration 12426, Loss: 0.054172128438949585\n",
      "Iteration 12427, Loss: 0.05398936569690704\n",
      "Iteration 12428, Loss: 0.054172128438949585\n",
      "Iteration 12429, Loss: 0.05398925393819809\n",
      "Iteration 12430, Loss: 0.0541720911860466\n",
      "Iteration 12431, Loss: 0.05398933216929436\n",
      "Iteration 12432, Loss: 0.05417205020785332\n",
      "Iteration 12433, Loss: 0.05398937314748764\n",
      "Iteration 12434, Loss: 0.05417204648256302\n",
      "Iteration 12435, Loss: 0.05398952215909958\n",
      "Iteration 12436, Loss: 0.054171930998563766\n",
      "Iteration 12437, Loss: 0.05398957058787346\n",
      "Iteration 12438, Loss: 0.054171930998563766\n",
      "Iteration 12439, Loss: 0.05398945137858391\n",
      "Iteration 12440, Loss: 0.054172005504369736\n",
      "Iteration 12441, Loss: 0.053989335894584656\n",
      "Iteration 12442, Loss: 0.05417191982269287\n",
      "Iteration 12443, Loss: 0.05398957058787346\n",
      "Iteration 12444, Loss: 0.05417177081108093\n",
      "Iteration 12445, Loss: 0.05398968607187271\n",
      "Iteration 12446, Loss: 0.05417173355817795\n",
      "Iteration 12447, Loss: 0.05398952588438988\n",
      "Iteration 12448, Loss: 0.054172005504369736\n",
      "Iteration 12449, Loss: 0.05398937314748764\n",
      "Iteration 12450, Loss: 0.05417197570204735\n",
      "Iteration 12451, Loss: 0.05398936569690704\n",
      "Iteration 12452, Loss: 0.05417202040553093\n",
      "Iteration 12453, Loss: 0.05398936569690704\n",
      "Iteration 12454, Loss: 0.05417202040553093\n",
      "Iteration 12455, Loss: 0.05398925393819809\n",
      "Iteration 12456, Loss: 0.05417216569185257\n",
      "Iteration 12457, Loss: 0.053989287465810776\n",
      "Iteration 12458, Loss: 0.0541720911860466\n",
      "Iteration 12459, Loss: 0.053989291191101074\n",
      "Iteration 12460, Loss: 0.054172083735466\n",
      "Iteration 12461, Loss: 0.053989410400390625\n",
      "Iteration 12462, Loss: 0.05417197197675705\n",
      "Iteration 12463, Loss: 0.05398945137858391\n",
      "Iteration 12464, Loss: 0.054172005504369736\n",
      "Iteration 12465, Loss: 0.05398949235677719\n",
      "Iteration 12466, Loss: 0.054171811789274216\n",
      "Iteration 12467, Loss: 0.05398944765329361\n",
      "Iteration 12468, Loss: 0.05417177081108093\n",
      "Iteration 12469, Loss: 0.05398960039019585\n",
      "Iteration 12470, Loss: 0.05417177826166153\n",
      "Iteration 12471, Loss: 0.05398968607187271\n",
      "Iteration 12472, Loss: 0.054171811789274216\n",
      "Iteration 12473, Loss: 0.05398945137858391\n",
      "Iteration 12474, Loss: 0.054171930998563766\n",
      "Iteration 12475, Loss: 0.05398937314748764\n",
      "Iteration 12476, Loss: 0.0541720911860466\n",
      "Iteration 12477, Loss: 0.05398925393819809\n",
      "Iteration 12478, Loss: 0.05417216569185257\n",
      "Iteration 12479, Loss: 0.053989216685295105\n",
      "Iteration 12480, Loss: 0.054172009229660034\n",
      "Iteration 12481, Loss: 0.053989291191101074\n",
      "Iteration 12482, Loss: 0.054172083735466\n",
      "Iteration 12483, Loss: 0.05398933216929436\n",
      "Iteration 12484, Loss: 0.054171930998563766\n",
      "Iteration 12485, Loss: 0.053989481180906296\n",
      "Iteration 12486, Loss: 0.054171960800886154\n",
      "Iteration 12487, Loss: 0.05398949235677719\n",
      "Iteration 12488, Loss: 0.054171860218048096\n",
      "Iteration 12489, Loss: 0.05398937314748764\n",
      "Iteration 12490, Loss: 0.0541720911860466\n",
      "Iteration 12491, Loss: 0.05398933216929436\n",
      "Iteration 12492, Loss: 0.05417205020785332\n",
      "Iteration 12493, Loss: 0.05398925393819809\n",
      "Iteration 12494, Loss: 0.054172128438949585\n",
      "Iteration 12495, Loss: 0.053989216685295105\n",
      "Iteration 12496, Loss: 0.05417216569185257\n",
      "Iteration 12497, Loss: 0.05398917943239212\n",
      "Iteration 12498, Loss: 0.05417189002037048\n",
      "Iteration 12499, Loss: 0.05398945137858391\n",
      "Iteration 12500, Loss: 0.05417180061340332\n",
      "Iteration 12501, Loss: 0.05398957058787346\n",
      "Iteration 12502, Loss: 0.05417173355817795\n",
      "Iteration 12503, Loss: 0.05398976057767868\n",
      "Iteration 12504, Loss: 0.05417177081108093\n",
      "Iteration 12505, Loss: 0.053989604115486145\n",
      "Iteration 12506, Loss: 0.054171930998563766\n",
      "Iteration 12507, Loss: 0.05398937687277794\n",
      "Iteration 12508, Loss: 0.05417205020785332\n",
      "Iteration 12509, Loss: 0.053989212960004807\n",
      "Iteration 12510, Loss: 0.054172083735466\n",
      "Iteration 12511, Loss: 0.05398933216929436\n",
      "Iteration 12512, Loss: 0.054172083735466\n",
      "Iteration 12513, Loss: 0.05398937314748764\n",
      "Iteration 12514, Loss: 0.05417205020785332\n",
      "Iteration 12515, Loss: 0.05398933216929436\n",
      "Iteration 12516, Loss: 0.05417197197675705\n",
      "Iteration 12517, Loss: 0.05398929864168167\n",
      "Iteration 12518, Loss: 0.0541718527674675\n",
      "Iteration 12519, Loss: 0.05398952588438988\n",
      "Iteration 12520, Loss: 0.05417169630527496\n",
      "Iteration 12521, Loss: 0.05398976057767868\n",
      "Iteration 12522, Loss: 0.05417165905237198\n",
      "Iteration 12523, Loss: 0.05398968979716301\n",
      "Iteration 12524, Loss: 0.05417173355817795\n",
      "Iteration 12525, Loss: 0.053989529609680176\n",
      "Iteration 12526, Loss: 0.054171930998563766\n",
      "Iteration 12527, Loss: 0.05398944765329361\n",
      "Iteration 12528, Loss: 0.0541720911860466\n",
      "Iteration 12529, Loss: 0.05398906394839287\n",
      "Iteration 12530, Loss: 0.05417227745056152\n",
      "Iteration 12531, Loss: 0.053989212960004807\n",
      "Iteration 12532, Loss: 0.054172202944755554\n",
      "Iteration 12533, Loss: 0.05398917198181152\n",
      "Iteration 12534, Loss: 0.054171960800886154\n",
      "Iteration 12535, Loss: 0.053989529609680176\n",
      "Iteration 12536, Loss: 0.05417177081108093\n",
      "Iteration 12537, Loss: 0.05398964136838913\n",
      "Iteration 12538, Loss: 0.05417177081108093\n",
      "Iteration 12539, Loss: 0.05398976057767868\n",
      "Iteration 12540, Loss: 0.05417177081108093\n",
      "Iteration 12541, Loss: 0.05398960039019585\n",
      "Iteration 12542, Loss: 0.05417189002037048\n",
      "Iteration 12543, Loss: 0.053989410400390625\n",
      "Iteration 12544, Loss: 0.05417197197675705\n",
      "Iteration 12545, Loss: 0.05398936569690704\n",
      "Iteration 12546, Loss: 0.05417212098836899\n",
      "Iteration 12547, Loss: 0.05398925393819809\n",
      "Iteration 12548, Loss: 0.054171960800886154\n",
      "Iteration 12549, Loss: 0.05398944765329361\n",
      "Iteration 12550, Loss: 0.0541718453168869\n",
      "Iteration 12551, Loss: 0.05398949235677719\n",
      "Iteration 12552, Loss: 0.054171737283468246\n",
      "Iteration 12553, Loss: 0.05398960039019585\n",
      "Iteration 12554, Loss: 0.054171811789274216\n",
      "Iteration 12555, Loss: 0.05398961156606674\n",
      "Iteration 12556, Loss: 0.05417170375585556\n",
      "Iteration 12557, Loss: 0.05398968979716301\n",
      "Iteration 12558, Loss: 0.0541718527674675\n",
      "Iteration 12559, Loss: 0.053989484906196594\n",
      "Iteration 12560, Loss: 0.05417197197675705\n",
      "Iteration 12561, Loss: 0.05398933216929436\n",
      "Iteration 12562, Loss: 0.05417206138372421\n",
      "Iteration 12563, Loss: 0.053989361971616745\n",
      "Iteration 12564, Loss: 0.054172128438949585\n",
      "Iteration 12565, Loss: 0.053989361971616745\n",
      "Iteration 12566, Loss: 0.054172199219465256\n",
      "Iteration 12567, Loss: 0.053989216685295105\n",
      "Iteration 12568, Loss: 0.05417216941714287\n",
      "Iteration 12569, Loss: 0.053989216685295105\n",
      "Iteration 12570, Loss: 0.05417216569185257\n",
      "Iteration 12571, Loss: 0.05398925393819809\n",
      "Iteration 12572, Loss: 0.05417216569185257\n",
      "Iteration 12573, Loss: 0.05398917198181152\n",
      "Iteration 12574, Loss: 0.0541720911860466\n",
      "Iteration 12575, Loss: 0.05398929864168167\n",
      "Iteration 12576, Loss: 0.05417192727327347\n",
      "Iteration 12577, Loss: 0.05398956686258316\n",
      "Iteration 12578, Loss: 0.05417173355817795\n",
      "Iteration 12579, Loss: 0.053989678621292114\n",
      "Iteration 12580, Loss: 0.05417180806398392\n",
      "Iteration 12581, Loss: 0.05398954078555107\n",
      "Iteration 12582, Loss: 0.05417177081108093\n",
      "Iteration 12583, Loss: 0.05398957058787346\n",
      "Iteration 12584, Loss: 0.054171811789274216\n",
      "Iteration 12585, Loss: 0.05398961156606674\n",
      "Iteration 12586, Loss: 0.05417189002037048\n",
      "Iteration 12587, Loss: 0.05398960039019585\n",
      "Iteration 12588, Loss: 0.05417200177907944\n",
      "Iteration 12589, Loss: 0.05398952215909958\n",
      "Iteration 12590, Loss: 0.054172009229660034\n",
      "Iteration 12591, Loss: 0.053989559412002563\n",
      "Iteration 12592, Loss: 0.05417201668024063\n",
      "Iteration 12593, Loss: 0.05398925393819809\n",
      "Iteration 12594, Loss: 0.05417221039533615\n",
      "Iteration 12595, Loss: 0.053989093750715256\n",
      "Iteration 12596, Loss: 0.054172366857528687\n",
      "Iteration 12597, Loss: 0.053989019244909286\n",
      "Iteration 12598, Loss: 0.0541723296046257\n",
      "Iteration 12599, Loss: 0.053988978266716\n",
      "Iteration 12600, Loss: 0.054172247648239136\n",
      "Iteration 12601, Loss: 0.05398918315768242\n",
      "Iteration 12602, Loss: 0.054172083735466\n",
      "Iteration 12603, Loss: 0.05398936569690704\n",
      "Iteration 12604, Loss: 0.05417177826166153\n",
      "Iteration 12605, Loss: 0.05398952588438988\n",
      "Iteration 12606, Loss: 0.05417170375585556\n",
      "Iteration 12607, Loss: 0.05398956686258316\n",
      "Iteration 12608, Loss: 0.05417165160179138\n",
      "Iteration 12609, Loss: 0.05398961156606674\n",
      "Iteration 12610, Loss: 0.05417172610759735\n",
      "Iteration 12611, Loss: 0.05398976057767868\n",
      "Iteration 12612, Loss: 0.05417170375585556\n",
      "Iteration 12613, Loss: 0.053989969193935394\n",
      "Iteration 12614, Loss: 0.05417189002037048\n",
      "Iteration 12615, Loss: 0.05398992821574211\n",
      "Iteration 12616, Loss: 0.054171930998563766\n",
      "Iteration 12617, Loss: 0.0539899617433548\n",
      "Iteration 12618, Loss: 0.054171256721019745\n",
      "Iteration 12619, Loss: 0.053990162909030914\n",
      "Iteration 12620, Loss: 0.05417129397392273\n",
      "Iteration 12621, Loss: 0.05399007722735405\n",
      "Iteration 12622, Loss: 0.05417142063379288\n",
      "Iteration 12623, Loss: 0.05398973450064659\n",
      "Iteration 12624, Loss: 0.05417170375585556\n",
      "Iteration 12625, Loss: 0.05398942157626152\n",
      "Iteration 12626, Loss: 0.05417216941714287\n",
      "Iteration 12627, Loss: 0.053989097476005554\n",
      "Iteration 12628, Loss: 0.05417221784591675\n",
      "Iteration 12629, Loss: 0.05398879200220108\n",
      "Iteration 12630, Loss: 0.05417249724268913\n",
      "Iteration 12631, Loss: 0.05398889631032944\n",
      "Iteration 12632, Loss: 0.05417248606681824\n",
      "Iteration 12633, Loss: 0.05398889631032944\n",
      "Iteration 12634, Loss: 0.05417256057262421\n",
      "Iteration 12635, Loss: 0.053989093750715256\n",
      "Iteration 12636, Loss: 0.05417221039533615\n",
      "Iteration 12637, Loss: 0.05398918315768242\n",
      "Iteration 12638, Loss: 0.05417200177907944\n",
      "Iteration 12639, Loss: 0.05398945137858391\n",
      "Iteration 12640, Loss: 0.054171692579984665\n",
      "Iteration 12641, Loss: 0.053989797830581665\n",
      "Iteration 12642, Loss: 0.05417146906256676\n",
      "Iteration 12643, Loss: 0.05398987978696823\n",
      "Iteration 12644, Loss: 0.05417165905237198\n",
      "Iteration 12645, Loss: 0.05398976802825928\n",
      "Iteration 12646, Loss: 0.05417166277766228\n",
      "Iteration 12647, Loss: 0.05398949235677719\n",
      "Iteration 12648, Loss: 0.054171979427337646\n",
      "Iteration 12649, Loss: 0.053989335894584656\n",
      "Iteration 12650, Loss: 0.05417206138372421\n",
      "Iteration 12651, Loss: 0.05398913472890854\n",
      "Iteration 12652, Loss: 0.054172247648239136\n",
      "Iteration 12653, Loss: 0.05398903042078018\n",
      "Iteration 12654, Loss: 0.0541723296046257\n",
      "Iteration 12655, Loss: 0.0539892241358757\n",
      "Iteration 12656, Loss: 0.0541720911860466\n",
      "Iteration 12657, Loss: 0.05398934334516525\n",
      "Iteration 12658, Loss: 0.05417197197675705\n",
      "Iteration 12659, Loss: 0.053989529609680176\n",
      "Iteration 12660, Loss: 0.05417177081108093\n",
      "Iteration 12661, Loss: 0.05398964881896973\n",
      "Iteration 12662, Loss: 0.0541716143488884\n",
      "Iteration 12663, Loss: 0.05398976802825928\n",
      "Iteration 12664, Loss: 0.054171618074178696\n",
      "Iteration 12665, Loss: 0.05398976802825928\n",
      "Iteration 12666, Loss: 0.05417177081108093\n",
      "Iteration 12667, Loss: 0.053989581763744354\n",
      "Iteration 12668, Loss: 0.05417197570204735\n",
      "Iteration 12669, Loss: 0.053989261388778687\n",
      "Iteration 12670, Loss: 0.0541720911860466\n",
      "Iteration 12671, Loss: 0.05398925766348839\n",
      "Iteration 12672, Loss: 0.05417213961482048\n",
      "Iteration 12673, Loss: 0.05398910865187645\n",
      "Iteration 12674, Loss: 0.0541720986366272\n",
      "Iteration 12675, Loss: 0.05398933216929436\n",
      "Iteration 12676, Loss: 0.054172009229660034\n",
      "Iteration 12677, Loss: 0.05398938059806824\n",
      "Iteration 12678, Loss: 0.054171737283468246\n",
      "Iteration 12679, Loss: 0.05398961156606674\n",
      "Iteration 12680, Loss: 0.054171621799468994\n",
      "Iteration 12681, Loss: 0.05398973077535629\n",
      "Iteration 12682, Loss: 0.05417165160179138\n",
      "Iteration 12683, Loss: 0.053989656269550323\n",
      "Iteration 12684, Loss: 0.054171692579984665\n",
      "Iteration 12685, Loss: 0.053989581763744354\n",
      "Iteration 12686, Loss: 0.054171741008758545\n",
      "Iteration 12687, Loss: 0.05398945510387421\n",
      "Iteration 12688, Loss: 0.0541718527674675\n",
      "Iteration 12689, Loss: 0.05398953706026077\n",
      "Iteration 12690, Loss: 0.0541718527674675\n",
      "Iteration 12691, Loss: 0.05398949608206749\n",
      "Iteration 12692, Loss: 0.05417197570204735\n",
      "Iteration 12693, Loss: 0.053989410400390625\n",
      "Iteration 12694, Loss: 0.05417205020785332\n",
      "Iteration 12695, Loss: 0.0539892241358757\n",
      "Iteration 12696, Loss: 0.054172009229660034\n",
      "Iteration 12697, Loss: 0.05398934334516525\n",
      "Iteration 12698, Loss: 0.05417197197675705\n",
      "Iteration 12699, Loss: 0.05398868769407272\n",
      "Iteration 12700, Loss: 0.054171860218048096\n",
      "Iteration 12701, Loss: 0.05398868769407272\n",
      "Iteration 12702, Loss: 0.054171428084373474\n",
      "Iteration 12703, Loss: 0.05398864671587944\n",
      "Iteration 12704, Loss: 0.05417139455676079\n",
      "Iteration 12705, Loss: 0.05398860573768616\n",
      "Iteration 12706, Loss: 0.05417139083147049\n",
      "Iteration 12707, Loss: 0.05398872122168541\n",
      "Iteration 12708, Loss: 0.05417118966579437\n",
      "Iteration 12709, Loss: 0.05398884043097496\n",
      "Iteration 12710, Loss: 0.054171036928892136\n",
      "Iteration 12711, Loss: 0.05398911237716675\n",
      "Iteration 12712, Loss: 0.05417107790708542\n",
      "Iteration 12713, Loss: 0.05398900434374809\n",
      "Iteration 12714, Loss: 0.054170962423086166\n",
      "Iteration 12715, Loss: 0.05398903414607048\n",
      "Iteration 12716, Loss: 0.05417108163237572\n",
      "Iteration 12717, Loss: 0.05398868769407272\n",
      "Iteration 12718, Loss: 0.05417143553495407\n",
      "Iteration 12719, Loss: 0.05398860201239586\n",
      "Iteration 12720, Loss: 0.05417151749134064\n",
      "Iteration 12721, Loss: 0.05398840829730034\n",
      "Iteration 12722, Loss: 0.054171741008758545\n",
      "Iteration 12723, Loss: 0.05398833006620407\n",
      "Iteration 12724, Loss: 0.05417170375585556\n",
      "Iteration 12725, Loss: 0.05398852750658989\n",
      "Iteration 12726, Loss: 0.054171428084373474\n",
      "Iteration 12727, Loss: 0.05398860573768616\n",
      "Iteration 12728, Loss: 0.0541711151599884\n",
      "Iteration 12729, Loss: 0.05398879945278168\n",
      "Iteration 12730, Loss: 0.05417099595069885\n",
      "Iteration 12731, Loss: 0.05398911237716675\n",
      "Iteration 12732, Loss: 0.05417092144489288\n",
      "Iteration 12733, Loss: 0.05398907884955406\n",
      "Iteration 12734, Loss: 0.05417099595069885\n",
      "Iteration 12735, Loss: 0.05398903787136078\n",
      "Iteration 12736, Loss: 0.05417107790708542\n",
      "Iteration 12737, Loss: 0.05398879945278168\n",
      "Iteration 12738, Loss: 0.054171353578567505\n",
      "Iteration 12739, Loss: 0.05398852750658989\n",
      "Iteration 12740, Loss: 0.054171666502952576\n",
      "Iteration 12741, Loss: 0.05398833006620407\n",
      "Iteration 12742, Loss: 0.05417156219482422\n",
      "Iteration 12743, Loss: 0.053988367319107056\n",
      "Iteration 12744, Loss: 0.05417166277766228\n",
      "Iteration 12745, Loss: 0.05398852750658989\n",
      "Iteration 12746, Loss: 0.05417139455676079\n",
      "Iteration 12747, Loss: 0.05398879572749138\n",
      "Iteration 12748, Loss: 0.054171156138181686\n",
      "Iteration 12749, Loss: 0.05398884043097496\n",
      "Iteration 12750, Loss: 0.05417118966579437\n",
      "Iteration 12751, Loss: 0.05398895964026451\n",
      "Iteration 12752, Loss: 0.0541711151599884\n",
      "Iteration 12753, Loss: 0.05398891493678093\n",
      "Iteration 12754, Loss: 0.05417122691869736\n",
      "Iteration 12755, Loss: 0.05398896336555481\n",
      "Iteration 12756, Loss: 0.05417116731405258\n",
      "Iteration 12757, Loss: 0.05398868769407272\n",
      "Iteration 12758, Loss: 0.0541713647544384\n",
      "Iteration 12759, Loss: 0.05398860573768616\n",
      "Iteration 12760, Loss: 0.05417151376605034\n",
      "Iteration 12761, Loss: 0.05398844927549362\n",
      "Iteration 12762, Loss: 0.054171547293663025\n",
      "Iteration 12763, Loss: 0.05398856848478317\n",
      "Iteration 12764, Loss: 0.05417143553495407\n",
      "Iteration 12765, Loss: 0.05398864299058914\n",
      "Iteration 12766, Loss: 0.05417138338088989\n",
      "Iteration 12767, Loss: 0.05398879945278168\n",
      "Iteration 12768, Loss: 0.05417127534747124\n",
      "Iteration 12769, Loss: 0.05398872494697571\n",
      "Iteration 12770, Loss: 0.05417119711637497\n",
      "Iteration 12771, Loss: 0.05398883670568466\n",
      "Iteration 12772, Loss: 0.054171234369277954\n",
      "Iteration 12773, Loss: 0.05398860573768616\n",
      "Iteration 12774, Loss: 0.05417120084166527\n",
      "Iteration 12775, Loss: 0.05398864671587944\n",
      "Iteration 12776, Loss: 0.05417139455676079\n",
      "Iteration 12777, Loss: 0.05398860573768616\n",
      "Iteration 12778, Loss: 0.05417139083147049\n",
      "Iteration 12779, Loss: 0.05398861691355705\n",
      "Iteration 12780, Loss: 0.05417127162218094\n",
      "Iteration 12781, Loss: 0.05398861691355705\n",
      "Iteration 12782, Loss: 0.054171230643987656\n",
      "Iteration 12783, Loss: 0.05398872494697571\n",
      "Iteration 12784, Loss: 0.054171159863471985\n",
      "Iteration 12785, Loss: 0.05398865044116974\n",
      "Iteration 12786, Loss: 0.05417119711637497\n",
      "Iteration 12787, Loss: 0.05398876219987869\n",
      "Iteration 12788, Loss: 0.054171234369277954\n",
      "Iteration 12789, Loss: 0.05398868769407272\n",
      "Iteration 12790, Loss: 0.05417132377624512\n",
      "Iteration 12791, Loss: 0.05398856848478317\n",
      "Iteration 12792, Loss: 0.054171591997146606\n",
      "Iteration 12793, Loss: 0.05398833006620407\n",
      "Iteration 12794, Loss: 0.05417167395353317\n",
      "Iteration 12795, Loss: 0.05398833006620407\n",
      "Iteration 12796, Loss: 0.054171666502952576\n",
      "Iteration 12797, Loss: 0.053988486528396606\n",
      "Iteration 12798, Loss: 0.05417143553495407\n",
      "Iteration 12799, Loss: 0.05398872122168541\n",
      "Iteration 12800, Loss: 0.05417119711637497\n",
      "Iteration 12801, Loss: 0.053988806903362274\n",
      "Iteration 12802, Loss: 0.054171036928892136\n",
      "Iteration 12803, Loss: 0.05398895964026451\n",
      "Iteration 12804, Loss: 0.054171036928892136\n",
      "Iteration 12805, Loss: 0.05398896336555481\n",
      "Iteration 12806, Loss: 0.05417099595069885\n",
      "Iteration 12807, Loss: 0.05398896336555481\n",
      "Iteration 12808, Loss: 0.05417099595069885\n",
      "Iteration 12809, Loss: 0.05398884415626526\n",
      "Iteration 12810, Loss: 0.05417107045650482\n",
      "Iteration 12811, Loss: 0.05398888513445854\n",
      "Iteration 12812, Loss: 0.05417099595069885\n",
      "Iteration 12813, Loss: 0.05398876592516899\n",
      "Iteration 12814, Loss: 0.054171040654182434\n",
      "Iteration 12815, Loss: 0.05398891866207123\n",
      "Iteration 12816, Loss: 0.05417095869779587\n",
      "Iteration 12817, Loss: 0.053989000618457794\n",
      "Iteration 12818, Loss: 0.05417095869779587\n",
      "Iteration 12819, Loss: 0.05398918688297272\n",
      "Iteration 12820, Loss: 0.05417060852050781\n",
      "Iteration 12821, Loss: 0.05398912355303764\n",
      "Iteration 12822, Loss: 0.054170798510313034\n",
      "Iteration 12823, Loss: 0.05398912355303764\n",
      "Iteration 12824, Loss: 0.054170768707990646\n",
      "Iteration 12825, Loss: 0.053988926112651825\n",
      "Iteration 12826, Loss: 0.05417080968618393\n",
      "Iteration 12827, Loss: 0.05398888513445854\n",
      "Iteration 12828, Loss: 0.05417092889547348\n",
      "Iteration 12829, Loss: 0.05398884415626526\n",
      "Iteration 12830, Loss: 0.054170966148376465\n",
      "Iteration 12831, Loss: 0.053988926112651825\n",
      "Iteration 12832, Loss: 0.05417095869779587\n",
      "Iteration 12833, Loss: 0.05398907884955406\n",
      "Iteration 12834, Loss: 0.05417083948850632\n",
      "Iteration 12835, Loss: 0.05398926883935928\n",
      "Iteration 12836, Loss: 0.05417076498270035\n",
      "Iteration 12837, Loss: 0.05398912355303764\n",
      "Iteration 12838, Loss: 0.0541708767414093\n",
      "Iteration 12839, Loss: 0.053989045321941376\n",
      "Iteration 12840, Loss: 0.05417097359895706\n",
      "Iteration 12841, Loss: 0.05398888513445854\n",
      "Iteration 12842, Loss: 0.05417100712656975\n",
      "Iteration 12843, Loss: 0.05398876592516899\n",
      "Iteration 12844, Loss: 0.054171085357666016\n",
      "Iteration 12845, Loss: 0.05398872494697571\n",
      "Iteration 12846, Loss: 0.05417100712656975\n",
      "Iteration 12847, Loss: 0.05398872494697571\n",
      "Iteration 12848, Loss: 0.054170962423086166\n",
      "Iteration 12849, Loss: 0.053988926112651825\n",
      "Iteration 12850, Loss: 0.054170846939086914\n",
      "Iteration 12851, Loss: 0.05398911237716675\n",
      "Iteration 12852, Loss: 0.0541706383228302\n",
      "Iteration 12853, Loss: 0.05398919805884361\n",
      "Iteration 12854, Loss: 0.05417057126760483\n",
      "Iteration 12855, Loss: 0.05398927628993988\n",
      "Iteration 12856, Loss: 0.054170720279216766\n",
      "Iteration 12857, Loss: 0.05398920178413391\n",
      "Iteration 12858, Loss: 0.054170798510313034\n",
      "Iteration 12859, Loss: 0.05398915708065033\n",
      "Iteration 12860, Loss: 0.05417085438966751\n",
      "Iteration 12861, Loss: 0.05398884415626526\n",
      "Iteration 12862, Loss: 0.054171159863471985\n",
      "Iteration 12863, Loss: 0.0539884939789772\n",
      "Iteration 12864, Loss: 0.05417143553495407\n",
      "Iteration 12865, Loss: 0.053988486528396606\n",
      "Iteration 12866, Loss: 0.05417143553495407\n",
      "Iteration 12867, Loss: 0.05398844927549362\n",
      "Iteration 12868, Loss: 0.05417128652334213\n",
      "Iteration 12869, Loss: 0.05398845672607422\n",
      "Iteration 12870, Loss: 0.05417120084166527\n",
      "Iteration 12871, Loss: 0.05398864671587944\n",
      "Iteration 12872, Loss: 0.054170966148376465\n",
      "Iteration 12873, Loss: 0.0539889931678772\n",
      "Iteration 12874, Loss: 0.05417072772979736\n",
      "Iteration 12875, Loss: 0.0539892315864563\n",
      "Iteration 12876, Loss: 0.05417072772979736\n",
      "Iteration 12877, Loss: 0.053989164531230927\n",
      "Iteration 12878, Loss: 0.05417069047689438\n",
      "Iteration 12879, Loss: 0.05398912355303764\n",
      "Iteration 12880, Loss: 0.05417069047689438\n",
      "Iteration 12881, Loss: 0.05398912355303764\n",
      "Iteration 12882, Loss: 0.054170798510313034\n",
      "Iteration 12883, Loss: 0.05398919805884361\n",
      "Iteration 12884, Loss: 0.05417067930102348\n",
      "Iteration 12885, Loss: 0.0539892315864563\n",
      "Iteration 12886, Loss: 0.05417068302631378\n",
      "Iteration 12887, Loss: 0.05398912355303764\n",
      "Iteration 12888, Loss: 0.05417072772979736\n",
      "Iteration 12889, Loss: 0.05398900434374809\n",
      "Iteration 12890, Loss: 0.054170843213796616\n",
      "Iteration 12891, Loss: 0.05398896336555481\n",
      "Iteration 12892, Loss: 0.05417092889547348\n",
      "Iteration 12893, Loss: 0.05398888513445854\n",
      "Iteration 12894, Loss: 0.054171040654182434\n",
      "Iteration 12895, Loss: 0.05398884415626526\n",
      "Iteration 12896, Loss: 0.05417100712656975\n",
      "Iteration 12897, Loss: 0.053988806903362274\n",
      "Iteration 12898, Loss: 0.05417100712656975\n",
      "Iteration 12899, Loss: 0.05398868769407272\n",
      "Iteration 12900, Loss: 0.05417100712656975\n",
      "Iteration 12901, Loss: 0.053988806903362274\n",
      "Iteration 12902, Loss: 0.054170962423086166\n",
      "Iteration 12903, Loss: 0.053988926112651825\n",
      "Iteration 12904, Loss: 0.0541708841919899\n",
      "Iteration 12905, Loss: 0.05398908257484436\n",
      "Iteration 12906, Loss: 0.05417069047689438\n",
      "Iteration 12907, Loss: 0.05398907884955406\n",
      "Iteration 12908, Loss: 0.05417092889547348\n",
      "Iteration 12909, Loss: 0.053988926112651825\n",
      "Iteration 12910, Loss: 0.054171040654182434\n",
      "Iteration 12911, Loss: 0.053988732397556305\n",
      "Iteration 12912, Loss: 0.05417116731405258\n",
      "Iteration 12913, Loss: 0.05398865044116974\n",
      "Iteration 12914, Loss: 0.05417132377624512\n",
      "Iteration 12915, Loss: 0.05398852750658989\n",
      "Iteration 12916, Loss: 0.054171472787857056\n",
      "Iteration 12917, Loss: 0.05398837476968765\n",
      "Iteration 12918, Loss: 0.05417143553495407\n",
      "Iteration 12919, Loss: 0.05398852750658989\n",
      "Iteration 12920, Loss: 0.054171305149793625\n",
      "Iteration 12921, Loss: 0.053988806903362274\n",
      "Iteration 12922, Loss: 0.05417095869779587\n",
      "Iteration 12923, Loss: 0.05398903414607048\n",
      "Iteration 12924, Loss: 0.05417095869779587\n",
      "Iteration 12925, Loss: 0.053989045321941376\n",
      "Iteration 12926, Loss: 0.05417080223560333\n",
      "Iteration 12927, Loss: 0.05398896336555481\n",
      "Iteration 12928, Loss: 0.054170966148376465\n",
      "Iteration 12929, Loss: 0.053988926112651825\n",
      "Iteration 12930, Loss: 0.0541711263358593\n",
      "Iteration 12931, Loss: 0.053988538682460785\n",
      "Iteration 12932, Loss: 0.054171398282051086\n",
      "Iteration 12933, Loss: 0.05398830026388168\n",
      "Iteration 12934, Loss: 0.05417156219482422\n",
      "Iteration 12935, Loss: 0.053988292813301086\n",
      "Iteration 12936, Loss: 0.054171591997146606\n",
      "Iteration 12937, Loss: 0.053988367319107056\n",
      "Iteration 12938, Loss: 0.05417143553495407\n",
      "Iteration 12939, Loss: 0.05398856848478317\n",
      "Iteration 12940, Loss: 0.05417131632566452\n",
      "Iteration 12941, Loss: 0.053988754749298096\n",
      "Iteration 12942, Loss: 0.0541711151599884\n",
      "Iteration 12943, Loss: 0.05398884415626526\n",
      "Iteration 12944, Loss: 0.054171036928892136\n",
      "Iteration 12945, Loss: 0.053989000618457794\n",
      "Iteration 12946, Loss: 0.05417100340127945\n",
      "Iteration 12947, Loss: 0.053988926112651825\n",
      "Iteration 12948, Loss: 0.0541711151599884\n",
      "Iteration 12949, Loss: 0.05398876965045929\n",
      "Iteration 12950, Loss: 0.054171085357666016\n",
      "Iteration 12951, Loss: 0.053988732397556305\n",
      "Iteration 12952, Loss: 0.054171159863471985\n",
      "Iteration 12953, Loss: 0.05398869514465332\n",
      "Iteration 12954, Loss: 0.054171234369277954\n",
      "Iteration 12955, Loss: 0.053988657891750336\n",
      "Iteration 12956, Loss: 0.054171353578567505\n",
      "Iteration 12957, Loss: 0.0539884977042675\n",
      "Iteration 12958, Loss: 0.05417128652334213\n",
      "Iteration 12959, Loss: 0.05398864671587944\n",
      "Iteration 12960, Loss: 0.054171204566955566\n",
      "Iteration 12961, Loss: 0.05398872494697571\n",
      "Iteration 12962, Loss: 0.054171122610569\n",
      "Iteration 12963, Loss: 0.05398881062865257\n",
      "Iteration 12964, Loss: 0.05417092889547348\n",
      "Iteration 12965, Loss: 0.053989045321941376\n",
      "Iteration 12966, Loss: 0.0541708841919899\n",
      "Iteration 12967, Loss: 0.053989045321941376\n",
      "Iteration 12968, Loss: 0.054171040654182434\n",
      "Iteration 12969, Loss: 0.05398884415626526\n",
      "Iteration 12970, Loss: 0.05417107790708542\n",
      "Iteration 12971, Loss: 0.0539887361228466\n",
      "Iteration 12972, Loss: 0.054171040654182434\n",
      "Iteration 12973, Loss: 0.053988806903362274\n",
      "Iteration 12974, Loss: 0.05417108163237572\n",
      "Iteration 12975, Loss: 0.05398884415626526\n",
      "Iteration 12976, Loss: 0.0541711263358593\n",
      "Iteration 12977, Loss: 0.05398876592516899\n",
      "Iteration 12978, Loss: 0.05417124554514885\n",
      "Iteration 12979, Loss: 0.05398868769407272\n",
      "Iteration 12980, Loss: 0.05417143553495407\n",
      "Iteration 12981, Loss: 0.05398856848478317\n",
      "Iteration 12982, Loss: 0.05417143553495407\n",
      "Iteration 12983, Loss: 0.05398852750658989\n",
      "Iteration 12984, Loss: 0.0541713647544384\n",
      "Iteration 12985, Loss: 0.05398856848478317\n",
      "Iteration 12986, Loss: 0.05417131632566452\n",
      "Iteration 12987, Loss: 0.05398876219987869\n",
      "Iteration 12988, Loss: 0.05417118966579437\n",
      "Iteration 12989, Loss: 0.05398888513445854\n",
      "Iteration 12990, Loss: 0.05417095869779587\n",
      "Iteration 12991, Loss: 0.05398900434374809\n",
      "Iteration 12992, Loss: 0.0541708841919899\n",
      "Iteration 12993, Loss: 0.05398900434374809\n",
      "Iteration 12994, Loss: 0.054170843213796616\n",
      "Iteration 12995, Loss: 0.0539892315864563\n",
      "Iteration 12996, Loss: 0.054170843213796616\n",
      "Iteration 12997, Loss: 0.05398908257484436\n",
      "Iteration 12998, Loss: 0.05417092144489288\n",
      "Iteration 12999, Loss: 0.05398888513445854\n",
      "Iteration 13000, Loss: 0.05417104810476303\n",
      "Iteration 13001, Loss: 0.05398869514465332\n",
      "Iteration 13002, Loss: 0.05417120084166527\n",
      "Iteration 13003, Loss: 0.05398861691355705\n",
      "Iteration 13004, Loss: 0.054171159863471985\n",
      "Iteration 13005, Loss: 0.05398869141936302\n",
      "Iteration 13006, Loss: 0.0541711151599884\n",
      "Iteration 13007, Loss: 0.053988873958587646\n",
      "Iteration 13008, Loss: 0.05417100712656975\n",
      "Iteration 13009, Loss: 0.05398884415626526\n",
      "Iteration 13010, Loss: 0.054171040654182434\n",
      "Iteration 13011, Loss: 0.053989000618457794\n",
      "Iteration 13012, Loss: 0.054170962423086166\n",
      "Iteration 13013, Loss: 0.05398900434374809\n",
      "Iteration 13014, Loss: 0.05417083948850632\n",
      "Iteration 13015, Loss: 0.0539892315864563\n",
      "Iteration 13016, Loss: 0.05417072772979736\n",
      "Iteration 13017, Loss: 0.053989242762327194\n",
      "Iteration 13018, Loss: 0.05417080968618393\n",
      "Iteration 13019, Loss: 0.053989164531230927\n",
      "Iteration 13020, Loss: 0.05417093262076378\n",
      "Iteration 13021, Loss: 0.05398876592516899\n",
      "Iteration 13022, Loss: 0.0541711300611496\n",
      "Iteration 13023, Loss: 0.05398857593536377\n",
      "Iteration 13024, Loss: 0.05417143553495407\n",
      "Iteration 13025, Loss: 0.053988419473171234\n",
      "Iteration 13026, Loss: 0.05417151376605034\n",
      "Iteration 13027, Loss: 0.05398853123188019\n",
      "Iteration 13028, Loss: 0.054171353578567505\n",
      "Iteration 13029, Loss: 0.05398868769407272\n",
      "Iteration 13030, Loss: 0.054171156138181686\n",
      "Iteration 13031, Loss: 0.05398895591497421\n",
      "Iteration 13032, Loss: 0.05417100340127945\n",
      "Iteration 13033, Loss: 0.05398900434374809\n",
      "Iteration 13034, Loss: 0.0541708879172802\n",
      "Iteration 13035, Loss: 0.05398911237716675\n",
      "Iteration 13036, Loss: 0.054170846939086914\n",
      "Iteration 13037, Loss: 0.053989119827747345\n",
      "Iteration 13038, Loss: 0.0541708879172802\n",
      "Iteration 13039, Loss: 0.05398911237716675\n",
      "Iteration 13040, Loss: 0.0541708879172802\n",
      "Iteration 13041, Loss: 0.053988851606845856\n",
      "Iteration 13042, Loss: 0.054171036928892136\n",
      "Iteration 13043, Loss: 0.0539887361228466\n",
      "Iteration 13044, Loss: 0.054170966148376465\n",
      "Iteration 13045, Loss: 0.05398876592516899\n",
      "Iteration 13046, Loss: 0.05417100340127945\n",
      "Iteration 13047, Loss: 0.05398876965045929\n",
      "Iteration 13048, Loss: 0.0541708879172802\n",
      "Iteration 13049, Loss: 0.053988926112651825\n",
      "Iteration 13050, Loss: 0.054171036928892136\n",
      "Iteration 13051, Loss: 0.053988926112651825\n",
      "Iteration 13052, Loss: 0.05417100340127945\n",
      "Iteration 13053, Loss: 0.05398888140916824\n",
      "Iteration 13054, Loss: 0.054171156138181686\n",
      "Iteration 13055, Loss: 0.05398869514465332\n",
      "Iteration 13056, Loss: 0.05417130887508392\n",
      "Iteration 13057, Loss: 0.05398861691355705\n",
      "Iteration 13058, Loss: 0.05417138338088989\n",
      "Iteration 13059, Loss: 0.05398876592516899\n",
      "Iteration 13060, Loss: 0.054171230643987656\n",
      "Iteration 13061, Loss: 0.05398876592516899\n",
      "Iteration 13062, Loss: 0.0541711151599884\n",
      "Iteration 13063, Loss: 0.05398891866207123\n",
      "Iteration 13064, Loss: 0.054171159863471985\n",
      "Iteration 13065, Loss: 0.05398876592516899\n",
      "Iteration 13066, Loss: 0.05417131632566452\n",
      "Iteration 13067, Loss: 0.05398872122168541\n",
      "Iteration 13068, Loss: 0.054171472787857056\n",
      "Iteration 13069, Loss: 0.05398845300078392\n",
      "Iteration 13070, Loss: 0.05417163297533989\n",
      "Iteration 13071, Loss: 0.05398837476968765\n",
      "Iteration 13072, Loss: 0.05417155846953392\n",
      "Iteration 13073, Loss: 0.05398837476968765\n",
      "Iteration 13074, Loss: 0.05417148396372795\n",
      "Iteration 13075, Loss: 0.05398833751678467\n",
      "Iteration 13076, Loss: 0.05417140573263168\n",
      "Iteration 13077, Loss: 0.053988486528396606\n",
      "Iteration 13078, Loss: 0.05417127162218094\n",
      "Iteration 13079, Loss: 0.05398884043097496\n",
      "Iteration 13080, Loss: 0.054171036928892136\n",
      "Iteration 13081, Loss: 0.05398907512426376\n",
      "Iteration 13082, Loss: 0.054170917719602585\n",
      "Iteration 13083, Loss: 0.053989194333553314\n",
      "Iteration 13084, Loss: 0.054170843213796616\n",
      "Iteration 13085, Loss: 0.05398911237716675\n",
      "Iteration 13086, Loss: 0.05417107790708542\n",
      "Iteration 13087, Loss: 0.05398891493678093\n",
      "Iteration 13088, Loss: 0.054171159863471985\n",
      "Iteration 13089, Loss: 0.05398876592516899\n",
      "Iteration 13090, Loss: 0.05417131632566452\n",
      "Iteration 13091, Loss: 0.053988680243492126\n",
      "Iteration 13092, Loss: 0.05417139455676079\n",
      "Iteration 13093, Loss: 0.05398844927549362\n",
      "Iteration 13094, Loss: 0.05417139455676079\n",
      "Iteration 13095, Loss: 0.05398860573768616\n",
      "Iteration 13096, Loss: 0.05417131632566452\n",
      "Iteration 13097, Loss: 0.053988635540008545\n",
      "Iteration 13098, Loss: 0.054171230643987656\n",
      "Iteration 13099, Loss: 0.053988806903362274\n",
      "Iteration 13100, Loss: 0.0541711151599884\n",
      "Iteration 13101, Loss: 0.053988926112651825\n",
      "Iteration 13102, Loss: 0.05417099595069885\n",
      "Iteration 13103, Loss: 0.05398900434374809\n",
      "Iteration 13104, Loss: 0.054170966148376465\n",
      "Iteration 13105, Loss: 0.05398907884955406\n",
      "Iteration 13106, Loss: 0.05417092889547348\n",
      "Iteration 13107, Loss: 0.05398903787136078\n",
      "Iteration 13108, Loss: 0.054171156138181686\n",
      "Iteration 13109, Loss: 0.05398872494697571\n",
      "Iteration 13110, Loss: 0.05417143553495407\n",
      "Iteration 13111, Loss: 0.05398852750658989\n",
      "Iteration 13112, Loss: 0.05417162925004959\n",
      "Iteration 13113, Loss: 0.053988367319107056\n",
      "Iteration 13114, Loss: 0.054171591997146606\n",
      "Iteration 13115, Loss: 0.05398844927549362\n",
      "Iteration 13116, Loss: 0.054171279072761536\n",
      "Iteration 13117, Loss: 0.053988754749298096\n",
      "Iteration 13118, Loss: 0.05417107790708542\n",
      "Iteration 13119, Loss: 0.0539889931678772\n",
      "Iteration 13120, Loss: 0.054170917719602585\n",
      "Iteration 13121, Loss: 0.05398915335536003\n",
      "Iteration 13122, Loss: 0.05417080968618393\n",
      "Iteration 13123, Loss: 0.05398915335536003\n",
      "Iteration 13124, Loss: 0.054170917719602585\n",
      "Iteration 13125, Loss: 0.05398911237716675\n",
      "Iteration 13126, Loss: 0.054171036928892136\n",
      "Iteration 13127, Loss: 0.0539889931678772\n",
      "Iteration 13128, Loss: 0.054171156138181686\n",
      "Iteration 13129, Loss: 0.05398876592516899\n",
      "Iteration 13130, Loss: 0.054171543568372726\n",
      "Iteration 13131, Loss: 0.05398844927549362\n",
      "Iteration 13132, Loss: 0.054171591997146606\n",
      "Iteration 13133, Loss: 0.05398837849497795\n",
      "Iteration 13134, Loss: 0.054171543568372726\n",
      "Iteration 13135, Loss: 0.05398852750658989\n",
      "Iteration 13136, Loss: 0.05417138338088989\n",
      "Iteration 13137, Loss: 0.053988754749298096\n",
      "Iteration 13138, Loss: 0.054171156138181686\n",
      "Iteration 13139, Loss: 0.05398888140916824\n",
      "Iteration 13140, Loss: 0.05417122691869736\n",
      "Iteration 13141, Loss: 0.05398895964026451\n",
      "Iteration 13142, Loss: 0.0541711151599884\n",
      "Iteration 13143, Loss: 0.05398888140916824\n",
      "Iteration 13144, Loss: 0.05417119711637497\n",
      "Iteration 13145, Loss: 0.05398876592516899\n",
      "Iteration 13146, Loss: 0.054171159863471985\n",
      "Iteration 13147, Loss: 0.05398876592516899\n",
      "Iteration 13148, Loss: 0.054171234369277954\n",
      "Iteration 13149, Loss: 0.05398883670568466\n",
      "Iteration 13150, Loss: 0.054171156138181686\n",
      "Iteration 13151, Loss: 0.05398891493678093\n",
      "Iteration 13152, Loss: 0.054171036928892136\n",
      "Iteration 13153, Loss: 0.05398883670568466\n",
      "Iteration 13154, Loss: 0.05417108163237572\n",
      "Iteration 13155, Loss: 0.05398891493678093\n",
      "Iteration 13156, Loss: 0.0541711151599884\n",
      "Iteration 13157, Loss: 0.05398881435394287\n",
      "Iteration 13158, Loss: 0.05417120084166527\n",
      "Iteration 13159, Loss: 0.05398865044116974\n",
      "Iteration 13160, Loss: 0.054171353578567505\n",
      "Iteration 13161, Loss: 0.05398860201239586\n",
      "Iteration 13162, Loss: 0.054171472787857056\n",
      "Iteration 13163, Loss: 0.053988486528396606\n",
      "Iteration 13164, Loss: 0.054171428084373474\n",
      "Iteration 13165, Loss: 0.05398860573768616\n",
      "Iteration 13166, Loss: 0.05417139083147049\n",
      "Iteration 13167, Loss: 0.053988754749298096\n",
      "Iteration 13168, Loss: 0.054171305149793625\n",
      "Iteration 13169, Loss: 0.053988873958587646\n",
      "Iteration 13170, Loss: 0.05417122691869736\n",
      "Iteration 13171, Loss: 0.05398891493678093\n",
      "Iteration 13172, Loss: 0.054171185940504074\n",
      "Iteration 13173, Loss: 0.05398891493678093\n",
      "Iteration 13174, Loss: 0.054171156138181686\n",
      "Iteration 13175, Loss: 0.0539889931678772\n",
      "Iteration 13176, Loss: 0.054171122610569\n",
      "Iteration 13177, Loss: 0.05398879572749138\n",
      "Iteration 13178, Loss: 0.054171353578567505\n",
      "Iteration 13179, Loss: 0.053988680243492126\n",
      "Iteration 13180, Loss: 0.05417139455676079\n",
      "Iteration 13181, Loss: 0.05398845300078392\n",
      "Iteration 13182, Loss: 0.05417148023843765\n",
      "Iteration 13183, Loss: 0.05398841202259064\n",
      "Iteration 13184, Loss: 0.054171472787857056\n",
      "Iteration 13185, Loss: 0.053988561034202576\n",
      "Iteration 13186, Loss: 0.05417150259017944\n",
      "Iteration 13187, Loss: 0.05398864299058914\n",
      "Iteration 13188, Loss: 0.05417119711637497\n",
      "Iteration 13189, Loss: 0.053989022970199585\n",
      "Iteration 13190, Loss: 0.054171185940504074\n",
      "Iteration 13191, Loss: 0.05398903414607048\n",
      "Iteration 13192, Loss: 0.05417107790708542\n",
      "Iteration 13193, Loss: 0.05398903787136078\n",
      "Iteration 13194, Loss: 0.05417119711637497\n",
      "Iteration 13195, Loss: 0.053988873958587646\n",
      "Iteration 13196, Loss: 0.05417139455676079\n",
      "Iteration 13197, Loss: 0.053988680243492126\n",
      "Iteration 13198, Loss: 0.05417151376605034\n",
      "Iteration 13199, Loss: 0.05398844927549362\n",
      "Iteration 13200, Loss: 0.05417163297533989\n",
      "Iteration 13201, Loss: 0.05398828908801079\n",
      "Iteration 13202, Loss: 0.05417166277766228\n",
      "Iteration 13203, Loss: 0.05398848280310631\n",
      "Iteration 13204, Loss: 0.054171472787857056\n",
      "Iteration 13205, Loss: 0.05398860573768616\n",
      "Iteration 13206, Loss: 0.05417127534747124\n",
      "Iteration 13207, Loss: 0.053988873958587646\n",
      "Iteration 13208, Loss: 0.054171156138181686\n",
      "Iteration 13209, Loss: 0.05398888140916824\n",
      "Iteration 13210, Loss: 0.054171036928892136\n",
      "Iteration 13211, Loss: 0.05398911237716675\n",
      "Iteration 13212, Loss: 0.054171036928892136\n",
      "Iteration 13213, Loss: 0.05398895591497421\n",
      "Iteration 13214, Loss: 0.05417127534747124\n",
      "Iteration 13215, Loss: 0.05398883670568466\n",
      "Iteration 13216, Loss: 0.05417132005095482\n",
      "Iteration 13217, Loss: 0.05398860201239586\n",
      "Iteration 13218, Loss: 0.05417163297533989\n",
      "Iteration 13219, Loss: 0.053988486528396606\n",
      "Iteration 13220, Loss: 0.05417171120643616\n",
      "Iteration 13221, Loss: 0.0539882555603981\n",
      "Iteration 13222, Loss: 0.05417163297533989\n",
      "Iteration 13223, Loss: 0.05398844927549362\n",
      "Iteration 13224, Loss: 0.05417155474424362\n",
      "Iteration 13225, Loss: 0.053988561034202576\n",
      "Iteration 13226, Loss: 0.054171424359083176\n",
      "Iteration 13227, Loss: 0.053988680243492126\n",
      "Iteration 13228, Loss: 0.05417119711637497\n",
      "Iteration 13229, Loss: 0.0539889931678772\n",
      "Iteration 13230, Loss: 0.054171230643987656\n",
      "Iteration 13231, Loss: 0.05398884043097496\n",
      "Iteration 13232, Loss: 0.05417127162218094\n",
      "Iteration 13233, Loss: 0.05398872494697571\n",
      "Iteration 13234, Loss: 0.05417127534747124\n",
      "Iteration 13235, Loss: 0.05398860573768616\n",
      "Iteration 13236, Loss: 0.05417124181985855\n",
      "Iteration 13237, Loss: 0.05398876592516899\n",
      "Iteration 13238, Loss: 0.054171234369277954\n",
      "Iteration 13239, Loss: 0.05398861691355705\n",
      "Iteration 13240, Loss: 0.05417127162218094\n",
      "Iteration 13241, Loss: 0.05398872494697571\n",
      "Iteration 13242, Loss: 0.054171156138181686\n",
      "Iteration 13243, Loss: 0.053988873958587646\n",
      "Iteration 13244, Loss: 0.054171156138181686\n",
      "Iteration 13245, Loss: 0.05398895591497421\n",
      "Iteration 13246, Loss: 0.054171156138181686\n",
      "Iteration 13247, Loss: 0.05398888513445854\n",
      "Iteration 13248, Loss: 0.0541711151599884\n",
      "Iteration 13249, Loss: 0.05398876592516899\n",
      "Iteration 13250, Loss: 0.05417127162218094\n",
      "Iteration 13251, Loss: 0.05398879572749138\n",
      "Iteration 13252, Loss: 0.05417130887508392\n",
      "Iteration 13253, Loss: 0.053988680243492126\n",
      "Iteration 13254, Loss: 0.05417145788669586\n",
      "Iteration 13255, Loss: 0.05398879572749138\n",
      "Iteration 13256, Loss: 0.05417127534747124\n",
      "Iteration 13257, Loss: 0.05398883670568466\n",
      "Iteration 13258, Loss: 0.0541711151599884\n",
      "Iteration 13259, Loss: 0.0539889931678772\n",
      "Iteration 13260, Loss: 0.054170966148376465\n",
      "Iteration 13261, Loss: 0.05398918688297272\n",
      "Iteration 13262, Loss: 0.05417095869779587\n",
      "Iteration 13263, Loss: 0.05398903414607048\n",
      "Iteration 13264, Loss: 0.05417092144489288\n",
      "Iteration 13265, Loss: 0.05398891866207123\n",
      "Iteration 13266, Loss: 0.054171085357666016\n",
      "Iteration 13267, Loss: 0.053988873958587646\n",
      "Iteration 13268, Loss: 0.05417127534747124\n",
      "Iteration 13269, Loss: 0.053988754749298096\n",
      "Iteration 13270, Loss: 0.05417124181985855\n",
      "Iteration 13271, Loss: 0.05398872494697571\n",
      "Iteration 13272, Loss: 0.05417127534747124\n",
      "Iteration 13273, Loss: 0.05398884043097496\n",
      "Iteration 13274, Loss: 0.054171230643987656\n",
      "Iteration 13275, Loss: 0.05398895591497421\n",
      "Iteration 13276, Loss: 0.054171156138181686\n",
      "Iteration 13277, Loss: 0.05398891493678093\n",
      "Iteration 13278, Loss: 0.054171156138181686\n",
      "Iteration 13279, Loss: 0.05398888140916824\n",
      "Iteration 13280, Loss: 0.05417108163237572\n",
      "Iteration 13281, Loss: 0.05398895591497421\n",
      "Iteration 13282, Loss: 0.054171230643987656\n",
      "Iteration 13283, Loss: 0.05398891493678093\n",
      "Iteration 13284, Loss: 0.054171279072761536\n",
      "Iteration 13285, Loss: 0.053988680243492126\n",
      "Iteration 13286, Loss: 0.05417140573263168\n",
      "Iteration 13287, Loss: 0.053988486528396606\n",
      "Iteration 13288, Loss: 0.054171591997146606\n",
      "Iteration 13289, Loss: 0.053988486528396606\n",
      "Iteration 13290, Loss: 0.05417151376605034\n",
      "Iteration 13291, Loss: 0.05398852750658989\n",
      "Iteration 13292, Loss: 0.05417127534747124\n",
      "Iteration 13293, Loss: 0.053988657891750336\n",
      "Iteration 13294, Loss: 0.054171107709407806\n",
      "Iteration 13295, Loss: 0.05398896336555481\n",
      "Iteration 13296, Loss: 0.0541708767414093\n",
      "Iteration 13297, Loss: 0.05398915708065033\n",
      "Iteration 13298, Loss: 0.0541708767414093\n",
      "Iteration 13299, Loss: 0.05398927256464958\n",
      "Iteration 13300, Loss: 0.054170917719602585\n",
      "Iteration 13301, Loss: 0.05398911237716675\n",
      "Iteration 13302, Loss: 0.054171036928892136\n",
      "Iteration 13303, Loss: 0.0539889931678772\n",
      "Iteration 13304, Loss: 0.05417127534747124\n",
      "Iteration 13305, Loss: 0.05398864299058914\n",
      "Iteration 13306, Loss: 0.054171472787857056\n",
      "Iteration 13307, Loss: 0.053988486528396606\n",
      "Iteration 13308, Loss: 0.05417170375585556\n",
      "Iteration 13309, Loss: 0.053988486528396606\n",
      "Iteration 13310, Loss: 0.05417151376605034\n",
      "Iteration 13311, Loss: 0.05398864299058914\n",
      "Iteration 13312, Loss: 0.05417139455676079\n",
      "Iteration 13313, Loss: 0.053988873958587646\n",
      "Iteration 13314, Loss: 0.05417119711637497\n",
      "Iteration 13315, Loss: 0.05398891493678093\n",
      "Iteration 13316, Loss: 0.05417107790708542\n",
      "Iteration 13317, Loss: 0.053989067673683167\n",
      "Iteration 13318, Loss: 0.054170966148376465\n",
      "Iteration 13319, Loss: 0.053988926112651825\n",
      "Iteration 13320, Loss: 0.054171085357666016\n",
      "Iteration 13321, Loss: 0.053988873958587646\n",
      "Iteration 13322, Loss: 0.05417132005095482\n",
      "Iteration 13323, Loss: 0.05398860201239586\n",
      "Iteration 13324, Loss: 0.054171621799468994\n",
      "Iteration 13325, Loss: 0.053988486528396606\n",
      "Iteration 13326, Loss: 0.05417158827185631\n",
      "Iteration 13327, Loss: 0.05398852750658989\n",
      "Iteration 13328, Loss: 0.05417146533727646\n",
      "Iteration 13329, Loss: 0.05398860573768616\n",
      "Iteration 13330, Loss: 0.05417138338088989\n",
      "Iteration 13331, Loss: 0.05398883670568466\n",
      "Iteration 13332, Loss: 0.05417118966579437\n",
      "Iteration 13333, Loss: 0.05398891866207123\n",
      "Iteration 13334, Loss: 0.0541711151599884\n",
      "Iteration 13335, Loss: 0.05398896336555481\n",
      "Iteration 13336, Loss: 0.054171036928892136\n",
      "Iteration 13337, Loss: 0.05398896336555481\n",
      "Iteration 13338, Loss: 0.054171085357666016\n",
      "Iteration 13339, Loss: 0.05398883670568466\n",
      "Iteration 13340, Loss: 0.054171279072761536\n",
      "Iteration 13341, Loss: 0.053988754749298096\n",
      "Iteration 13342, Loss: 0.05417150259017944\n",
      "Iteration 13343, Loss: 0.053988486528396606\n",
      "Iteration 13344, Loss: 0.054171472787857056\n",
      "Iteration 13345, Loss: 0.05398859828710556\n",
      "Iteration 13346, Loss: 0.05417139083147049\n",
      "Iteration 13347, Loss: 0.05398872494697571\n",
      "Iteration 13348, Loss: 0.0541711151599884\n",
      "Iteration 13349, Loss: 0.0539889931678772\n",
      "Iteration 13350, Loss: 0.054170917719602585\n",
      "Iteration 13351, Loss: 0.05398918688297272\n",
      "Iteration 13352, Loss: 0.05417080223560333\n",
      "Iteration 13353, Loss: 0.05398911237716675\n",
      "Iteration 13354, Loss: 0.054171036928892136\n",
      "Iteration 13355, Loss: 0.05398891866207123\n",
      "Iteration 13356, Loss: 0.05417119711637497\n",
      "Iteration 13357, Loss: 0.05398872122168541\n",
      "Iteration 13358, Loss: 0.05417155474424362\n",
      "Iteration 13359, Loss: 0.05398837476968765\n",
      "Iteration 13360, Loss: 0.054171741008758545\n",
      "Iteration 13361, Loss: 0.05398840457201004\n",
      "Iteration 13362, Loss: 0.05417155474424362\n",
      "Iteration 13363, Loss: 0.053988516330718994\n",
      "Iteration 13364, Loss: 0.05417138338088989\n",
      "Iteration 13365, Loss: 0.05398883670568466\n",
      "Iteration 13366, Loss: 0.0541711151599884\n",
      "Iteration 13367, Loss: 0.053989067673683167\n",
      "Iteration 13368, Loss: 0.05417100340127945\n",
      "Iteration 13369, Loss: 0.05398907512426376\n",
      "Iteration 13370, Loss: 0.054170962423086166\n",
      "Iteration 13371, Loss: 0.05398895964026451\n",
      "Iteration 13372, Loss: 0.054171122610569\n",
      "Iteration 13373, Loss: 0.05398876219987869\n",
      "Iteration 13374, Loss: 0.05417139455676079\n",
      "Iteration 13375, Loss: 0.05398856848478317\n",
      "Iteration 13376, Loss: 0.054171472787857056\n",
      "Iteration 13377, Loss: 0.05398844927549362\n",
      "Iteration 13378, Loss: 0.05417158827185631\n",
      "Iteration 13379, Loss: 0.05398852750658989\n",
      "Iteration 13380, Loss: 0.05417143553495407\n",
      "Iteration 13381, Loss: 0.05398856848478317\n",
      "Iteration 13382, Loss: 0.054171353578567505\n",
      "Iteration 13383, Loss: 0.05398883670568466\n",
      "Iteration 13384, Loss: 0.05417131632566452\n",
      "Iteration 13385, Loss: 0.05398883670568466\n",
      "Iteration 13386, Loss: 0.05417127534747124\n",
      "Iteration 13387, Loss: 0.05398883670568466\n",
      "Iteration 13388, Loss: 0.05417126417160034\n",
      "Iteration 13389, Loss: 0.05398895591497421\n",
      "Iteration 13390, Loss: 0.05417107790708542\n",
      "Iteration 13391, Loss: 0.05398891866207123\n",
      "Iteration 13392, Loss: 0.0541711151599884\n",
      "Iteration 13393, Loss: 0.05398876219987869\n",
      "Iteration 13394, Loss: 0.05417131632566452\n",
      "Iteration 13395, Loss: 0.05398867651820183\n",
      "Iteration 13396, Loss: 0.0541713610291481\n",
      "Iteration 13397, Loss: 0.05398867651820183\n",
      "Iteration 13398, Loss: 0.05417131632566452\n",
      "Iteration 13399, Loss: 0.053988754749298096\n",
      "Iteration 13400, Loss: 0.05417127534747124\n",
      "Iteration 13401, Loss: 0.053988806903362274\n",
      "Iteration 13402, Loss: 0.05417119711637497\n",
      "Iteration 13403, Loss: 0.05398879945278168\n",
      "Iteration 13404, Loss: 0.054171234369277954\n",
      "Iteration 13405, Loss: 0.053988873958587646\n",
      "Iteration 13406, Loss: 0.054171156138181686\n",
      "Iteration 13407, Loss: 0.05398876592516899\n",
      "Iteration 13408, Loss: 0.054171156138181686\n",
      "Iteration 13409, Loss: 0.05398895591497421\n",
      "Iteration 13410, Loss: 0.05417119711637497\n",
      "Iteration 13411, Loss: 0.053988806903362274\n",
      "Iteration 13412, Loss: 0.054171305149793625\n",
      "Iteration 13413, Loss: 0.05398876592516899\n",
      "Iteration 13414, Loss: 0.054171349853277206\n",
      "Iteration 13415, Loss: 0.05398872494697571\n",
      "Iteration 13416, Loss: 0.05417130887508392\n",
      "Iteration 13417, Loss: 0.05398883670568466\n",
      "Iteration 13418, Loss: 0.05417119711637497\n",
      "Iteration 13419, Loss: 0.05398884043097496\n",
      "Iteration 13420, Loss: 0.0541711151599884\n",
      "Iteration 13421, Loss: 0.05398888513445854\n",
      "Iteration 13422, Loss: 0.054171156138181686\n",
      "Iteration 13423, Loss: 0.05398884415626526\n",
      "Iteration 13424, Loss: 0.054171234369277954\n",
      "Iteration 13425, Loss: 0.05398883670568466\n",
      "Iteration 13426, Loss: 0.05417127534747124\n",
      "Iteration 13427, Loss: 0.05398887023329735\n",
      "Iteration 13428, Loss: 0.054171353578567505\n",
      "Iteration 13429, Loss: 0.05398864671587944\n",
      "Iteration 13430, Loss: 0.054171279072761536\n",
      "Iteration 13431, Loss: 0.05398876219987869\n",
      "Iteration 13432, Loss: 0.05417138338088989\n",
      "Iteration 13433, Loss: 0.05398884043097496\n",
      "Iteration 13434, Loss: 0.054171156138181686\n",
      "Iteration 13435, Loss: 0.053988806903362274\n",
      "Iteration 13436, Loss: 0.054171156138181686\n",
      "Iteration 13437, Loss: 0.05398895591497421\n",
      "Iteration 13438, Loss: 0.05417115241289139\n",
      "Iteration 13439, Loss: 0.05398888140916824\n",
      "Iteration 13440, Loss: 0.05417122691869736\n",
      "Iteration 13441, Loss: 0.05398876592516899\n",
      "Iteration 13442, Loss: 0.05417131632566452\n",
      "Iteration 13443, Loss: 0.05398879572749138\n",
      "Iteration 13444, Loss: 0.05417146906256676\n",
      "Iteration 13445, Loss: 0.05398856848478317\n",
      "Iteration 13446, Loss: 0.05417143553495407\n",
      "Iteration 13447, Loss: 0.053988486528396606\n",
      "Iteration 13448, Loss: 0.05417158827185631\n",
      "Iteration 13449, Loss: 0.05398859828710556\n",
      "Iteration 13450, Loss: 0.05417132005095482\n",
      "Iteration 13451, Loss: 0.05398867651820183\n",
      "Iteration 13452, Loss: 0.05417126417160034\n",
      "Iteration 13453, Loss: 0.053988806903362274\n",
      "Iteration 13454, Loss: 0.05417107045650482\n",
      "Iteration 13455, Loss: 0.05398918315768242\n",
      "Iteration 13456, Loss: 0.054170843213796616\n",
      "Iteration 13457, Loss: 0.05398915335536003\n",
      "Iteration 13458, Loss: 0.05417107045650482\n",
      "Iteration 13459, Loss: 0.05398896336555481\n",
      "Iteration 13460, Loss: 0.054171156138181686\n",
      "Iteration 13461, Loss: 0.05398876592516899\n",
      "Iteration 13462, Loss: 0.054171234369277954\n",
      "Iteration 13463, Loss: 0.05398872122168541\n",
      "Iteration 13464, Loss: 0.054171472787857056\n",
      "Iteration 13465, Loss: 0.053988486528396606\n",
      "Iteration 13466, Loss: 0.054171591997146606\n",
      "Iteration 13467, Loss: 0.053988367319107056\n",
      "Iteration 13468, Loss: 0.054171666502952576\n",
      "Iteration 13469, Loss: 0.05398847907781601\n",
      "Iteration 13470, Loss: 0.054171547293663025\n",
      "Iteration 13471, Loss: 0.053988486528396606\n",
      "Iteration 13472, Loss: 0.054171428084373474\n",
      "Iteration 13473, Loss: 0.05398867651820183\n",
      "Iteration 13474, Loss: 0.05417134612798691\n",
      "Iteration 13475, Loss: 0.05398891493678093\n",
      "Iteration 13476, Loss: 0.054171122610569\n",
      "Iteration 13477, Loss: 0.05398895591497421\n",
      "Iteration 13478, Loss: 0.054171234369277954\n",
      "Iteration 13479, Loss: 0.05398876592516899\n",
      "Iteration 13480, Loss: 0.05417127534747124\n",
      "Iteration 13481, Loss: 0.053988754749298096\n",
      "Iteration 13482, Loss: 0.054171353578567505\n",
      "Iteration 13483, Loss: 0.05398871749639511\n",
      "Iteration 13484, Loss: 0.05417131632566452\n",
      "Iteration 13485, Loss: 0.05398872494697571\n",
      "Iteration 13486, Loss: 0.05417122691869736\n",
      "Iteration 13487, Loss: 0.05398884043097496\n",
      "Iteration 13488, Loss: 0.05417118966579437\n",
      "Iteration 13489, Loss: 0.05398891866207123\n",
      "Iteration 13490, Loss: 0.05417095869779587\n",
      "Iteration 13491, Loss: 0.0539889931678772\n",
      "Iteration 13492, Loss: 0.05417092144489288\n",
      "Iteration 13493, Loss: 0.0539889931678772\n",
      "Iteration 13494, Loss: 0.054171036928892136\n",
      "Iteration 13495, Loss: 0.053988806903362274\n",
      "Iteration 13496, Loss: 0.05417119711637497\n",
      "Iteration 13497, Loss: 0.05398876592516899\n",
      "Iteration 13498, Loss: 0.05417130887508392\n",
      "Iteration 13499, Loss: 0.05398879572749138\n",
      "Iteration 13500, Loss: 0.05417127534747124\n",
      "Iteration 13501, Loss: 0.05398884043097496\n",
      "Iteration 13502, Loss: 0.05417126417160034\n",
      "Iteration 13503, Loss: 0.053988806903362274\n",
      "Iteration 13504, Loss: 0.05417119711637497\n",
      "Iteration 13505, Loss: 0.05398884043097496\n",
      "Iteration 13506, Loss: 0.054171234369277954\n",
      "Iteration 13507, Loss: 0.05398879945278168\n",
      "Iteration 13508, Loss: 0.054171305149793625\n",
      "Iteration 13509, Loss: 0.05398884043097496\n",
      "Iteration 13510, Loss: 0.054171156138181686\n",
      "Iteration 13511, Loss: 0.05398888140916824\n",
      "Iteration 13512, Loss: 0.054171156138181686\n",
      "Iteration 13513, Loss: 0.05398888140916824\n",
      "Iteration 13514, Loss: 0.054171156138181686\n",
      "Iteration 13515, Loss: 0.05398884415626526\n",
      "Iteration 13516, Loss: 0.054171085357666016\n",
      "Iteration 13517, Loss: 0.05398884043097496\n",
      "Iteration 13518, Loss: 0.05417119711637497\n",
      "Iteration 13519, Loss: 0.05398888513445854\n",
      "Iteration 13520, Loss: 0.05417119711637497\n",
      "Iteration 13521, Loss: 0.05398888513445854\n",
      "Iteration 13522, Loss: 0.054171230643987656\n",
      "Iteration 13523, Loss: 0.05398888513445854\n",
      "Iteration 13524, Loss: 0.054171156138181686\n",
      "Iteration 13525, Loss: 0.0539889931678772\n",
      "Iteration 13526, Loss: 0.054171156138181686\n",
      "Iteration 13527, Loss: 0.0539889931678772\n",
      "Iteration 13528, Loss: 0.05417118966579437\n",
      "Iteration 13529, Loss: 0.05398884043097496\n",
      "Iteration 13530, Loss: 0.05417127534747124\n",
      "Iteration 13531, Loss: 0.05398879572749138\n",
      "Iteration 13532, Loss: 0.05417131632566452\n",
      "Iteration 13533, Loss: 0.05398871749639511\n",
      "Iteration 13534, Loss: 0.0541713610291481\n",
      "Iteration 13535, Loss: 0.053988561034202576\n",
      "Iteration 13536, Loss: 0.054171398282051086\n",
      "Iteration 13537, Loss: 0.05398907512426376\n",
      "Iteration 13538, Loss: 0.05417139455676079\n",
      "Iteration 13539, Loss: 0.0539892315864563\n",
      "Iteration 13540, Loss: 0.05417175218462944\n",
      "Iteration 13541, Loss: 0.05398927256464958\n",
      "Iteration 13542, Loss: 0.05417182296514511\n",
      "Iteration 13543, Loss: 0.053989313542842865\n",
      "Iteration 13544, Loss: 0.054171591997146606\n",
      "Iteration 13545, Loss: 0.05398931726813316\n",
      "Iteration 13546, Loss: 0.054171666502952576\n",
      "Iteration 13547, Loss: 0.053989313542842865\n",
      "Iteration 13548, Loss: 0.05417171120643616\n",
      "Iteration 13549, Loss: 0.053989239037036896\n",
      "Iteration 13550, Loss: 0.05417171120643616\n",
      "Iteration 13551, Loss: 0.053989239037036896\n",
      "Iteration 13552, Loss: 0.05417175218462944\n",
      "Iteration 13553, Loss: 0.05398934707045555\n",
      "Iteration 13554, Loss: 0.05417175218462944\n",
      "Iteration 13555, Loss: 0.05398927256464958\n",
      "Iteration 13556, Loss: 0.05417182296514511\n",
      "Iteration 13557, Loss: 0.05398934707045555\n",
      "Iteration 13558, Loss: 0.054171741008758545\n",
      "Iteration 13559, Loss: 0.05398935079574585\n",
      "Iteration 13560, Loss: 0.05417170375585556\n",
      "Iteration 13561, Loss: 0.0539894662797451\n",
      "Iteration 13562, Loss: 0.054171547293663025\n",
      "Iteration 13563, Loss: 0.05398954451084137\n",
      "Iteration 13564, Loss: 0.054171591997146606\n",
      "Iteration 13565, Loss: 0.0539894662797451\n",
      "Iteration 13566, Loss: 0.05417171120643616\n",
      "Iteration 13567, Loss: 0.053989194333553314\n",
      "Iteration 13568, Loss: 0.05417199060320854\n",
      "Iteration 13569, Loss: 0.05398891493678093\n",
      "Iteration 13570, Loss: 0.054172225296497345\n",
      "Iteration 13571, Loss: 0.05398884043097496\n",
      "Iteration 13572, Loss: 0.05417221784591675\n",
      "Iteration 13573, Loss: 0.05398888513445854\n",
      "Iteration 13574, Loss: 0.05417182669043541\n",
      "Iteration 13575, Loss: 0.053989242762327194\n",
      "Iteration 13576, Loss: 0.054171621799468994\n",
      "Iteration 13577, Loss: 0.05398955196142197\n",
      "Iteration 13578, Loss: 0.05417127162218094\n",
      "Iteration 13579, Loss: 0.053989820182323456\n",
      "Iteration 13580, Loss: 0.05417122691869736\n",
      "Iteration 13581, Loss: 0.05398985743522644\n",
      "Iteration 13582, Loss: 0.05417134612798691\n",
      "Iteration 13583, Loss: 0.05398967117071152\n",
      "Iteration 13584, Loss: 0.05417151004076004\n",
      "Iteration 13585, Loss: 0.053989432752132416\n",
      "Iteration 13586, Loss: 0.054171785712242126\n",
      "Iteration 13587, Loss: 0.053989194333553314\n",
      "Iteration 13588, Loss: 0.05417206883430481\n",
      "Iteration 13589, Loss: 0.05398891866207123\n",
      "Iteration 13590, Loss: 0.05417249724268913\n",
      "Iteration 13591, Loss: 0.05398864299058914\n",
      "Iteration 13592, Loss: 0.05417242646217346\n",
      "Iteration 13593, Loss: 0.05398867651820183\n",
      "Iteration 13594, Loss: 0.05417230725288391\n",
      "Iteration 13595, Loss: 0.053988754749298096\n",
      "Iteration 13596, Loss: 0.05417194589972496\n",
      "Iteration 13597, Loss: 0.053989119827747345\n",
      "Iteration 13598, Loss: 0.05417170375585556\n",
      "Iteration 13599, Loss: 0.0539894700050354\n",
      "Iteration 13600, Loss: 0.05417151376605034\n",
      "Iteration 13601, Loss: 0.053989432752132416\n",
      "Iteration 13602, Loss: 0.054171398282051086\n",
      "Iteration 13603, Loss: 0.0539894700050354\n",
      "Iteration 13604, Loss: 0.05417151749134064\n",
      "Iteration 13605, Loss: 0.053989432752132416\n",
      "Iteration 13606, Loss: 0.05417155846953392\n",
      "Iteration 13607, Loss: 0.05398935079574585\n",
      "Iteration 13608, Loss: 0.05417170375585556\n",
      "Iteration 13609, Loss: 0.05398939177393913\n",
      "Iteration 13610, Loss: 0.054171741008758545\n",
      "Iteration 13611, Loss: 0.05398935079574585\n",
      "Iteration 13612, Loss: 0.05417186766862869\n",
      "Iteration 13613, Loss: 0.05398915335536003\n",
      "Iteration 13614, Loss: 0.0541720911860466\n",
      "Iteration 13615, Loss: 0.05398907512426376\n",
      "Iteration 13616, Loss: 0.054171912372112274\n",
      "Iteration 13617, Loss: 0.0539889931678772\n",
      "Iteration 13618, Loss: 0.05417199060320854\n",
      "Iteration 13619, Loss: 0.05398903787136078\n",
      "Iteration 13620, Loss: 0.05417183041572571\n",
      "Iteration 13621, Loss: 0.05398927256464958\n",
      "Iteration 13622, Loss: 0.054171785712242126\n",
      "Iteration 13623, Loss: 0.05398919805884361\n",
      "Iteration 13624, Loss: 0.05417171120643616\n",
      "Iteration 13625, Loss: 0.053989313542842865\n",
      "Iteration 13626, Loss: 0.05417170375585556\n",
      "Iteration 13627, Loss: 0.0539892315864563\n",
      "Iteration 13628, Loss: 0.054171666502952576\n",
      "Iteration 13629, Loss: 0.05398939177393913\n",
      "Iteration 13630, Loss: 0.05417158454656601\n",
      "Iteration 13631, Loss: 0.053989626467227936\n",
      "Iteration 13632, Loss: 0.054171353578567505\n",
      "Iteration 13633, Loss: 0.05398955196142197\n",
      "Iteration 13634, Loss: 0.05417166277766228\n",
      "Iteration 13635, Loss: 0.05398939177393913\n",
      "Iteration 13636, Loss: 0.054171785712242126\n",
      "Iteration 13637, Loss: 0.05398915335536003\n",
      "Iteration 13638, Loss: 0.05417194962501526\n",
      "Iteration 13639, Loss: 0.05398907512426376\n",
      "Iteration 13640, Loss: 0.05417218059301376\n",
      "Iteration 13641, Loss: 0.05398884043097496\n",
      "Iteration 13642, Loss: 0.05417221784591675\n",
      "Iteration 13643, Loss: 0.05398895591497421\n",
      "Iteration 13644, Loss: 0.0541720986366272\n",
      "Iteration 13645, Loss: 0.05398895964026451\n",
      "Iteration 13646, Loss: 0.05417194217443466\n",
      "Iteration 13647, Loss: 0.05398900434374809\n",
      "Iteration 13648, Loss: 0.05417174845933914\n",
      "Iteration 13649, Loss: 0.05398942157626152\n",
      "Iteration 13650, Loss: 0.05417155474424362\n",
      "Iteration 13651, Loss: 0.053989946842193604\n",
      "Iteration 13652, Loss: 0.05417158454656601\n",
      "Iteration 13653, Loss: 0.05399002879858017\n",
      "Iteration 13654, Loss: 0.054171591997146606\n",
      "Iteration 13655, Loss: 0.053989872336387634\n",
      "Iteration 13656, Loss: 0.05417210981249809\n",
      "Iteration 13657, Loss: 0.05398979038000107\n",
      "Iteration 13658, Loss: 0.05417242646217346\n",
      "Iteration 13659, Loss: 0.05398958921432495\n",
      "Iteration 13660, Loss: 0.054172582924366\n",
      "Iteration 13661, Loss: 0.05398939549922943\n",
      "Iteration 13662, Loss: 0.05417269468307495\n",
      "Iteration 13663, Loss: 0.0539894700050354\n",
      "Iteration 13664, Loss: 0.05417249724268913\n",
      "Iteration 13665, Loss: 0.05398952215909958\n",
      "Iteration 13666, Loss: 0.05417218804359436\n",
      "Iteration 13667, Loss: 0.053989797830581665\n",
      "Iteration 13668, Loss: 0.054172031581401825\n",
      "Iteration 13669, Loss: 0.05398935079574585\n",
      "Iteration 13670, Loss: 0.054172031581401825\n",
      "Iteration 13671, Loss: 0.05398919805884361\n",
      "Iteration 13672, Loss: 0.054172348231077194\n",
      "Iteration 13673, Loss: 0.05398891493678093\n",
      "Iteration 13674, Loss: 0.05417337268590927\n",
      "Iteration 13675, Loss: 0.05398860573768616\n",
      "Iteration 13676, Loss: 0.05417337641119957\n",
      "Iteration 13677, Loss: 0.05398864671587944\n",
      "Iteration 13678, Loss: 0.05417313799262047\n",
      "Iteration 13679, Loss: 0.05398881435394287\n",
      "Iteration 13680, Loss: 0.05417269468307495\n",
      "Iteration 13681, Loss: 0.05398935824632645\n",
      "Iteration 13682, Loss: 0.05417225882411003\n",
      "Iteration 13683, Loss: 0.0539897195994854\n",
      "Iteration 13684, Loss: 0.05417182296514511\n",
      "Iteration 13685, Loss: 0.05399010702967644\n",
      "Iteration 13686, Loss: 0.054171666502952576\n",
      "Iteration 13687, Loss: 0.05399022623896599\n",
      "Iteration 13688, Loss: 0.05417163297533989\n",
      "Iteration 13689, Loss: 0.05399014800786972\n",
      "Iteration 13690, Loss: 0.0541718415915966\n",
      "Iteration 13691, Loss: 0.053989898413419724\n",
      "Iteration 13692, Loss: 0.05417219549417496\n",
      "Iteration 13693, Loss: 0.053989700973033905\n",
      "Iteration 13694, Loss: 0.054172392934560776\n",
      "Iteration 13695, Loss: 0.053989313542842865\n",
      "Iteration 13696, Loss: 0.054172541946172714\n",
      "Iteration 13697, Loss: 0.053989432752132416\n",
      "Iteration 13698, Loss: 0.054172348231077194\n",
      "Iteration 13699, Loss: 0.05398954451084137\n",
      "Iteration 13700, Loss: 0.05417215824127197\n",
      "Iteration 13701, Loss: 0.05398967117071152\n",
      "Iteration 13702, Loss: 0.05417206883430481\n",
      "Iteration 13703, Loss: 0.053989898413419724\n",
      "Iteration 13704, Loss: 0.05417194962501526\n",
      "Iteration 13705, Loss: 0.05398990958929062\n",
      "Iteration 13706, Loss: 0.05417187511920929\n",
      "Iteration 13707, Loss: 0.053989898413419724\n",
      "Iteration 13708, Loss: 0.054171957075595856\n",
      "Iteration 13709, Loss: 0.0539897084236145\n",
      "Iteration 13710, Loss: 0.05417218804359436\n",
      "Iteration 13711, Loss: 0.053989626467227936\n",
      "Iteration 13712, Loss: 0.05417219549417496\n",
      "Iteration 13713, Loss: 0.0539894700050354\n",
      "Iteration 13714, Loss: 0.05417231470346451\n",
      "Iteration 13715, Loss: 0.0539894700050354\n",
      "Iteration 13716, Loss: 0.05417230725288391\n",
      "Iteration 13717, Loss: 0.0539894700050354\n",
      "Iteration 13718, Loss: 0.05417222902178764\n",
      "Iteration 13719, Loss: 0.053989749401807785\n",
      "Iteration 13720, Loss: 0.05417226254940033\n",
      "Iteration 13721, Loss: 0.05398967117071152\n",
      "Iteration 13722, Loss: 0.05417210981249809\n",
      "Iteration 13723, Loss: 0.05398967117071152\n",
      "Iteration 13724, Loss: 0.054172150790691376\n",
      "Iteration 13725, Loss: 0.05398967117071152\n",
      "Iteration 13726, Loss: 0.05417219549417496\n",
      "Iteration 13727, Loss: 0.05398961901664734\n",
      "Iteration 13728, Loss: 0.05417224019765854\n",
      "Iteration 13729, Loss: 0.05398955196142197\n",
      "Iteration 13730, Loss: 0.054172273725271225\n",
      "Iteration 13731, Loss: 0.05398954451084137\n",
      "Iteration 13732, Loss: 0.05417219549417496\n",
      "Iteration 13733, Loss: 0.053989630192518234\n",
      "Iteration 13734, Loss: 0.05417206883430481\n",
      "Iteration 13735, Loss: 0.05398979038000107\n",
      "Iteration 13736, Loss: 0.05417187139391899\n",
      "Iteration 13737, Loss: 0.05398990958929062\n",
      "Iteration 13738, Loss: 0.054171763360500336\n",
      "Iteration 13739, Loss: 0.05399005860090256\n",
      "Iteration 13740, Loss: 0.05417180061340332\n",
      "Iteration 13741, Loss: 0.05399009585380554\n",
      "Iteration 13742, Loss: 0.054171912372112274\n",
      "Iteration 13743, Loss: 0.05398986488580704\n",
      "Iteration 13744, Loss: 0.05417203530669212\n",
      "Iteration 13745, Loss: 0.053989749401807785\n",
      "Iteration 13746, Loss: 0.05417222902178764\n",
      "Iteration 13747, Loss: 0.05398955196142197\n",
      "Iteration 13748, Loss: 0.05417218804359436\n",
      "Iteration 13749, Loss: 0.05398967117071152\n",
      "Iteration 13750, Loss: 0.05417218804359436\n",
      "Iteration 13751, Loss: 0.053989630192518234\n",
      "Iteration 13752, Loss: 0.05417210981249809\n",
      "Iteration 13753, Loss: 0.0539897084236145\n",
      "Iteration 13754, Loss: 0.054172031581401825\n",
      "Iteration 13755, Loss: 0.053989823907613754\n",
      "Iteration 13756, Loss: 0.054171912372112274\n",
      "Iteration 13757, Loss: 0.053990017622709274\n",
      "Iteration 13758, Loss: 0.05417187511920929\n",
      "Iteration 13759, Loss: 0.05398997664451599\n",
      "Iteration 13760, Loss: 0.05417191982269287\n",
      "Iteration 13761, Loss: 0.05398997291922569\n",
      "Iteration 13762, Loss: 0.05417222902178764\n",
      "Iteration 13763, Loss: 0.05398959666490555\n",
      "Iteration 13764, Loss: 0.05417235940694809\n",
      "Iteration 13765, Loss: 0.053989436477422714\n",
      "Iteration 13766, Loss: 0.054172586649656296\n",
      "Iteration 13767, Loss: 0.05398920178413391\n",
      "Iteration 13768, Loss: 0.05417270585894585\n",
      "Iteration 13769, Loss: 0.05398915708065033\n",
      "Iteration 13770, Loss: 0.054172661155462265\n",
      "Iteration 13771, Loss: 0.05398931726813316\n",
      "Iteration 13772, Loss: 0.054172467440366745\n",
      "Iteration 13773, Loss: 0.05398935824632645\n",
      "Iteration 13774, Loss: 0.05417242273688316\n",
      "Iteration 13775, Loss: 0.05398944020271301\n",
      "Iteration 13776, Loss: 0.05417222902178764\n",
      "Iteration 13777, Loss: 0.05398964136838913\n",
      "Iteration 13778, Loss: 0.05417206883430481\n",
      "Iteration 13779, Loss: 0.053989753127098083\n",
      "Iteration 13780, Loss: 0.054171960800886154\n",
      "Iteration 13781, Loss: 0.05398990958929062\n",
      "Iteration 13782, Loss: 0.05417191982269287\n",
      "Iteration 13783, Loss: 0.0539897084236145\n",
      "Iteration 13784, Loss: 0.05417210981249809\n",
      "Iteration 13785, Loss: 0.05398979038000107\n",
      "Iteration 13786, Loss: 0.05417200177907944\n",
      "Iteration 13787, Loss: 0.05398963391780853\n",
      "Iteration 13788, Loss: 0.05417218804359436\n",
      "Iteration 13789, Loss: 0.05398958921432495\n",
      "Iteration 13790, Loss: 0.05417218804359436\n",
      "Iteration 13791, Loss: 0.05398952215909958\n",
      "Iteration 13792, Loss: 0.05417206883430481\n",
      "Iteration 13793, Loss: 0.0539897195994854\n",
      "Iteration 13794, Loss: 0.05417191982269287\n",
      "Iteration 13795, Loss: 0.05398979038000107\n",
      "Iteration 13796, Loss: 0.054171960800886154\n",
      "Iteration 13797, Loss: 0.05398982763290405\n",
      "Iteration 13798, Loss: 0.05417203530669212\n",
      "Iteration 13799, Loss: 0.05398959666490555\n",
      "Iteration 13800, Loss: 0.054172080010175705\n",
      "Iteration 13801, Loss: 0.05398952215909958\n",
      "Iteration 13802, Loss: 0.054172225296497345\n",
      "Iteration 13803, Loss: 0.053989674896001816\n",
      "Iteration 13804, Loss: 0.05417199432849884\n",
      "Iteration 13805, Loss: 0.05398979038000107\n",
      "Iteration 13806, Loss: 0.05417206510901451\n",
      "Iteration 13807, Loss: 0.053989872336387634\n",
      "Iteration 13808, Loss: 0.05417202413082123\n",
      "Iteration 13809, Loss: 0.053989797830581665\n",
      "Iteration 13810, Loss: 0.05417211353778839\n",
      "Iteration 13811, Loss: 0.053989630192518234\n",
      "Iteration 13812, Loss: 0.05417224019765854\n",
      "Iteration 13813, Loss: 0.0539894700050354\n",
      "Iteration 13814, Loss: 0.05417250841856003\n",
      "Iteration 13815, Loss: 0.05398935824632645\n",
      "Iteration 13816, Loss: 0.05417262762784958\n",
      "Iteration 13817, Loss: 0.0539892315864563\n",
      "Iteration 13818, Loss: 0.05417255684733391\n",
      "Iteration 13819, Loss: 0.053989529609680176\n",
      "Iteration 13820, Loss: 0.05417250841856003\n",
      "Iteration 13821, Loss: 0.05398980900645256\n",
      "Iteration 13822, Loss: 0.054171591997146606\n",
      "Iteration 13823, Loss: 0.05399034917354584\n",
      "Iteration 13824, Loss: 0.05417127534747124\n",
      "Iteration 13825, Loss: 0.053990475833415985\n",
      "Iteration 13826, Loss: 0.05417131632566452\n",
      "Iteration 13827, Loss: 0.05399058014154434\n",
      "Iteration 13828, Loss: 0.054171591997146606\n",
      "Iteration 13829, Loss: 0.053990304470062256\n",
      "Iteration 13830, Loss: 0.05417187139391899\n",
      "Iteration 13831, Loss: 0.05398990958929062\n",
      "Iteration 13832, Loss: 0.05417222902178764\n",
      "Iteration 13833, Loss: 0.05398959666490555\n",
      "Iteration 13834, Loss: 0.05417250841856003\n",
      "Iteration 13835, Loss: 0.05398928374052048\n",
      "Iteration 13836, Loss: 0.05417254567146301\n",
      "Iteration 13837, Loss: 0.05398940294981003\n",
      "Iteration 13838, Loss: 0.05417242646217346\n",
      "Iteration 13839, Loss: 0.053989481180906296\n",
      "Iteration 13840, Loss: 0.05417222902178764\n",
      "Iteration 13841, Loss: 0.05398964136838913\n",
      "Iteration 13842, Loss: 0.05417200177907944\n",
      "Iteration 13843, Loss: 0.05398982763290405\n",
      "Iteration 13844, Loss: 0.05417203530669212\n",
      "Iteration 13845, Loss: 0.053989797830581665\n",
      "Iteration 13846, Loss: 0.054171882569789886\n",
      "Iteration 13847, Loss: 0.053989868611097336\n",
      "Iteration 13848, Loss: 0.054172031581401825\n",
      "Iteration 13849, Loss: 0.0539899542927742\n",
      "Iteration 13850, Loss: 0.054171960800886154\n",
      "Iteration 13851, Loss: 0.053989678621292114\n",
      "Iteration 13852, Loss: 0.054172199219465256\n",
      "Iteration 13853, Loss: 0.05398944020271301\n",
      "Iteration 13854, Loss: 0.05417242646217346\n",
      "Iteration 13855, Loss: 0.05398940294981003\n",
      "Iteration 13856, Loss: 0.05417250096797943\n",
      "Iteration 13857, Loss: 0.05398928374052048\n",
      "Iteration 13858, Loss: 0.054172467440366745\n",
      "Iteration 13859, Loss: 0.05398940294981003\n",
      "Iteration 13860, Loss: 0.05417230352759361\n",
      "Iteration 13861, Loss: 0.05398960039019585\n",
      "Iteration 13862, Loss: 0.054172106087207794\n",
      "Iteration 13863, Loss: 0.05398983880877495\n",
      "Iteration 13864, Loss: 0.05417202413082123\n",
      "Iteration 13865, Loss: 0.0539899542927742\n",
      "Iteration 13866, Loss: 0.054171882569789886\n",
      "Iteration 13867, Loss: 0.05398991331458092\n",
      "Iteration 13868, Loss: 0.05417191982269287\n",
      "Iteration 13869, Loss: 0.05398983880877495\n",
      "Iteration 13870, Loss: 0.05417203903198242\n",
      "Iteration 13871, Loss: 0.05398982763290405\n",
      "Iteration 13872, Loss: 0.05417212098836899\n",
      "Iteration 13873, Loss: 0.053989674896001816\n",
      "Iteration 13874, Loss: 0.054172269999980927\n",
      "Iteration 13875, Loss: 0.05398944020271301\n",
      "Iteration 13876, Loss: 0.05417230725288391\n",
      "Iteration 13877, Loss: 0.05398952215909958\n",
      "Iteration 13878, Loss: 0.054172150790691376\n",
      "Iteration 13879, Loss: 0.05398982763290405\n",
      "Iteration 13880, Loss: 0.05417194962501526\n",
      "Iteration 13881, Loss: 0.0539899542927742\n",
      "Iteration 13882, Loss: 0.05417187139391899\n",
      "Iteration 13883, Loss: 0.05398998782038689\n",
      "Iteration 13884, Loss: 0.05417194962501526\n",
      "Iteration 13885, Loss: 0.05398991331458092\n",
      "Iteration 13886, Loss: 0.05417191982269287\n",
      "Iteration 13887, Loss: 0.053989872336387634\n",
      "Iteration 13888, Loss: 0.054172150790691376\n",
      "Iteration 13889, Loss: 0.05398960039019585\n",
      "Iteration 13890, Loss: 0.05417230725288391\n",
      "Iteration 13891, Loss: 0.053989559412002563\n",
      "Iteration 13892, Loss: 0.05417230725288391\n",
      "Iteration 13893, Loss: 0.053989481180906296\n",
      "Iteration 13894, Loss: 0.05417230725288391\n",
      "Iteration 13895, Loss: 0.05398944020271301\n",
      "Iteration 13896, Loss: 0.05417210981249809\n",
      "Iteration 13897, Loss: 0.0539897195994854\n",
      "Iteration 13898, Loss: 0.05417194962501526\n",
      "Iteration 13899, Loss: 0.05399002879858017\n",
      "Iteration 13900, Loss: 0.05417163670063019\n",
      "Iteration 13901, Loss: 0.05399015173316002\n",
      "Iteration 13902, Loss: 0.05417143926024437\n",
      "Iteration 13903, Loss: 0.053990304470062256\n",
      "Iteration 13904, Loss: 0.054171591997146606\n",
      "Iteration 13905, Loss: 0.05399014800786972\n",
      "Iteration 13906, Loss: 0.05417172238230705\n",
      "Iteration 13907, Loss: 0.0539899580180645\n",
      "Iteration 13908, Loss: 0.054171960800886154\n",
      "Iteration 13909, Loss: 0.05398982763290405\n",
      "Iteration 13910, Loss: 0.05417203903198242\n",
      "Iteration 13911, Loss: 0.053989559412002563\n",
      "Iteration 13912, Loss: 0.05417218804359436\n",
      "Iteration 13913, Loss: 0.05398963391780853\n",
      "Iteration 13914, Loss: 0.05417218804359436\n",
      "Iteration 13915, Loss: 0.053989749401807785\n",
      "Iteration 13916, Loss: 0.054172150790691376\n",
      "Iteration 13917, Loss: 0.053989868611097336\n",
      "Iteration 13918, Loss: 0.054172031581401825\n",
      "Iteration 13919, Loss: 0.05398976057767868\n",
      "Iteration 13920, Loss: 0.054171960800886154\n",
      "Iteration 13921, Loss: 0.053989872336387634\n",
      "Iteration 13922, Loss: 0.05417191609740257\n",
      "Iteration 13923, Loss: 0.053989946842193604\n",
      "Iteration 13924, Loss: 0.05417172238230705\n",
      "Iteration 13925, Loss: 0.053989946842193604\n",
      "Iteration 13926, Loss: 0.054171763360500336\n",
      "Iteration 13927, Loss: 0.05399002879858017\n",
      "Iteration 13928, Loss: 0.05417175218462944\n",
      "Iteration 13929, Loss: 0.05399007350206375\n",
      "Iteration 13930, Loss: 0.05417171120643616\n",
      "Iteration 13931, Loss: 0.05399007722735405\n",
      "Iteration 13932, Loss: 0.05417163670063019\n",
      "Iteration 13933, Loss: 0.05399014800786972\n",
      "Iteration 13934, Loss: 0.05417163670063019\n",
      "Iteration 13935, Loss: 0.05399002879858017\n",
      "Iteration 13936, Loss: 0.054171763360500336\n",
      "Iteration 13937, Loss: 0.053990066051483154\n",
      "Iteration 13938, Loss: 0.05417206883430481\n",
      "Iteration 13939, Loss: 0.053989678621292114\n",
      "Iteration 13940, Loss: 0.05417227745056152\n",
      "Iteration 13941, Loss: 0.05398955196142197\n",
      "Iteration 13942, Loss: 0.05417238920927048\n",
      "Iteration 13943, Loss: 0.053989361971616745\n",
      "Iteration 13944, Loss: 0.054172348231077194\n",
      "Iteration 13945, Loss: 0.05398944020271301\n",
      "Iteration 13946, Loss: 0.05417222902178764\n",
      "Iteration 13947, Loss: 0.05398964136838913\n",
      "Iteration 13948, Loss: 0.054172076284885406\n",
      "Iteration 13949, Loss: 0.053989797830581665\n",
      "Iteration 13950, Loss: 0.05417179316282272\n",
      "Iteration 13951, Loss: 0.05399003624916077\n",
      "Iteration 13952, Loss: 0.05417155846953392\n",
      "Iteration 13953, Loss: 0.05399034172296524\n",
      "Iteration 13954, Loss: 0.054171524941921234\n",
      "Iteration 13955, Loss: 0.05399026721715927\n",
      "Iteration 13956, Loss: 0.05417148768901825\n",
      "Iteration 13957, Loss: 0.053990185260772705\n",
      "Iteration 13958, Loss: 0.05417172238230705\n",
      "Iteration 13959, Loss: 0.05399003252387047\n",
      "Iteration 13960, Loss: 0.054171882569789886\n",
      "Iteration 13961, Loss: 0.05398979038000107\n",
      "Iteration 13962, Loss: 0.05417206883430481\n",
      "Iteration 13963, Loss: 0.053989678621292114\n",
      "Iteration 13964, Loss: 0.05417210981249809\n",
      "Iteration 13965, Loss: 0.053989674896001816\n",
      "Iteration 13966, Loss: 0.05417206883430481\n",
      "Iteration 13967, Loss: 0.053989749401807785\n",
      "Iteration 13968, Loss: 0.054172031581401825\n",
      "Iteration 13969, Loss: 0.053989723324775696\n",
      "Iteration 13970, Loss: 0.054171979427337646\n",
      "Iteration 13971, Loss: 0.053990066051483154\n",
      "Iteration 13972, Loss: 0.054171718657016754\n",
      "Iteration 13973, Loss: 0.05399018153548241\n",
      "Iteration 13974, Loss: 0.05417175590991974\n",
      "Iteration 13975, Loss: 0.05399010702967644\n",
      "Iteration 13976, Loss: 0.05417172238230705\n",
      "Iteration 13977, Loss: 0.05398983508348465\n",
      "Iteration 13978, Loss: 0.05417199432849884\n",
      "Iteration 13979, Loss: 0.05398982763290405\n",
      "Iteration 13980, Loss: 0.05417191982269287\n",
      "Iteration 13981, Loss: 0.053989868611097336\n",
      "Iteration 13982, Loss: 0.05417199432849884\n",
      "Iteration 13983, Loss: 0.0539897195994854\n",
      "Iteration 13984, Loss: 0.05417191982269287\n",
      "Iteration 13985, Loss: 0.05398964136838913\n",
      "Iteration 13986, Loss: 0.054171960800886154\n",
      "Iteration 13987, Loss: 0.05398961156606674\n",
      "Iteration 13988, Loss: 0.054171960800886154\n",
      "Iteration 13989, Loss: 0.05398968607187271\n",
      "Iteration 13990, Loss: 0.054172031581401825\n",
      "Iteration 13991, Loss: 0.05398968979716301\n",
      "Iteration 13992, Loss: 0.054171882569789886\n",
      "Iteration 13993, Loss: 0.0539897195994854\n",
      "Iteration 13994, Loss: 0.054172080010175705\n",
      "Iteration 13995, Loss: 0.05398960039019585\n",
      "Iteration 13996, Loss: 0.0541720911860466\n",
      "Iteration 13997, Loss: 0.05398952215909958\n",
      "Iteration 13998, Loss: 0.05417218804359436\n",
      "Iteration 13999, Loss: 0.053989410400390625\n",
      "Iteration 14000, Loss: 0.05417211353778839\n",
      "Iteration 14001, Loss: 0.05398961156606674\n",
      "Iteration 14002, Loss: 0.054172031581401825\n",
      "Iteration 14003, Loss: 0.05398987978696823\n",
      "Iteration 14004, Loss: 0.05417180806398392\n",
      "Iteration 14005, Loss: 0.053989917039871216\n",
      "Iteration 14006, Loss: 0.0541718415915966\n",
      "Iteration 14007, Loss: 0.05398976057767868\n",
      "Iteration 14008, Loss: 0.05417200177907944\n",
      "Iteration 14009, Loss: 0.05398979410529137\n",
      "Iteration 14010, Loss: 0.05417212098836899\n",
      "Iteration 14011, Loss: 0.05398959666490555\n",
      "Iteration 14012, Loss: 0.05417227745056152\n",
      "Iteration 14013, Loss: 0.053989291191101074\n",
      "Iteration 14014, Loss: 0.05417251214385033\n",
      "Iteration 14015, Loss: 0.05398920923471451\n",
      "Iteration 14016, Loss: 0.05417243763804436\n",
      "Iteration 14017, Loss: 0.05398928374052048\n",
      "Iteration 14018, Loss: 0.05417238920927048\n",
      "Iteration 14019, Loss: 0.053989410400390625\n",
      "Iteration 14020, Loss: 0.054172150790691376\n",
      "Iteration 14021, Loss: 0.053989529609680176\n",
      "Iteration 14022, Loss: 0.054171882569789886\n",
      "Iteration 14023, Loss: 0.05398988351225853\n",
      "Iteration 14024, Loss: 0.054171644151210785\n",
      "Iteration 14025, Loss: 0.05399015173316002\n",
      "Iteration 14026, Loss: 0.0541716143488884\n",
      "Iteration 14027, Loss: 0.05399007350206375\n",
      "Iteration 14028, Loss: 0.054171882569789886\n",
      "Iteration 14029, Loss: 0.05398979410529137\n",
      "Iteration 14030, Loss: 0.05417212098836899\n",
      "Iteration 14031, Loss: 0.05398960039019585\n",
      "Iteration 14032, Loss: 0.054172154515981674\n",
      "Iteration 14033, Loss: 0.053989559412002563\n",
      "Iteration 14034, Loss: 0.05417212098836899\n",
      "Iteration 14035, Loss: 0.053989481180906296\n",
      "Iteration 14036, Loss: 0.05417219549417496\n",
      "Iteration 14037, Loss: 0.053989559412002563\n",
      "Iteration 14038, Loss: 0.054172076284885406\n",
      "Iteration 14039, Loss: 0.05398964136838913\n",
      "Iteration 14040, Loss: 0.054171960800886154\n",
      "Iteration 14041, Loss: 0.053989678621292114\n",
      "Iteration 14042, Loss: 0.05417203530669212\n",
      "Iteration 14043, Loss: 0.0539897195994854\n",
      "Iteration 14044, Loss: 0.05417212098836899\n",
      "Iteration 14045, Loss: 0.05398960039019585\n",
      "Iteration 14046, Loss: 0.05417219549417496\n",
      "Iteration 14047, Loss: 0.05398952215909958\n",
      "Iteration 14048, Loss: 0.054172199219465256\n",
      "Iteration 14049, Loss: 0.05398940294981003\n",
      "Iteration 14050, Loss: 0.05417222902178764\n",
      "Iteration 14051, Loss: 0.05398933216929436\n",
      "Iteration 14052, Loss: 0.05417222902178764\n",
      "Iteration 14053, Loss: 0.05398964136838913\n",
      "Iteration 14054, Loss: 0.05417199060320854\n",
      "Iteration 14055, Loss: 0.05398976430296898\n",
      "Iteration 14056, Loss: 0.05417180061340332\n",
      "Iteration 14057, Loss: 0.05398999899625778\n",
      "Iteration 14058, Loss: 0.05417175218462944\n",
      "Iteration 14059, Loss: 0.05398999899625778\n",
      "Iteration 14060, Loss: 0.05417175590991974\n",
      "Iteration 14061, Loss: 0.05398983880877495\n",
      "Iteration 14062, Loss: 0.05417180806398392\n",
      "Iteration 14063, Loss: 0.05398976057767868\n",
      "Iteration 14064, Loss: 0.05417203903198242\n",
      "Iteration 14065, Loss: 0.05398960039019585\n",
      "Iteration 14066, Loss: 0.054172009229660034\n",
      "Iteration 14067, Loss: 0.05398944765329361\n",
      "Iteration 14068, Loss: 0.05417223274707794\n",
      "Iteration 14069, Loss: 0.05398952215909958\n",
      "Iteration 14070, Loss: 0.05417228490114212\n",
      "Iteration 14071, Loss: 0.05398937314748764\n",
      "Iteration 14072, Loss: 0.054172392934560776\n",
      "Iteration 14073, Loss: 0.05398940294981003\n",
      "Iteration 14074, Loss: 0.05417224019765854\n",
      "Iteration 14075, Loss: 0.05398944020271301\n",
      "Iteration 14076, Loss: 0.054172348231077194\n",
      "Iteration 14077, Loss: 0.05398940667510033\n",
      "Iteration 14078, Loss: 0.05417242646217346\n",
      "Iteration 14079, Loss: 0.053989481180906296\n",
      "Iteration 14080, Loss: 0.05417215824127197\n",
      "Iteration 14081, Loss: 0.05398952215909958\n",
      "Iteration 14082, Loss: 0.05417224019765854\n",
      "Iteration 14083, Loss: 0.05398944020271301\n",
      "Iteration 14084, Loss: 0.054172348231077194\n",
      "Iteration 14085, Loss: 0.053989361971616745\n",
      "Iteration 14086, Loss: 0.05417242646217346\n",
      "Iteration 14087, Loss: 0.05398944765329361\n",
      "Iteration 14088, Loss: 0.05417222902178764\n",
      "Iteration 14089, Loss: 0.05398952588438988\n",
      "Iteration 14090, Loss: 0.05417219549417496\n",
      "Iteration 14091, Loss: 0.05398964136838913\n",
      "Iteration 14092, Loss: 0.05417204648256302\n",
      "Iteration 14093, Loss: 0.05398963391780853\n",
      "Iteration 14094, Loss: 0.054172199219465256\n",
      "Iteration 14095, Loss: 0.05398952215909958\n",
      "Iteration 14096, Loss: 0.054172348231077194\n",
      "Iteration 14097, Loss: 0.05398940294981003\n",
      "Iteration 14098, Loss: 0.05417231470346451\n",
      "Iteration 14099, Loss: 0.05398952215909958\n",
      "Iteration 14100, Loss: 0.05417238920927048\n",
      "Iteration 14101, Loss: 0.05398932844400406\n",
      "Iteration 14102, Loss: 0.054172348231077194\n",
      "Iteration 14103, Loss: 0.053989481180906296\n",
      "Iteration 14104, Loss: 0.05417230725288391\n",
      "Iteration 14105, Loss: 0.053989410400390625\n",
      "Iteration 14106, Loss: 0.05417212098836899\n",
      "Iteration 14107, Loss: 0.05398960039019585\n",
      "Iteration 14108, Loss: 0.05417203530669212\n",
      "Iteration 14109, Loss: 0.05398968979716301\n",
      "Iteration 14110, Loss: 0.054171837866306305\n",
      "Iteration 14111, Loss: 0.05398987978696823\n",
      "Iteration 14112, Loss: 0.054171763360500336\n",
      "Iteration 14113, Loss: 0.05398937314748764\n",
      "Iteration 14114, Loss: 0.05417191982269287\n",
      "Iteration 14115, Loss: 0.05398920923471451\n",
      "Iteration 14116, Loss: 0.05417134612798691\n",
      "Iteration 14117, Loss: 0.053988900035619736\n",
      "Iteration 14118, Loss: 0.05417158454656601\n",
      "Iteration 14119, Loss: 0.053988777101039886\n",
      "Iteration 14120, Loss: 0.05417189002037048\n",
      "Iteration 14121, Loss: 0.053988464176654816\n",
      "Iteration 14122, Loss: 0.0541718527674675\n",
      "Iteration 14123, Loss: 0.053988587111234665\n",
      "Iteration 14124, Loss: 0.05417165160179138\n",
      "Iteration 14125, Loss: 0.05398866534233093\n",
      "Iteration 14126, Loss: 0.05417141318321228\n",
      "Iteration 14127, Loss: 0.053988978266716\n",
      "Iteration 14128, Loss: 0.05417121574282646\n",
      "Iteration 14129, Loss: 0.053989142179489136\n",
      "Iteration 14130, Loss: 0.05417109653353691\n",
      "Iteration 14131, Loss: 0.0539892241358757\n",
      "Iteration 14132, Loss: 0.05417102575302124\n",
      "Iteration 14133, Loss: 0.053989216685295105\n",
      "Iteration 14134, Loss: 0.05417133495211601\n",
      "Iteration 14135, Loss: 0.0539889857172966\n",
      "Iteration 14136, Loss: 0.05417153984308243\n",
      "Iteration 14137, Loss: 0.05398866534233093\n",
      "Iteration 14138, Loss: 0.05417178198695183\n",
      "Iteration 14139, Loss: 0.0539885014295578\n",
      "Iteration 14140, Loss: 0.05417189002037048\n",
      "Iteration 14141, Loss: 0.05398842692375183\n",
      "Iteration 14142, Loss: 0.054171811789274216\n",
      "Iteration 14143, Loss: 0.05398854613304138\n",
      "Iteration 14144, Loss: 0.05417164787650108\n",
      "Iteration 14145, Loss: 0.05398886650800705\n",
      "Iteration 14146, Loss: 0.054171331226825714\n",
      "Iteration 14147, Loss: 0.053989212960004807\n",
      "Iteration 14148, Loss: 0.05417105555534363\n",
      "Iteration 14149, Loss: 0.05398937314748764\n",
      "Iteration 14150, Loss: 0.05417097732424736\n",
      "Iteration 14151, Loss: 0.05398930236697197\n",
      "Iteration 14152, Loss: 0.05417102575302124\n",
      "Iteration 14153, Loss: 0.05398910492658615\n",
      "Iteration 14154, Loss: 0.05417126417160034\n",
      "Iteration 14155, Loss: 0.05398905277252197\n",
      "Iteration 14156, Loss: 0.05417153239250183\n",
      "Iteration 14157, Loss: 0.05398878455162048\n",
      "Iteration 14158, Loss: 0.05417165905237198\n",
      "Iteration 14159, Loss: 0.05398862808942795\n",
      "Iteration 14160, Loss: 0.0541716143488884\n",
      "Iteration 14161, Loss: 0.053988516330718994\n",
      "Iteration 14162, Loss: 0.054171767085790634\n",
      "Iteration 14163, Loss: 0.05398866534233093\n",
      "Iteration 14164, Loss: 0.054171763360500336\n",
      "Iteration 14165, Loss: 0.05398881435394287\n",
      "Iteration 14166, Loss: 0.054171450436115265\n",
      "Iteration 14167, Loss: 0.05398905277252197\n",
      "Iteration 14168, Loss: 0.0541713684797287\n",
      "Iteration 14169, Loss: 0.053989097476005554\n",
      "Iteration 14170, Loss: 0.05417122691869736\n",
      "Iteration 14171, Loss: 0.053989093750715256\n",
      "Iteration 14172, Loss: 0.05417122691869736\n",
      "Iteration 14173, Loss: 0.0539889857172966\n",
      "Iteration 14174, Loss: 0.054171375930309296\n",
      "Iteration 14175, Loss: 0.05398894473910332\n",
      "Iteration 14176, Loss: 0.05417141318321228\n",
      "Iteration 14177, Loss: 0.053988903760910034\n",
      "Iteration 14178, Loss: 0.054171379655599594\n",
      "Iteration 14179, Loss: 0.05398891121149063\n",
      "Iteration 14180, Loss: 0.054171375930309296\n",
      "Iteration 14181, Loss: 0.05398905277252197\n",
      "Iteration 14182, Loss: 0.05417140945792198\n",
      "Iteration 14183, Loss: 0.05398894101381302\n",
      "Iteration 14184, Loss: 0.054171495139598846\n",
      "Iteration 14185, Loss: 0.05398882180452347\n",
      "Iteration 14186, Loss: 0.05417173355817795\n",
      "Iteration 14187, Loss: 0.053988661617040634\n",
      "Iteration 14188, Loss: 0.05417173355817795\n",
      "Iteration 14189, Loss: 0.053988587111234665\n",
      "Iteration 14190, Loss: 0.054171692579984665\n",
      "Iteration 14191, Loss: 0.053988706320524216\n",
      "Iteration 14192, Loss: 0.05417172238230705\n",
      "Iteration 14193, Loss: 0.05398885905742645\n",
      "Iteration 14194, Loss: 0.054171375930309296\n",
      "Iteration 14195, Loss: 0.05398905277252197\n",
      "Iteration 14196, Loss: 0.05417140573263168\n",
      "Iteration 14197, Loss: 0.05398906394839287\n",
      "Iteration 14198, Loss: 0.05417114496231079\n",
      "Iteration 14199, Loss: 0.053989097476005554\n",
      "Iteration 14200, Loss: 0.05417122691869736\n",
      "Iteration 14201, Loss: 0.053988978266716\n",
      "Iteration 14202, Loss: 0.054171543568372726\n",
      "Iteration 14203, Loss: 0.05398893356323242\n",
      "Iteration 14204, Loss: 0.05417166277766228\n",
      "Iteration 14205, Loss: 0.05398862063884735\n",
      "Iteration 14206, Loss: 0.05417189002037048\n",
      "Iteration 14207, Loss: 0.05398831516504288\n",
      "Iteration 14208, Loss: 0.05417205020785332\n",
      "Iteration 14209, Loss: 0.05398842319846153\n",
      "Iteration 14210, Loss: 0.05417189002037048\n",
      "Iteration 14211, Loss: 0.0539885088801384\n",
      "Iteration 14212, Loss: 0.05417177081108093\n",
      "Iteration 14213, Loss: 0.05398855358362198\n",
      "Iteration 14214, Loss: 0.05417164787650108\n",
      "Iteration 14215, Loss: 0.053988900035619736\n",
      "Iteration 14216, Loss: 0.05417141318321228\n",
      "Iteration 14217, Loss: 0.0539889857172966\n",
      "Iteration 14218, Loss: 0.05417133495211601\n",
      "Iteration 14219, Loss: 0.05398906394839287\n",
      "Iteration 14220, Loss: 0.05417126417160034\n",
      "Iteration 14221, Loss: 0.053989019244909286\n",
      "Iteration 14222, Loss: 0.054171573370695114\n",
      "Iteration 14223, Loss: 0.05398889631032944\n",
      "Iteration 14224, Loss: 0.054171618074178696\n",
      "Iteration 14225, Loss: 0.053988706320524216\n",
      "Iteration 14226, Loss: 0.054171741008758545\n",
      "Iteration 14227, Loss: 0.053988583385944366\n",
      "Iteration 14228, Loss: 0.05417189002037048\n",
      "Iteration 14229, Loss: 0.05398870259523392\n",
      "Iteration 14230, Loss: 0.054171573370695114\n",
      "Iteration 14231, Loss: 0.05398882180452347\n",
      "Iteration 14232, Loss: 0.05417141318321228\n",
      "Iteration 14233, Loss: 0.05398894473910332\n",
      "Iteration 14234, Loss: 0.05417121574282646\n",
      "Iteration 14235, Loss: 0.05398906394839287\n",
      "Iteration 14236, Loss: 0.05417132377624512\n",
      "Iteration 14237, Loss: 0.05398910492658615\n",
      "Iteration 14238, Loss: 0.05417121574282646\n",
      "Iteration 14239, Loss: 0.053989097476005554\n",
      "Iteration 14240, Loss: 0.05417121574282646\n",
      "Iteration 14241, Loss: 0.0539889857172966\n",
      "Iteration 14242, Loss: 0.05417141318321228\n",
      "Iteration 14243, Loss: 0.0539889857172966\n",
      "Iteration 14244, Loss: 0.05417141318321228\n",
      "Iteration 14245, Loss: 0.053989019244909286\n",
      "Iteration 14246, Loss: 0.05417133495211601\n",
      "Iteration 14247, Loss: 0.053988974541425705\n",
      "Iteration 14248, Loss: 0.05417152866721153\n",
      "Iteration 14249, Loss: 0.05398893356323242\n",
      "Iteration 14250, Loss: 0.05417156219482422\n",
      "Iteration 14251, Loss: 0.05398889631032944\n",
      "Iteration 14252, Loss: 0.054171495139598846\n",
      "Iteration 14253, Loss: 0.053988855332136154\n",
      "Iteration 14254, Loss: 0.054171573370695114\n",
      "Iteration 14255, Loss: 0.05398889631032944\n",
      "Iteration 14256, Loss: 0.054171573370695114\n",
      "Iteration 14257, Loss: 0.05398889631032944\n",
      "Iteration 14258, Loss: 0.054171498864889145\n",
      "Iteration 14259, Loss: 0.05398881435394287\n",
      "Iteration 14260, Loss: 0.0541716143488884\n",
      "Iteration 14261, Loss: 0.05398885905742645\n",
      "Iteration 14262, Loss: 0.0541716143488884\n",
      "Iteration 14263, Loss: 0.0539887472987175\n",
      "Iteration 14264, Loss: 0.05417153984308243\n",
      "Iteration 14265, Loss: 0.05398893356323242\n",
      "Iteration 14266, Loss: 0.054171495139598846\n",
      "Iteration 14267, Loss: 0.05398894101381302\n",
      "Iteration 14268, Loss: 0.05417145416140556\n",
      "Iteration 14269, Loss: 0.05398893356323242\n",
      "Iteration 14270, Loss: 0.05417153239250183\n",
      "Iteration 14271, Loss: 0.053988855332136154\n",
      "Iteration 14272, Loss: 0.0541716068983078\n",
      "Iteration 14273, Loss: 0.05398885905742645\n",
      "Iteration 14274, Loss: 0.054171573370695114\n",
      "Iteration 14275, Loss: 0.05398882180452347\n",
      "Iteration 14276, Loss: 0.054171692579984665\n",
      "Iteration 14277, Loss: 0.0539887361228466\n",
      "Iteration 14278, Loss: 0.05417173355817795\n",
      "Iteration 14279, Loss: 0.05398869514465332\n",
      "Iteration 14280, Loss: 0.05417173355817795\n",
      "Iteration 14281, Loss: 0.05398881435394287\n",
      "Iteration 14282, Loss: 0.0541716143488884\n",
      "Iteration 14283, Loss: 0.05398866534233093\n",
      "Iteration 14284, Loss: 0.05417145416140556\n",
      "Iteration 14285, Loss: 0.05398893356323242\n",
      "Iteration 14286, Loss: 0.05417141318321228\n",
      "Iteration 14287, Loss: 0.05398917198181152\n",
      "Iteration 14288, Loss: 0.05417129397392273\n",
      "Iteration 14289, Loss: 0.05398928374052048\n",
      "Iteration 14290, Loss: 0.05417122691869736\n",
      "Iteration 14291, Loss: 0.053989164531230927\n",
      "Iteration 14292, Loss: 0.05417146533727646\n",
      "Iteration 14293, Loss: 0.053988974541425705\n",
      "Iteration 14294, Loss: 0.054171621799468994\n",
      "Iteration 14295, Loss: 0.053988587111234665\n",
      "Iteration 14296, Loss: 0.05417177081108093\n",
      "Iteration 14297, Loss: 0.05398847162723541\n",
      "Iteration 14298, Loss: 0.05417177081108093\n",
      "Iteration 14299, Loss: 0.05398862808942795\n",
      "Iteration 14300, Loss: 0.05417180061340332\n",
      "Iteration 14301, Loss: 0.05398881435394287\n",
      "Iteration 14302, Loss: 0.05417164787650108\n",
      "Iteration 14303, Loss: 0.05398893356323242\n",
      "Iteration 14304, Loss: 0.054171495139598846\n",
      "Iteration 14305, Loss: 0.053989049047231674\n",
      "Iteration 14306, Loss: 0.054171524941921234\n",
      "Iteration 14307, Loss: 0.05398901551961899\n",
      "Iteration 14308, Loss: 0.05417141318321228\n",
      "Iteration 14309, Loss: 0.053989093750715256\n",
      "Iteration 14310, Loss: 0.05417142063379288\n",
      "Iteration 14311, Loss: 0.053988970816135406\n",
      "Iteration 14312, Loss: 0.054171543568372726\n",
      "Iteration 14313, Loss: 0.053988777101039886\n",
      "Iteration 14314, Loss: 0.05417169630527496\n",
      "Iteration 14315, Loss: 0.053988657891750336\n",
      "Iteration 14316, Loss: 0.05417177081108093\n",
      "Iteration 14317, Loss: 0.05398881435394287\n",
      "Iteration 14318, Loss: 0.05417153239250183\n",
      "Iteration 14319, Loss: 0.05398893356323242\n",
      "Iteration 14320, Loss: 0.054171375930309296\n",
      "Iteration 14321, Loss: 0.05398920923471451\n",
      "Iteration 14322, Loss: 0.054171256721019745\n",
      "Iteration 14323, Loss: 0.05398913472890854\n",
      "Iteration 14324, Loss: 0.05417129397392273\n",
      "Iteration 14325, Loss: 0.05398913472890854\n",
      "Iteration 14326, Loss: 0.05417133495211601\n",
      "Iteration 14327, Loss: 0.053988974541425705\n",
      "Iteration 14328, Loss: 0.05417146533727646\n",
      "Iteration 14329, Loss: 0.05398892983794212\n",
      "Iteration 14330, Loss: 0.0541716143488884\n",
      "Iteration 14331, Loss: 0.05398881435394287\n",
      "Iteration 14332, Loss: 0.054171692579984665\n",
      "Iteration 14333, Loss: 0.0539887472987175\n",
      "Iteration 14334, Loss: 0.054171573370695114\n",
      "Iteration 14335, Loss: 0.05398878455162048\n",
      "Iteration 14336, Loss: 0.05417129397392273\n",
      "Iteration 14337, Loss: 0.0539889857172966\n",
      "Iteration 14338, Loss: 0.054171204566955566\n",
      "Iteration 14339, Loss: 0.05398929864168167\n",
      "Iteration 14340, Loss: 0.05417086184024811\n",
      "Iteration 14341, Loss: 0.053989410400390625\n",
      "Iteration 14342, Loss: 0.05417089909315109\n",
      "Iteration 14343, Loss: 0.05398956686258316\n",
      "Iteration 14344, Loss: 0.05417105555534363\n",
      "Iteration 14345, Loss: 0.053989291191101074\n",
      "Iteration 14346, Loss: 0.05417114496231079\n",
      "Iteration 14347, Loss: 0.05398905277252197\n",
      "Iteration 14348, Loss: 0.054171424359083176\n",
      "Iteration 14349, Loss: 0.053988777101039886\n",
      "Iteration 14350, Loss: 0.05417165905237198\n",
      "Iteration 14351, Loss: 0.05398854613304138\n",
      "Iteration 14352, Loss: 0.05417180806398392\n",
      "Iteration 14353, Loss: 0.05398869514465332\n",
      "Iteration 14354, Loss: 0.054171692579984665\n",
      "Iteration 14355, Loss: 0.053988780826330185\n",
      "Iteration 14356, Loss: 0.05417157709598541\n",
      "Iteration 14357, Loss: 0.053988855332136154\n",
      "Iteration 14358, Loss: 0.054171524941921234\n",
      "Iteration 14359, Loss: 0.05398913472890854\n",
      "Iteration 14360, Loss: 0.054171256721019745\n",
      "Iteration 14361, Loss: 0.053989093750715256\n",
      "Iteration 14362, Loss: 0.05417129397392273\n",
      "Iteration 14363, Loss: 0.053989097476005554\n",
      "Iteration 14364, Loss: 0.05417126417160034\n",
      "Iteration 14365, Loss: 0.053989164531230927\n",
      "Iteration 14366, Loss: 0.054171424359083176\n",
      "Iteration 14367, Loss: 0.0539887398481369\n",
      "Iteration 14368, Loss: 0.054171573370695114\n",
      "Iteration 14369, Loss: 0.0539887361228466\n",
      "Iteration 14370, Loss: 0.054171692579984665\n",
      "Iteration 14371, Loss: 0.053988780826330185\n",
      "Iteration 14372, Loss: 0.05417153239250183\n",
      "Iteration 14373, Loss: 0.053988855332136154\n",
      "Iteration 14374, Loss: 0.05417153239250183\n",
      "Iteration 14375, Loss: 0.053988974541425705\n",
      "Iteration 14376, Loss: 0.054171495139598846\n",
      "Iteration 14377, Loss: 0.053988903760910034\n",
      "Iteration 14378, Loss: 0.054171498864889145\n",
      "Iteration 14379, Loss: 0.05398882180452347\n",
      "Iteration 14380, Loss: 0.05417153239250183\n",
      "Iteration 14381, Loss: 0.05398878455162048\n",
      "Iteration 14382, Loss: 0.054171573370695114\n",
      "Iteration 14383, Loss: 0.053988777101039886\n",
      "Iteration 14384, Loss: 0.05417165160179138\n",
      "Iteration 14385, Loss: 0.053988855332136154\n",
      "Iteration 14386, Loss: 0.0541716068983078\n",
      "Iteration 14387, Loss: 0.05398889631032944\n",
      "Iteration 14388, Loss: 0.05417145416140556\n",
      "Iteration 14389, Loss: 0.053989019244909286\n",
      "Iteration 14390, Loss: 0.05417132377624512\n",
      "Iteration 14391, Loss: 0.053989212960004807\n",
      "Iteration 14392, Loss: 0.05417109653353691\n",
      "Iteration 14393, Loss: 0.05398925393819809\n",
      "Iteration 14394, Loss: 0.054171137511730194\n",
      "Iteration 14395, Loss: 0.05398917198181152\n",
      "Iteration 14396, Loss: 0.054171256721019745\n",
      "Iteration 14397, Loss: 0.05398905277252197\n",
      "Iteration 14398, Loss: 0.05417146533727646\n",
      "Iteration 14399, Loss: 0.05398900434374809\n",
      "Iteration 14400, Loss: 0.05417166277766228\n",
      "Iteration 14401, Loss: 0.05398854613304138\n",
      "Iteration 14402, Loss: 0.054171811789274216\n",
      "Iteration 14403, Loss: 0.05398861691355705\n",
      "Iteration 14404, Loss: 0.054171811789274216\n",
      "Iteration 14405, Loss: 0.05398862808942795\n",
      "Iteration 14406, Loss: 0.05417180061340332\n",
      "Iteration 14407, Loss: 0.05398881435394287\n",
      "Iteration 14408, Loss: 0.05417165160179138\n",
      "Iteration 14409, Loss: 0.05398893356323242\n",
      "Iteration 14410, Loss: 0.0541716031730175\n",
      "Iteration 14411, Loss: 0.053988903760910034\n",
      "Iteration 14412, Loss: 0.054171495139598846\n",
      "Iteration 14413, Loss: 0.053988900035619736\n",
      "Iteration 14414, Loss: 0.054171495139598846\n",
      "Iteration 14415, Loss: 0.05398878455162048\n",
      "Iteration 14416, Loss: 0.054171692579984665\n",
      "Iteration 14417, Loss: 0.0539887398481369\n",
      "Iteration 14418, Loss: 0.054171811789274216\n",
      "Iteration 14419, Loss: 0.053988661617040634\n",
      "Iteration 14420, Loss: 0.05417173355817795\n",
      "Iteration 14421, Loss: 0.05398869514465332\n",
      "Iteration 14422, Loss: 0.05417169630527496\n",
      "Iteration 14423, Loss: 0.05398870259523392\n",
      "Iteration 14424, Loss: 0.054171692579984665\n",
      "Iteration 14425, Loss: 0.053988855332136154\n",
      "Iteration 14426, Loss: 0.05417133495211601\n",
      "Iteration 14427, Loss: 0.05398901551961899\n",
      "Iteration 14428, Loss: 0.05417129397392273\n",
      "Iteration 14429, Loss: 0.05398917943239212\n",
      "Iteration 14430, Loss: 0.05417121574282646\n",
      "Iteration 14431, Loss: 0.053989142179489136\n",
      "Iteration 14432, Loss: 0.05417117476463318\n",
      "Iteration 14433, Loss: 0.053989291191101074\n",
      "Iteration 14434, Loss: 0.05417126044631004\n",
      "Iteration 14435, Loss: 0.05398905277252197\n",
      "Iteration 14436, Loss: 0.05417145416140556\n",
      "Iteration 14437, Loss: 0.05398881435394287\n",
      "Iteration 14438, Loss: 0.05417165160179138\n",
      "Iteration 14439, Loss: 0.0539887398481369\n",
      "Iteration 14440, Loss: 0.054171692579984665\n",
      "Iteration 14441, Loss: 0.05398878455162048\n",
      "Iteration 14442, Loss: 0.05417157709598541\n",
      "Iteration 14443, Loss: 0.0539887398481369\n",
      "Iteration 14444, Loss: 0.05417172610759735\n",
      "Iteration 14445, Loss: 0.053988777101039886\n",
      "Iteration 14446, Loss: 0.0541716143488884\n",
      "Iteration 14447, Loss: 0.05398893356323242\n",
      "Iteration 14448, Loss: 0.05417134612798691\n",
      "Iteration 14449, Loss: 0.05398905277252197\n",
      "Iteration 14450, Loss: 0.05417129397392273\n",
      "Iteration 14451, Loss: 0.05398906022310257\n",
      "Iteration 14452, Loss: 0.05417129397392273\n",
      "Iteration 14453, Loss: 0.053988441824913025\n",
      "Iteration 14454, Loss: 0.05417142063379288\n",
      "Iteration 14455, Loss: 0.05398831516504288\n",
      "Iteration 14456, Loss: 0.05417146533727646\n",
      "Iteration 14457, Loss: 0.05398815870285034\n",
      "Iteration 14458, Loss: 0.054170992225408554\n",
      "Iteration 14459, Loss: 0.053988080471754074\n",
      "Iteration 14460, Loss: 0.054170917719602585\n",
      "Iteration 14461, Loss: 0.053988080471754074\n",
      "Iteration 14462, Loss: 0.054170917719602585\n",
      "Iteration 14463, Loss: 0.053988080471754074\n",
      "Iteration 14464, Loss: 0.054171036928892136\n",
      "Iteration 14465, Loss: 0.05398803949356079\n",
      "Iteration 14466, Loss: 0.054171111434698105\n",
      "Iteration 14467, Loss: 0.053988080471754074\n",
      "Iteration 14468, Loss: 0.054171036928892136\n",
      "Iteration 14469, Loss: 0.05398812144994736\n",
      "Iteration 14470, Loss: 0.054170962423086166\n",
      "Iteration 14471, Loss: 0.053988080471754074\n",
      "Iteration 14472, Loss: 0.054171036928892136\n",
      "Iteration 14473, Loss: 0.05398803949356079\n",
      "Iteration 14474, Loss: 0.054171107709407806\n",
      "Iteration 14475, Loss: 0.05398809164762497\n",
      "Iteration 14476, Loss: 0.0541708767414093\n",
      "Iteration 14477, Loss: 0.05398827791213989\n",
      "Iteration 14478, Loss: 0.05417067930102348\n",
      "Iteration 14479, Loss: 0.053988441824913025\n",
      "Iteration 14480, Loss: 0.054170556366443634\n",
      "Iteration 14481, Loss: 0.05398859828710556\n",
      "Iteration 14482, Loss: 0.054170481860637665\n",
      "Iteration 14483, Loss: 0.05398867651820183\n",
      "Iteration 14484, Loss: 0.05417044460773468\n",
      "Iteration 14485, Loss: 0.053988635540008545\n",
      "Iteration 14486, Loss: 0.0541706383228302\n",
      "Iteration 14487, Loss: 0.05398843437433243\n",
      "Iteration 14488, Loss: 0.054170917719602585\n",
      "Iteration 14489, Loss: 0.053988199681043625\n",
      "Iteration 14490, Loss: 0.0541711151599884\n",
      "Iteration 14491, Loss: 0.053987812250852585\n",
      "Iteration 14492, Loss: 0.05417126417160034\n",
      "Iteration 14493, Loss: 0.05398792028427124\n",
      "Iteration 14494, Loss: 0.05417106673121452\n",
      "Iteration 14495, Loss: 0.053988080471754074\n",
      "Iteration 14496, Loss: 0.05417090654373169\n",
      "Iteration 14497, Loss: 0.053988318890333176\n",
      "Iteration 14498, Loss: 0.05417066812515259\n",
      "Iteration 14499, Loss: 0.053988516330718994\n",
      "Iteration 14500, Loss: 0.05417056009173393\n",
      "Iteration 14501, Loss: 0.05398859083652496\n",
      "Iteration 14502, Loss: 0.0541706383228302\n",
      "Iteration 14503, Loss: 0.05398839712142944\n",
      "Iteration 14504, Loss: 0.05417083948850632\n",
      "Iteration 14505, Loss: 0.05398824065923691\n",
      "Iteration 14506, Loss: 0.054171036928892136\n",
      "Iteration 14507, Loss: 0.05398815497756004\n",
      "Iteration 14508, Loss: 0.054171234369277954\n",
      "Iteration 14509, Loss: 0.05398784205317497\n",
      "Iteration 14510, Loss: 0.05417127534747124\n",
      "Iteration 14511, Loss: 0.05398785322904587\n",
      "Iteration 14512, Loss: 0.05417107790708542\n",
      "Iteration 14513, Loss: 0.05398789048194885\n",
      "Iteration 14514, Loss: 0.05417099595069885\n",
      "Iteration 14515, Loss: 0.05398809164762497\n",
      "Iteration 14516, Loss: 0.05417075753211975\n",
      "Iteration 14517, Loss: 0.05398836359381676\n",
      "Iteration 14518, Loss: 0.05417051538825035\n",
      "Iteration 14519, Loss: 0.05398859828710556\n",
      "Iteration 14520, Loss: 0.054170481860637665\n",
      "Iteration 14521, Loss: 0.05398871749639511\n",
      "Iteration 14522, Loss: 0.05417056009173393\n",
      "Iteration 14523, Loss: 0.05398839712142944\n",
      "Iteration 14524, Loss: 0.054170720279216766\n",
      "Iteration 14525, Loss: 0.05398821085691452\n",
      "Iteration 14526, Loss: 0.05417083203792572\n",
      "Iteration 14527, Loss: 0.05398824065923691\n",
      "Iteration 14528, Loss: 0.05417099595069885\n",
      "Iteration 14529, Loss: 0.053988050669431686\n",
      "Iteration 14530, Loss: 0.05417107790708542\n",
      "Iteration 14531, Loss: 0.05398797243833542\n",
      "Iteration 14532, Loss: 0.05417099595069885\n",
      "Iteration 14533, Loss: 0.05398796498775482\n",
      "Iteration 14534, Loss: 0.05417099595069885\n",
      "Iteration 14535, Loss: 0.05398815870285034\n",
      "Iteration 14536, Loss: 0.05417076498270035\n",
      "Iteration 14537, Loss: 0.053988393396139145\n",
      "Iteration 14538, Loss: 0.054170794785022736\n",
      "Iteration 14539, Loss: 0.053988248109817505\n",
      "Iteration 14540, Loss: 0.05417071282863617\n",
      "Iteration 14541, Loss: 0.05398839712142944\n",
      "Iteration 14542, Loss: 0.05417075753211975\n",
      "Iteration 14543, Loss: 0.05398820340633392\n",
      "Iteration 14544, Loss: 0.05417083948850632\n",
      "Iteration 14545, Loss: 0.05398824065923691\n",
      "Iteration 14546, Loss: 0.054170750081539154\n",
      "Iteration 14547, Loss: 0.053988318890333176\n",
      "Iteration 14548, Loss: 0.0541706383228302\n",
      "Iteration 14549, Loss: 0.05398840457201004\n",
      "Iteration 14550, Loss: 0.0541706308722496\n",
      "Iteration 14551, Loss: 0.053988516330718994\n",
      "Iteration 14552, Loss: 0.054170556366443634\n",
      "Iteration 14553, Loss: 0.05398859828710556\n",
      "Iteration 14554, Loss: 0.05417056009173393\n",
      "Iteration 14555, Loss: 0.05398839712142944\n",
      "Iteration 14556, Loss: 0.05417083948850632\n",
      "Iteration 14557, Loss: 0.053988270461559296\n",
      "Iteration 14558, Loss: 0.0541708841919899\n",
      "Iteration 14559, Loss: 0.05398815870285034\n",
      "Iteration 14560, Loss: 0.05417100340127945\n",
      "Iteration 14561, Loss: 0.053988002240657806\n",
      "Iteration 14562, Loss: 0.054171156138181686\n",
      "Iteration 14563, Loss: 0.053988002240657806\n",
      "Iteration 14564, Loss: 0.054171111434698105\n",
      "Iteration 14565, Loss: 0.05398803949356079\n",
      "Iteration 14566, Loss: 0.05417099595069885\n",
      "Iteration 14567, Loss: 0.053988050669431686\n",
      "Iteration 14568, Loss: 0.0541708767414093\n",
      "Iteration 14569, Loss: 0.05398827791213989\n",
      "Iteration 14570, Loss: 0.05417067930102348\n",
      "Iteration 14571, Loss: 0.05398847907781601\n",
      "Iteration 14572, Loss: 0.0541706308722496\n",
      "Iteration 14573, Loss: 0.05398859828710556\n",
      "Iteration 14574, Loss: 0.05417051911354065\n",
      "Iteration 14575, Loss: 0.05398840829730034\n",
      "Iteration 14576, Loss: 0.05417075753211975\n",
      "Iteration 14577, Loss: 0.05398824065923691\n",
      "Iteration 14578, Loss: 0.05417099595069885\n",
      "Iteration 14579, Loss: 0.05398808419704437\n",
      "Iteration 14580, Loss: 0.05417107045650482\n",
      "Iteration 14581, Loss: 0.0539880096912384\n",
      "Iteration 14582, Loss: 0.054171107709407806\n",
      "Iteration 14583, Loss: 0.053988128900527954\n",
      "Iteration 14584, Loss: 0.05417090654373169\n",
      "Iteration 14585, Loss: 0.05398827791213989\n",
      "Iteration 14586, Loss: 0.054170798510313034\n",
      "Iteration 14587, Loss: 0.053988393396139145\n",
      "Iteration 14588, Loss: 0.054170720279216766\n",
      "Iteration 14589, Loss: 0.053988393396139145\n",
      "Iteration 14590, Loss: 0.05417067930102348\n",
      "Iteration 14591, Loss: 0.053988467901945114\n",
      "Iteration 14592, Loss: 0.05417067930102348\n",
      "Iteration 14593, Loss: 0.05398854613304138\n",
      "Iteration 14594, Loss: 0.054170601069927216\n",
      "Iteration 14595, Loss: 0.05398844927549362\n",
      "Iteration 14596, Loss: 0.05417069047689438\n",
      "Iteration 14597, Loss: 0.053988322615623474\n",
      "Iteration 14598, Loss: 0.054170917719602585\n",
      "Iteration 14599, Loss: 0.05398808419704437\n",
      "Iteration 14600, Loss: 0.054171036928892136\n",
      "Iteration 14601, Loss: 0.053988002240657806\n",
      "Iteration 14602, Loss: 0.05417099595069885\n",
      "Iteration 14603, Loss: 0.05398812144994736\n",
      "Iteration 14604, Loss: 0.054170913994312286\n",
      "Iteration 14605, Loss: 0.05398827791213989\n",
      "Iteration 14606, Loss: 0.05417078360915184\n",
      "Iteration 14607, Loss: 0.05398839712142944\n",
      "Iteration 14608, Loss: 0.05417066812515259\n",
      "Iteration 14609, Loss: 0.053988587111234665\n",
      "Iteration 14610, Loss: 0.0541706308722496\n",
      "Iteration 14611, Loss: 0.05398859828710556\n",
      "Iteration 14612, Loss: 0.0541706308722496\n",
      "Iteration 14613, Loss: 0.05398855358362198\n",
      "Iteration 14614, Loss: 0.054170530289411545\n",
      "Iteration 14615, Loss: 0.05398839712142944\n",
      "Iteration 14616, Loss: 0.05417067930102348\n",
      "Iteration 14617, Loss: 0.05398839712142944\n",
      "Iteration 14618, Loss: 0.054170794785022736\n",
      "Iteration 14619, Loss: 0.05398835986852646\n",
      "Iteration 14620, Loss: 0.05417075753211975\n",
      "Iteration 14621, Loss: 0.05398835986852646\n",
      "Iteration 14622, Loss: 0.05417082831263542\n",
      "Iteration 14623, Loss: 0.053988318890333176\n",
      "Iteration 14624, Loss: 0.054170798510313034\n",
      "Iteration 14625, Loss: 0.05398815870285034\n",
      "Iteration 14626, Loss: 0.054170917719602585\n",
      "Iteration 14627, Loss: 0.05398808419704437\n",
      "Iteration 14628, Loss: 0.05417095124721527\n",
      "Iteration 14629, Loss: 0.05398815870285034\n",
      "Iteration 14630, Loss: 0.0541708767414093\n",
      "Iteration 14631, Loss: 0.05398815870285034\n",
      "Iteration 14632, Loss: 0.05417095124721527\n",
      "Iteration 14633, Loss: 0.053988199681043625\n",
      "Iteration 14634, Loss: 0.05417090654373169\n",
      "Iteration 14635, Loss: 0.05398827791213989\n",
      "Iteration 14636, Loss: 0.054170750081539154\n",
      "Iteration 14637, Loss: 0.05398847907781601\n",
      "Iteration 14638, Loss: 0.05417054891586304\n",
      "Iteration 14639, Loss: 0.05398860201239586\n",
      "Iteration 14640, Loss: 0.05417043715715408\n",
      "Iteration 14641, Loss: 0.0539887472987175\n",
      "Iteration 14642, Loss: 0.05417048558592796\n",
      "Iteration 14643, Loss: 0.05398835986852646\n",
      "Iteration 14644, Loss: 0.05417068302631378\n",
      "Iteration 14645, Loss: 0.05398831516504288\n",
      "Iteration 14646, Loss: 0.05417080223560333\n",
      "Iteration 14647, Loss: 0.05398823320865631\n",
      "Iteration 14648, Loss: 0.054170917719602585\n",
      "Iteration 14649, Loss: 0.053988199681043625\n",
      "Iteration 14650, Loss: 0.054170988500118256\n",
      "Iteration 14651, Loss: 0.053988199681043625\n",
      "Iteration 14652, Loss: 0.054170869290828705\n",
      "Iteration 14653, Loss: 0.05398827791213989\n",
      "Iteration 14654, Loss: 0.05417082831263542\n",
      "Iteration 14655, Loss: 0.05398827791213989\n",
      "Iteration 14656, Loss: 0.05417067930102348\n",
      "Iteration 14657, Loss: 0.05398833006620407\n",
      "Iteration 14658, Loss: 0.054170720279216766\n",
      "Iteration 14659, Loss: 0.05398828908801079\n",
      "Iteration 14660, Loss: 0.0541706383228302\n",
      "Iteration 14661, Loss: 0.053988441824913025\n",
      "Iteration 14662, Loss: 0.05417082831263542\n",
      "Iteration 14663, Loss: 0.05398835986852646\n",
      "Iteration 14664, Loss: 0.054170917719602585\n",
      "Iteration 14665, Loss: 0.05398816615343094\n",
      "Iteration 14666, Loss: 0.05417106673121452\n",
      "Iteration 14667, Loss: 0.05398797243833542\n",
      "Iteration 14668, Loss: 0.05417102575302124\n",
      "Iteration 14669, Loss: 0.053988080471754074\n",
      "Iteration 14670, Loss: 0.054170992225408554\n",
      "Iteration 14671, Loss: 0.05398824065923691\n",
      "Iteration 14672, Loss: 0.054170869290828705\n",
      "Iteration 14673, Loss: 0.05398835986852646\n",
      "Iteration 14674, Loss: 0.05417071282863617\n",
      "Iteration 14675, Loss: 0.05398854613304138\n",
      "Iteration 14676, Loss: 0.0541706383228302\n",
      "Iteration 14677, Loss: 0.053988438099622726\n",
      "Iteration 14678, Loss: 0.054170601069927216\n",
      "Iteration 14679, Loss: 0.05398847907781601\n",
      "Iteration 14680, Loss: 0.054170530289411545\n",
      "Iteration 14681, Loss: 0.05398828536272049\n",
      "Iteration 14682, Loss: 0.05417075753211975\n",
      "Iteration 14683, Loss: 0.05398820340633392\n",
      "Iteration 14684, Loss: 0.05417080223560333\n",
      "Iteration 14685, Loss: 0.053988199681043625\n",
      "Iteration 14686, Loss: 0.0541708841919899\n",
      "Iteration 14687, Loss: 0.05398812144994736\n",
      "Iteration 14688, Loss: 0.05417092144489288\n",
      "Iteration 14689, Loss: 0.05398811399936676\n",
      "Iteration 14690, Loss: 0.05417106673121452\n",
      "Iteration 14691, Loss: 0.05398815870285034\n",
      "Iteration 14692, Loss: 0.05417095869779587\n",
      "Iteration 14693, Loss: 0.05398824065923691\n",
      "Iteration 14694, Loss: 0.054170869290828705\n",
      "Iteration 14695, Loss: 0.05398842692375183\n",
      "Iteration 14696, Loss: 0.054170720279216766\n",
      "Iteration 14697, Loss: 0.05398847907781601\n",
      "Iteration 14698, Loss: 0.054170601069927216\n",
      "Iteration 14699, Loss: 0.053988516330718994\n",
      "Iteration 14700, Loss: 0.05417067930102348\n",
      "Iteration 14701, Loss: 0.05398847162723541\n",
      "Iteration 14702, Loss: 0.054170798510313034\n",
      "Iteration 14703, Loss: 0.05398824065923691\n",
      "Iteration 14704, Loss: 0.054170724004507065\n",
      "Iteration 14705, Loss: 0.05398824065923691\n",
      "Iteration 14706, Loss: 0.05417083948850632\n",
      "Iteration 14707, Loss: 0.053988244384527206\n",
      "Iteration 14708, Loss: 0.05417075753211975\n",
      "Iteration 14709, Loss: 0.053988248109817505\n",
      "Iteration 14710, Loss: 0.0541706383228302\n",
      "Iteration 14711, Loss: 0.05398847907781601\n",
      "Iteration 14712, Loss: 0.054170601069927216\n",
      "Iteration 14713, Loss: 0.053988587111234665\n",
      "Iteration 14714, Loss: 0.05417048558592796\n",
      "Iteration 14715, Loss: 0.053988516330718994\n",
      "Iteration 14716, Loss: 0.05417067930102348\n",
      "Iteration 14717, Loss: 0.05398835986852646\n",
      "Iteration 14718, Loss: 0.05417083948850632\n",
      "Iteration 14719, Loss: 0.053988199681043625\n",
      "Iteration 14720, Loss: 0.054171036928892136\n",
      "Iteration 14721, Loss: 0.053988002240657806\n",
      "Iteration 14722, Loss: 0.05417106673121452\n",
      "Iteration 14723, Loss: 0.05398797243833542\n",
      "Iteration 14724, Loss: 0.054171111434698105\n",
      "Iteration 14725, Loss: 0.05398816987872124\n",
      "Iteration 14726, Loss: 0.05417083948850632\n",
      "Iteration 14727, Loss: 0.05398816615343094\n",
      "Iteration 14728, Loss: 0.0541708841919899\n",
      "Iteration 14729, Loss: 0.05398815870285034\n",
      "Iteration 14730, Loss: 0.05417083948850632\n",
      "Iteration 14731, Loss: 0.053988199681043625\n",
      "Iteration 14732, Loss: 0.054170913994312286\n",
      "Iteration 14733, Loss: 0.053988199681043625\n",
      "Iteration 14734, Loss: 0.054170869290828705\n",
      "Iteration 14735, Loss: 0.053988389670848846\n",
      "Iteration 14736, Loss: 0.05417078733444214\n",
      "Iteration 14737, Loss: 0.05398835986852646\n",
      "Iteration 14738, Loss: 0.05417071282863617\n",
      "Iteration 14739, Loss: 0.05398839712142944\n",
      "Iteration 14740, Loss: 0.05417078733444214\n",
      "Iteration 14741, Loss: 0.05398839712142944\n",
      "Iteration 14742, Loss: 0.054170750081539154\n",
      "Iteration 14743, Loss: 0.05398839712142944\n",
      "Iteration 14744, Loss: 0.054170675575733185\n",
      "Iteration 14745, Loss: 0.053988512605428696\n",
      "Iteration 14746, Loss: 0.054170869290828705\n",
      "Iteration 14747, Loss: 0.05398835986852646\n",
      "Iteration 14748, Loss: 0.0541708767414093\n",
      "Iteration 14749, Loss: 0.05398830771446228\n",
      "Iteration 14750, Loss: 0.054170913994312286\n",
      "Iteration 14751, Loss: 0.053988270461559296\n",
      "Iteration 14752, Loss: 0.054170992225408554\n",
      "Iteration 14753, Loss: 0.053988195955753326\n",
      "Iteration 14754, Loss: 0.05417099595069885\n",
      "Iteration 14755, Loss: 0.05398803949356079\n",
      "Iteration 14756, Loss: 0.05417099595069885\n",
      "Iteration 14757, Loss: 0.05398815870285034\n",
      "Iteration 14758, Loss: 0.05417090654373169\n",
      "Iteration 14759, Loss: 0.05398822948336601\n",
      "Iteration 14760, Loss: 0.054170798510313034\n",
      "Iteration 14761, Loss: 0.05398835986852646\n",
      "Iteration 14762, Loss: 0.05417078733444214\n",
      "Iteration 14763, Loss: 0.05398847907781601\n",
      "Iteration 14764, Loss: 0.05417070537805557\n",
      "Iteration 14765, Loss: 0.05398855730891228\n",
      "Iteration 14766, Loss: 0.05417051538825035\n",
      "Iteration 14767, Loss: 0.05398855730891228\n",
      "Iteration 14768, Loss: 0.05417051538825035\n",
      "Iteration 14769, Loss: 0.053988516330718994\n",
      "Iteration 14770, Loss: 0.05417048558592796\n",
      "Iteration 14771, Loss: 0.053988486528396606\n",
      "Iteration 14772, Loss: 0.05417071282863617\n",
      "Iteration 14773, Loss: 0.053988318890333176\n",
      "Iteration 14774, Loss: 0.05417083948850632\n",
      "Iteration 14775, Loss: 0.05398815870285034\n",
      "Iteration 14776, Loss: 0.054171107709407806\n",
      "Iteration 14777, Loss: 0.05398812144994736\n",
      "Iteration 14778, Loss: 0.054171107709407806\n",
      "Iteration 14779, Loss: 0.053988195955753326\n",
      "Iteration 14780, Loss: 0.05417099595069885\n",
      "Iteration 14781, Loss: 0.05398815870285034\n",
      "Iteration 14782, Loss: 0.05417094752192497\n",
      "Iteration 14783, Loss: 0.05398834869265556\n",
      "Iteration 14784, Loss: 0.054170750081539154\n",
      "Iteration 14785, Loss: 0.05398842692375183\n",
      "Iteration 14786, Loss: 0.0541706383228302\n",
      "Iteration 14787, Loss: 0.05398835986852646\n",
      "Iteration 14788, Loss: 0.0541706383228302\n",
      "Iteration 14789, Loss: 0.05398835986852646\n",
      "Iteration 14790, Loss: 0.05417083948850632\n",
      "Iteration 14791, Loss: 0.05398816615343094\n",
      "Iteration 14792, Loss: 0.05417107790708542\n",
      "Iteration 14793, Loss: 0.053988002240657806\n",
      "Iteration 14794, Loss: 0.05417118966579437\n",
      "Iteration 14795, Loss: 0.05398796126246452\n",
      "Iteration 14796, Loss: 0.05417106673121452\n",
      "Iteration 14797, Loss: 0.053988050669431686\n",
      "Iteration 14798, Loss: 0.05417078733444214\n",
      "Iteration 14799, Loss: 0.05398833006620407\n",
      "Iteration 14800, Loss: 0.054170429706573486\n",
      "Iteration 14801, Loss: 0.053988825529813766\n",
      "Iteration 14802, Loss: 0.05417028069496155\n",
      "Iteration 14803, Loss: 0.05398894473910332\n",
      "Iteration 14804, Loss: 0.0541703999042511\n",
      "Iteration 14805, Loss: 0.053988710045814514\n",
      "Iteration 14806, Loss: 0.05417056009173393\n",
      "Iteration 14807, Loss: 0.0539885088801384\n",
      "Iteration 14808, Loss: 0.054170846939086914\n",
      "Iteration 14809, Loss: 0.05398823320865631\n",
      "Iteration 14810, Loss: 0.05417108163237572\n",
      "Iteration 14811, Loss: 0.05398835986852646\n",
      "Iteration 14812, Loss: 0.05417115241289139\n",
      "Iteration 14813, Loss: 0.05398847907781601\n",
      "Iteration 14814, Loss: 0.05417155474424362\n",
      "Iteration 14815, Loss: 0.05398855730891228\n",
      "Iteration 14816, Loss: 0.05417139455676079\n",
      "Iteration 14817, Loss: 0.05398879572749138\n",
      "Iteration 14818, Loss: 0.05417119711637497\n",
      "Iteration 14819, Loss: 0.05398879572749138\n",
      "Iteration 14820, Loss: 0.054171156138181686\n",
      "Iteration 14821, Loss: 0.05398891493678093\n",
      "Iteration 14822, Loss: 0.054171234369277954\n",
      "Iteration 14823, Loss: 0.053988903760910034\n",
      "Iteration 14824, Loss: 0.054171353578567505\n",
      "Iteration 14825, Loss: 0.053988710045814514\n",
      "Iteration 14826, Loss: 0.05417143553495407\n",
      "Iteration 14827, Loss: 0.05398859828710556\n",
      "Iteration 14828, Loss: 0.054171543568372726\n",
      "Iteration 14829, Loss: 0.053988516330718994\n",
      "Iteration 14830, Loss: 0.05417155474424362\n",
      "Iteration 14831, Loss: 0.05398847907781601\n",
      "Iteration 14832, Loss: 0.054171543568372726\n",
      "Iteration 14833, Loss: 0.053988516330718994\n",
      "Iteration 14834, Loss: 0.054171472787857056\n",
      "Iteration 14835, Loss: 0.05398871749639511\n",
      "Iteration 14836, Loss: 0.05417131632566452\n",
      "Iteration 14837, Loss: 0.05398879200220108\n",
      "Iteration 14838, Loss: 0.05417107790708542\n",
      "Iteration 14839, Loss: 0.0539889857172966\n",
      "Iteration 14840, Loss: 0.054171036928892136\n",
      "Iteration 14841, Loss: 0.05398903042078018\n",
      "Iteration 14842, Loss: 0.054171036928892136\n",
      "Iteration 14843, Loss: 0.053988873958587646\n",
      "Iteration 14844, Loss: 0.0541711151599884\n",
      "Iteration 14845, Loss: 0.053988829255104065\n",
      "Iteration 14846, Loss: 0.05417120084166527\n",
      "Iteration 14847, Loss: 0.05398871749639511\n",
      "Iteration 14848, Loss: 0.05417127534747124\n",
      "Iteration 14849, Loss: 0.05398871749639511\n",
      "Iteration 14850, Loss: 0.05417130887508392\n",
      "Iteration 14851, Loss: 0.05398871749639511\n",
      "Iteration 14852, Loss: 0.054171156138181686\n",
      "Iteration 14853, Loss: 0.053988903760910034\n",
      "Iteration 14854, Loss: 0.05417126417160034\n",
      "Iteration 14855, Loss: 0.053988825529813766\n",
      "Iteration 14856, Loss: 0.05417122691869736\n",
      "Iteration 14857, Loss: 0.05398883670568466\n",
      "Iteration 14858, Loss: 0.054171234369277954\n",
      "Iteration 14859, Loss: 0.053988903760910034\n",
      "Iteration 14860, Loss: 0.05417127534747124\n",
      "Iteration 14861, Loss: 0.05398871749639511\n",
      "Iteration 14862, Loss: 0.05417131632566452\n",
      "Iteration 14863, Loss: 0.05398855730891228\n",
      "Iteration 14864, Loss: 0.054171353578567505\n",
      "Iteration 14865, Loss: 0.053988561034202576\n",
      "Iteration 14866, Loss: 0.054171353578567505\n",
      "Iteration 14867, Loss: 0.05398867279291153\n",
      "Iteration 14868, Loss: 0.05417127534747124\n",
      "Iteration 14869, Loss: 0.05398859828710556\n",
      "Iteration 14870, Loss: 0.05417118966579437\n",
      "Iteration 14871, Loss: 0.05398883670568466\n",
      "Iteration 14872, Loss: 0.05417107045650482\n",
      "Iteration 14873, Loss: 0.053989000618457794\n",
      "Iteration 14874, Loss: 0.0541708767414093\n",
      "Iteration 14875, Loss: 0.05398903414607048\n",
      "Iteration 14876, Loss: 0.05417103320360184\n",
      "Iteration 14877, Loss: 0.05398891493678093\n",
      "Iteration 14878, Loss: 0.054171156138181686\n",
      "Iteration 14879, Loss: 0.05398883670568466\n",
      "Iteration 14880, Loss: 0.054171234369277954\n",
      "Iteration 14881, Loss: 0.0539887510240078\n",
      "Iteration 14882, Loss: 0.054171424359083176\n",
      "Iteration 14883, Loss: 0.05398852750658989\n",
      "Iteration 14884, Loss: 0.054171424359083176\n",
      "Iteration 14885, Loss: 0.053988561034202576\n",
      "Iteration 14886, Loss: 0.05417134612798691\n",
      "Iteration 14887, Loss: 0.05398867651820183\n",
      "Iteration 14888, Loss: 0.054171234369277954\n",
      "Iteration 14889, Loss: 0.05398883670568466\n",
      "Iteration 14890, Loss: 0.0541711151599884\n",
      "Iteration 14891, Loss: 0.05398895591497421\n",
      "Iteration 14892, Loss: 0.0541711151599884\n",
      "Iteration 14893, Loss: 0.05398891121149063\n",
      "Iteration 14894, Loss: 0.054171156138181686\n",
      "Iteration 14895, Loss: 0.053988829255104065\n",
      "Iteration 14896, Loss: 0.05417127534747124\n",
      "Iteration 14897, Loss: 0.053988635540008545\n",
      "Iteration 14898, Loss: 0.05417134612798691\n",
      "Iteration 14899, Loss: 0.05398855730891228\n",
      "Iteration 14900, Loss: 0.054171230643987656\n",
      "Iteration 14901, Loss: 0.05398868769407272\n",
      "Iteration 14902, Loss: 0.054171185940504074\n",
      "Iteration 14903, Loss: 0.053988873958587646\n",
      "Iteration 14904, Loss: 0.054171036928892136\n",
      "Iteration 14905, Loss: 0.05398891493678093\n",
      "Iteration 14906, Loss: 0.05417107790708542\n",
      "Iteration 14907, Loss: 0.053988873958587646\n",
      "Iteration 14908, Loss: 0.054171234369277954\n",
      "Iteration 14909, Loss: 0.05398879572749138\n",
      "Iteration 14910, Loss: 0.05417131632566452\n",
      "Iteration 14911, Loss: 0.05398867651820183\n",
      "Iteration 14912, Loss: 0.054171472787857056\n",
      "Iteration 14913, Loss: 0.05398859828710556\n",
      "Iteration 14914, Loss: 0.05417155474424362\n",
      "Iteration 14915, Loss: 0.053988438099622726\n",
      "Iteration 14916, Loss: 0.05417151004076004\n",
      "Iteration 14917, Loss: 0.05398848280310631\n",
      "Iteration 14918, Loss: 0.05417146533727646\n",
      "Iteration 14919, Loss: 0.05398856848478317\n",
      "Iteration 14920, Loss: 0.054171234369277954\n",
      "Iteration 14921, Loss: 0.05398871749639511\n",
      "Iteration 14922, Loss: 0.05417107790708542\n",
      "Iteration 14923, Loss: 0.0539889857172966\n",
      "Iteration 14924, Loss: 0.05417115241289139\n",
      "Iteration 14925, Loss: 0.053988873958587646\n",
      "Iteration 14926, Loss: 0.05417107790708542\n",
      "Iteration 14927, Loss: 0.053988873958587646\n",
      "Iteration 14928, Loss: 0.05417127162218094\n",
      "Iteration 14929, Loss: 0.05398879572749138\n",
      "Iteration 14930, Loss: 0.05417127534747124\n",
      "Iteration 14931, Loss: 0.05398860201239586\n",
      "Iteration 14932, Loss: 0.05417143553495407\n",
      "Iteration 14933, Loss: 0.05398855730891228\n",
      "Iteration 14934, Loss: 0.054171472787857056\n",
      "Iteration 14935, Loss: 0.05398859828710556\n",
      "Iteration 14936, Loss: 0.054171543568372726\n",
      "Iteration 14937, Loss: 0.05398855730891228\n",
      "Iteration 14938, Loss: 0.05417146533727646\n",
      "Iteration 14939, Loss: 0.053988635540008545\n",
      "Iteration 14940, Loss: 0.05417146533727646\n",
      "Iteration 14941, Loss: 0.053988706320524216\n",
      "Iteration 14942, Loss: 0.05417138338088989\n",
      "Iteration 14943, Loss: 0.05398867651820183\n",
      "Iteration 14944, Loss: 0.05417127534747124\n",
      "Iteration 14945, Loss: 0.053988873958587646\n",
      "Iteration 14946, Loss: 0.0541711151599884\n",
      "Iteration 14947, Loss: 0.0539889931678772\n",
      "Iteration 14948, Loss: 0.054171159863471985\n",
      "Iteration 14949, Loss: 0.05398883670568466\n",
      "Iteration 14950, Loss: 0.05417127534747124\n",
      "Iteration 14951, Loss: 0.053988561034202576\n",
      "Iteration 14952, Loss: 0.05417127534747124\n",
      "Iteration 14953, Loss: 0.05398864671587944\n",
      "Iteration 14954, Loss: 0.05417127162218094\n",
      "Iteration 14955, Loss: 0.05398864671587944\n",
      "Iteration 14956, Loss: 0.05417126417160034\n",
      "Iteration 14957, Loss: 0.05398879572749138\n",
      "Iteration 14958, Loss: 0.05417122691869736\n",
      "Iteration 14959, Loss: 0.053988873958587646\n",
      "Iteration 14960, Loss: 0.05417107790708542\n",
      "Iteration 14961, Loss: 0.053988948464393616\n",
      "Iteration 14962, Loss: 0.0541711151599884\n",
      "Iteration 14963, Loss: 0.05398883670568466\n",
      "Iteration 14964, Loss: 0.05417127534747124\n",
      "Iteration 14965, Loss: 0.053988635540008545\n",
      "Iteration 14966, Loss: 0.05417143553495407\n",
      "Iteration 14967, Loss: 0.05398859828710556\n",
      "Iteration 14968, Loss: 0.05417150259017944\n",
      "Iteration 14969, Loss: 0.053988635540008545\n",
      "Iteration 14970, Loss: 0.05417146906256676\n",
      "Iteration 14971, Loss: 0.053988635540008545\n",
      "Iteration 14972, Loss: 0.05417146533727646\n",
      "Iteration 14973, Loss: 0.05398860573768616\n",
      "Iteration 14974, Loss: 0.05417134612798691\n",
      "Iteration 14975, Loss: 0.05398871749639511\n",
      "Iteration 14976, Loss: 0.054171036928892136\n",
      "Iteration 14977, Loss: 0.05398891866207123\n",
      "Iteration 14978, Loss: 0.05417083948850632\n",
      "Iteration 14979, Loss: 0.05398915335536003\n",
      "Iteration 14980, Loss: 0.054170720279216766\n",
      "Iteration 14981, Loss: 0.053989194333553314\n",
      "Iteration 14982, Loss: 0.054170720279216766\n",
      "Iteration 14983, Loss: 0.05398930236697197\n",
      "Iteration 14984, Loss: 0.054170798510313034\n",
      "Iteration 14985, Loss: 0.053989194333553314\n",
      "Iteration 14986, Loss: 0.054170917719602585\n",
      "Iteration 14987, Loss: 0.05398903414607048\n",
      "Iteration 14988, Loss: 0.054170992225408554\n",
      "Iteration 14989, Loss: 0.0539889931678772\n",
      "Iteration 14990, Loss: 0.054171036928892136\n",
      "Iteration 14991, Loss: 0.05398891493678093\n",
      "Iteration 14992, Loss: 0.05417095869779587\n",
      "Iteration 14993, Loss: 0.05398888140916824\n",
      "Iteration 14994, Loss: 0.054170917719602585\n",
      "Iteration 14995, Loss: 0.05398903414607048\n",
      "Iteration 14996, Loss: 0.0541708767414093\n",
      "Iteration 14997, Loss: 0.05398914963006973\n",
      "Iteration 14998, Loss: 0.054170917719602585\n",
      "Iteration 14999, Loss: 0.05398903414607048\n",
      "Iteration 15000, Loss: 0.054171036928892136\n",
      "Iteration 15001, Loss: 0.05398891493678093\n",
      "Iteration 15002, Loss: 0.054170846939086914\n",
      "Iteration 15003, Loss: 0.05398891493678093\n",
      "Iteration 15004, Loss: 0.05417095869779587\n",
      "Iteration 15005, Loss: 0.05398895964026451\n",
      "Iteration 15006, Loss: 0.054170917719602585\n",
      "Iteration 15007, Loss: 0.05398918315768242\n",
      "Iteration 15008, Loss: 0.054170913994312286\n",
      "Iteration 15009, Loss: 0.053989142179489136\n",
      "Iteration 15010, Loss: 0.05417095869779587\n",
      "Iteration 15011, Loss: 0.05398903787136078\n",
      "Iteration 15012, Loss: 0.05417099595069885\n",
      "Iteration 15013, Loss: 0.05398903414607048\n",
      "Iteration 15014, Loss: 0.054171036928892136\n",
      "Iteration 15015, Loss: 0.05398887023329735\n",
      "Iteration 15016, Loss: 0.0541711151599884\n",
      "Iteration 15017, Loss: 0.05398883670568466\n",
      "Iteration 15018, Loss: 0.054171122610569\n",
      "Iteration 15019, Loss: 0.053988754749298096\n",
      "Iteration 15020, Loss: 0.054171156138181686\n",
      "Iteration 15021, Loss: 0.053988754749298096\n",
      "Iteration 15022, Loss: 0.05417099595069885\n",
      "Iteration 15023, Loss: 0.05398927628993988\n",
      "Iteration 15024, Loss: 0.05417094752192497\n",
      "Iteration 15025, Loss: 0.05398951470851898\n",
      "Iteration 15026, Loss: 0.054170191287994385\n",
      "Iteration 15027, Loss: 0.05398982763290405\n",
      "Iteration 15028, Loss: 0.05416996777057648\n",
      "Iteration 15029, Loss: 0.053990017622709274\n",
      "Iteration 15030, Loss: 0.054170168936252594\n",
      "Iteration 15031, Loss: 0.05398966372013092\n",
      "Iteration 15032, Loss: 0.05417048931121826\n",
      "Iteration 15033, Loss: 0.05398935079574585\n",
      "Iteration 15034, Loss: 0.054171036928892136\n",
      "Iteration 15035, Loss: 0.0539889894425869\n",
      "Iteration 15036, Loss: 0.05417116731405258\n",
      "Iteration 15037, Loss: 0.05398859828710556\n",
      "Iteration 15038, Loss: 0.05417175218462944\n",
      "Iteration 15039, Loss: 0.053988248109817505\n",
      "Iteration 15040, Loss: 0.054171591997146606\n",
      "Iteration 15041, Loss: 0.053988248109817505\n",
      "Iteration 15042, Loss: 0.05417158454656601\n",
      "Iteration 15043, Loss: 0.05398844927549362\n",
      "Iteration 15044, Loss: 0.05417127162218094\n",
      "Iteration 15045, Loss: 0.05398879945278168\n",
      "Iteration 15046, Loss: 0.054170917719602585\n",
      "Iteration 15047, Loss: 0.053989119827747345\n",
      "Iteration 15048, Loss: 0.05417082831263542\n",
      "Iteration 15049, Loss: 0.0539892315864563\n",
      "Iteration 15050, Loss: 0.0541706383228302\n",
      "Iteration 15051, Loss: 0.0539894662797451\n",
      "Iteration 15052, Loss: 0.0541706383228302\n",
      "Iteration 15053, Loss: 0.05398935079574585\n",
      "Iteration 15054, Loss: 0.05417083948850632\n",
      "Iteration 15055, Loss: 0.0539892315864563\n",
      "Iteration 15056, Loss: 0.054171156138181686\n",
      "Iteration 15057, Loss: 0.053988873958587646\n",
      "Iteration 15058, Loss: 0.05417131632566452\n",
      "Iteration 15059, Loss: 0.05398871749639511\n",
      "Iteration 15060, Loss: 0.05417131632566452\n",
      "Iteration 15061, Loss: 0.05398867651820183\n",
      "Iteration 15062, Loss: 0.054171349853277206\n",
      "Iteration 15063, Loss: 0.05398868769407272\n",
      "Iteration 15064, Loss: 0.05417127162218094\n",
      "Iteration 15065, Loss: 0.05398883670568466\n",
      "Iteration 15066, Loss: 0.0541711151599884\n",
      "Iteration 15067, Loss: 0.05398896336555481\n",
      "Iteration 15068, Loss: 0.054171036928892136\n",
      "Iteration 15069, Loss: 0.05398915335536003\n",
      "Iteration 15070, Loss: 0.054170992225408554\n",
      "Iteration 15071, Loss: 0.05398915335536003\n",
      "Iteration 15072, Loss: 0.0541711151599884\n",
      "Iteration 15073, Loss: 0.0539889931678772\n",
      "Iteration 15074, Loss: 0.054171040654182434\n",
      "Iteration 15075, Loss: 0.053988873958587646\n",
      "Iteration 15076, Loss: 0.054171156138181686\n",
      "Iteration 15077, Loss: 0.053988806903362274\n",
      "Iteration 15078, Loss: 0.05417118966579437\n",
      "Iteration 15079, Loss: 0.053988806903362274\n",
      "Iteration 15080, Loss: 0.05417134612798691\n",
      "Iteration 15081, Loss: 0.05398876219987869\n",
      "Iteration 15082, Loss: 0.05417126417160034\n",
      "Iteration 15083, Loss: 0.05398891493678093\n",
      "Iteration 15084, Loss: 0.05417126417160034\n",
      "Iteration 15085, Loss: 0.05398895591497421\n",
      "Iteration 15086, Loss: 0.054171156138181686\n",
      "Iteration 15087, Loss: 0.05398895964026451\n",
      "Iteration 15088, Loss: 0.05417099595069885\n",
      "Iteration 15089, Loss: 0.05398903042078018\n",
      "Iteration 15090, Loss: 0.0541711151599884\n",
      "Iteration 15091, Loss: 0.05398883670568466\n",
      "Iteration 15092, Loss: 0.05417131632566452\n",
      "Iteration 15093, Loss: 0.05398872122168541\n",
      "Iteration 15094, Loss: 0.054171349853277206\n",
      "Iteration 15095, Loss: 0.05398872494697571\n",
      "Iteration 15096, Loss: 0.054171230643987656\n",
      "Iteration 15097, Loss: 0.05398891493678093\n",
      "Iteration 15098, Loss: 0.05417115241289139\n",
      "Iteration 15099, Loss: 0.053988926112651825\n",
      "Iteration 15100, Loss: 0.05417099595069885\n",
      "Iteration 15101, Loss: 0.05398907512426376\n",
      "Iteration 15102, Loss: 0.05417107790708542\n",
      "Iteration 15103, Loss: 0.05398895591497421\n",
      "Iteration 15104, Loss: 0.0541711151599884\n",
      "Iteration 15105, Loss: 0.05398883670568466\n",
      "Iteration 15106, Loss: 0.05417139455676079\n",
      "Iteration 15107, Loss: 0.05398864299058914\n",
      "Iteration 15108, Loss: 0.054171398282051086\n",
      "Iteration 15109, Loss: 0.05398844927549362\n",
      "Iteration 15110, Loss: 0.054171472787857056\n",
      "Iteration 15111, Loss: 0.053988561034202576\n",
      "Iteration 15112, Loss: 0.054171353578567505\n",
      "Iteration 15113, Loss: 0.05398872122168541\n",
      "Iteration 15114, Loss: 0.05417119711637497\n",
      "Iteration 15115, Loss: 0.053988926112651825\n",
      "Iteration 15116, Loss: 0.054170917719602585\n",
      "Iteration 15117, Loss: 0.05398907884955406\n",
      "Iteration 15118, Loss: 0.054170869290828705\n",
      "Iteration 15119, Loss: 0.0539892241358757\n",
      "Iteration 15120, Loss: 0.054170917719602585\n",
      "Iteration 15121, Loss: 0.05398911237716675\n",
      "Iteration 15122, Loss: 0.05417100340127945\n",
      "Iteration 15123, Loss: 0.05398884415626526\n",
      "Iteration 15124, Loss: 0.05417119711637497\n",
      "Iteration 15125, Loss: 0.05398883670568466\n",
      "Iteration 15126, Loss: 0.0541713610291481\n",
      "Iteration 15127, Loss: 0.05398859828710556\n",
      "Iteration 15128, Loss: 0.054171472787857056\n",
      "Iteration 15129, Loss: 0.053988367319107056\n",
      "Iteration 15130, Loss: 0.05417146906256676\n",
      "Iteration 15131, Loss: 0.05398867651820183\n",
      "Iteration 15132, Loss: 0.05417131632566452\n",
      "Iteration 15133, Loss: 0.05398884043097496\n",
      "Iteration 15134, Loss: 0.05417122691869736\n",
      "Iteration 15135, Loss: 0.05398891493678093\n",
      "Iteration 15136, Loss: 0.054171156138181686\n",
      "Iteration 15137, Loss: 0.0539889931678772\n",
      "Iteration 15138, Loss: 0.0541711151599884\n",
      "Iteration 15139, Loss: 0.05398895591497421\n",
      "Iteration 15140, Loss: 0.054171234369277954\n",
      "Iteration 15141, Loss: 0.053988873958587646\n",
      "Iteration 15142, Loss: 0.054171591997146606\n",
      "Iteration 15143, Loss: 0.053988367319107056\n",
      "Iteration 15144, Loss: 0.05417182296514511\n",
      "Iteration 15145, Loss: 0.05398828536272049\n",
      "Iteration 15146, Loss: 0.05417179316282272\n",
      "Iteration 15147, Loss: 0.05398816987872124\n",
      "Iteration 15148, Loss: 0.05417167395353317\n",
      "Iteration 15149, Loss: 0.05398833006620407\n",
      "Iteration 15150, Loss: 0.054171547293663025\n",
      "Iteration 15151, Loss: 0.05398852750658989\n",
      "Iteration 15152, Loss: 0.054171305149793625\n",
      "Iteration 15153, Loss: 0.053988806903362274\n",
      "Iteration 15154, Loss: 0.0541711151599884\n",
      "Iteration 15155, Loss: 0.05398910865187645\n",
      "Iteration 15156, Loss: 0.05417099595069885\n",
      "Iteration 15157, Loss: 0.05398907512426376\n",
      "Iteration 15158, Loss: 0.054171036928892136\n",
      "Iteration 15159, Loss: 0.05398895591497421\n",
      "Iteration 15160, Loss: 0.0541711151599884\n",
      "Iteration 15161, Loss: 0.05398891493678093\n",
      "Iteration 15162, Loss: 0.05417119711637497\n",
      "Iteration 15163, Loss: 0.05398883670568466\n",
      "Iteration 15164, Loss: 0.054171353578567505\n",
      "Iteration 15165, Loss: 0.05398864671587944\n",
      "Iteration 15166, Loss: 0.054171472787857056\n",
      "Iteration 15167, Loss: 0.05398859828710556\n",
      "Iteration 15168, Loss: 0.05417151376605034\n",
      "Iteration 15169, Loss: 0.053988486528396606\n",
      "Iteration 15170, Loss: 0.0541713647544384\n",
      "Iteration 15171, Loss: 0.053988561034202576\n",
      "Iteration 15172, Loss: 0.05417131632566452\n",
      "Iteration 15173, Loss: 0.05398864671587944\n",
      "Iteration 15174, Loss: 0.054171204566955566\n",
      "Iteration 15175, Loss: 0.05398872494697571\n",
      "Iteration 15176, Loss: 0.054171085357666016\n",
      "Iteration 15177, Loss: 0.053988732397556305\n",
      "Iteration 15178, Loss: 0.054171156138181686\n",
      "Iteration 15179, Loss: 0.053988873958587646\n",
      "Iteration 15180, Loss: 0.054171156138181686\n",
      "Iteration 15181, Loss: 0.05398879945278168\n",
      "Iteration 15182, Loss: 0.054171234369277954\n",
      "Iteration 15183, Loss: 0.05398876219987869\n",
      "Iteration 15184, Loss: 0.05417127534747124\n",
      "Iteration 15185, Loss: 0.05398865044116974\n",
      "Iteration 15186, Loss: 0.054171204566955566\n",
      "Iteration 15187, Loss: 0.05398861691355705\n",
      "Iteration 15188, Loss: 0.05417127534747124\n",
      "Iteration 15189, Loss: 0.05398872494697571\n",
      "Iteration 15190, Loss: 0.05417131632566452\n",
      "Iteration 15191, Loss: 0.053988829255104065\n",
      "Iteration 15192, Loss: 0.05417139455676079\n",
      "Iteration 15193, Loss: 0.05398856848478317\n",
      "Iteration 15194, Loss: 0.054171398282051086\n",
      "Iteration 15195, Loss: 0.05398852750658989\n",
      "Iteration 15196, Loss: 0.05417129024863243\n",
      "Iteration 15197, Loss: 0.053988486528396606\n",
      "Iteration 15198, Loss: 0.05417128652334213\n",
      "Iteration 15199, Loss: 0.05398856848478317\n",
      "Iteration 15200, Loss: 0.05417128652334213\n",
      "Iteration 15201, Loss: 0.05398860573768616\n",
      "Iteration 15202, Loss: 0.05417131632566452\n",
      "Iteration 15203, Loss: 0.05398860573768616\n",
      "Iteration 15204, Loss: 0.05417119711637497\n",
      "Iteration 15205, Loss: 0.05398876592516899\n",
      "Iteration 15206, Loss: 0.05417100712656975\n",
      "Iteration 15207, Loss: 0.05398940294981003\n",
      "Iteration 15208, Loss: 0.05417099595069885\n",
      "Iteration 15209, Loss: 0.05398944020271301\n",
      "Iteration 15210, Loss: 0.054171591997146606\n",
      "Iteration 15211, Loss: 0.05398932099342346\n",
      "Iteration 15212, Loss: 0.05417168140411377\n",
      "Iteration 15213, Loss: 0.053989242762327194\n",
      "Iteration 15214, Loss: 0.05417180061340332\n",
      "Iteration 15215, Loss: 0.053989045321941376\n",
      "Iteration 15216, Loss: 0.05417190492153168\n",
      "Iteration 15217, Loss: 0.05398908257484436\n",
      "Iteration 15218, Loss: 0.054171860218048096\n",
      "Iteration 15219, Loss: 0.05398912355303764\n",
      "Iteration 15220, Loss: 0.05417163297533989\n",
      "Iteration 15221, Loss: 0.05398928374052048\n",
      "Iteration 15222, Loss: 0.05417158827185631\n",
      "Iteration 15223, Loss: 0.05398936569690704\n",
      "Iteration 15224, Loss: 0.05417143926024437\n",
      "Iteration 15225, Loss: 0.053989555686712265\n",
      "Iteration 15226, Loss: 0.05417143926024437\n",
      "Iteration 15227, Loss: 0.05398944020271301\n",
      "Iteration 15228, Loss: 0.054171591997146606\n",
      "Iteration 15229, Loss: 0.05398932099342346\n",
      "Iteration 15230, Loss: 0.054171912372112274\n",
      "Iteration 15231, Loss: 0.05398909002542496\n",
      "Iteration 15232, Loss: 0.05417194962501526\n",
      "Iteration 15233, Loss: 0.05398889631032944\n",
      "Iteration 15234, Loss: 0.05417199060320854\n",
      "Iteration 15235, Loss: 0.05398892983794212\n",
      "Iteration 15236, Loss: 0.05417187139391899\n",
      "Iteration 15237, Loss: 0.053989045321941376\n",
      "Iteration 15238, Loss: 0.054171912372112274\n",
      "Iteration 15239, Loss: 0.05398908257484436\n",
      "Iteration 15240, Loss: 0.05417186766862869\n",
      "Iteration 15241, Loss: 0.05398912355303764\n",
      "Iteration 15242, Loss: 0.05417179316282272\n",
      "Iteration 15243, Loss: 0.053989164531230927\n",
      "Iteration 15244, Loss: 0.05417175218462944\n",
      "Iteration 15245, Loss: 0.05398927256464958\n",
      "Iteration 15246, Loss: 0.05417175218462944\n",
      "Iteration 15247, Loss: 0.053989164531230927\n",
      "Iteration 15248, Loss: 0.05417183041572571\n",
      "Iteration 15249, Loss: 0.05398919805884361\n",
      "Iteration 15250, Loss: 0.05417187139391899\n",
      "Iteration 15251, Loss: 0.05398900434374809\n",
      "Iteration 15252, Loss: 0.05417191982269287\n",
      "Iteration 15253, Loss: 0.053989045321941376\n",
      "Iteration 15254, Loss: 0.05417187139391899\n",
      "Iteration 15255, Loss: 0.05398908257484436\n",
      "Iteration 15256, Loss: 0.054171785712242126\n",
      "Iteration 15257, Loss: 0.053989164531230927\n",
      "Iteration 15258, Loss: 0.05417171120643616\n",
      "Iteration 15259, Loss: 0.053989164531230927\n",
      "Iteration 15260, Loss: 0.05417167395353317\n",
      "Iteration 15261, Loss: 0.05398932099342346\n",
      "Iteration 15262, Loss: 0.05417148023843765\n",
      "Iteration 15263, Loss: 0.053989436477422714\n",
      "Iteration 15264, Loss: 0.05417151376605034\n",
      "Iteration 15265, Loss: 0.05398944020271301\n",
      "Iteration 15266, Loss: 0.054171524941921234\n",
      "Iteration 15267, Loss: 0.05398940294981003\n",
      "Iteration 15268, Loss: 0.05417167767882347\n",
      "Iteration 15269, Loss: 0.05398928374052048\n",
      "Iteration 15270, Loss: 0.05417187139391899\n",
      "Iteration 15271, Loss: 0.05398900806903839\n",
      "Iteration 15272, Loss: 0.05417199060320854\n",
      "Iteration 15273, Loss: 0.05398908257484436\n",
      "Iteration 15274, Loss: 0.05417190492153168\n",
      "Iteration 15275, Loss: 0.05398908257484436\n",
      "Iteration 15276, Loss: 0.05417175218462944\n",
      "Iteration 15277, Loss: 0.05398927256464958\n",
      "Iteration 15278, Loss: 0.05417190492153168\n",
      "Iteration 15279, Loss: 0.05398927256464958\n",
      "Iteration 15280, Loss: 0.05417183041572571\n",
      "Iteration 15281, Loss: 0.05398912355303764\n",
      "Iteration 15282, Loss: 0.05417183041572571\n",
      "Iteration 15283, Loss: 0.05398912355303764\n",
      "Iteration 15284, Loss: 0.05417183041572571\n",
      "Iteration 15285, Loss: 0.05398912355303764\n",
      "Iteration 15286, Loss: 0.05417183041572571\n",
      "Iteration 15287, Loss: 0.053988974541425705\n",
      "Iteration 15288, Loss: 0.05417182669043541\n",
      "Iteration 15289, Loss: 0.05398912727832794\n",
      "Iteration 15290, Loss: 0.05417175218462944\n",
      "Iteration 15291, Loss: 0.053989242762327194\n",
      "Iteration 15292, Loss: 0.054171666502952576\n",
      "Iteration 15293, Loss: 0.05398944020271301\n",
      "Iteration 15294, Loss: 0.054171472787857056\n",
      "Iteration 15295, Loss: 0.05398940667510033\n",
      "Iteration 15296, Loss: 0.054171524941921234\n",
      "Iteration 15297, Loss: 0.053989432752132416\n",
      "Iteration 15298, Loss: 0.05417163297533989\n",
      "Iteration 15299, Loss: 0.05398927628993988\n",
      "Iteration 15300, Loss: 0.05417171120643616\n",
      "Iteration 15301, Loss: 0.05398908257484436\n",
      "Iteration 15302, Loss: 0.054171912372112274\n",
      "Iteration 15303, Loss: 0.053989045321941376\n",
      "Iteration 15304, Loss: 0.054171882569789886\n",
      "Iteration 15305, Loss: 0.053988970816135406\n",
      "Iteration 15306, Loss: 0.05417187139391899\n",
      "Iteration 15307, Loss: 0.05398912355303764\n",
      "Iteration 15308, Loss: 0.05417175218462944\n",
      "Iteration 15309, Loss: 0.05398920178413391\n",
      "Iteration 15310, Loss: 0.05417163297533989\n",
      "Iteration 15311, Loss: 0.05398928374052048\n",
      "Iteration 15312, Loss: 0.05417163297533989\n",
      "Iteration 15313, Loss: 0.05398928374052048\n",
      "Iteration 15314, Loss: 0.05417167395353317\n",
      "Iteration 15315, Loss: 0.05398932099342346\n",
      "Iteration 15316, Loss: 0.054171785712242126\n",
      "Iteration 15317, Loss: 0.05398920178413391\n",
      "Iteration 15318, Loss: 0.05417194589972496\n",
      "Iteration 15319, Loss: 0.05398841202259064\n",
      "Iteration 15320, Loss: 0.05417203903198242\n",
      "Iteration 15321, Loss: 0.053988099098205566\n",
      "Iteration 15322, Loss: 0.05417287349700928\n",
      "Iteration 15323, Loss: 0.05398782342672348\n",
      "Iteration 15324, Loss: 0.05417286604642868\n",
      "Iteration 15325, Loss: 0.05398798733949661\n",
      "Iteration 15326, Loss: 0.05417238920927048\n",
      "Iteration 15327, Loss: 0.05398868769407272\n",
      "Iteration 15328, Loss: 0.05417178198695183\n",
      "Iteration 15329, Loss: 0.053989242762327194\n",
      "Iteration 15330, Loss: 0.054171234369277954\n",
      "Iteration 15331, Loss: 0.05398960039019585\n",
      "Iteration 15332, Loss: 0.05417099595069885\n",
      "Iteration 15333, Loss: 0.053989868611097336\n",
      "Iteration 15334, Loss: 0.0541708879172802\n",
      "Iteration 15335, Loss: 0.05398993939161301\n",
      "Iteration 15336, Loss: 0.054171156138181686\n",
      "Iteration 15337, Loss: 0.053989630192518234\n",
      "Iteration 15338, Loss: 0.0541713647544384\n",
      "Iteration 15339, Loss: 0.053989313542842865\n",
      "Iteration 15340, Loss: 0.05417179316282272\n",
      "Iteration 15341, Loss: 0.053989119827747345\n",
      "Iteration 15342, Loss: 0.054171837866306305\n",
      "Iteration 15343, Loss: 0.05398895964026451\n",
      "Iteration 15344, Loss: 0.05417183041572571\n",
      "Iteration 15345, Loss: 0.053988926112651825\n",
      "Iteration 15346, Loss: 0.05417187511920929\n",
      "Iteration 15347, Loss: 0.05398893356323242\n",
      "Iteration 15348, Loss: 0.05417179316282272\n",
      "Iteration 15349, Loss: 0.05398909002542496\n",
      "Iteration 15350, Loss: 0.05417163670063019\n",
      "Iteration 15351, Loss: 0.053989164531230927\n",
      "Iteration 15352, Loss: 0.0541716031730175\n",
      "Iteration 15353, Loss: 0.05398912727832794\n",
      "Iteration 15354, Loss: 0.05417156219482422\n",
      "Iteration 15355, Loss: 0.05398920178413391\n",
      "Iteration 15356, Loss: 0.05417155474424362\n",
      "Iteration 15357, Loss: 0.053989242762327194\n",
      "Iteration 15358, Loss: 0.05417155474424362\n",
      "Iteration 15359, Loss: 0.05398928374052048\n",
      "Iteration 15360, Loss: 0.054171591997146606\n",
      "Iteration 15361, Loss: 0.053989432752132416\n",
      "Iteration 15362, Loss: 0.05417144298553467\n",
      "Iteration 15363, Loss: 0.05398939549922943\n",
      "Iteration 15364, Loss: 0.05417144298553467\n",
      "Iteration 15365, Loss: 0.053989242762327194\n",
      "Iteration 15366, Loss: 0.054171644151210785\n",
      "Iteration 15367, Loss: 0.05398926883935928\n",
      "Iteration 15368, Loss: 0.0541716031730175\n",
      "Iteration 15369, Loss: 0.05398915335536003\n",
      "Iteration 15370, Loss: 0.054171718657016754\n",
      "Iteration 15371, Loss: 0.053989164531230927\n",
      "Iteration 15372, Loss: 0.0541715994477272\n",
      "Iteration 15373, Loss: 0.05398912727832794\n",
      "Iteration 15374, Loss: 0.054171591997146606\n",
      "Iteration 15375, Loss: 0.053989093750715256\n",
      "Iteration 15376, Loss: 0.05417144298553467\n",
      "Iteration 15377, Loss: 0.05398928374052048\n",
      "Iteration 15378, Loss: 0.05417148023843765\n",
      "Iteration 15379, Loss: 0.053989242762327194\n",
      "Iteration 15380, Loss: 0.05417155474424362\n",
      "Iteration 15381, Loss: 0.05398935824632645\n",
      "Iteration 15382, Loss: 0.05417143553495407\n",
      "Iteration 15383, Loss: 0.053989361971616745\n",
      "Iteration 15384, Loss: 0.05417143553495407\n",
      "Iteration 15385, Loss: 0.0539894700050354\n",
      "Iteration 15386, Loss: 0.05417143926024437\n",
      "Iteration 15387, Loss: 0.05398935824632645\n",
      "Iteration 15388, Loss: 0.05417163297533989\n",
      "Iteration 15389, Loss: 0.05398935079574585\n",
      "Iteration 15390, Loss: 0.05417168140411377\n",
      "Iteration 15391, Loss: 0.053989045321941376\n",
      "Iteration 15392, Loss: 0.054171882569789886\n",
      "Iteration 15393, Loss: 0.05398895964026451\n",
      "Iteration 15394, Loss: 0.05417191982269287\n",
      "Iteration 15395, Loss: 0.053988806903362274\n",
      "Iteration 15396, Loss: 0.05417199060320854\n",
      "Iteration 15397, Loss: 0.05398896336555481\n",
      "Iteration 15398, Loss: 0.05417183041572571\n",
      "Iteration 15399, Loss: 0.05398912355303764\n",
      "Iteration 15400, Loss: 0.054171591997146606\n",
      "Iteration 15401, Loss: 0.05398928374052048\n",
      "Iteration 15402, Loss: 0.05417155474424362\n",
      "Iteration 15403, Loss: 0.053989242762327194\n",
      "Iteration 15404, Loss: 0.05417171120643616\n",
      "Iteration 15405, Loss: 0.05398920178413391\n",
      "Iteration 15406, Loss: 0.054171912372112274\n",
      "Iteration 15407, Loss: 0.05398900434374809\n",
      "Iteration 15408, Loss: 0.05417191609740257\n",
      "Iteration 15409, Loss: 0.05398888513445854\n",
      "Iteration 15410, Loss: 0.05417202413082123\n",
      "Iteration 15411, Loss: 0.053988926112651825\n",
      "Iteration 15412, Loss: 0.05417190119624138\n",
      "Iteration 15413, Loss: 0.053989045321941376\n",
      "Iteration 15414, Loss: 0.05417163297533989\n",
      "Iteration 15415, Loss: 0.053989242762327194\n",
      "Iteration 15416, Loss: 0.05417143926024437\n",
      "Iteration 15417, Loss: 0.053989242762327194\n",
      "Iteration 15418, Loss: 0.05417140573263168\n",
      "Iteration 15419, Loss: 0.05398935824632645\n",
      "Iteration 15420, Loss: 0.05417140573263168\n",
      "Iteration 15421, Loss: 0.05398931726813316\n",
      "Iteration 15422, Loss: 0.05417163297533989\n",
      "Iteration 15423, Loss: 0.05398915708065033\n",
      "Iteration 15424, Loss: 0.054171644151210785\n",
      "Iteration 15425, Loss: 0.05398915708065033\n",
      "Iteration 15426, Loss: 0.054171718657016754\n",
      "Iteration 15427, Loss: 0.05398930609226227\n",
      "Iteration 15428, Loss: 0.05417172238230705\n",
      "Iteration 15429, Loss: 0.05398912355303764\n",
      "Iteration 15430, Loss: 0.05417183041572571\n",
      "Iteration 15431, Loss: 0.05398907884955406\n",
      "Iteration 15432, Loss: 0.05417183041572571\n",
      "Iteration 15433, Loss: 0.053988970816135406\n",
      "Iteration 15434, Loss: 0.054171718657016754\n",
      "Iteration 15435, Loss: 0.05398908257484436\n",
      "Iteration 15436, Loss: 0.05417175218462944\n",
      "Iteration 15437, Loss: 0.05398920178413391\n",
      "Iteration 15438, Loss: 0.0541715994477272\n",
      "Iteration 15439, Loss: 0.053989239037036896\n",
      "Iteration 15440, Loss: 0.05417168140411377\n",
      "Iteration 15441, Loss: 0.053989164531230927\n",
      "Iteration 15442, Loss: 0.054171912372112274\n",
      "Iteration 15443, Loss: 0.05398903414607048\n",
      "Iteration 15444, Loss: 0.05417210981249809\n",
      "Iteration 15445, Loss: 0.05398888513445854\n",
      "Iteration 15446, Loss: 0.054171960800886154\n",
      "Iteration 15447, Loss: 0.053988806903362274\n",
      "Iteration 15448, Loss: 0.05417194589972496\n",
      "Iteration 15449, Loss: 0.05398896336555481\n",
      "Iteration 15450, Loss: 0.05417179688811302\n",
      "Iteration 15451, Loss: 0.053989164531230927\n",
      "Iteration 15452, Loss: 0.05417167767882347\n",
      "Iteration 15453, Loss: 0.05398908257484436\n",
      "Iteration 15454, Loss: 0.0541715994477272\n",
      "Iteration 15455, Loss: 0.05398912727832794\n",
      "Iteration 15456, Loss: 0.05417163297533989\n",
      "Iteration 15457, Loss: 0.05398920178413391\n",
      "Iteration 15458, Loss: 0.05417170748114586\n",
      "Iteration 15459, Loss: 0.05398912727832794\n",
      "Iteration 15460, Loss: 0.054171524941921234\n",
      "Iteration 15461, Loss: 0.05398917198181152\n",
      "Iteration 15462, Loss: 0.05417140573263168\n",
      "Iteration 15463, Loss: 0.05398920178413391\n",
      "Iteration 15464, Loss: 0.054171524941921234\n",
      "Iteration 15465, Loss: 0.053989164531230927\n",
      "Iteration 15466, Loss: 0.05417155846953392\n",
      "Iteration 15467, Loss: 0.05398919805884361\n",
      "Iteration 15468, Loss: 0.05417155474424362\n",
      "Iteration 15469, Loss: 0.05398928374052048\n",
      "Iteration 15470, Loss: 0.05417155474424362\n",
      "Iteration 15471, Loss: 0.05398932099342346\n",
      "Iteration 15472, Loss: 0.05417163297533989\n",
      "Iteration 15473, Loss: 0.053989242762327194\n",
      "Iteration 15474, Loss: 0.05417171120643616\n",
      "Iteration 15475, Loss: 0.053989194333553314\n",
      "Iteration 15476, Loss: 0.05417199060320854\n",
      "Iteration 15477, Loss: 0.053989000618457794\n",
      "Iteration 15478, Loss: 0.054172031581401825\n",
      "Iteration 15479, Loss: 0.05398884043097496\n",
      "Iteration 15480, Loss: 0.054171960800886154\n",
      "Iteration 15481, Loss: 0.05398884415626526\n",
      "Iteration 15482, Loss: 0.054171882569789886\n",
      "Iteration 15483, Loss: 0.05398889631032944\n",
      "Iteration 15484, Loss: 0.05417171120643616\n",
      "Iteration 15485, Loss: 0.053989049047231674\n",
      "Iteration 15486, Loss: 0.054171524941921234\n",
      "Iteration 15487, Loss: 0.053989242762327194\n",
      "Iteration 15488, Loss: 0.05417151376605034\n",
      "Iteration 15489, Loss: 0.05398935824632645\n",
      "Iteration 15490, Loss: 0.05417143553495407\n",
      "Iteration 15491, Loss: 0.05398924648761749\n",
      "Iteration 15492, Loss: 0.054171472787857056\n",
      "Iteration 15493, Loss: 0.05398944020271301\n",
      "Iteration 15494, Loss: 0.05417151376605034\n",
      "Iteration 15495, Loss: 0.053989313542842865\n",
      "Iteration 15496, Loss: 0.05417172238230705\n",
      "Iteration 15497, Loss: 0.05398915335536003\n",
      "Iteration 15498, Loss: 0.05417191609740257\n",
      "Iteration 15499, Loss: 0.053988926112651825\n",
      "Iteration 15500, Loss: 0.05417206883430481\n",
      "Iteration 15501, Loss: 0.05398888513445854\n",
      "Iteration 15502, Loss: 0.054172031581401825\n",
      "Iteration 15503, Loss: 0.05398896336555481\n",
      "Iteration 15504, Loss: 0.05417175218462944\n",
      "Iteration 15505, Loss: 0.053989045321941376\n",
      "Iteration 15506, Loss: 0.05417155474424362\n",
      "Iteration 15507, Loss: 0.053989242762327194\n",
      "Iteration 15508, Loss: 0.054171472787857056\n",
      "Iteration 15509, Loss: 0.05398939177393913\n",
      "Iteration 15510, Loss: 0.054171398282051086\n",
      "Iteration 15511, Loss: 0.05398935824632645\n",
      "Iteration 15512, Loss: 0.05417143553495407\n",
      "Iteration 15513, Loss: 0.05398940294981003\n",
      "Iteration 15514, Loss: 0.05417143926024437\n",
      "Iteration 15515, Loss: 0.053989432752132416\n",
      "Iteration 15516, Loss: 0.05417171120643616\n",
      "Iteration 15517, Loss: 0.05398927256464958\n",
      "Iteration 15518, Loss: 0.05417190492153168\n",
      "Iteration 15519, Loss: 0.05398911237716675\n",
      "Iteration 15520, Loss: 0.05417194962501526\n",
      "Iteration 15521, Loss: 0.053988926112651825\n",
      "Iteration 15522, Loss: 0.05417194962501526\n",
      "Iteration 15523, Loss: 0.05398888513445854\n",
      "Iteration 15524, Loss: 0.0541718415915966\n",
      "Iteration 15525, Loss: 0.05398900434374809\n",
      "Iteration 15526, Loss: 0.05417167395353317\n",
      "Iteration 15527, Loss: 0.05398920178413391\n",
      "Iteration 15528, Loss: 0.054171472787857056\n",
      "Iteration 15529, Loss: 0.05398940294981003\n",
      "Iteration 15530, Loss: 0.05417143553495407\n",
      "Iteration 15531, Loss: 0.05398944020271301\n",
      "Iteration 15532, Loss: 0.054171398282051086\n",
      "Iteration 15533, Loss: 0.053989477455616\n",
      "Iteration 15534, Loss: 0.0541713647544384\n",
      "Iteration 15535, Loss: 0.05398928374052048\n",
      "Iteration 15536, Loss: 0.05417178198695183\n",
      "Iteration 15537, Loss: 0.05398926883935928\n",
      "Iteration 15538, Loss: 0.05417187139391899\n",
      "Iteration 15539, Loss: 0.05398900434374809\n",
      "Iteration 15540, Loss: 0.05417199060320854\n",
      "Iteration 15541, Loss: 0.05398888140916824\n",
      "Iteration 15542, Loss: 0.05417191982269287\n",
      "Iteration 15543, Loss: 0.05398888885974884\n",
      "Iteration 15544, Loss: 0.05417194962501526\n",
      "Iteration 15545, Loss: 0.05398903787136078\n",
      "Iteration 15546, Loss: 0.05417167395353317\n",
      "Iteration 15547, Loss: 0.05398912727832794\n",
      "Iteration 15548, Loss: 0.05417167395353317\n",
      "Iteration 15549, Loss: 0.05398912727832794\n",
      "Iteration 15550, Loss: 0.054171644151210785\n",
      "Iteration 15551, Loss: 0.05398919805884361\n",
      "Iteration 15552, Loss: 0.05417171120643616\n",
      "Iteration 15553, Loss: 0.053989045321941376\n",
      "Iteration 15554, Loss: 0.05417167395353317\n",
      "Iteration 15555, Loss: 0.05398912355303764\n",
      "Iteration 15556, Loss: 0.05417171120643616\n",
      "Iteration 15557, Loss: 0.053989093750715256\n",
      "Iteration 15558, Loss: 0.05417151749134064\n",
      "Iteration 15559, Loss: 0.053989287465810776\n",
      "Iteration 15560, Loss: 0.05417148023843765\n",
      "Iteration 15561, Loss: 0.053989361971616745\n",
      "Iteration 15562, Loss: 0.05417144298553467\n",
      "Iteration 15563, Loss: 0.053989432752132416\n",
      "Iteration 15564, Loss: 0.05417148396372795\n",
      "Iteration 15565, Loss: 0.05398931726813316\n",
      "Iteration 15566, Loss: 0.05417168140411377\n",
      "Iteration 15567, Loss: 0.05398900434374809\n",
      "Iteration 15568, Loss: 0.054171960800886154\n",
      "Iteration 15569, Loss: 0.05398884043097496\n",
      "Iteration 15570, Loss: 0.05417211353778839\n",
      "Iteration 15571, Loss: 0.05398872494697571\n",
      "Iteration 15572, Loss: 0.054172150790691376\n",
      "Iteration 15573, Loss: 0.0539887361228466\n",
      "Iteration 15574, Loss: 0.05417194962501526\n",
      "Iteration 15575, Loss: 0.053988777101039886\n",
      "Iteration 15576, Loss: 0.054171741008758545\n",
      "Iteration 15577, Loss: 0.053989242762327194\n",
      "Iteration 15578, Loss: 0.05417128652334213\n",
      "Iteration 15579, Loss: 0.053989559412002563\n",
      "Iteration 15580, Loss: 0.054171234369277954\n",
      "Iteration 15581, Loss: 0.053989674896001816\n",
      "Iteration 15582, Loss: 0.05417128652334213\n",
      "Iteration 15583, Loss: 0.0539894700050354\n",
      "Iteration 15584, Loss: 0.05417145416140556\n",
      "Iteration 15585, Loss: 0.05398927256464958\n",
      "Iteration 15586, Loss: 0.05417191609740257\n",
      "Iteration 15587, Loss: 0.05398895591497421\n",
      "Iteration 15588, Loss: 0.05417200177907944\n",
      "Iteration 15589, Loss: 0.05398872494697571\n",
      "Iteration 15590, Loss: 0.05417223274707794\n",
      "Iteration 15591, Loss: 0.05398857593536377\n",
      "Iteration 15592, Loss: 0.054172031581401825\n",
      "Iteration 15593, Loss: 0.05398869514465332\n",
      "Iteration 15594, Loss: 0.05417175218462944\n",
      "Iteration 15595, Loss: 0.053989049047231674\n",
      "Iteration 15596, Loss: 0.05417155474424362\n",
      "Iteration 15597, Loss: 0.05398932844400406\n",
      "Iteration 15598, Loss: 0.0541713610291481\n",
      "Iteration 15599, Loss: 0.05398951470851898\n",
      "Iteration 15600, Loss: 0.0541713647544384\n",
      "Iteration 15601, Loss: 0.05398940294981003\n",
      "Iteration 15602, Loss: 0.054171591997146606\n",
      "Iteration 15603, Loss: 0.053989242762327194\n",
      "Iteration 15604, Loss: 0.05417175218462944\n",
      "Iteration 15605, Loss: 0.05398896336555481\n",
      "Iteration 15606, Loss: 0.05417187511920929\n",
      "Iteration 15607, Loss: 0.053988851606845856\n",
      "Iteration 15608, Loss: 0.05417187139391899\n",
      "Iteration 15609, Loss: 0.053989045321941376\n",
      "Iteration 15610, Loss: 0.05417167395353317\n",
      "Iteration 15611, Loss: 0.05398920178413391\n",
      "Iteration 15612, Loss: 0.05417144298553467\n",
      "Iteration 15613, Loss: 0.05398939549922943\n",
      "Iteration 15614, Loss: 0.0541713647544384\n",
      "Iteration 15615, Loss: 0.053989481180906296\n",
      "Iteration 15616, Loss: 0.05417132005095482\n",
      "Iteration 15617, Loss: 0.05398951470851898\n",
      "Iteration 15618, Loss: 0.05417132377624512\n",
      "Iteration 15619, Loss: 0.05398944020271301\n",
      "Iteration 15620, Loss: 0.05417140573263168\n",
      "Iteration 15621, Loss: 0.05398927628993988\n",
      "Iteration 15622, Loss: 0.05417144298553467\n",
      "Iteration 15623, Loss: 0.053989242762327194\n",
      "Iteration 15624, Loss: 0.05417179316282272\n",
      "Iteration 15625, Loss: 0.05398911237716675\n",
      "Iteration 15626, Loss: 0.054171912372112274\n",
      "Iteration 15627, Loss: 0.053988926112651825\n",
      "Iteration 15628, Loss: 0.05417187139391899\n",
      "Iteration 15629, Loss: 0.053988851606845856\n",
      "Iteration 15630, Loss: 0.05417183041572571\n",
      "Iteration 15631, Loss: 0.05398908257484436\n",
      "Iteration 15632, Loss: 0.05417163297533989\n",
      "Iteration 15633, Loss: 0.053989313542842865\n",
      "Iteration 15634, Loss: 0.05417143926024437\n",
      "Iteration 15635, Loss: 0.05398932099342346\n",
      "Iteration 15636, Loss: 0.05417143926024437\n",
      "Iteration 15637, Loss: 0.053989361971616745\n",
      "Iteration 15638, Loss: 0.05417148023843765\n",
      "Iteration 15639, Loss: 0.05398939177393913\n",
      "Iteration 15640, Loss: 0.05417148396372795\n",
      "Iteration 15641, Loss: 0.05398920178413391\n",
      "Iteration 15642, Loss: 0.0541716031730175\n",
      "Iteration 15643, Loss: 0.053989261388778687\n",
      "Iteration 15644, Loss: 0.05417171120643616\n",
      "Iteration 15645, Loss: 0.053989164531230927\n",
      "Iteration 15646, Loss: 0.05417171120643616\n",
      "Iteration 15647, Loss: 0.05398912355303764\n",
      "Iteration 15648, Loss: 0.05417167395353317\n",
      "Iteration 15649, Loss: 0.053989164531230927\n",
      "Iteration 15650, Loss: 0.054171524941921234\n",
      "Iteration 15651, Loss: 0.05398912727832794\n",
      "Iteration 15652, Loss: 0.054171644151210785\n",
      "Iteration 15653, Loss: 0.0539892315864563\n",
      "Iteration 15654, Loss: 0.05417167395353317\n",
      "Iteration 15655, Loss: 0.053989164531230927\n",
      "Iteration 15656, Loss: 0.054171837866306305\n",
      "Iteration 15657, Loss: 0.05398903787136078\n",
      "Iteration 15658, Loss: 0.05417206883430481\n",
      "Iteration 15659, Loss: 0.05398884415626526\n",
      "Iteration 15660, Loss: 0.054172150790691376\n",
      "Iteration 15661, Loss: 0.053988657891750336\n",
      "Iteration 15662, Loss: 0.05417202413082123\n",
      "Iteration 15663, Loss: 0.05398881435394287\n",
      "Iteration 15664, Loss: 0.05417179316282272\n",
      "Iteration 15665, Loss: 0.05398908257484436\n",
      "Iteration 15666, Loss: 0.05417156219482422\n",
      "Iteration 15667, Loss: 0.05398928374052048\n",
      "Iteration 15668, Loss: 0.05417155474424362\n",
      "Iteration 15669, Loss: 0.05398928374052048\n",
      "Iteration 15670, Loss: 0.05417151749134064\n",
      "Iteration 15671, Loss: 0.05398931726813316\n",
      "Iteration 15672, Loss: 0.05417151749134064\n",
      "Iteration 15673, Loss: 0.05398928374052048\n",
      "Iteration 15674, Loss: 0.05417144298553467\n",
      "Iteration 15675, Loss: 0.05398920178413391\n",
      "Iteration 15676, Loss: 0.05417151749134064\n",
      "Iteration 15677, Loss: 0.05398928374052048\n",
      "Iteration 15678, Loss: 0.05417151749134064\n",
      "Iteration 15679, Loss: 0.05398920178413391\n",
      "Iteration 15680, Loss: 0.054171591997146606\n",
      "Iteration 15681, Loss: 0.05398912355303764\n",
      "Iteration 15682, Loss: 0.05417156219482422\n",
      "Iteration 15683, Loss: 0.05398920178413391\n",
      "Iteration 15684, Loss: 0.054171472787857056\n",
      "Iteration 15685, Loss: 0.053989361971616745\n",
      "Iteration 15686, Loss: 0.05417140573263168\n",
      "Iteration 15687, Loss: 0.053989432752132416\n",
      "Iteration 15688, Loss: 0.054171591997146606\n",
      "Iteration 15689, Loss: 0.05398927628993988\n",
      "Iteration 15690, Loss: 0.05417167767882347\n",
      "Iteration 15691, Loss: 0.05398915335536003\n",
      "Iteration 15692, Loss: 0.05417187139391899\n",
      "Iteration 15693, Loss: 0.05398900434374809\n",
      "Iteration 15694, Loss: 0.0541718415915966\n",
      "Iteration 15695, Loss: 0.05398892983794212\n",
      "Iteration 15696, Loss: 0.05417179316282272\n",
      "Iteration 15697, Loss: 0.053989045321941376\n",
      "Iteration 15698, Loss: 0.05417167395353317\n",
      "Iteration 15699, Loss: 0.053989164531230927\n",
      "Iteration 15700, Loss: 0.05417148396372795\n",
      "Iteration 15701, Loss: 0.053989361971616745\n",
      "Iteration 15702, Loss: 0.05417140573263168\n",
      "Iteration 15703, Loss: 0.0539894700050354\n",
      "Iteration 15704, Loss: 0.05417144298553467\n",
      "Iteration 15705, Loss: 0.05398939177393913\n",
      "Iteration 15706, Loss: 0.054171524941921234\n",
      "Iteration 15707, Loss: 0.05398920178413391\n",
      "Iteration 15708, Loss: 0.054171644151210785\n",
      "Iteration 15709, Loss: 0.05398919805884361\n",
      "Iteration 15710, Loss: 0.05417172238230705\n",
      "Iteration 15711, Loss: 0.053989119827747345\n",
      "Iteration 15712, Loss: 0.054171763360500336\n",
      "Iteration 15713, Loss: 0.05398900434374809\n",
      "Iteration 15714, Loss: 0.05417194962501526\n",
      "Iteration 15715, Loss: 0.05398896336555481\n",
      "Iteration 15716, Loss: 0.05417194589972496\n",
      "Iteration 15717, Loss: 0.05398888885974884\n",
      "Iteration 15718, Loss: 0.05417187139391899\n",
      "Iteration 15719, Loss: 0.05398900434374809\n",
      "Iteration 15720, Loss: 0.05417167395353317\n",
      "Iteration 15721, Loss: 0.05398912355303764\n",
      "Iteration 15722, Loss: 0.05417151749134064\n",
      "Iteration 15723, Loss: 0.053989361971616745\n",
      "Iteration 15724, Loss: 0.05417148396372795\n",
      "Iteration 15725, Loss: 0.05398927628993988\n",
      "Iteration 15726, Loss: 0.0541716031730175\n",
      "Iteration 15727, Loss: 0.05398927256464958\n",
      "Iteration 15728, Loss: 0.054171644151210785\n",
      "Iteration 15729, Loss: 0.053989045321941376\n",
      "Iteration 15730, Loss: 0.05417172238230705\n",
      "Iteration 15731, Loss: 0.05398907884955406\n",
      "Iteration 15732, Loss: 0.05417172238230705\n",
      "Iteration 15733, Loss: 0.05398896336555481\n",
      "Iteration 15734, Loss: 0.05417171120643616\n",
      "Iteration 15735, Loss: 0.053989049047231674\n",
      "Iteration 15736, Loss: 0.05417175218462944\n",
      "Iteration 15737, Loss: 0.05398912355303764\n",
      "Iteration 15738, Loss: 0.0541715994477272\n",
      "Iteration 15739, Loss: 0.053989242762327194\n",
      "Iteration 15740, Loss: 0.05417144298553467\n",
      "Iteration 15741, Loss: 0.05398928374052048\n",
      "Iteration 15742, Loss: 0.05417148396372795\n",
      "Iteration 15743, Loss: 0.05398935824632645\n",
      "Iteration 15744, Loss: 0.05417167395353317\n",
      "Iteration 15745, Loss: 0.0539892315864563\n",
      "Iteration 15746, Loss: 0.05417175590991974\n",
      "Iteration 15747, Loss: 0.053989045321941376\n",
      "Iteration 15748, Loss: 0.05417194962501526\n",
      "Iteration 15749, Loss: 0.05398888885974884\n",
      "Iteration 15750, Loss: 0.05417206510901451\n",
      "Iteration 15751, Loss: 0.053988851606845856\n",
      "Iteration 15752, Loss: 0.054171957075595856\n",
      "Iteration 15753, Loss: 0.053988926112651825\n",
      "Iteration 15754, Loss: 0.05417167395353317\n",
      "Iteration 15755, Loss: 0.053989164531230927\n",
      "Iteration 15756, Loss: 0.05417163297533989\n",
      "Iteration 15757, Loss: 0.05398920178413391\n",
      "Iteration 15758, Loss: 0.05417155846953392\n",
      "Iteration 15759, Loss: 0.053989168256521225\n",
      "Iteration 15760, Loss: 0.0541716031730175\n",
      "Iteration 15761, Loss: 0.0539892315864563\n",
      "Iteration 15762, Loss: 0.05417171120643616\n",
      "Iteration 15763, Loss: 0.053989045321941376\n",
      "Iteration 15764, Loss: 0.05417175590991974\n",
      "Iteration 15765, Loss: 0.05398900434374809\n",
      "Iteration 15766, Loss: 0.05417179688811302\n",
      "Iteration 15767, Loss: 0.05398900434374809\n",
      "Iteration 15768, Loss: 0.05417187139391899\n",
      "Iteration 15769, Loss: 0.05398901551961899\n",
      "Iteration 15770, Loss: 0.054171666502952576\n",
      "Iteration 15771, Loss: 0.05398927628993988\n",
      "Iteration 15772, Loss: 0.054171398282051086\n",
      "Iteration 15773, Loss: 0.05398939549922943\n",
      "Iteration 15774, Loss: 0.054171353578567505\n",
      "Iteration 15775, Loss: 0.053989481180906296\n",
      "Iteration 15776, Loss: 0.054171472787857056\n",
      "Iteration 15777, Loss: 0.053989361971616745\n",
      "Iteration 15778, Loss: 0.054171644151210785\n",
      "Iteration 15779, Loss: 0.05398911237716675\n",
      "Iteration 15780, Loss: 0.0541718415915966\n",
      "Iteration 15781, Loss: 0.05398895591497421\n",
      "Iteration 15782, Loss: 0.05417203903198242\n",
      "Iteration 15783, Loss: 0.0539887361228466\n",
      "Iteration 15784, Loss: 0.05417199432849884\n",
      "Iteration 15785, Loss: 0.05398888513445854\n",
      "Iteration 15786, Loss: 0.05417179316282272\n",
      "Iteration 15787, Loss: 0.05398908257484436\n",
      "Iteration 15788, Loss: 0.054171718657016754\n",
      "Iteration 15789, Loss: 0.053989168256521225\n",
      "Iteration 15790, Loss: 0.05417163297533989\n",
      "Iteration 15791, Loss: 0.05398920178413391\n",
      "Iteration 15792, Loss: 0.05417167395353317\n",
      "Iteration 15793, Loss: 0.053989164531230927\n",
      "Iteration 15794, Loss: 0.05417168140411377\n",
      "Iteration 15795, Loss: 0.05398908257484436\n",
      "Iteration 15796, Loss: 0.05417172238230705\n",
      "Iteration 15797, Loss: 0.053989045321941376\n",
      "Iteration 15798, Loss: 0.05417180061340332\n",
      "Iteration 15799, Loss: 0.05398896336555481\n",
      "Iteration 15800, Loss: 0.05417187139391899\n",
      "Iteration 15801, Loss: 0.053989045321941376\n",
      "Iteration 15802, Loss: 0.05417183041572571\n",
      "Iteration 15803, Loss: 0.053989045321941376\n",
      "Iteration 15804, Loss: 0.05417175218462944\n",
      "Iteration 15805, Loss: 0.053989045321941376\n",
      "Iteration 15806, Loss: 0.05417163297533989\n",
      "Iteration 15807, Loss: 0.05398912727832794\n",
      "Iteration 15808, Loss: 0.05417148023843765\n",
      "Iteration 15809, Loss: 0.053989361971616745\n",
      "Iteration 15810, Loss: 0.0541713647544384\n",
      "Iteration 15811, Loss: 0.05398940294981003\n",
      "Iteration 15812, Loss: 0.05417144298553467\n",
      "Iteration 15813, Loss: 0.05398927628993988\n",
      "Iteration 15814, Loss: 0.05417155474424362\n",
      "Iteration 15815, Loss: 0.05398930609226227\n",
      "Iteration 15816, Loss: 0.05417168140411377\n",
      "Iteration 15817, Loss: 0.053989045321941376\n",
      "Iteration 15818, Loss: 0.054171837866306305\n",
      "Iteration 15819, Loss: 0.05398888885974884\n",
      "Iteration 15820, Loss: 0.05417179316282272\n",
      "Iteration 15821, Loss: 0.053989045321941376\n",
      "Iteration 15822, Loss: 0.05417171120643616\n",
      "Iteration 15823, Loss: 0.053989168256521225\n",
      "Iteration 15824, Loss: 0.05417155846953392\n",
      "Iteration 15825, Loss: 0.053989242762327194\n",
      "Iteration 15826, Loss: 0.05417148023843765\n",
      "Iteration 15827, Loss: 0.05398940294981003\n",
      "Iteration 15828, Loss: 0.05417151376605034\n",
      "Iteration 15829, Loss: 0.05398944020271301\n",
      "Iteration 15830, Loss: 0.0541713647544384\n",
      "Iteration 15831, Loss: 0.053989436477422714\n",
      "Iteration 15832, Loss: 0.05417151376605034\n",
      "Iteration 15833, Loss: 0.05398935079574585\n",
      "Iteration 15834, Loss: 0.05417168140411377\n",
      "Iteration 15835, Loss: 0.05398903414607048\n",
      "Iteration 15836, Loss: 0.05417191982269287\n",
      "Iteration 15837, Loss: 0.05398884043097496\n",
      "Iteration 15838, Loss: 0.05417206883430481\n",
      "Iteration 15839, Loss: 0.053988806903362274\n",
      "Iteration 15840, Loss: 0.05417194962501526\n",
      "Iteration 15841, Loss: 0.05398881435394287\n",
      "Iteration 15842, Loss: 0.05417168140411377\n",
      "Iteration 15843, Loss: 0.05398908257484436\n",
      "Iteration 15844, Loss: 0.0541716031730175\n",
      "Iteration 15845, Loss: 0.05398920178413391\n",
      "Iteration 15846, Loss: 0.054171591997146606\n",
      "Iteration 15847, Loss: 0.05398928374052048\n",
      "Iteration 15848, Loss: 0.05417151749134064\n",
      "Iteration 15849, Loss: 0.05398932099342346\n",
      "Iteration 15850, Loss: 0.054171524941921234\n",
      "Iteration 15851, Loss: 0.05398927628993988\n",
      "Iteration 15852, Loss: 0.0541716031730175\n",
      "Iteration 15853, Loss: 0.05398912355303764\n",
      "Iteration 15854, Loss: 0.054171837866306305\n",
      "Iteration 15855, Loss: 0.05398896336555481\n",
      "Iteration 15856, Loss: 0.05417187139391899\n",
      "Iteration 15857, Loss: 0.05398888885974884\n",
      "Iteration 15858, Loss: 0.05417175590991974\n",
      "Iteration 15859, Loss: 0.053988974541425705\n",
      "Iteration 15860, Loss: 0.05417163297533989\n",
      "Iteration 15861, Loss: 0.053989242762327194\n",
      "Iteration 15862, Loss: 0.05417148396372795\n",
      "Iteration 15863, Loss: 0.053989361971616745\n",
      "Iteration 15864, Loss: 0.05417140573263168\n",
      "Iteration 15865, Loss: 0.053989436477422714\n",
      "Iteration 15866, Loss: 0.05417144298553467\n",
      "Iteration 15867, Loss: 0.053989313542842865\n",
      "Iteration 15868, Loss: 0.05417156219482422\n",
      "Iteration 15869, Loss: 0.053989119827747345\n",
      "Iteration 15870, Loss: 0.05417194962501526\n",
      "Iteration 15871, Loss: 0.05398888513445854\n",
      "Iteration 15872, Loss: 0.054171912372112274\n",
      "Iteration 15873, Loss: 0.053988926112651825\n",
      "Iteration 15874, Loss: 0.05417175218462944\n",
      "Iteration 15875, Loss: 0.053989049047231674\n",
      "Iteration 15876, Loss: 0.05417148396372795\n",
      "Iteration 15877, Loss: 0.05398925393819809\n",
      "Iteration 15878, Loss: 0.05417124554514885\n",
      "Iteration 15879, Loss: 0.053989481180906296\n",
      "Iteration 15880, Loss: 0.054171137511730194\n",
      "Iteration 15881, Loss: 0.053989481180906296\n",
      "Iteration 15882, Loss: 0.0541713647544384\n",
      "Iteration 15883, Loss: 0.05398939177393913\n",
      "Iteration 15884, Loss: 0.05417148396372795\n",
      "Iteration 15885, Loss: 0.053989313542842865\n",
      "Iteration 15886, Loss: 0.0541716031730175\n",
      "Iteration 15887, Loss: 0.0539892315864563\n",
      "Iteration 15888, Loss: 0.05417172238230705\n",
      "Iteration 15889, Loss: 0.053989045321941376\n",
      "Iteration 15890, Loss: 0.05417187139391899\n",
      "Iteration 15891, Loss: 0.05398888885974884\n",
      "Iteration 15892, Loss: 0.05417183041572571\n",
      "Iteration 15893, Loss: 0.05398896336555481\n",
      "Iteration 15894, Loss: 0.05417155846953392\n",
      "Iteration 15895, Loss: 0.05398912355303764\n",
      "Iteration 15896, Loss: 0.05417140573263168\n",
      "Iteration 15897, Loss: 0.05398920178413391\n",
      "Iteration 15898, Loss: 0.05417121574282646\n",
      "Iteration 15899, Loss: 0.05398940294981003\n",
      "Iteration 15900, Loss: 0.05417132377624512\n",
      "Iteration 15901, Loss: 0.053989510983228683\n",
      "Iteration 15902, Loss: 0.05417144298553467\n",
      "Iteration 15903, Loss: 0.05398928374052048\n",
      "Iteration 15904, Loss: 0.05417172238230705\n",
      "Iteration 15905, Loss: 0.053989119827747345\n",
      "Iteration 15906, Loss: 0.054171882569789886\n",
      "Iteration 15907, Loss: 0.053988926112651825\n",
      "Iteration 15908, Loss: 0.05417180061340332\n",
      "Iteration 15909, Loss: 0.053988970816135406\n",
      "Iteration 15910, Loss: 0.05417180061340332\n",
      "Iteration 15911, Loss: 0.05398896336555481\n",
      "Iteration 15912, Loss: 0.05417179316282272\n",
      "Iteration 15913, Loss: 0.053989045321941376\n",
      "Iteration 15914, Loss: 0.05417179316282272\n",
      "Iteration 15915, Loss: 0.053989239037036896\n",
      "Iteration 15916, Loss: 0.05417163297533989\n",
      "Iteration 15917, Loss: 0.05398931726813316\n",
      "Iteration 15918, Loss: 0.054171524941921234\n",
      "Iteration 15919, Loss: 0.05398932099342346\n",
      "Iteration 15920, Loss: 0.05417148396372795\n",
      "Iteration 15921, Loss: 0.05398935079574585\n",
      "Iteration 15922, Loss: 0.05417148023843765\n",
      "Iteration 15923, Loss: 0.05398932099342346\n",
      "Iteration 15924, Loss: 0.05417140573263168\n",
      "Iteration 15925, Loss: 0.05398920178413391\n",
      "Iteration 15926, Loss: 0.05417151749134064\n",
      "Iteration 15927, Loss: 0.05398920178413391\n",
      "Iteration 15928, Loss: 0.0541716031730175\n",
      "Iteration 15929, Loss: 0.05398912355303764\n",
      "Iteration 15930, Loss: 0.05417171120643616\n",
      "Iteration 15931, Loss: 0.05398908257484436\n",
      "Iteration 15932, Loss: 0.05417179316282272\n",
      "Iteration 15933, Loss: 0.05398915708065033\n",
      "Iteration 15934, Loss: 0.054171912372112274\n",
      "Iteration 15935, Loss: 0.05398907512426376\n",
      "Iteration 15936, Loss: 0.054171912372112274\n",
      "Iteration 15937, Loss: 0.053989000618457794\n",
      "Iteration 15938, Loss: 0.05417180061340332\n",
      "Iteration 15939, Loss: 0.05398903787136078\n",
      "Iteration 15940, Loss: 0.05417179688811302\n",
      "Iteration 15941, Loss: 0.053989045321941376\n",
      "Iteration 15942, Loss: 0.05417183041572571\n",
      "Iteration 15943, Loss: 0.053989119827747345\n",
      "Iteration 15944, Loss: 0.05417155474424362\n",
      "Iteration 15945, Loss: 0.05398920178413391\n",
      "Iteration 15946, Loss: 0.05417144298553467\n",
      "Iteration 15947, Loss: 0.05398931726813316\n",
      "Iteration 15948, Loss: 0.05417148396372795\n",
      "Iteration 15949, Loss: 0.053989436477422714\n",
      "Iteration 15950, Loss: 0.05417148396372795\n",
      "Iteration 15951, Loss: 0.05398928374052048\n",
      "Iteration 15952, Loss: 0.05417167767882347\n",
      "Iteration 15953, Loss: 0.05398930609226227\n",
      "Iteration 15954, Loss: 0.05417172610759735\n",
      "Iteration 15955, Loss: 0.053988926112651825\n",
      "Iteration 15956, Loss: 0.054172031581401825\n",
      "Iteration 15957, Loss: 0.05398876965045929\n",
      "Iteration 15958, Loss: 0.05417180806398392\n",
      "Iteration 15959, Loss: 0.05398881062865257\n",
      "Iteration 15960, Loss: 0.054171763360500336\n",
      "Iteration 15961, Loss: 0.053988970816135406\n",
      "Iteration 15962, Loss: 0.054171591997146606\n",
      "Iteration 15963, Loss: 0.05398928374052048\n",
      "Iteration 15964, Loss: 0.054171472787857056\n",
      "Iteration 15965, Loss: 0.05398932099342346\n",
      "Iteration 15966, Loss: 0.05417124181985855\n",
      "Iteration 15967, Loss: 0.053989481180906296\n",
      "Iteration 15968, Loss: 0.054171279072761536\n",
      "Iteration 15969, Loss: 0.05398958921432495\n",
      "Iteration 15970, Loss: 0.05417143926024437\n",
      "Iteration 15971, Loss: 0.05398928374052048\n",
      "Iteration 15972, Loss: 0.05417175218462944\n",
      "Iteration 15973, Loss: 0.05398908257484436\n",
      "Iteration 15974, Loss: 0.05417180061340332\n",
      "Iteration 15975, Loss: 0.0539889931678772\n",
      "Iteration 15976, Loss: 0.05417210981249809\n",
      "Iteration 15977, Loss: 0.05398872494697571\n",
      "Iteration 15978, Loss: 0.054172031581401825\n",
      "Iteration 15979, Loss: 0.053988806903362274\n",
      "Iteration 15980, Loss: 0.054171912372112274\n",
      "Iteration 15981, Loss: 0.05398889631032944\n",
      "Iteration 15982, Loss: 0.054171591997146606\n",
      "Iteration 15983, Loss: 0.05398901551961899\n",
      "Iteration 15984, Loss: 0.05417151376605034\n",
      "Iteration 15985, Loss: 0.05398932099342346\n",
      "Iteration 15986, Loss: 0.05417124554514885\n",
      "Iteration 15987, Loss: 0.05398940294981003\n",
      "Iteration 15988, Loss: 0.05417128652334213\n",
      "Iteration 15989, Loss: 0.05398940294981003\n",
      "Iteration 15990, Loss: 0.05417140573263168\n",
      "Iteration 15991, Loss: 0.05398928374052048\n",
      "Iteration 15992, Loss: 0.054171591997146606\n",
      "Iteration 15993, Loss: 0.05398912355303764\n",
      "Iteration 15994, Loss: 0.054171718657016754\n",
      "Iteration 15995, Loss: 0.053989119827747345\n",
      "Iteration 15996, Loss: 0.05417175218462944\n",
      "Iteration 15997, Loss: 0.05398900434374809\n",
      "Iteration 15998, Loss: 0.05417179316282272\n",
      "Iteration 15999, Loss: 0.05398896336555481\n",
      "Iteration 16000, Loss: 0.054171763360500336\n",
      "Iteration 16001, Loss: 0.05398900434374809\n",
      "Iteration 16002, Loss: 0.05417163297533989\n",
      "Iteration 16003, Loss: 0.0539892315864563\n",
      "Iteration 16004, Loss: 0.05417140573263168\n",
      "Iteration 16005, Loss: 0.05398939549922943\n",
      "Iteration 16006, Loss: 0.05417148396372795\n",
      "Iteration 16007, Loss: 0.053989477455616\n",
      "Iteration 16008, Loss: 0.05417144298553467\n",
      "Iteration 16009, Loss: 0.053989242762327194\n",
      "Iteration 16010, Loss: 0.05417171120643616\n",
      "Iteration 16011, Loss: 0.053989045321941376\n",
      "Iteration 16012, Loss: 0.05417183041572571\n",
      "Iteration 16013, Loss: 0.053988926112651825\n",
      "Iteration 16014, Loss: 0.054171912372112274\n",
      "Iteration 16015, Loss: 0.053988851606845856\n",
      "Iteration 16016, Loss: 0.054171912372112274\n",
      "Iteration 16017, Loss: 0.05398889631032944\n",
      "Iteration 16018, Loss: 0.05417167395353317\n",
      "Iteration 16019, Loss: 0.05398928374052048\n",
      "Iteration 16020, Loss: 0.05417148023843765\n",
      "Iteration 16021, Loss: 0.053989287465810776\n",
      "Iteration 16022, Loss: 0.05417151376605034\n",
      "Iteration 16023, Loss: 0.053989477455616\n",
      "Iteration 16024, Loss: 0.05417151376605034\n",
      "Iteration 16025, Loss: 0.053989313542842865\n",
      "Iteration 16026, Loss: 0.05417168140411377\n",
      "Iteration 16027, Loss: 0.05398907512426376\n",
      "Iteration 16028, Loss: 0.054171767085790634\n",
      "Iteration 16029, Loss: 0.05398884043097496\n",
      "Iteration 16030, Loss: 0.054171882569789886\n",
      "Iteration 16031, Loss: 0.05398881062865257\n",
      "Iteration 16032, Loss: 0.054171837866306305\n",
      "Iteration 16033, Loss: 0.053989045321941376\n",
      "Iteration 16034, Loss: 0.05417183041572571\n",
      "Iteration 16035, Loss: 0.05398915708065033\n",
      "Iteration 16036, Loss: 0.05417163297533989\n",
      "Iteration 16037, Loss: 0.05398913472890854\n",
      "Iteration 16038, Loss: 0.054171591997146606\n",
      "Iteration 16039, Loss: 0.053989242762327194\n",
      "Iteration 16040, Loss: 0.05417148023843765\n",
      "Iteration 16041, Loss: 0.053989361971616745\n",
      "Iteration 16042, Loss: 0.054171524941921234\n",
      "Iteration 16043, Loss: 0.05398935824632645\n",
      "Iteration 16044, Loss: 0.05417163670063019\n",
      "Iteration 16045, Loss: 0.05398927256464958\n",
      "Iteration 16046, Loss: 0.054171763360500336\n",
      "Iteration 16047, Loss: 0.05398896336555481\n",
      "Iteration 16048, Loss: 0.0541718415915966\n",
      "Iteration 16049, Loss: 0.05398881435394287\n",
      "Iteration 16050, Loss: 0.054171912372112274\n",
      "Iteration 16051, Loss: 0.05398892983794212\n",
      "Iteration 16052, Loss: 0.05417163297533989\n",
      "Iteration 16053, Loss: 0.053989242762327194\n",
      "Iteration 16054, Loss: 0.05417151376605034\n",
      "Iteration 16055, Loss: 0.05398932099342346\n",
      "Iteration 16056, Loss: 0.05417140573263168\n",
      "Iteration 16057, Loss: 0.053989477455616\n",
      "Iteration 16058, Loss: 0.05417144298553467\n",
      "Iteration 16059, Loss: 0.05398931726813316\n",
      "Iteration 16060, Loss: 0.05417167767882347\n",
      "Iteration 16061, Loss: 0.053989239037036896\n",
      "Iteration 16062, Loss: 0.05417165160179138\n",
      "Iteration 16063, Loss: 0.05398896336555481\n",
      "Iteration 16064, Loss: 0.054171882569789886\n",
      "Iteration 16065, Loss: 0.05398869514465332\n",
      "Iteration 16066, Loss: 0.05417191982269287\n",
      "Iteration 16067, Loss: 0.053988851606845856\n",
      "Iteration 16068, Loss: 0.054171644151210785\n",
      "Iteration 16069, Loss: 0.05398901551961899\n",
      "Iteration 16070, Loss: 0.05417143553495407\n",
      "Iteration 16071, Loss: 0.05398924648761749\n",
      "Iteration 16072, Loss: 0.054171279072761536\n",
      "Iteration 16073, Loss: 0.05398940294981003\n",
      "Iteration 16074, Loss: 0.05417131632566452\n",
      "Iteration 16075, Loss: 0.05398952215909958\n",
      "Iteration 16076, Loss: 0.05417116731405258\n",
      "Iteration 16077, Loss: 0.05398952215909958\n",
      "Iteration 16078, Loss: 0.05417140573263168\n",
      "Iteration 16079, Loss: 0.053989361971616745\n",
      "Iteration 16080, Loss: 0.05417163297533989\n",
      "Iteration 16081, Loss: 0.05398928374052048\n",
      "Iteration 16082, Loss: 0.05417167395353317\n",
      "Iteration 16083, Loss: 0.05398912355303764\n",
      "Iteration 16084, Loss: 0.05417172238230705\n",
      "Iteration 16085, Loss: 0.053989555686712265\n",
      "Iteration 16086, Loss: 0.05417168140411377\n",
      "Iteration 16087, Loss: 0.053989559412002563\n",
      "Iteration 16088, Loss: 0.054172031581401825\n",
      "Iteration 16089, Loss: 0.0539897195994854\n",
      "Iteration 16090, Loss: 0.05417179688811302\n",
      "Iteration 16091, Loss: 0.05398990958929062\n",
      "Iteration 16092, Loss: 0.05417175218462944\n",
      "Iteration 16093, Loss: 0.05398987978696823\n",
      "Iteration 16094, Loss: 0.05417175590991974\n",
      "Iteration 16095, Loss: 0.053989917039871216\n",
      "Iteration 16096, Loss: 0.054171644151210785\n",
      "Iteration 16097, Loss: 0.05398987978696823\n",
      "Iteration 16098, Loss: 0.05417164787650108\n",
      "Iteration 16099, Loss: 0.0539899580180645\n",
      "Iteration 16100, Loss: 0.0541718415915966\n",
      "Iteration 16101, Loss: 0.05398982763290405\n",
      "Iteration 16102, Loss: 0.05417199432849884\n",
      "Iteration 16103, Loss: 0.05398957058787346\n",
      "Iteration 16104, Loss: 0.054172031581401825\n",
      "Iteration 16105, Loss: 0.05398964136838913\n",
      "Iteration 16106, Loss: 0.054172031581401825\n",
      "Iteration 16107, Loss: 0.05398964136838913\n",
      "Iteration 16108, Loss: 0.054171957075595856\n",
      "Iteration 16109, Loss: 0.0539897195994854\n",
      "Iteration 16110, Loss: 0.0541718415915966\n",
      "Iteration 16111, Loss: 0.0539897195994854\n",
      "Iteration 16112, Loss: 0.05417172238230705\n",
      "Iteration 16113, Loss: 0.053989917039871216\n",
      "Iteration 16114, Loss: 0.054171763360500336\n",
      "Iteration 16115, Loss: 0.05398983880877495\n",
      "Iteration 16116, Loss: 0.05417180061340332\n",
      "Iteration 16117, Loss: 0.053989872336387634\n",
      "Iteration 16118, Loss: 0.05417180061340332\n",
      "Iteration 16119, Loss: 0.053989797830581665\n",
      "Iteration 16120, Loss: 0.05417194589972496\n",
      "Iteration 16121, Loss: 0.053989723324775696\n",
      "Iteration 16122, Loss: 0.054171718657016754\n",
      "Iteration 16123, Loss: 0.05398984253406525\n",
      "Iteration 16124, Loss: 0.0541716031730175\n",
      "Iteration 16125, Loss: 0.05398992449045181\n",
      "Iteration 16126, Loss: 0.0541715994477272\n",
      "Iteration 16127, Loss: 0.05399014800786972\n",
      "Iteration 16128, Loss: 0.05417167767882347\n",
      "Iteration 16129, Loss: 0.05398998782038689\n",
      "Iteration 16130, Loss: 0.054171882569789886\n",
      "Iteration 16131, Loss: 0.053989749401807785\n",
      "Iteration 16132, Loss: 0.054172080010175705\n",
      "Iteration 16133, Loss: 0.053989481180906296\n",
      "Iteration 16134, Loss: 0.054172348231077194\n",
      "Iteration 16135, Loss: 0.05398940294981003\n",
      "Iteration 16136, Loss: 0.05417223274707794\n",
      "Iteration 16137, Loss: 0.05398936569690704\n",
      "Iteration 16138, Loss: 0.05417210981249809\n",
      "Iteration 16139, Loss: 0.053989678621292114\n",
      "Iteration 16140, Loss: 0.05417179316282272\n",
      "Iteration 16141, Loss: 0.053989991545677185\n",
      "Iteration 16142, Loss: 0.054171666502952576\n",
      "Iteration 16143, Loss: 0.05399011820554733\n",
      "Iteration 16144, Loss: 0.05417132377624512\n",
      "Iteration 16145, Loss: 0.05399022623896599\n",
      "Iteration 16146, Loss: 0.05417144298553467\n",
      "Iteration 16147, Loss: 0.053990259766578674\n",
      "Iteration 16148, Loss: 0.05417152866721153\n",
      "Iteration 16149, Loss: 0.0539899580180645\n",
      "Iteration 16150, Loss: 0.054171886295080185\n",
      "Iteration 16151, Loss: 0.053989674896001816\n",
      "Iteration 16152, Loss: 0.054172124713659286\n",
      "Iteration 16153, Loss: 0.05398940294981003\n",
      "Iteration 16154, Loss: 0.054172322154045105\n",
      "Iteration 16155, Loss: 0.05398920923471451\n",
      "Iteration 16156, Loss: 0.05417254567146301\n",
      "Iteration 16157, Loss: 0.05398917198181152\n",
      "Iteration 16158, Loss: 0.054172467440366745\n",
      "Iteration 16159, Loss: 0.05398917943239212\n",
      "Iteration 16160, Loss: 0.054172269999980927\n",
      "Iteration 16161, Loss: 0.05398937314748764\n",
      "Iteration 16162, Loss: 0.05417206883430481\n",
      "Iteration 16163, Loss: 0.053989678621292114\n",
      "Iteration 16164, Loss: 0.05417187511920929\n",
      "Iteration 16165, Loss: 0.05398976802825928\n",
      "Iteration 16166, Loss: 0.0541718415915966\n",
      "Iteration 16167, Loss: 0.053989917039871216\n",
      "Iteration 16168, Loss: 0.0541718415915966\n",
      "Iteration 16169, Loss: 0.053989872336387634\n",
      "Iteration 16170, Loss: 0.054171886295080185\n",
      "Iteration 16171, Loss: 0.0539897084236145\n",
      "Iteration 16172, Loss: 0.054172199219465256\n",
      "Iteration 16173, Loss: 0.0539894700050354\n",
      "Iteration 16174, Loss: 0.05417227745056152\n",
      "Iteration 16175, Loss: 0.05398920923471451\n",
      "Iteration 16176, Loss: 0.05417238920927048\n",
      "Iteration 16177, Loss: 0.053989216685295105\n",
      "Iteration 16178, Loss: 0.05417230725288391\n",
      "Iteration 16179, Loss: 0.05398944020271301\n",
      "Iteration 16180, Loss: 0.05417218804359436\n",
      "Iteration 16181, Loss: 0.053989484906196594\n",
      "Iteration 16182, Loss: 0.054172076284885406\n",
      "Iteration 16183, Loss: 0.05398960039019585\n",
      "Iteration 16184, Loss: 0.054172076284885406\n",
      "Iteration 16185, Loss: 0.05398960039019585\n",
      "Iteration 16186, Loss: 0.054172225296497345\n",
      "Iteration 16187, Loss: 0.053989559412002563\n",
      "Iteration 16188, Loss: 0.05417218804359436\n",
      "Iteration 16189, Loss: 0.053989481180906296\n",
      "Iteration 16190, Loss: 0.054172080010175705\n",
      "Iteration 16191, Loss: 0.05398960039019585\n",
      "Iteration 16192, Loss: 0.05417214334011078\n",
      "Iteration 16193, Loss: 0.053989604115486145\n",
      "Iteration 16194, Loss: 0.054171960800886154\n",
      "Iteration 16195, Loss: 0.053989678621292114\n",
      "Iteration 16196, Loss: 0.05417191982269287\n",
      "Iteration 16197, Loss: 0.053989630192518234\n",
      "Iteration 16198, Loss: 0.054171960800886154\n",
      "Iteration 16199, Loss: 0.053989559412002563\n",
      "Iteration 16200, Loss: 0.05417212098836899\n",
      "Iteration 16201, Loss: 0.05398964136838913\n",
      "Iteration 16202, Loss: 0.05417203530669212\n",
      "Iteration 16203, Loss: 0.05398960039019585\n",
      "Iteration 16204, Loss: 0.05417226254940033\n",
      "Iteration 16205, Loss: 0.053989481180906296\n",
      "Iteration 16206, Loss: 0.05417218804359436\n",
      "Iteration 16207, Loss: 0.053989484906196594\n",
      "Iteration 16208, Loss: 0.05417203530669212\n",
      "Iteration 16209, Loss: 0.053988873958587646\n",
      "Iteration 16210, Loss: 0.054172031581401825\n",
      "Iteration 16211, Loss: 0.0539889894425869\n",
      "Iteration 16212, Loss: 0.054171256721019745\n",
      "Iteration 16213, Loss: 0.05398918315768242\n",
      "Iteration 16214, Loss: 0.05417117476463318\n",
      "Iteration 16215, Loss: 0.053989291191101074\n",
      "Iteration 16216, Loss: 0.05417122691869736\n",
      "Iteration 16217, Loss: 0.053989097476005554\n",
      "Iteration 16218, Loss: 0.05417145788669586\n",
      "Iteration 16219, Loss: 0.05398893356323242\n",
      "Iteration 16220, Loss: 0.05417158454656601\n",
      "Iteration 16221, Loss: 0.05398870259523392\n",
      "Iteration 16222, Loss: 0.05417178198695183\n",
      "Iteration 16223, Loss: 0.053988467901945114\n",
      "Iteration 16224, Loss: 0.0541718564927578\n",
      "Iteration 16225, Loss: 0.05398862808942795\n",
      "Iteration 16226, Loss: 0.05417165160179138\n",
      "Iteration 16227, Loss: 0.05398878455162048\n",
      "Iteration 16228, Loss: 0.054171569645404816\n",
      "Iteration 16229, Loss: 0.05398934334516525\n",
      "Iteration 16230, Loss: 0.0541713684797287\n",
      "Iteration 16231, Loss: 0.05398968979716301\n",
      "Iteration 16232, Loss: 0.054170578718185425\n",
      "Iteration 16233, Loss: 0.05398985370993614\n",
      "Iteration 16234, Loss: 0.05417061969637871\n",
      "Iteration 16235, Loss: 0.05398981273174286\n",
      "Iteration 16236, Loss: 0.05417070537805557\n",
      "Iteration 16237, Loss: 0.05398964881896973\n",
      "Iteration 16238, Loss: 0.054171107709407806\n",
      "Iteration 16239, Loss: 0.053989216685295105\n",
      "Iteration 16240, Loss: 0.05417157709598541\n",
      "Iteration 16241, Loss: 0.05398885905742645\n",
      "Iteration 16242, Loss: 0.05417182669043541\n",
      "Iteration 16243, Loss: 0.053988467901945114\n",
      "Iteration 16244, Loss: 0.05417215824127197\n",
      "Iteration 16245, Loss: 0.053988393396139145\n",
      "Iteration 16246, Loss: 0.054171811789274216\n",
      "Iteration 16247, Loss: 0.053988635540008545\n",
      "Iteration 16248, Loss: 0.054171644151210785\n",
      "Iteration 16249, Loss: 0.05398894473910332\n",
      "Iteration 16250, Loss: 0.05417129024863243\n",
      "Iteration 16251, Loss: 0.05398929864168167\n",
      "Iteration 16252, Loss: 0.05417102575302124\n",
      "Iteration 16253, Loss: 0.05398945137858391\n",
      "Iteration 16254, Loss: 0.05417110025882721\n",
      "Iteration 16255, Loss: 0.05398949235677719\n",
      "Iteration 16256, Loss: 0.05417126044631004\n",
      "Iteration 16257, Loss: 0.05398925393819809\n",
      "Iteration 16258, Loss: 0.05417153239250183\n",
      "Iteration 16259, Loss: 0.05398910492658615\n",
      "Iteration 16260, Loss: 0.054171543568372726\n",
      "Iteration 16261, Loss: 0.0539887472987175\n",
      "Iteration 16262, Loss: 0.05417169630527496\n",
      "Iteration 16263, Loss: 0.053988706320524216\n",
      "Iteration 16264, Loss: 0.054171930998563766\n",
      "Iteration 16265, Loss: 0.05398862808942795\n",
      "Iteration 16266, Loss: 0.0541718453168869\n",
      "Iteration 16267, Loss: 0.053988635540008545\n",
      "Iteration 16268, Loss: 0.05417165160179138\n",
      "Iteration 16269, Loss: 0.053988903760910034\n",
      "Iteration 16270, Loss: 0.05417145416140556\n",
      "Iteration 16271, Loss: 0.05398917943239212\n",
      "Iteration 16272, Loss: 0.054171375930309296\n",
      "Iteration 16273, Loss: 0.053989212960004807\n",
      "Iteration 16274, Loss: 0.054171375930309296\n",
      "Iteration 16275, Loss: 0.053989097476005554\n",
      "Iteration 16276, Loss: 0.054171375930309296\n",
      "Iteration 16277, Loss: 0.053989022970199585\n",
      "Iteration 16278, Loss: 0.054171301424503326\n",
      "Iteration 16279, Loss: 0.05398910492658615\n",
      "Iteration 16280, Loss: 0.054171256721019745\n",
      "Iteration 16281, Loss: 0.05398930236697197\n",
      "Iteration 16282, Loss: 0.05417114496231079\n",
      "Iteration 16283, Loss: 0.05398933216929436\n",
      "Iteration 16284, Loss: 0.05417129397392273\n",
      "Iteration 16285, Loss: 0.053989097476005554\n",
      "Iteration 16286, Loss: 0.054171450436115265\n",
      "Iteration 16287, Loss: 0.05398906022310257\n",
      "Iteration 16288, Loss: 0.054171375930309296\n",
      "Iteration 16289, Loss: 0.05398910492658615\n",
      "Iteration 16290, Loss: 0.05417152866721153\n",
      "Iteration 16291, Loss: 0.053989093750715256\n",
      "Iteration 16292, Loss: 0.0541716143488884\n",
      "Iteration 16293, Loss: 0.05398886650800705\n",
      "Iteration 16294, Loss: 0.054171688854694366\n",
      "Iteration 16295, Loss: 0.05398879200220108\n",
      "Iteration 16296, Loss: 0.05417168140411377\n",
      "Iteration 16297, Loss: 0.053988903760910034\n",
      "Iteration 16298, Loss: 0.054171569645404816\n",
      "Iteration 16299, Loss: 0.0539889857172966\n",
      "Iteration 16300, Loss: 0.05417145416140556\n",
      "Iteration 16301, Loss: 0.05398894473910332\n",
      "Iteration 16302, Loss: 0.054171569645404816\n",
      "Iteration 16303, Loss: 0.05398901551961899\n",
      "Iteration 16304, Loss: 0.05417153239250183\n",
      "Iteration 16305, Loss: 0.053988974541425705\n",
      "Iteration 16306, Loss: 0.05417168140411377\n",
      "Iteration 16307, Loss: 0.05398901551961899\n",
      "Iteration 16308, Loss: 0.05417165905237198\n",
      "Iteration 16309, Loss: 0.0539887472987175\n",
      "Iteration 16310, Loss: 0.0541718527674675\n",
      "Iteration 16311, Loss: 0.05398866534233093\n",
      "Iteration 16312, Loss: 0.05417203903198242\n",
      "Iteration 16313, Loss: 0.053988583385944366\n",
      "Iteration 16314, Loss: 0.05417192727327347\n",
      "Iteration 16315, Loss: 0.05398862808942795\n",
      "Iteration 16316, Loss: 0.054171763360500336\n",
      "Iteration 16317, Loss: 0.053988829255104065\n",
      "Iteration 16318, Loss: 0.0541715994477272\n",
      "Iteration 16319, Loss: 0.0539889931678772\n",
      "Iteration 16320, Loss: 0.054171450436115265\n",
      "Iteration 16321, Loss: 0.053989142179489136\n",
      "Iteration 16322, Loss: 0.05417129397392273\n",
      "Iteration 16323, Loss: 0.053989142179489136\n",
      "Iteration 16324, Loss: 0.054171331226825714\n",
      "Iteration 16325, Loss: 0.0539892241358757\n",
      "Iteration 16326, Loss: 0.05417133495211601\n",
      "Iteration 16327, Loss: 0.05398910492658615\n",
      "Iteration 16328, Loss: 0.054171573370695114\n",
      "Iteration 16329, Loss: 0.05398885905742645\n",
      "Iteration 16330, Loss: 0.054171763360500336\n",
      "Iteration 16331, Loss: 0.053988855332136154\n",
      "Iteration 16332, Loss: 0.05417180061340332\n",
      "Iteration 16333, Loss: 0.05398885905742645\n",
      "Iteration 16334, Loss: 0.05417168140411377\n",
      "Iteration 16335, Loss: 0.053989022970199585\n",
      "Iteration 16336, Loss: 0.05417133867740631\n",
      "Iteration 16337, Loss: 0.05398913845419884\n",
      "Iteration 16338, Loss: 0.054171375930309296\n",
      "Iteration 16339, Loss: 0.05398917943239212\n",
      "Iteration 16340, Loss: 0.05417133495211601\n",
      "Iteration 16341, Loss: 0.053989212960004807\n",
      "Iteration 16342, Loss: 0.05417145416140556\n",
      "Iteration 16343, Loss: 0.05398906394839287\n",
      "Iteration 16344, Loss: 0.054171495139598846\n",
      "Iteration 16345, Loss: 0.05398906394839287\n",
      "Iteration 16346, Loss: 0.0541716143488884\n",
      "Iteration 16347, Loss: 0.05398883670568466\n",
      "Iteration 16348, Loss: 0.05417173355817795\n",
      "Iteration 16349, Loss: 0.053988825529813766\n",
      "Iteration 16350, Loss: 0.05417165160179138\n",
      "Iteration 16351, Loss: 0.05398859828710556\n",
      "Iteration 16352, Loss: 0.05417153239250183\n",
      "Iteration 16353, Loss: 0.05398876219987869\n",
      "Iteration 16354, Loss: 0.05417145416140556\n",
      "Iteration 16355, Loss: 0.05398903042078018\n",
      "Iteration 16356, Loss: 0.054171375930309296\n",
      "Iteration 16357, Loss: 0.05398917943239212\n",
      "Iteration 16358, Loss: 0.05417141318321228\n",
      "Iteration 16359, Loss: 0.05398906394839287\n",
      "Iteration 16360, Loss: 0.054171495139598846\n",
      "Iteration 16361, Loss: 0.05398906394839287\n",
      "Iteration 16362, Loss: 0.054171573370695114\n",
      "Iteration 16363, Loss: 0.05398894473910332\n",
      "Iteration 16364, Loss: 0.05417165905237198\n",
      "Iteration 16365, Loss: 0.05398871749639511\n",
      "Iteration 16366, Loss: 0.05417166277766228\n",
      "Iteration 16367, Loss: 0.053988706320524216\n",
      "Iteration 16368, Loss: 0.05417177081108093\n",
      "Iteration 16369, Loss: 0.05398867651820183\n",
      "Iteration 16370, Loss: 0.0541716143488884\n",
      "Iteration 16371, Loss: 0.053988754749298096\n",
      "Iteration 16372, Loss: 0.05417153984308243\n",
      "Iteration 16373, Loss: 0.05398883670568466\n",
      "Iteration 16374, Loss: 0.054171305149793625\n",
      "Iteration 16375, Loss: 0.053988873958587646\n",
      "Iteration 16376, Loss: 0.05417133495211601\n",
      "Iteration 16377, Loss: 0.053989142179489136\n",
      "Iteration 16378, Loss: 0.054171375930309296\n",
      "Iteration 16379, Loss: 0.05398925393819809\n",
      "Iteration 16380, Loss: 0.054171450436115265\n",
      "Iteration 16381, Loss: 0.05398906394839287\n",
      "Iteration 16382, Loss: 0.05417153984308243\n",
      "Iteration 16383, Loss: 0.053988825529813766\n",
      "Iteration 16384, Loss: 0.054171621799468994\n",
      "Iteration 16385, Loss: 0.053988706320524216\n",
      "Iteration 16386, Loss: 0.054171811789274216\n",
      "Iteration 16387, Loss: 0.053988706320524216\n",
      "Iteration 16388, Loss: 0.054171767085790634\n",
      "Iteration 16389, Loss: 0.05398879572749138\n",
      "Iteration 16390, Loss: 0.05417153239250183\n",
      "Iteration 16391, Loss: 0.053988903760910034\n",
      "Iteration 16392, Loss: 0.054171305149793625\n",
      "Iteration 16393, Loss: 0.05398906394839287\n",
      "Iteration 16394, Loss: 0.054171305149793625\n",
      "Iteration 16395, Loss: 0.0539889931678772\n",
      "Iteration 16396, Loss: 0.05417134612798691\n",
      "Iteration 16397, Loss: 0.0539889857172966\n",
      "Iteration 16398, Loss: 0.054171424359083176\n",
      "Iteration 16399, Loss: 0.05398827791213989\n",
      "Iteration 16400, Loss: 0.054171573370695114\n",
      "Iteration 16401, Loss: 0.05398816615343094\n",
      "Iteration 16402, Loss: 0.05417099595069885\n",
      "Iteration 16403, Loss: 0.053987935185432434\n",
      "Iteration 16404, Loss: 0.054170962423086166\n",
      "Iteration 16405, Loss: 0.053988050669431686\n",
      "Iteration 16406, Loss: 0.05417102575302124\n",
      "Iteration 16407, Loss: 0.05398809164762497\n",
      "Iteration 16408, Loss: 0.054170869290828705\n",
      "Iteration 16409, Loss: 0.053988173604011536\n",
      "Iteration 16410, Loss: 0.05417071282863617\n",
      "Iteration 16411, Loss: 0.05398839712142944\n",
      "Iteration 16412, Loss: 0.05417056381702423\n",
      "Iteration 16413, Loss: 0.05398847907781601\n",
      "Iteration 16414, Loss: 0.05417067930102348\n",
      "Iteration 16415, Loss: 0.05398843437433243\n",
      "Iteration 16416, Loss: 0.05417075753211975\n",
      "Iteration 16417, Loss: 0.05398824065923691\n",
      "Iteration 16418, Loss: 0.054170917719602585\n",
      "Iteration 16419, Loss: 0.05398812144994736\n",
      "Iteration 16420, Loss: 0.0541708841919899\n",
      "Iteration 16421, Loss: 0.0539880096912384\n",
      "Iteration 16422, Loss: 0.054170917719602585\n",
      "Iteration 16423, Loss: 0.05398809164762497\n",
      "Iteration 16424, Loss: 0.054170917719602585\n",
      "Iteration 16425, Loss: 0.05398806184530258\n",
      "Iteration 16426, Loss: 0.05417083948850632\n",
      "Iteration 16427, Loss: 0.053988128900527954\n",
      "Iteration 16428, Loss: 0.05417083948850632\n",
      "Iteration 16429, Loss: 0.05398821085691452\n",
      "Iteration 16430, Loss: 0.05417076498270035\n",
      "Iteration 16431, Loss: 0.053988248109817505\n",
      "Iteration 16432, Loss: 0.05417075753211975\n",
      "Iteration 16433, Loss: 0.053988248109817505\n",
      "Iteration 16434, Loss: 0.054170798510313034\n",
      "Iteration 16435, Loss: 0.05398827791213989\n",
      "Iteration 16436, Loss: 0.054170917719602585\n",
      "Iteration 16437, Loss: 0.053988128900527954\n",
      "Iteration 16438, Loss: 0.05417106673121452\n",
      "Iteration 16439, Loss: 0.05398797243833542\n",
      "Iteration 16440, Loss: 0.0541711151599884\n",
      "Iteration 16441, Loss: 0.05398804694414139\n",
      "Iteration 16442, Loss: 0.05417102575302124\n",
      "Iteration 16443, Loss: 0.053988050669431686\n",
      "Iteration 16444, Loss: 0.054170917719602585\n",
      "Iteration 16445, Loss: 0.05398820340633392\n",
      "Iteration 16446, Loss: 0.054170720279216766\n",
      "Iteration 16447, Loss: 0.05398835986852646\n",
      "Iteration 16448, Loss: 0.054170530289411545\n",
      "Iteration 16449, Loss: 0.05398833006620407\n",
      "Iteration 16450, Loss: 0.05417075753211975\n",
      "Iteration 16451, Loss: 0.05398839712142944\n",
      "Iteration 16452, Loss: 0.05417082831263542\n",
      "Iteration 16453, Loss: 0.053988199681043625\n",
      "Iteration 16454, Loss: 0.05417083948850632\n",
      "Iteration 16455, Loss: 0.05398809164762497\n",
      "Iteration 16456, Loss: 0.05417095869779587\n",
      "Iteration 16457, Loss: 0.053988248109817505\n",
      "Iteration 16458, Loss: 0.05417075753211975\n",
      "Iteration 16459, Loss: 0.053988248109817505\n",
      "Iteration 16460, Loss: 0.05417075753211975\n",
      "Iteration 16461, Loss: 0.05398828908801079\n",
      "Iteration 16462, Loss: 0.05417075753211975\n",
      "Iteration 16463, Loss: 0.05398827791213989\n",
      "Iteration 16464, Loss: 0.05417083948850632\n",
      "Iteration 16465, Loss: 0.053988318890333176\n",
      "Iteration 16466, Loss: 0.054170873016119\n",
      "Iteration 16467, Loss: 0.053988199681043625\n",
      "Iteration 16468, Loss: 0.054170913994312286\n",
      "Iteration 16469, Loss: 0.05398816987872124\n",
      "Iteration 16470, Loss: 0.0541708767414093\n",
      "Iteration 16471, Loss: 0.05398816987872124\n",
      "Iteration 16472, Loss: 0.05417075753211975\n",
      "Iteration 16473, Loss: 0.053988318890333176\n",
      "Iteration 16474, Loss: 0.054170724004507065\n",
      "Iteration 16475, Loss: 0.053988248109817505\n",
      "Iteration 16476, Loss: 0.054170873016119\n",
      "Iteration 16477, Loss: 0.05398828908801079\n",
      "Iteration 16478, Loss: 0.054170720279216766\n",
      "Iteration 16479, Loss: 0.053988318890333176\n",
      "Iteration 16480, Loss: 0.054170604795217514\n",
      "Iteration 16481, Loss: 0.05398828908801079\n",
      "Iteration 16482, Loss: 0.0541706383228302\n",
      "Iteration 16483, Loss: 0.05398833006620407\n",
      "Iteration 16484, Loss: 0.054170530289411545\n",
      "Iteration 16485, Loss: 0.053988367319107056\n",
      "Iteration 16486, Loss: 0.0541706383228302\n",
      "Iteration 16487, Loss: 0.05398840829730034\n",
      "Iteration 16488, Loss: 0.05417067930102348\n",
      "Iteration 16489, Loss: 0.05398839712142944\n",
      "Iteration 16490, Loss: 0.054170720279216766\n",
      "Iteration 16491, Loss: 0.05398828908801079\n",
      "Iteration 16492, Loss: 0.054170794785022736\n",
      "Iteration 16493, Loss: 0.05398835986852646\n",
      "Iteration 16494, Loss: 0.05417067930102348\n",
      "Iteration 16495, Loss: 0.05398833006620407\n",
      "Iteration 16496, Loss: 0.05417075753211975\n",
      "Iteration 16497, Loss: 0.05398833006620407\n",
      "Iteration 16498, Loss: 0.05417075380682945\n",
      "Iteration 16499, Loss: 0.053988367319107056\n",
      "Iteration 16500, Loss: 0.05417076498270035\n",
      "Iteration 16501, Loss: 0.05398827791213989\n",
      "Iteration 16502, Loss: 0.054170913994312286\n",
      "Iteration 16503, Loss: 0.05398816615343094\n",
      "Iteration 16504, Loss: 0.05417095869779587\n",
      "Iteration 16505, Loss: 0.053988050669431686\n",
      "Iteration 16506, Loss: 0.054171036928892136\n",
      "Iteration 16507, Loss: 0.05398812144994736\n",
      "Iteration 16508, Loss: 0.054171040654182434\n",
      "Iteration 16509, Loss: 0.05398789048194885\n",
      "Iteration 16510, Loss: 0.05417106673121452\n",
      "Iteration 16511, Loss: 0.053988050669431686\n",
      "Iteration 16512, Loss: 0.05417090654373169\n",
      "Iteration 16513, Loss: 0.05398813635110855\n",
      "Iteration 16514, Loss: 0.054170601069927216\n",
      "Iteration 16515, Loss: 0.05398840829730034\n",
      "Iteration 16516, Loss: 0.05417051166296005\n",
      "Iteration 16517, Loss: 0.05398844927549362\n",
      "Iteration 16518, Loss: 0.05417044088244438\n",
      "Iteration 16519, Loss: 0.053988561034202576\n",
      "Iteration 16520, Loss: 0.05417048558592796\n",
      "Iteration 16521, Loss: 0.053988635540008545\n",
      "Iteration 16522, Loss: 0.05417067930102348\n",
      "Iteration 16523, Loss: 0.05398847907781601\n",
      "Iteration 16524, Loss: 0.05417075753211975\n",
      "Iteration 16525, Loss: 0.05398809164762497\n",
      "Iteration 16526, Loss: 0.05417107045650482\n",
      "Iteration 16527, Loss: 0.05398797243833542\n",
      "Iteration 16528, Loss: 0.054171156138181686\n",
      "Iteration 16529, Loss: 0.05398797243833542\n",
      "Iteration 16530, Loss: 0.05417107790708542\n",
      "Iteration 16531, Loss: 0.053987931460142136\n",
      "Iteration 16532, Loss: 0.05417103320360184\n",
      "Iteration 16533, Loss: 0.05398812144994736\n",
      "Iteration 16534, Loss: 0.05417083948850632\n",
      "Iteration 16535, Loss: 0.05398828908801079\n",
      "Iteration 16536, Loss: 0.05417066812515259\n",
      "Iteration 16537, Loss: 0.053988367319107056\n",
      "Iteration 16538, Loss: 0.05417056009173393\n",
      "Iteration 16539, Loss: 0.05398844927549362\n",
      "Iteration 16540, Loss: 0.054170601069927216\n",
      "Iteration 16541, Loss: 0.05398839712142944\n",
      "Iteration 16542, Loss: 0.05417075753211975\n",
      "Iteration 16543, Loss: 0.05398835986852646\n",
      "Iteration 16544, Loss: 0.05417095869779587\n",
      "Iteration 16545, Loss: 0.053988017141819\n",
      "Iteration 16546, Loss: 0.05417103320360184\n",
      "Iteration 16547, Loss: 0.053988050669431686\n",
      "Iteration 16548, Loss: 0.05417107790708542\n",
      "Iteration 16549, Loss: 0.05398797243833542\n",
      "Iteration 16550, Loss: 0.05417102575302124\n",
      "Iteration 16551, Loss: 0.053988125175237656\n",
      "Iteration 16552, Loss: 0.05417094752192497\n",
      "Iteration 16553, Loss: 0.05398824065923691\n",
      "Iteration 16554, Loss: 0.05417075753211975\n",
      "Iteration 16555, Loss: 0.0539882555603981\n",
      "Iteration 16556, Loss: 0.054170720279216766\n",
      "Iteration 16557, Loss: 0.0539882555603981\n",
      "Iteration 16558, Loss: 0.054170720279216766\n",
      "Iteration 16559, Loss: 0.053988318890333176\n",
      "Iteration 16560, Loss: 0.05417095124721527\n",
      "Iteration 16561, Loss: 0.053988199681043625\n",
      "Iteration 16562, Loss: 0.05417099595069885\n",
      "Iteration 16563, Loss: 0.05398808419704437\n",
      "Iteration 16564, Loss: 0.054170917719602585\n",
      "Iteration 16565, Loss: 0.05398821085691452\n",
      "Iteration 16566, Loss: 0.054170869290828705\n",
      "Iteration 16567, Loss: 0.053988248109817505\n",
      "Iteration 16568, Loss: 0.054170601069927216\n",
      "Iteration 16569, Loss: 0.05398833006620407\n",
      "Iteration 16570, Loss: 0.05417070910334587\n",
      "Iteration 16571, Loss: 0.05398848280310631\n",
      "Iteration 16572, Loss: 0.05417051911354065\n",
      "Iteration 16573, Loss: 0.05398855730891228\n",
      "Iteration 16574, Loss: 0.05417048931121826\n",
      "Iteration 16575, Loss: 0.053988512605428696\n",
      "Iteration 16576, Loss: 0.05417075753211975\n",
      "Iteration 16577, Loss: 0.053988318890333176\n",
      "Iteration 16578, Loss: 0.05417090654373169\n",
      "Iteration 16579, Loss: 0.05398808419704437\n",
      "Iteration 16580, Loss: 0.05417095124721527\n",
      "Iteration 16581, Loss: 0.05398809164762497\n",
      "Iteration 16582, Loss: 0.054170917719602585\n",
      "Iteration 16583, Loss: 0.053988128900527954\n",
      "Iteration 16584, Loss: 0.0541708767414093\n",
      "Iteration 16585, Loss: 0.05398827791213989\n",
      "Iteration 16586, Loss: 0.05417067930102348\n",
      "Iteration 16587, Loss: 0.053988438099622726\n",
      "Iteration 16588, Loss: 0.05417067930102348\n",
      "Iteration 16589, Loss: 0.053988367319107056\n",
      "Iteration 16590, Loss: 0.0541706383228302\n",
      "Iteration 16591, Loss: 0.053988389670848846\n",
      "Iteration 16592, Loss: 0.05417083948850632\n",
      "Iteration 16593, Loss: 0.053988199681043625\n",
      "Iteration 16594, Loss: 0.054170992225408554\n",
      "Iteration 16595, Loss: 0.05398808419704437\n",
      "Iteration 16596, Loss: 0.05417095869779587\n",
      "Iteration 16597, Loss: 0.0539880096912384\n",
      "Iteration 16598, Loss: 0.054171107709407806\n",
      "Iteration 16599, Loss: 0.05398797616362572\n",
      "Iteration 16600, Loss: 0.0541708767414093\n",
      "Iteration 16601, Loss: 0.05398808419704437\n",
      "Iteration 16602, Loss: 0.05417090654373169\n",
      "Iteration 16603, Loss: 0.053988248109817505\n",
      "Iteration 16604, Loss: 0.05417071282863617\n",
      "Iteration 16605, Loss: 0.053988322615623474\n",
      "Iteration 16606, Loss: 0.0541706383228302\n",
      "Iteration 16607, Loss: 0.053988438099622726\n",
      "Iteration 16608, Loss: 0.05417056009173393\n",
      "Iteration 16609, Loss: 0.05398855730891228\n",
      "Iteration 16610, Loss: 0.0541706383228302\n",
      "Iteration 16611, Loss: 0.05398839712142944\n",
      "Iteration 16612, Loss: 0.054170720279216766\n",
      "Iteration 16613, Loss: 0.05398835241794586\n",
      "Iteration 16614, Loss: 0.05417080223560333\n",
      "Iteration 16615, Loss: 0.05398816615343094\n",
      "Iteration 16616, Loss: 0.054170917719602585\n",
      "Iteration 16617, Loss: 0.05398808419704437\n",
      "Iteration 16618, Loss: 0.054170843213796616\n",
      "Iteration 16619, Loss: 0.05398815870285034\n",
      "Iteration 16620, Loss: 0.05417075753211975\n",
      "Iteration 16621, Loss: 0.05398828908801079\n",
      "Iteration 16622, Loss: 0.05417067930102348\n",
      "Iteration 16623, Loss: 0.05398839712142944\n",
      "Iteration 16624, Loss: 0.0541706383228302\n",
      "Iteration 16625, Loss: 0.053988367319107056\n",
      "Iteration 16626, Loss: 0.05417056009173393\n",
      "Iteration 16627, Loss: 0.053988516330718994\n",
      "Iteration 16628, Loss: 0.05417051911354065\n",
      "Iteration 16629, Loss: 0.05398852378129959\n",
      "Iteration 16630, Loss: 0.05417048931121826\n",
      "Iteration 16631, Loss: 0.053988512605428696\n",
      "Iteration 16632, Loss: 0.054170798510313034\n",
      "Iteration 16633, Loss: 0.053988322615623474\n",
      "Iteration 16634, Loss: 0.05417083948850632\n",
      "Iteration 16635, Loss: 0.05398812144994736\n",
      "Iteration 16636, Loss: 0.05417095124721527\n",
      "Iteration 16637, Loss: 0.053988050669431686\n",
      "Iteration 16638, Loss: 0.054170917719602585\n",
      "Iteration 16639, Loss: 0.0539880096912384\n",
      "Iteration 16640, Loss: 0.05417083948850632\n",
      "Iteration 16641, Loss: 0.05398824065923691\n",
      "Iteration 16642, Loss: 0.054170601069927216\n",
      "Iteration 16643, Loss: 0.053988438099622726\n",
      "Iteration 16644, Loss: 0.0541706383228302\n",
      "Iteration 16645, Loss: 0.053988441824913025\n",
      "Iteration 16646, Loss: 0.054170604795217514\n",
      "Iteration 16647, Loss: 0.05398843437433243\n",
      "Iteration 16648, Loss: 0.054170724004507065\n",
      "Iteration 16649, Loss: 0.05398821085691452\n",
      "Iteration 16650, Loss: 0.05417075753211975\n",
      "Iteration 16651, Loss: 0.05398828908801079\n",
      "Iteration 16652, Loss: 0.05417067930102348\n",
      "Iteration 16653, Loss: 0.05398828908801079\n",
      "Iteration 16654, Loss: 0.05417067930102348\n",
      "Iteration 16655, Loss: 0.0539882555603981\n",
      "Iteration 16656, Loss: 0.054170720279216766\n",
      "Iteration 16657, Loss: 0.053988367319107056\n",
      "Iteration 16658, Loss: 0.0541706383228302\n",
      "Iteration 16659, Loss: 0.05398835986852646\n",
      "Iteration 16660, Loss: 0.05417056381702423\n",
      "Iteration 16661, Loss: 0.053988441824913025\n",
      "Iteration 16662, Loss: 0.054170604795217514\n",
      "Iteration 16663, Loss: 0.05398833006620407\n",
      "Iteration 16664, Loss: 0.054170720279216766\n",
      "Iteration 16665, Loss: 0.05398828908801079\n",
      "Iteration 16666, Loss: 0.05417067930102348\n",
      "Iteration 16667, Loss: 0.05398833006620407\n",
      "Iteration 16668, Loss: 0.054170720279216766\n",
      "Iteration 16669, Loss: 0.05398833006620407\n",
      "Iteration 16670, Loss: 0.054170720279216766\n",
      "Iteration 16671, Loss: 0.05398828908801079\n",
      "Iteration 16672, Loss: 0.05417067930102348\n",
      "Iteration 16673, Loss: 0.05398828908801079\n",
      "Iteration 16674, Loss: 0.05417067930102348\n",
      "Iteration 16675, Loss: 0.05398828908801079\n",
      "Iteration 16676, Loss: 0.054170720279216766\n",
      "Iteration 16677, Loss: 0.053988248109817505\n",
      "Iteration 16678, Loss: 0.05417069047689438\n",
      "Iteration 16679, Loss: 0.053988248109817505\n",
      "Iteration 16680, Loss: 0.05417075753211975\n",
      "Iteration 16681, Loss: 0.05398821085691452\n",
      "Iteration 16682, Loss: 0.0541708767414093\n",
      "Iteration 16683, Loss: 0.053988050669431686\n",
      "Iteration 16684, Loss: 0.054170917719602585\n",
      "Iteration 16685, Loss: 0.053988125175237656\n",
      "Iteration 16686, Loss: 0.054170843213796616\n",
      "Iteration 16687, Loss: 0.05398809164762497\n",
      "Iteration 16688, Loss: 0.054170873016119\n",
      "Iteration 16689, Loss: 0.053988248109817505\n",
      "Iteration 16690, Loss: 0.05417071282863617\n",
      "Iteration 16691, Loss: 0.05398835986852646\n",
      "Iteration 16692, Loss: 0.0541706345975399\n",
      "Iteration 16693, Loss: 0.05398840457201004\n",
      "Iteration 16694, Loss: 0.05417056381702423\n",
      "Iteration 16695, Loss: 0.053988516330718994\n",
      "Iteration 16696, Loss: 0.054170601069927216\n",
      "Iteration 16697, Loss: 0.05398855358362198\n",
      "Iteration 16698, Loss: 0.05417067930102348\n",
      "Iteration 16699, Loss: 0.05398839712142944\n",
      "Iteration 16700, Loss: 0.054170649498701096\n",
      "Iteration 16701, Loss: 0.05398824065923691\n",
      "Iteration 16702, Loss: 0.0541708767414093\n",
      "Iteration 16703, Loss: 0.05398797243833542\n",
      "Iteration 16704, Loss: 0.0541711151599884\n",
      "Iteration 16705, Loss: 0.0539880096912384\n",
      "Iteration 16706, Loss: 0.05417099595069885\n",
      "Iteration 16707, Loss: 0.05398797616362572\n",
      "Iteration 16708, Loss: 0.05417095124721527\n",
      "Iteration 16709, Loss: 0.053988128900527954\n",
      "Iteration 16710, Loss: 0.054170750081539154\n",
      "Iteration 16711, Loss: 0.05398833006620407\n",
      "Iteration 16712, Loss: 0.0541706308722496\n",
      "Iteration 16713, Loss: 0.05398855730891228\n",
      "Iteration 16714, Loss: 0.054170481860637665\n",
      "Iteration 16715, Loss: 0.05398855730891228\n",
      "Iteration 16716, Loss: 0.054170720279216766\n",
      "Iteration 16717, Loss: 0.053988438099622726\n",
      "Iteration 16718, Loss: 0.054170798510313034\n",
      "Iteration 16719, Loss: 0.05398824065923691\n",
      "Iteration 16720, Loss: 0.054171040654182434\n",
      "Iteration 16721, Loss: 0.053988002240657806\n",
      "Iteration 16722, Loss: 0.054171156138181686\n",
      "Iteration 16723, Loss: 0.05398780107498169\n",
      "Iteration 16724, Loss: 0.05417119711637497\n",
      "Iteration 16725, Loss: 0.0539877787232399\n",
      "Iteration 16726, Loss: 0.054171156138181686\n",
      "Iteration 16727, Loss: 0.053987856954336166\n",
      "Iteration 16728, Loss: 0.054170917719602585\n",
      "Iteration 16729, Loss: 0.053988128900527954\n",
      "Iteration 16730, Loss: 0.0541706383228302\n",
      "Iteration 16731, Loss: 0.05398848280310631\n",
      "Iteration 16732, Loss: 0.054170362651348114\n",
      "Iteration 16733, Loss: 0.05398860573768616\n",
      "Iteration 16734, Loss: 0.054170407354831696\n",
      "Iteration 16735, Loss: 0.05398859828710556\n",
      "Iteration 16736, Loss: 0.05417078733444214\n",
      "Iteration 16737, Loss: 0.05398839712142944\n",
      "Iteration 16738, Loss: 0.054170798510313034\n",
      "Iteration 16739, Loss: 0.05398824065923691\n",
      "Iteration 16740, Loss: 0.054171107709407806\n",
      "Iteration 16741, Loss: 0.0539880096912384\n",
      "Iteration 16742, Loss: 0.05417126417160034\n",
      "Iteration 16743, Loss: 0.0539880096912384\n",
      "Iteration 16744, Loss: 0.05417107790708542\n",
      "Iteration 16745, Loss: 0.05398785322904587\n",
      "Iteration 16746, Loss: 0.05417092889547348\n",
      "Iteration 16747, Loss: 0.05398789793252945\n",
      "Iteration 16748, Loss: 0.054170873016119\n",
      "Iteration 16749, Loss: 0.05398821085691452\n",
      "Iteration 16750, Loss: 0.05417069047689438\n",
      "Iteration 16751, Loss: 0.05398828908801079\n",
      "Iteration 16752, Loss: 0.05417069047689438\n",
      "Iteration 16753, Loss: 0.053988248109817505\n",
      "Iteration 16754, Loss: 0.05417075753211975\n",
      "Iteration 16755, Loss: 0.05398820340633392\n",
      "Iteration 16756, Loss: 0.054170798510313034\n",
      "Iteration 16757, Loss: 0.05398821085691452\n",
      "Iteration 16758, Loss: 0.05417076498270035\n",
      "Iteration 16759, Loss: 0.05398821085691452\n",
      "Iteration 16760, Loss: 0.05417075753211975\n",
      "Iteration 16761, Loss: 0.05398872494697571\n",
      "Iteration 16762, Loss: 0.054170601069927216\n",
      "Iteration 16763, Loss: 0.053988806903362274\n",
      "Iteration 16764, Loss: 0.054170601069927216\n",
      "Iteration 16765, Loss: 0.05398888513445854\n",
      "Iteration 16766, Loss: 0.05417107790708542\n",
      "Iteration 16767, Loss: 0.05398896336555481\n",
      "Iteration 16768, Loss: 0.05417100340127945\n",
      "Iteration 16769, Loss: 0.05398889631032944\n",
      "Iteration 16770, Loss: 0.05417118966579437\n",
      "Iteration 16771, Loss: 0.05398884043097496\n",
      "Iteration 16772, Loss: 0.05417127162218094\n",
      "Iteration 16773, Loss: 0.053988754749298096\n",
      "Iteration 16774, Loss: 0.0541713647544384\n",
      "Iteration 16775, Loss: 0.05398871749639511\n",
      "Iteration 16776, Loss: 0.054171428084373474\n",
      "Iteration 16777, Loss: 0.05398852750658989\n",
      "Iteration 16778, Loss: 0.05417139455676079\n",
      "Iteration 16779, Loss: 0.05398860573768616\n",
      "Iteration 16780, Loss: 0.0541713610291481\n",
      "Iteration 16781, Loss: 0.05398864299058914\n",
      "Iteration 16782, Loss: 0.05417127534747124\n",
      "Iteration 16783, Loss: 0.05398868769407272\n",
      "Iteration 16784, Loss: 0.054171234369277954\n",
      "Iteration 16785, Loss: 0.053988806903362274\n",
      "Iteration 16786, Loss: 0.0541711151599884\n",
      "Iteration 16787, Loss: 0.05398888885974884\n",
      "Iteration 16788, Loss: 0.0541711151599884\n",
      "Iteration 16789, Loss: 0.05398872494697571\n",
      "Iteration 16790, Loss: 0.054171156138181686\n",
      "Iteration 16791, Loss: 0.05398872494697571\n",
      "Iteration 16792, Loss: 0.054171156138181686\n",
      "Iteration 16793, Loss: 0.05398872494697571\n",
      "Iteration 16794, Loss: 0.05417119711637497\n",
      "Iteration 16795, Loss: 0.053988806903362274\n",
      "Iteration 16796, Loss: 0.054171159863471985\n",
      "Iteration 16797, Loss: 0.05398876592516899\n",
      "Iteration 16798, Loss: 0.054171156138181686\n",
      "Iteration 16799, Loss: 0.053988806903362274\n",
      "Iteration 16800, Loss: 0.05417119711637497\n",
      "Iteration 16801, Loss: 0.053988806903362274\n",
      "Iteration 16802, Loss: 0.05417119711637497\n",
      "Iteration 16803, Loss: 0.05398876592516899\n",
      "Iteration 16804, Loss: 0.05417119711637497\n",
      "Iteration 16805, Loss: 0.053988806903362274\n",
      "Iteration 16806, Loss: 0.054171159863471985\n",
      "Iteration 16807, Loss: 0.053988806903362274\n",
      "Iteration 16808, Loss: 0.05417120084166527\n",
      "Iteration 16809, Loss: 0.053988806903362274\n",
      "Iteration 16810, Loss: 0.054171156138181686\n",
      "Iteration 16811, Loss: 0.05398876592516899\n",
      "Iteration 16812, Loss: 0.0541711151599884\n",
      "Iteration 16813, Loss: 0.053988851606845856\n",
      "Iteration 16814, Loss: 0.05417107790708542\n",
      "Iteration 16815, Loss: 0.05398888885974884\n",
      "Iteration 16816, Loss: 0.054170962423086166\n",
      "Iteration 16817, Loss: 0.05398900434374809\n",
      "Iteration 16818, Loss: 0.05417099595069885\n",
      "Iteration 16819, Loss: 0.05398907512426376\n",
      "Iteration 16820, Loss: 0.05417100712656975\n",
      "Iteration 16821, Loss: 0.05398884415626526\n",
      "Iteration 16822, Loss: 0.054171353578567505\n",
      "Iteration 16823, Loss: 0.05398864671587944\n",
      "Iteration 16824, Loss: 0.05417155474424362\n",
      "Iteration 16825, Loss: 0.05398852378129959\n",
      "Iteration 16826, Loss: 0.05417182296514511\n",
      "Iteration 16827, Loss: 0.053988367319107056\n",
      "Iteration 16828, Loss: 0.05417167395353317\n",
      "Iteration 16829, Loss: 0.05398840829730034\n",
      "Iteration 16830, Loss: 0.05417151376605034\n",
      "Iteration 16831, Loss: 0.05398844927549362\n",
      "Iteration 16832, Loss: 0.05417132005095482\n",
      "Iteration 16833, Loss: 0.05398868769407272\n",
      "Iteration 16834, Loss: 0.05417124181985855\n",
      "Iteration 16835, Loss: 0.05398876592516899\n",
      "Iteration 16836, Loss: 0.054171156138181686\n",
      "Iteration 16837, Loss: 0.05398884415626526\n",
      "Iteration 16838, Loss: 0.05417108163237572\n",
      "Iteration 16839, Loss: 0.05398891493678093\n",
      "Iteration 16840, Loss: 0.05417127162218094\n",
      "Iteration 16841, Loss: 0.05398876219987869\n",
      "Iteration 16842, Loss: 0.054171424359083176\n",
      "Iteration 16843, Loss: 0.05398864299058914\n",
      "Iteration 16844, Loss: 0.05417146906256676\n",
      "Iteration 16845, Loss: 0.05398856848478317\n",
      "Iteration 16846, Loss: 0.05417151004076004\n",
      "Iteration 16847, Loss: 0.05398853123188019\n",
      "Iteration 16848, Loss: 0.054171353578567505\n",
      "Iteration 16849, Loss: 0.05398856848478317\n",
      "Iteration 16850, Loss: 0.05417131632566452\n",
      "Iteration 16851, Loss: 0.05398872494697571\n",
      "Iteration 16852, Loss: 0.05417119711637497\n",
      "Iteration 16853, Loss: 0.053988806903362274\n",
      "Iteration 16854, Loss: 0.054171122610569\n",
      "Iteration 16855, Loss: 0.05398888513445854\n",
      "Iteration 16856, Loss: 0.054171156138181686\n",
      "Iteration 16857, Loss: 0.053988806903362274\n",
      "Iteration 16858, Loss: 0.054171230643987656\n",
      "Iteration 16859, Loss: 0.053988873958587646\n",
      "Iteration 16860, Loss: 0.0541711300611496\n",
      "Iteration 16861, Loss: 0.05398864299058914\n",
      "Iteration 16862, Loss: 0.05417143553495407\n",
      "Iteration 16863, Loss: 0.05398856848478317\n",
      "Iteration 16864, Loss: 0.05417139455676079\n",
      "Iteration 16865, Loss: 0.05398860573768616\n",
      "Iteration 16866, Loss: 0.05417127534747124\n",
      "Iteration 16867, Loss: 0.05398879572749138\n",
      "Iteration 16868, Loss: 0.05417115241289139\n",
      "Iteration 16869, Loss: 0.053988926112651825\n",
      "Iteration 16870, Loss: 0.05417100340127945\n",
      "Iteration 16871, Loss: 0.05398892983794212\n",
      "Iteration 16872, Loss: 0.05417107790708542\n",
      "Iteration 16873, Loss: 0.05398895964026451\n",
      "Iteration 16874, Loss: 0.05417131632566452\n",
      "Iteration 16875, Loss: 0.053988635540008545\n",
      "Iteration 16876, Loss: 0.05417148396372795\n",
      "Iteration 16877, Loss: 0.05398840457201004\n",
      "Iteration 16878, Loss: 0.05417172238230705\n",
      "Iteration 16879, Loss: 0.053988128900527954\n",
      "Iteration 16880, Loss: 0.05417183041572571\n",
      "Iteration 16881, Loss: 0.05398821830749512\n",
      "Iteration 16882, Loss: 0.05417162925004959\n",
      "Iteration 16883, Loss: 0.05398841202259064\n",
      "Iteration 16884, Loss: 0.05417134612798691\n",
      "Iteration 16885, Loss: 0.05398872494697571\n",
      "Iteration 16886, Loss: 0.054171185940504074\n",
      "Iteration 16887, Loss: 0.05398895964026451\n",
      "Iteration 16888, Loss: 0.05417100340127945\n",
      "Iteration 16889, Loss: 0.05398907512426376\n",
      "Iteration 16890, Loss: 0.05417115241289139\n",
      "Iteration 16891, Loss: 0.05398903042078018\n",
      "Iteration 16892, Loss: 0.05417108163237572\n",
      "Iteration 16893, Loss: 0.05398884043097496\n",
      "Iteration 16894, Loss: 0.054171234369277954\n",
      "Iteration 16895, Loss: 0.05398872122168541\n",
      "Iteration 16896, Loss: 0.05417131632566452\n",
      "Iteration 16897, Loss: 0.05398864671587944\n",
      "Iteration 16898, Loss: 0.054171353578567505\n",
      "Iteration 16899, Loss: 0.05398861691355705\n",
      "Iteration 16900, Loss: 0.05417124554514885\n",
      "Iteration 16901, Loss: 0.05398869141936302\n",
      "Iteration 16902, Loss: 0.054171159863471985\n",
      "Iteration 16903, Loss: 0.05398876592516899\n",
      "Iteration 16904, Loss: 0.05417119711637497\n",
      "Iteration 16905, Loss: 0.053988806903362274\n",
      "Iteration 16906, Loss: 0.054171040654182434\n",
      "Iteration 16907, Loss: 0.053988926112651825\n",
      "Iteration 16908, Loss: 0.054171040654182434\n",
      "Iteration 16909, Loss: 0.05398907512426376\n",
      "Iteration 16910, Loss: 0.05417107790708542\n",
      "Iteration 16911, Loss: 0.053989000618457794\n",
      "Iteration 16912, Loss: 0.05417107790708542\n",
      "Iteration 16913, Loss: 0.05398891493678093\n",
      "Iteration 16914, Loss: 0.05417127162218094\n",
      "Iteration 16915, Loss: 0.053988806903362274\n",
      "Iteration 16916, Loss: 0.05417132377624512\n",
      "Iteration 16917, Loss: 0.053988635540008545\n",
      "Iteration 16918, Loss: 0.05417156219482422\n",
      "Iteration 16919, Loss: 0.053988322615623474\n",
      "Iteration 16920, Loss: 0.05417168140411377\n",
      "Iteration 16921, Loss: 0.05398821085691452\n",
      "Iteration 16922, Loss: 0.05417171120643616\n",
      "Iteration 16923, Loss: 0.0539882555603981\n",
      "Iteration 16924, Loss: 0.054171472787857056\n",
      "Iteration 16925, Loss: 0.0539884939789772\n",
      "Iteration 16926, Loss: 0.05417119711637497\n",
      "Iteration 16927, Loss: 0.05398876592516899\n",
      "Iteration 16928, Loss: 0.054171040654182434\n",
      "Iteration 16929, Loss: 0.05398888885974884\n",
      "Iteration 16930, Loss: 0.05417100340127945\n",
      "Iteration 16931, Loss: 0.05398900434374809\n",
      "Iteration 16932, Loss: 0.05417100340127945\n",
      "Iteration 16933, Loss: 0.05398891493678093\n",
      "Iteration 16934, Loss: 0.054171185940504074\n",
      "Iteration 16935, Loss: 0.05398884043097496\n",
      "Iteration 16936, Loss: 0.05417127162218094\n",
      "Iteration 16937, Loss: 0.05398876219987869\n",
      "Iteration 16938, Loss: 0.05417139455676079\n",
      "Iteration 16939, Loss: 0.05398868769407272\n",
      "Iteration 16940, Loss: 0.054171353578567505\n",
      "Iteration 16941, Loss: 0.053988486528396606\n",
      "Iteration 16942, Loss: 0.05417151376605034\n",
      "Iteration 16943, Loss: 0.05398852750658989\n",
      "Iteration 16944, Loss: 0.05417143553495407\n",
      "Iteration 16945, Loss: 0.05398845300078392\n",
      "Iteration 16946, Loss: 0.054171398282051086\n",
      "Iteration 16947, Loss: 0.053988538682460785\n",
      "Iteration 16948, Loss: 0.054171204566955566\n",
      "Iteration 16949, Loss: 0.05398868769407272\n",
      "Iteration 16950, Loss: 0.054171085357666016\n",
      "Iteration 16951, Loss: 0.05398883670568466\n",
      "Iteration 16952, Loss: 0.054171156138181686\n",
      "Iteration 16953, Loss: 0.053988926112651825\n",
      "Iteration 16954, Loss: 0.054171036928892136\n",
      "Iteration 16955, Loss: 0.053989045321941376\n",
      "Iteration 16956, Loss: 0.05417095124721527\n",
      "Iteration 16957, Loss: 0.05398900434374809\n",
      "Iteration 16958, Loss: 0.05417095869779587\n",
      "Iteration 16959, Loss: 0.053989119827747345\n",
      "Iteration 16960, Loss: 0.054171156138181686\n",
      "Iteration 16961, Loss: 0.05398895591497421\n",
      "Iteration 16962, Loss: 0.05417104810476303\n",
      "Iteration 16963, Loss: 0.05398876219987869\n",
      "Iteration 16964, Loss: 0.05417127534747124\n",
      "Iteration 16965, Loss: 0.05398860573768616\n",
      "Iteration 16966, Loss: 0.054171543568372726\n",
      "Iteration 16967, Loss: 0.05398852750658989\n",
      "Iteration 16968, Loss: 0.05417143553495407\n",
      "Iteration 16969, Loss: 0.05398790165781975\n",
      "Iteration 16970, Loss: 0.0541713647544384\n",
      "Iteration 16971, Loss: 0.05398789048194885\n",
      "Iteration 16972, Loss: 0.05417204648256302\n",
      "Iteration 16973, Loss: 0.053987737745046616\n",
      "Iteration 16974, Loss: 0.05417191982269287\n",
      "Iteration 16975, Loss: 0.053987935185432434\n",
      "Iteration 16976, Loss: 0.054171644151210785\n",
      "Iteration 16977, Loss: 0.05398833006620407\n",
      "Iteration 16978, Loss: 0.05417139455676079\n",
      "Iteration 16979, Loss: 0.053988680243492126\n",
      "Iteration 16980, Loss: 0.05417104810476303\n",
      "Iteration 16981, Loss: 0.053988926112651825\n",
      "Iteration 16982, Loss: 0.054171036928892136\n",
      "Iteration 16983, Loss: 0.05398884415626526\n",
      "Iteration 16984, Loss: 0.05417100340127945\n",
      "Iteration 16985, Loss: 0.05398868769407272\n",
      "Iteration 16986, Loss: 0.054170966148376465\n",
      "Iteration 16987, Loss: 0.05398857221007347\n",
      "Iteration 16988, Loss: 0.05417105183005333\n",
      "Iteration 16989, Loss: 0.05398860573768616\n",
      "Iteration 16990, Loss: 0.05417117103934288\n",
      "Iteration 16991, Loss: 0.05398845672607422\n",
      "Iteration 16992, Loss: 0.054171204566955566\n",
      "Iteration 16993, Loss: 0.05398845300078392\n",
      "Iteration 16994, Loss: 0.05417124181985855\n",
      "Iteration 16995, Loss: 0.053988486528396606\n",
      "Iteration 16996, Loss: 0.0541713610291481\n",
      "Iteration 16997, Loss: 0.053988635540008545\n",
      "Iteration 16998, Loss: 0.05417124181985855\n",
      "Iteration 16999, Loss: 0.05398860573768616\n",
      "Iteration 17000, Loss: 0.054171204566955566\n",
      "Iteration 17001, Loss: 0.05398860573768616\n",
      "Iteration 17002, Loss: 0.05417120084166527\n",
      "Iteration 17003, Loss: 0.05398860573768616\n",
      "Iteration 17004, Loss: 0.0541711263358593\n",
      "Iteration 17005, Loss: 0.05398867651820183\n",
      "Iteration 17006, Loss: 0.05417120084166527\n",
      "Iteration 17007, Loss: 0.05398864671587944\n",
      "Iteration 17008, Loss: 0.054171234369277954\n",
      "Iteration 17009, Loss: 0.05398864671587944\n",
      "Iteration 17010, Loss: 0.05417119711637497\n",
      "Iteration 17011, Loss: 0.05398868769407272\n",
      "Iteration 17012, Loss: 0.054170966148376465\n",
      "Iteration 17013, Loss: 0.05398876592516899\n",
      "Iteration 17014, Loss: 0.0541708879172802\n",
      "Iteration 17015, Loss: 0.05398881062865257\n",
      "Iteration 17016, Loss: 0.05417099595069885\n",
      "Iteration 17017, Loss: 0.053988873958587646\n",
      "Iteration 17018, Loss: 0.05417093262076378\n",
      "Iteration 17019, Loss: 0.05398872122168541\n",
      "Iteration 17020, Loss: 0.05417105183005333\n",
      "Iteration 17021, Loss: 0.05398856848478317\n",
      "Iteration 17022, Loss: 0.05417127534747124\n",
      "Iteration 17023, Loss: 0.05398856848478317\n",
      "Iteration 17024, Loss: 0.05417119711637497\n",
      "Iteration 17025, Loss: 0.05398879572749138\n",
      "Iteration 17026, Loss: 0.054171040654182434\n",
      "Iteration 17027, Loss: 0.05398884043097496\n",
      "Iteration 17028, Loss: 0.054170962423086166\n",
      "Iteration 17029, Loss: 0.053988926112651825\n",
      "Iteration 17030, Loss: 0.05417080968618393\n",
      "Iteration 17031, Loss: 0.053989000618457794\n",
      "Iteration 17032, Loss: 0.054170768707990646\n",
      "Iteration 17033, Loss: 0.05398895964026451\n",
      "Iteration 17034, Loss: 0.0541708879172802\n",
      "Iteration 17035, Loss: 0.05398884415626526\n",
      "Iteration 17036, Loss: 0.054171085357666016\n",
      "Iteration 17037, Loss: 0.05398856848478317\n",
      "Iteration 17038, Loss: 0.05417139455676079\n",
      "Iteration 17039, Loss: 0.05398837476968765\n",
      "Iteration 17040, Loss: 0.05417139455676079\n",
      "Iteration 17041, Loss: 0.05398830026388168\n",
      "Iteration 17042, Loss: 0.05417131632566452\n",
      "Iteration 17043, Loss: 0.05398856848478317\n",
      "Iteration 17044, Loss: 0.0541711151599884\n",
      "Iteration 17045, Loss: 0.05398868769407272\n",
      "Iteration 17046, Loss: 0.054170966148376465\n",
      "Iteration 17047, Loss: 0.05398895964026451\n",
      "Iteration 17048, Loss: 0.05417099595069885\n",
      "Iteration 17049, Loss: 0.05398876965045929\n",
      "Iteration 17050, Loss: 0.054170891642570496\n",
      "Iteration 17051, Loss: 0.05398868769407272\n",
      "Iteration 17052, Loss: 0.054171085357666016\n",
      "Iteration 17053, Loss: 0.05398860573768616\n",
      "Iteration 17054, Loss: 0.05417124554514885\n",
      "Iteration 17055, Loss: 0.05398840829730034\n",
      "Iteration 17056, Loss: 0.05417129397392273\n",
      "Iteration 17057, Loss: 0.053988367319107056\n",
      "Iteration 17058, Loss: 0.054171472787857056\n",
      "Iteration 17059, Loss: 0.05398830026388168\n",
      "Iteration 17060, Loss: 0.05417132377624512\n",
      "Iteration 17061, Loss: 0.05398860573768616\n",
      "Iteration 17062, Loss: 0.05417104810476303\n",
      "Iteration 17063, Loss: 0.05398864671587944\n",
      "Iteration 17064, Loss: 0.05417104810476303\n",
      "Iteration 17065, Loss: 0.053988732397556305\n",
      "Iteration 17066, Loss: 0.05417100712656975\n",
      "Iteration 17067, Loss: 0.05398876592516899\n",
      "Iteration 17068, Loss: 0.05417100712656975\n",
      "Iteration 17069, Loss: 0.053988777101039886\n",
      "Iteration 17070, Loss: 0.054170891642570496\n",
      "Iteration 17071, Loss: 0.053988657891750336\n",
      "Iteration 17072, Loss: 0.054171204566955566\n",
      "Iteration 17073, Loss: 0.053988486528396606\n",
      "Iteration 17074, Loss: 0.05417139455676079\n",
      "Iteration 17075, Loss: 0.053988367319107056\n",
      "Iteration 17076, Loss: 0.05417132377624512\n",
      "Iteration 17077, Loss: 0.05398830026388168\n",
      "Iteration 17078, Loss: 0.054171353578567505\n",
      "Iteration 17079, Loss: 0.0539884939789772\n",
      "Iteration 17080, Loss: 0.054171156138181686\n",
      "Iteration 17081, Loss: 0.05398872494697571\n",
      "Iteration 17082, Loss: 0.05417089909315109\n",
      "Iteration 17083, Loss: 0.05398891493678093\n",
      "Iteration 17084, Loss: 0.05417092889547348\n",
      "Iteration 17085, Loss: 0.053988926112651825\n",
      "Iteration 17086, Loss: 0.05417099595069885\n",
      "Iteration 17087, Loss: 0.053988873958587646\n",
      "Iteration 17088, Loss: 0.05417108163237572\n",
      "Iteration 17089, Loss: 0.05398872122168541\n",
      "Iteration 17090, Loss: 0.054171204566955566\n",
      "Iteration 17091, Loss: 0.053988486528396606\n",
      "Iteration 17092, Loss: 0.054171204566955566\n",
      "Iteration 17093, Loss: 0.05398845300078392\n",
      "Iteration 17094, Loss: 0.05417139455676079\n",
      "Iteration 17095, Loss: 0.05398833751678467\n",
      "Iteration 17096, Loss: 0.0541713610291481\n",
      "Iteration 17097, Loss: 0.05398821830749512\n",
      "Iteration 17098, Loss: 0.054171353578567505\n",
      "Iteration 17099, Loss: 0.053988419473171234\n",
      "Iteration 17100, Loss: 0.05417107790708542\n",
      "Iteration 17101, Loss: 0.05398872494697571\n",
      "Iteration 17102, Loss: 0.05417099595069885\n",
      "Iteration 17103, Loss: 0.053988926112651825\n",
      "Iteration 17104, Loss: 0.05417081341147423\n",
      "Iteration 17105, Loss: 0.05398884415626526\n",
      "Iteration 17106, Loss: 0.054171085357666016\n",
      "Iteration 17107, Loss: 0.053988754749298096\n",
      "Iteration 17108, Loss: 0.054171353578567505\n",
      "Iteration 17109, Loss: 0.05398852378129959\n",
      "Iteration 17110, Loss: 0.054171331226825714\n",
      "Iteration 17111, Loss: 0.053988367319107056\n",
      "Iteration 17112, Loss: 0.054171472787857056\n",
      "Iteration 17113, Loss: 0.05398844927549362\n",
      "Iteration 17114, Loss: 0.05417143553495407\n",
      "Iteration 17115, Loss: 0.053988486528396606\n",
      "Iteration 17116, Loss: 0.054171428084373474\n",
      "Iteration 17117, Loss: 0.053988680243492126\n",
      "Iteration 17118, Loss: 0.0541711151599884\n",
      "Iteration 17119, Loss: 0.05398872494697571\n",
      "Iteration 17120, Loss: 0.0541711151599884\n",
      "Iteration 17121, Loss: 0.05398883670568466\n",
      "Iteration 17122, Loss: 0.054171036928892136\n",
      "Iteration 17123, Loss: 0.05398868769407272\n",
      "Iteration 17124, Loss: 0.05417109280824661\n",
      "Iteration 17125, Loss: 0.05398860573768616\n",
      "Iteration 17126, Loss: 0.05417131632566452\n",
      "Iteration 17127, Loss: 0.05398864299058914\n",
      "Iteration 17128, Loss: 0.05417124181985855\n",
      "Iteration 17129, Loss: 0.05398856848478317\n",
      "Iteration 17130, Loss: 0.054171230643987656\n",
      "Iteration 17131, Loss: 0.05398868769407272\n",
      "Iteration 17132, Loss: 0.054171085357666016\n",
      "Iteration 17133, Loss: 0.05398876592516899\n",
      "Iteration 17134, Loss: 0.0541711151599884\n",
      "Iteration 17135, Loss: 0.05398876592516899\n",
      "Iteration 17136, Loss: 0.054170966148376465\n",
      "Iteration 17137, Loss: 0.05398879572749138\n",
      "Iteration 17138, Loss: 0.05417116731405258\n",
      "Iteration 17139, Loss: 0.05398860573768616\n",
      "Iteration 17140, Loss: 0.0541711263358593\n",
      "Iteration 17141, Loss: 0.053988486528396606\n",
      "Iteration 17142, Loss: 0.05417127534747124\n",
      "Iteration 17143, Loss: 0.05398864299058914\n",
      "Iteration 17144, Loss: 0.05417127534747124\n",
      "Iteration 17145, Loss: 0.05398860573768616\n",
      "Iteration 17146, Loss: 0.05417124181985855\n",
      "Iteration 17147, Loss: 0.05398857221007347\n",
      "Iteration 17148, Loss: 0.05417127534747124\n",
      "Iteration 17149, Loss: 0.053988613188266754\n",
      "Iteration 17150, Loss: 0.05417104810476303\n",
      "Iteration 17151, Loss: 0.05398868769407272\n",
      "Iteration 17152, Loss: 0.0541711151599884\n",
      "Iteration 17153, Loss: 0.05398879572749138\n",
      "Iteration 17154, Loss: 0.0541711151599884\n",
      "Iteration 17155, Loss: 0.05398872494697571\n",
      "Iteration 17156, Loss: 0.05417108163237572\n",
      "Iteration 17157, Loss: 0.05398879572749138\n",
      "Iteration 17158, Loss: 0.054171036928892136\n",
      "Iteration 17159, Loss: 0.053988754749298096\n",
      "Iteration 17160, Loss: 0.054171085357666016\n",
      "Iteration 17161, Loss: 0.05398860573768616\n",
      "Iteration 17162, Loss: 0.05417107790708542\n",
      "Iteration 17163, Loss: 0.05398872494697571\n",
      "Iteration 17164, Loss: 0.054171036928892136\n",
      "Iteration 17165, Loss: 0.05398876592516899\n",
      "Iteration 17166, Loss: 0.05417080223560333\n",
      "Iteration 17167, Loss: 0.05398896336555481\n",
      "Iteration 17168, Loss: 0.05417060852050781\n",
      "Iteration 17169, Loss: 0.053988974541425705\n",
      "Iteration 17170, Loss: 0.054170530289411545\n",
      "Iteration 17171, Loss: 0.0539892315864563\n",
      "Iteration 17172, Loss: 0.054170526564121246\n",
      "Iteration 17173, Loss: 0.05398920178413391\n",
      "Iteration 17174, Loss: 0.05417060852050781\n",
      "Iteration 17175, Loss: 0.053989045321941376\n",
      "Iteration 17176, Loss: 0.0541708841919899\n",
      "Iteration 17177, Loss: 0.05398888140916824\n",
      "Iteration 17178, Loss: 0.05417100712656975\n",
      "Iteration 17179, Loss: 0.05398868769407272\n",
      "Iteration 17180, Loss: 0.05417116731405258\n",
      "Iteration 17181, Loss: 0.05398856848478317\n",
      "Iteration 17182, Loss: 0.05417116731405258\n",
      "Iteration 17183, Loss: 0.05398844927549362\n",
      "Iteration 17184, Loss: 0.05417127162218094\n",
      "Iteration 17185, Loss: 0.05398860573768616\n",
      "Iteration 17186, Loss: 0.05417099595069885\n",
      "Iteration 17187, Loss: 0.05398876965045929\n",
      "Iteration 17188, Loss: 0.0541708767414093\n",
      "Iteration 17189, Loss: 0.05398903414607048\n",
      "Iteration 17190, Loss: 0.05417072772979736\n",
      "Iteration 17191, Loss: 0.05398911237716675\n",
      "Iteration 17192, Loss: 0.054170798510313034\n",
      "Iteration 17193, Loss: 0.05398903787136078\n",
      "Iteration 17194, Loss: 0.05417066067457199\n",
      "Iteration 17195, Loss: 0.053988926112651825\n",
      "Iteration 17196, Loss: 0.054170891642570496\n",
      "Iteration 17197, Loss: 0.05398876592516899\n",
      "Iteration 17198, Loss: 0.054170966148376465\n",
      "Iteration 17199, Loss: 0.053988754749298096\n",
      "Iteration 17200, Loss: 0.054171085357666016\n",
      "Iteration 17201, Loss: 0.05398864671587944\n",
      "Iteration 17202, Loss: 0.05417127534747124\n",
      "Iteration 17203, Loss: 0.05398857221007347\n",
      "Iteration 17204, Loss: 0.054170966148376465\n",
      "Iteration 17205, Loss: 0.05398865044116974\n",
      "Iteration 17206, Loss: 0.054171036928892136\n",
      "Iteration 17207, Loss: 0.05398876592516899\n",
      "Iteration 17208, Loss: 0.054170843213796616\n",
      "Iteration 17209, Loss: 0.05398896336555481\n",
      "Iteration 17210, Loss: 0.054170768707990646\n",
      "Iteration 17211, Loss: 0.05398908257484436\n",
      "Iteration 17212, Loss: 0.054170798510313034\n",
      "Iteration 17213, Loss: 0.053989045321941376\n",
      "Iteration 17214, Loss: 0.054170913994312286\n",
      "Iteration 17215, Loss: 0.05398896336555481\n",
      "Iteration 17216, Loss: 0.05417083948850632\n",
      "Iteration 17217, Loss: 0.05398895964026451\n",
      "Iteration 17218, Loss: 0.054170917719602585\n",
      "Iteration 17219, Loss: 0.05398903414607048\n",
      "Iteration 17220, Loss: 0.05417095869779587\n",
      "Iteration 17221, Loss: 0.053988926112651825\n",
      "Iteration 17222, Loss: 0.0541708841919899\n",
      "Iteration 17223, Loss: 0.05398895964026451\n",
      "Iteration 17224, Loss: 0.054170846939086914\n",
      "Iteration 17225, Loss: 0.05398879572749138\n",
      "Iteration 17226, Loss: 0.05417104810476303\n",
      "Iteration 17227, Loss: 0.05398868769407272\n",
      "Iteration 17228, Loss: 0.0541711263358593\n",
      "Iteration 17229, Loss: 0.053988486528396606\n",
      "Iteration 17230, Loss: 0.05417127534747124\n",
      "Iteration 17231, Loss: 0.05398845300078392\n",
      "Iteration 17232, Loss: 0.05417127534747124\n",
      "Iteration 17233, Loss: 0.05398864671587944\n",
      "Iteration 17234, Loss: 0.05417100712656975\n",
      "Iteration 17235, Loss: 0.05398865044116974\n",
      "Iteration 17236, Loss: 0.054170846939086914\n",
      "Iteration 17237, Loss: 0.05398893356323242\n",
      "Iteration 17238, Loss: 0.054170649498701096\n",
      "Iteration 17239, Loss: 0.053989194333553314\n",
      "Iteration 17240, Loss: 0.05417067930102348\n",
      "Iteration 17241, Loss: 0.05398919805884361\n",
      "Iteration 17242, Loss: 0.054170798510313034\n",
      "Iteration 17243, Loss: 0.05398888513445854\n",
      "Iteration 17244, Loss: 0.054170966148376465\n",
      "Iteration 17245, Loss: 0.053988806903362274\n",
      "Iteration 17246, Loss: 0.05417130887508392\n",
      "Iteration 17247, Loss: 0.05398860201239586\n",
      "Iteration 17248, Loss: 0.054171375930309296\n",
      "Iteration 17249, Loss: 0.0539882555603981\n",
      "Iteration 17250, Loss: 0.054171495139598846\n",
      "Iteration 17251, Loss: 0.053987979888916016\n",
      "Iteration 17252, Loss: 0.05417155846953392\n",
      "Iteration 17253, Loss: 0.0539882592856884\n",
      "Iteration 17254, Loss: 0.05417131632566452\n",
      "Iteration 17255, Loss: 0.05398852750658989\n",
      "Iteration 17256, Loss: 0.05417107790708542\n",
      "Iteration 17257, Loss: 0.053988926112651825\n",
      "Iteration 17258, Loss: 0.054170798510313034\n",
      "Iteration 17259, Loss: 0.053988970816135406\n",
      "Iteration 17260, Loss: 0.05417068302631378\n",
      "Iteration 17261, Loss: 0.05398912355303764\n",
      "Iteration 17262, Loss: 0.054170649498701096\n",
      "Iteration 17263, Loss: 0.053988926112651825\n",
      "Iteration 17264, Loss: 0.0541708879172802\n",
      "Iteration 17265, Loss: 0.05398884415626526\n",
      "Iteration 17266, Loss: 0.05417116731405258\n",
      "Iteration 17267, Loss: 0.05398864299058914\n",
      "Iteration 17268, Loss: 0.054171398282051086\n",
      "Iteration 17269, Loss: 0.053988367319107056\n",
      "Iteration 17270, Loss: 0.05417155846953392\n",
      "Iteration 17271, Loss: 0.05398818105459213\n",
      "Iteration 17272, Loss: 0.05417158827185631\n",
      "Iteration 17273, Loss: 0.05398833751678467\n",
      "Iteration 17274, Loss: 0.05417131632566452\n",
      "Iteration 17275, Loss: 0.05398868769407272\n",
      "Iteration 17276, Loss: 0.05417099595069885\n",
      "Iteration 17277, Loss: 0.05398895964026451\n",
      "Iteration 17278, Loss: 0.05417072772979736\n",
      "Iteration 17279, Loss: 0.05398919805884361\n",
      "Iteration 17280, Loss: 0.054170578718185425\n",
      "Iteration 17281, Loss: 0.05398907512426376\n",
      "Iteration 17282, Loss: 0.05417085438966751\n",
      "Iteration 17283, Loss: 0.05398879945278168\n",
      "Iteration 17284, Loss: 0.0541711300611496\n",
      "Iteration 17285, Loss: 0.05398860201239586\n",
      "Iteration 17286, Loss: 0.05417132377624512\n",
      "Iteration 17287, Loss: 0.0539882592856884\n",
      "Iteration 17288, Loss: 0.0541713647544384\n",
      "Iteration 17289, Loss: 0.05398844927549362\n",
      "Iteration 17290, Loss: 0.0541713647544384\n",
      "Iteration 17291, Loss: 0.053988486528396606\n",
      "Iteration 17292, Loss: 0.05417127534747124\n",
      "Iteration 17293, Loss: 0.05398852750658989\n",
      "Iteration 17294, Loss: 0.054171156138181686\n",
      "Iteration 17295, Loss: 0.05398872494697571\n",
      "Iteration 17296, Loss: 0.054170966148376465\n",
      "Iteration 17297, Loss: 0.05398883670568466\n",
      "Iteration 17298, Loss: 0.054170966148376465\n",
      "Iteration 17299, Loss: 0.05398868769407272\n",
      "Iteration 17300, Loss: 0.054171204566955566\n",
      "Iteration 17301, Loss: 0.05398864299058914\n",
      "Iteration 17302, Loss: 0.05417143553495407\n",
      "Iteration 17303, Loss: 0.053988367319107056\n",
      "Iteration 17304, Loss: 0.054171450436115265\n",
      "Iteration 17305, Loss: 0.05398830026388168\n",
      "Iteration 17306, Loss: 0.05417144298553467\n",
      "Iteration 17307, Loss: 0.05398840457201004\n",
      "Iteration 17308, Loss: 0.054171398282051086\n",
      "Iteration 17309, Loss: 0.05398852750658989\n",
      "Iteration 17310, Loss: 0.0541711151599884\n",
      "Iteration 17311, Loss: 0.05398868769407272\n",
      "Iteration 17312, Loss: 0.0541708879172802\n",
      "Iteration 17313, Loss: 0.053988873958587646\n",
      "Iteration 17314, Loss: 0.054170966148376465\n",
      "Iteration 17315, Loss: 0.053988732397556305\n",
      "Iteration 17316, Loss: 0.054171122610569\n",
      "Iteration 17317, Loss: 0.05398860573768616\n",
      "Iteration 17318, Loss: 0.054171204566955566\n",
      "Iteration 17319, Loss: 0.05398845672607422\n",
      "Iteration 17320, Loss: 0.05417124554514885\n",
      "Iteration 17321, Loss: 0.05398852750658989\n",
      "Iteration 17322, Loss: 0.054171234369277954\n",
      "Iteration 17323, Loss: 0.05398857221007347\n",
      "Iteration 17324, Loss: 0.0541711151599884\n",
      "Iteration 17325, Loss: 0.05398876592516899\n",
      "Iteration 17326, Loss: 0.05417095869779587\n",
      "Iteration 17327, Loss: 0.05398891866207123\n",
      "Iteration 17328, Loss: 0.05417095869779587\n",
      "Iteration 17329, Loss: 0.05398903787136078\n",
      "Iteration 17330, Loss: 0.054170843213796616\n",
      "Iteration 17331, Loss: 0.05398895964026451\n",
      "Iteration 17332, Loss: 0.05417092889547348\n",
      "Iteration 17333, Loss: 0.053988873958587646\n",
      "Iteration 17334, Loss: 0.05417104810476303\n",
      "Iteration 17335, Loss: 0.05398864671587944\n",
      "Iteration 17336, Loss: 0.05417131632566452\n",
      "Iteration 17337, Loss: 0.05398844927549362\n",
      "Iteration 17338, Loss: 0.0541713647544384\n",
      "Iteration 17339, Loss: 0.05398840829730034\n",
      "Iteration 17340, Loss: 0.054171353578567505\n",
      "Iteration 17341, Loss: 0.053988367319107056\n",
      "Iteration 17342, Loss: 0.05417128652334213\n",
      "Iteration 17343, Loss: 0.05398845300078392\n",
      "Iteration 17344, Loss: 0.0541711300611496\n",
      "Iteration 17345, Loss: 0.05398852750658989\n",
      "Iteration 17346, Loss: 0.05417127162218094\n",
      "Iteration 17347, Loss: 0.05398857221007347\n",
      "Iteration 17348, Loss: 0.05417108163237572\n",
      "Iteration 17349, Loss: 0.05398884415626526\n",
      "Iteration 17350, Loss: 0.05417100340127945\n",
      "Iteration 17351, Loss: 0.0539889931678772\n",
      "Iteration 17352, Loss: 0.054170966148376465\n",
      "Iteration 17353, Loss: 0.05398884415626526\n",
      "Iteration 17354, Loss: 0.054171122610569\n",
      "Iteration 17355, Loss: 0.05398864671587944\n",
      "Iteration 17356, Loss: 0.0541711300611496\n",
      "Iteration 17357, Loss: 0.0539884939789772\n",
      "Iteration 17358, Loss: 0.05417124554514885\n",
      "Iteration 17359, Loss: 0.05398907884955406\n",
      "Iteration 17360, Loss: 0.0541711263358593\n",
      "Iteration 17361, Loss: 0.053989045321941376\n",
      "Iteration 17362, Loss: 0.054171591997146606\n",
      "Iteration 17363, Loss: 0.05398908257484436\n",
      "Iteration 17364, Loss: 0.05417148396372795\n",
      "Iteration 17365, Loss: 0.053989164531230927\n",
      "Iteration 17366, Loss: 0.05417143926024437\n",
      "Iteration 17367, Loss: 0.05398939549922943\n",
      "Iteration 17368, Loss: 0.05417128652334213\n",
      "Iteration 17369, Loss: 0.05398927628993988\n",
      "Iteration 17370, Loss: 0.05417140573263168\n",
      "Iteration 17371, Loss: 0.053989239037036896\n",
      "Iteration 17372, Loss: 0.0541716031730175\n",
      "Iteration 17373, Loss: 0.05398903787136078\n",
      "Iteration 17374, Loss: 0.05417175218462944\n",
      "Iteration 17375, Loss: 0.05398908257484436\n",
      "Iteration 17376, Loss: 0.05417167395353317\n",
      "Iteration 17377, Loss: 0.05398912355303764\n",
      "Iteration 17378, Loss: 0.0541716031730175\n",
      "Iteration 17379, Loss: 0.05398907884955406\n",
      "Iteration 17380, Loss: 0.05417167395353317\n",
      "Iteration 17381, Loss: 0.053989119827747345\n",
      "Iteration 17382, Loss: 0.05417163297533989\n",
      "Iteration 17383, Loss: 0.05398915708065033\n",
      "Iteration 17384, Loss: 0.05417167395353317\n",
      "Iteration 17385, Loss: 0.05398915708065033\n",
      "Iteration 17386, Loss: 0.05417167395353317\n",
      "Iteration 17387, Loss: 0.05398912355303764\n",
      "Iteration 17388, Loss: 0.05417156219482422\n",
      "Iteration 17389, Loss: 0.05398908257484436\n",
      "Iteration 17390, Loss: 0.0541716031730175\n",
      "Iteration 17391, Loss: 0.05398908257484436\n",
      "Iteration 17392, Loss: 0.05417148396372795\n",
      "Iteration 17393, Loss: 0.05398911237716675\n",
      "Iteration 17394, Loss: 0.05417177826166153\n",
      "Iteration 17395, Loss: 0.05398919805884361\n",
      "Iteration 17396, Loss: 0.054171547293663025\n",
      "Iteration 17397, Loss: 0.05398928374052048\n",
      "Iteration 17398, Loss: 0.05417151004076004\n",
      "Iteration 17399, Loss: 0.053989361971616745\n",
      "Iteration 17400, Loss: 0.05417134612798691\n",
      "Iteration 17401, Loss: 0.053989477455616\n",
      "Iteration 17402, Loss: 0.054171204566955566\n",
      "Iteration 17403, Loss: 0.053989477455616\n",
      "Iteration 17404, Loss: 0.0541713610291481\n",
      "Iteration 17405, Loss: 0.05398935079574585\n",
      "Iteration 17406, Loss: 0.05417148023843765\n",
      "Iteration 17407, Loss: 0.05398927256464958\n",
      "Iteration 17408, Loss: 0.05417171120643616\n",
      "Iteration 17409, Loss: 0.05398896336555481\n",
      "Iteration 17410, Loss: 0.05417175218462944\n",
      "Iteration 17411, Loss: 0.053988926112651825\n",
      "Iteration 17412, Loss: 0.05417179316282272\n",
      "Iteration 17413, Loss: 0.05398888513445854\n",
      "Iteration 17414, Loss: 0.054171785712242126\n",
      "Iteration 17415, Loss: 0.05398900434374809\n",
      "Iteration 17416, Loss: 0.054171591997146606\n",
      "Iteration 17417, Loss: 0.053989164531230927\n",
      "Iteration 17418, Loss: 0.05417158827185631\n",
      "Iteration 17419, Loss: 0.053989242762327194\n",
      "Iteration 17420, Loss: 0.0541713647544384\n",
      "Iteration 17421, Loss: 0.05398935079574585\n",
      "Iteration 17422, Loss: 0.05417155474424362\n",
      "Iteration 17423, Loss: 0.05398919805884361\n",
      "Iteration 17424, Loss: 0.05417163670063019\n",
      "Iteration 17425, Loss: 0.05398908257484436\n",
      "Iteration 17426, Loss: 0.05417168140411377\n",
      "Iteration 17427, Loss: 0.053988926112651825\n",
      "Iteration 17428, Loss: 0.05417187139391899\n",
      "Iteration 17429, Loss: 0.05398888513445854\n",
      "Iteration 17430, Loss: 0.05417183041572571\n",
      "Iteration 17431, Loss: 0.05398891866207123\n",
      "Iteration 17432, Loss: 0.05417190119624138\n",
      "Iteration 17433, Loss: 0.053989361971616745\n",
      "Iteration 17434, Loss: 0.05417167395353317\n",
      "Iteration 17435, Loss: 0.053989678621292114\n",
      "Iteration 17436, Loss: 0.05417083203792572\n",
      "Iteration 17437, Loss: 0.05399010702967644\n",
      "Iteration 17438, Loss: 0.054170720279216766\n",
      "Iteration 17439, Loss: 0.053990066051483154\n",
      "Iteration 17440, Loss: 0.0541708767414093\n",
      "Iteration 17441, Loss: 0.053989868611097336\n",
      "Iteration 17442, Loss: 0.05417127534747124\n",
      "Iteration 17443, Loss: 0.05398955196142197\n",
      "Iteration 17444, Loss: 0.054171569645404816\n",
      "Iteration 17445, Loss: 0.05398911237716675\n",
      "Iteration 17446, Loss: 0.054172150790691376\n",
      "Iteration 17447, Loss: 0.05398868769407272\n",
      "Iteration 17448, Loss: 0.05417222902178764\n",
      "Iteration 17449, Loss: 0.0539884977042675\n",
      "Iteration 17450, Loss: 0.05417218804359436\n",
      "Iteration 17451, Loss: 0.05398861691355705\n",
      "Iteration 17452, Loss: 0.05417194962501526\n",
      "Iteration 17453, Loss: 0.05398889631032944\n",
      "Iteration 17454, Loss: 0.05417171120643616\n",
      "Iteration 17455, Loss: 0.053989164531230927\n",
      "Iteration 17456, Loss: 0.05417155474424362\n",
      "Iteration 17457, Loss: 0.05398939549922943\n",
      "Iteration 17458, Loss: 0.054171472787857056\n",
      "Iteration 17459, Loss: 0.05398928374052048\n",
      "Iteration 17460, Loss: 0.05417148396372795\n",
      "Iteration 17461, Loss: 0.053989242762327194\n",
      "Iteration 17462, Loss: 0.05417175218462944\n",
      "Iteration 17463, Loss: 0.05398915708065033\n",
      "Iteration 17464, Loss: 0.05417172238230705\n",
      "Iteration 17465, Loss: 0.05398903414607048\n",
      "Iteration 17466, Loss: 0.054172031581401825\n",
      "Iteration 17467, Loss: 0.05398881062865257\n",
      "Iteration 17468, Loss: 0.05417218431830406\n",
      "Iteration 17469, Loss: 0.05398872494697571\n",
      "Iteration 17470, Loss: 0.05417213961482048\n",
      "Iteration 17471, Loss: 0.05398884415626526\n",
      "Iteration 17472, Loss: 0.054171912372112274\n",
      "Iteration 17473, Loss: 0.05398903787136078\n",
      "Iteration 17474, Loss: 0.054171666502952576\n",
      "Iteration 17475, Loss: 0.05398928374052048\n",
      "Iteration 17476, Loss: 0.05417155474424362\n",
      "Iteration 17477, Loss: 0.053989477455616\n",
      "Iteration 17478, Loss: 0.05417131632566452\n",
      "Iteration 17479, Loss: 0.05398952215909958\n",
      "Iteration 17480, Loss: 0.05417131632566452\n",
      "Iteration 17481, Loss: 0.053989481180906296\n",
      "Iteration 17482, Loss: 0.05417144298553467\n",
      "Iteration 17483, Loss: 0.05398931726813316\n",
      "Iteration 17484, Loss: 0.05417171120643616\n",
      "Iteration 17485, Loss: 0.05398908257484436\n",
      "Iteration 17486, Loss: 0.05417194589972496\n",
      "Iteration 17487, Loss: 0.053988926112651825\n",
      "Iteration 17488, Loss: 0.05417187139391899\n",
      "Iteration 17489, Loss: 0.05398896336555481\n",
      "Iteration 17490, Loss: 0.05417187139391899\n",
      "Iteration 17491, Loss: 0.05398896336555481\n",
      "Iteration 17492, Loss: 0.05417199060320854\n",
      "Iteration 17493, Loss: 0.05398891866207123\n",
      "Iteration 17494, Loss: 0.054171837866306305\n",
      "Iteration 17495, Loss: 0.053988926112651825\n",
      "Iteration 17496, Loss: 0.05417206138372421\n",
      "Iteration 17497, Loss: 0.05398896336555481\n",
      "Iteration 17498, Loss: 0.05417183041572571\n",
      "Iteration 17499, Loss: 0.05398915708065033\n",
      "Iteration 17500, Loss: 0.05417163297533989\n",
      "Iteration 17501, Loss: 0.053989239037036896\n",
      "Iteration 17502, Loss: 0.054171591997146606\n",
      "Iteration 17503, Loss: 0.05398932099342346\n",
      "Iteration 17504, Loss: 0.054171472787857056\n",
      "Iteration 17505, Loss: 0.05398932099342346\n",
      "Iteration 17506, Loss: 0.05417144298553467\n",
      "Iteration 17507, Loss: 0.05398931726813316\n",
      "Iteration 17508, Loss: 0.05417175218462944\n",
      "Iteration 17509, Loss: 0.053989049047231674\n",
      "Iteration 17510, Loss: 0.054171763360500336\n",
      "Iteration 17511, Loss: 0.053989045321941376\n",
      "Iteration 17512, Loss: 0.054171882569789886\n",
      "Iteration 17513, Loss: 0.053988926112651825\n",
      "Iteration 17514, Loss: 0.05417203530669212\n",
      "Iteration 17515, Loss: 0.05398876592516899\n",
      "Iteration 17516, Loss: 0.054172106087207794\n",
      "Iteration 17517, Loss: 0.053988926112651825\n",
      "Iteration 17518, Loss: 0.054171912372112274\n",
      "Iteration 17519, Loss: 0.053989045321941376\n",
      "Iteration 17520, Loss: 0.05417163297533989\n",
      "Iteration 17521, Loss: 0.05398920923471451\n",
      "Iteration 17522, Loss: 0.05417151376605034\n",
      "Iteration 17523, Loss: 0.053989361971616745\n",
      "Iteration 17524, Loss: 0.05417151376605034\n",
      "Iteration 17525, Loss: 0.05398940294981003\n",
      "Iteration 17526, Loss: 0.05417143553495407\n",
      "Iteration 17527, Loss: 0.053989361971616745\n",
      "Iteration 17528, Loss: 0.054171375930309296\n",
      "Iteration 17529, Loss: 0.05398935824632645\n",
      "Iteration 17530, Loss: 0.0541716031730175\n",
      "Iteration 17531, Loss: 0.05398900434374809\n",
      "Iteration 17532, Loss: 0.05417187511920929\n",
      "Iteration 17533, Loss: 0.05398888885974884\n",
      "Iteration 17534, Loss: 0.05417200177907944\n",
      "Iteration 17535, Loss: 0.0539887361228466\n",
      "Iteration 17536, Loss: 0.054171886295080185\n",
      "Iteration 17537, Loss: 0.053988657891750336\n",
      "Iteration 17538, Loss: 0.05417199060320854\n",
      "Iteration 17539, Loss: 0.05398888513445854\n",
      "Iteration 17540, Loss: 0.05417183041572571\n",
      "Iteration 17541, Loss: 0.05398908257484436\n",
      "Iteration 17542, Loss: 0.05417163297533989\n",
      "Iteration 17543, Loss: 0.053989242762327194\n",
      "Iteration 17544, Loss: 0.054171524941921234\n",
      "Iteration 17545, Loss: 0.05398932099342346\n",
      "Iteration 17546, Loss: 0.05417140573263168\n",
      "Iteration 17547, Loss: 0.05398920178413391\n",
      "Iteration 17548, Loss: 0.05417156219482422\n",
      "Iteration 17549, Loss: 0.053989049047231674\n",
      "Iteration 17550, Loss: 0.054171644151210785\n",
      "Iteration 17551, Loss: 0.05398892983794212\n",
      "Iteration 17552, Loss: 0.054171644151210785\n",
      "Iteration 17553, Loss: 0.05398900434374809\n",
      "Iteration 17554, Loss: 0.054171718657016754\n",
      "Iteration 17555, Loss: 0.05398893356323242\n",
      "Iteration 17556, Loss: 0.05417172238230705\n",
      "Iteration 17557, Loss: 0.05398896336555481\n",
      "Iteration 17558, Loss: 0.05417183041572571\n",
      "Iteration 17559, Loss: 0.05398900434374809\n",
      "Iteration 17560, Loss: 0.05417168140411377\n",
      "Iteration 17561, Loss: 0.053989168256521225\n",
      "Iteration 17562, Loss: 0.05417144298553467\n",
      "Iteration 17563, Loss: 0.05398940294981003\n",
      "Iteration 17564, Loss: 0.0541713647544384\n",
      "Iteration 17565, Loss: 0.053989361971616745\n",
      "Iteration 17566, Loss: 0.0541713610291481\n",
      "Iteration 17567, Loss: 0.05398933216929436\n",
      "Iteration 17568, Loss: 0.054171256721019745\n",
      "Iteration 17569, Loss: 0.05398928374052048\n",
      "Iteration 17570, Loss: 0.05417148396372795\n",
      "Iteration 17571, Loss: 0.053989164531230927\n",
      "Iteration 17572, Loss: 0.054171644151210785\n",
      "Iteration 17573, Loss: 0.05398900806903839\n",
      "Iteration 17574, Loss: 0.05417179316282272\n",
      "Iteration 17575, Loss: 0.05398900806903839\n",
      "Iteration 17576, Loss: 0.054171763360500336\n",
      "Iteration 17577, Loss: 0.05398896336555481\n",
      "Iteration 17578, Loss: 0.05417183041572571\n",
      "Iteration 17579, Loss: 0.05398892983794212\n",
      "Iteration 17580, Loss: 0.054171763360500336\n",
      "Iteration 17581, Loss: 0.053988926112651825\n",
      "Iteration 17582, Loss: 0.054171763360500336\n",
      "Iteration 17583, Loss: 0.05398888885974884\n",
      "Iteration 17584, Loss: 0.054171763360500336\n",
      "Iteration 17585, Loss: 0.053988851606845856\n",
      "Iteration 17586, Loss: 0.0541718415915966\n",
      "Iteration 17587, Loss: 0.05398893356323242\n",
      "Iteration 17588, Loss: 0.05417168140411377\n",
      "Iteration 17589, Loss: 0.05398893356323242\n",
      "Iteration 17590, Loss: 0.054171718657016754\n",
      "Iteration 17591, Loss: 0.05398889631032944\n",
      "Iteration 17592, Loss: 0.05417175590991974\n",
      "Iteration 17593, Loss: 0.05398889631032944\n",
      "Iteration 17594, Loss: 0.05417183041572571\n",
      "Iteration 17595, Loss: 0.05398909002542496\n",
      "Iteration 17596, Loss: 0.05417167767882347\n",
      "Iteration 17597, Loss: 0.053989164531230927\n",
      "Iteration 17598, Loss: 0.05417179316282272\n",
      "Iteration 17599, Loss: 0.05398896336555481\n",
      "Iteration 17600, Loss: 0.05417180061340332\n",
      "Iteration 17601, Loss: 0.053988851606845856\n",
      "Iteration 17602, Loss: 0.05417180061340332\n",
      "Iteration 17603, Loss: 0.05398881435394287\n",
      "Iteration 17604, Loss: 0.05417194962501526\n",
      "Iteration 17605, Loss: 0.05398888885974884\n",
      "Iteration 17606, Loss: 0.05417179316282272\n",
      "Iteration 17607, Loss: 0.053989164531230927\n",
      "Iteration 17608, Loss: 0.054171591997146606\n",
      "Iteration 17609, Loss: 0.053989361971616745\n",
      "Iteration 17610, Loss: 0.0541713647544384\n",
      "Iteration 17611, Loss: 0.05398944020271301\n",
      "Iteration 17612, Loss: 0.05417124181985855\n",
      "Iteration 17613, Loss: 0.053989559412002563\n",
      "Iteration 17614, Loss: 0.05417117476463318\n",
      "Iteration 17615, Loss: 0.05398951470851898\n",
      "Iteration 17616, Loss: 0.05417148396372795\n",
      "Iteration 17617, Loss: 0.053989242762327194\n",
      "Iteration 17618, Loss: 0.05417175218462944\n",
      "Iteration 17619, Loss: 0.053988970816135406\n",
      "Iteration 17620, Loss: 0.05417180061340332\n",
      "Iteration 17621, Loss: 0.053988855332136154\n",
      "Iteration 17622, Loss: 0.0541718415915966\n",
      "Iteration 17623, Loss: 0.05398881435394287\n",
      "Iteration 17624, Loss: 0.05417187139391899\n",
      "Iteration 17625, Loss: 0.05398900434374809\n",
      "Iteration 17626, Loss: 0.05417186766862869\n",
      "Iteration 17627, Loss: 0.05398893356323242\n",
      "Iteration 17628, Loss: 0.05417178198695183\n",
      "Iteration 17629, Loss: 0.053989049047231674\n",
      "Iteration 17630, Loss: 0.05417163297533989\n",
      "Iteration 17631, Loss: 0.05398931726813316\n",
      "Iteration 17632, Loss: 0.05417151749134064\n",
      "Iteration 17633, Loss: 0.05398932099342346\n",
      "Iteration 17634, Loss: 0.05417151376605034\n",
      "Iteration 17635, Loss: 0.05398940294981003\n",
      "Iteration 17636, Loss: 0.05417143926024437\n",
      "Iteration 17637, Loss: 0.053989361971616745\n",
      "Iteration 17638, Loss: 0.054171256721019745\n",
      "Iteration 17639, Loss: 0.05398924648761749\n",
      "Iteration 17640, Loss: 0.054171375930309296\n",
      "Iteration 17641, Loss: 0.053989313542842865\n",
      "Iteration 17642, Loss: 0.054171591997146606\n",
      "Iteration 17643, Loss: 0.05398908257484436\n",
      "Iteration 17644, Loss: 0.05417156219482422\n",
      "Iteration 17645, Loss: 0.05398896336555481\n",
      "Iteration 17646, Loss: 0.0541716031730175\n",
      "Iteration 17647, Loss: 0.05398893356323242\n",
      "Iteration 17648, Loss: 0.05417171120643616\n",
      "Iteration 17649, Loss: 0.05398909002542496\n",
      "Iteration 17650, Loss: 0.05417179316282272\n",
      "Iteration 17651, Loss: 0.05398908257484436\n",
      "Iteration 17652, Loss: 0.054171524941921234\n",
      "Iteration 17653, Loss: 0.05398900806903839\n",
      "Iteration 17654, Loss: 0.054171569645404816\n",
      "Iteration 17655, Loss: 0.05398893356323242\n",
      "Iteration 17656, Loss: 0.054171718657016754\n",
      "Iteration 17657, Loss: 0.053989045321941376\n",
      "Iteration 17658, Loss: 0.05417164787650108\n",
      "Iteration 17659, Loss: 0.05398888885974884\n",
      "Iteration 17660, Loss: 0.05417183041572571\n",
      "Iteration 17661, Loss: 0.053989045321941376\n",
      "Iteration 17662, Loss: 0.05417175218462944\n",
      "Iteration 17663, Loss: 0.05398908257484436\n",
      "Iteration 17664, Loss: 0.054171644151210785\n",
      "Iteration 17665, Loss: 0.05398920178413391\n",
      "Iteration 17666, Loss: 0.05417140945792198\n",
      "Iteration 17667, Loss: 0.053989239037036896\n",
      "Iteration 17668, Loss: 0.054171524941921234\n",
      "Iteration 17669, Loss: 0.05398912355303764\n",
      "Iteration 17670, Loss: 0.0541716031730175\n",
      "Iteration 17671, Loss: 0.05398909002542496\n",
      "Iteration 17672, Loss: 0.05417168140411377\n",
      "Iteration 17673, Loss: 0.053988970816135406\n",
      "Iteration 17674, Loss: 0.05417163670063019\n",
      "Iteration 17675, Loss: 0.053988855332136154\n",
      "Iteration 17676, Loss: 0.05417172238230705\n",
      "Iteration 17677, Loss: 0.05398889631032944\n",
      "Iteration 17678, Loss: 0.05417180061340332\n",
      "Iteration 17679, Loss: 0.05398881435394287\n",
      "Iteration 17680, Loss: 0.05417183041572571\n",
      "Iteration 17681, Loss: 0.053989045321941376\n",
      "Iteration 17682, Loss: 0.05417190119624138\n",
      "Iteration 17683, Loss: 0.053989049047231674\n",
      "Iteration 17684, Loss: 0.05417175218462944\n",
      "Iteration 17685, Loss: 0.053989045321941376\n",
      "Iteration 17686, Loss: 0.05417167767882347\n",
      "Iteration 17687, Loss: 0.05398908257484436\n",
      "Iteration 17688, Loss: 0.05417175218462944\n",
      "Iteration 17689, Loss: 0.05398908257484436\n",
      "Iteration 17690, Loss: 0.05417171120643616\n",
      "Iteration 17691, Loss: 0.05398912355303764\n",
      "Iteration 17692, Loss: 0.054171591997146606\n",
      "Iteration 17693, Loss: 0.05398920178413391\n",
      "Iteration 17694, Loss: 0.05417155474424362\n",
      "Iteration 17695, Loss: 0.053989242762327194\n",
      "Iteration 17696, Loss: 0.054171331226825714\n",
      "Iteration 17697, Loss: 0.05398940294981003\n",
      "Iteration 17698, Loss: 0.0541713610291481\n",
      "Iteration 17699, Loss: 0.05398940294981003\n",
      "Iteration 17700, Loss: 0.05417143553495407\n",
      "Iteration 17701, Loss: 0.05398940294981003\n",
      "Iteration 17702, Loss: 0.054171398282051086\n",
      "Iteration 17703, Loss: 0.053989361971616745\n",
      "Iteration 17704, Loss: 0.05417133495211601\n",
      "Iteration 17705, Loss: 0.05398932099342346\n",
      "Iteration 17706, Loss: 0.05417167395353317\n",
      "Iteration 17707, Loss: 0.05398900806903839\n",
      "Iteration 17708, Loss: 0.05417183041572571\n",
      "Iteration 17709, Loss: 0.05398888513445854\n",
      "Iteration 17710, Loss: 0.054172076284885406\n",
      "Iteration 17711, Loss: 0.05398865044116974\n",
      "Iteration 17712, Loss: 0.05417214334011078\n",
      "Iteration 17713, Loss: 0.05398888513445854\n",
      "Iteration 17714, Loss: 0.05417187139391899\n",
      "Iteration 17715, Loss: 0.053989049047231674\n",
      "Iteration 17716, Loss: 0.05417162925004959\n",
      "Iteration 17717, Loss: 0.05398928374052048\n",
      "Iteration 17718, Loss: 0.05417148023843765\n",
      "Iteration 17719, Loss: 0.05398951470851898\n",
      "Iteration 17720, Loss: 0.05417124554514885\n",
      "Iteration 17721, Loss: 0.053989559412002563\n",
      "Iteration 17722, Loss: 0.05417117103934288\n",
      "Iteration 17723, Loss: 0.05398940667510033\n",
      "Iteration 17724, Loss: 0.05417140573263168\n",
      "Iteration 17725, Loss: 0.053989239037036896\n",
      "Iteration 17726, Loss: 0.054171644151210785\n",
      "Iteration 17727, Loss: 0.05398912355303764\n",
      "Iteration 17728, Loss: 0.05417180061340332\n",
      "Iteration 17729, Loss: 0.053988855332136154\n",
      "Iteration 17730, Loss: 0.05417191609740257\n",
      "Iteration 17731, Loss: 0.05398881062865257\n",
      "Iteration 17732, Loss: 0.054171882569789886\n",
      "Iteration 17733, Loss: 0.05398881435394287\n",
      "Iteration 17734, Loss: 0.05417180061340332\n",
      "Iteration 17735, Loss: 0.05398888513445854\n",
      "Iteration 17736, Loss: 0.05417183041572571\n",
      "Iteration 17737, Loss: 0.05398908257484436\n",
      "Iteration 17738, Loss: 0.054171718657016754\n",
      "Iteration 17739, Loss: 0.053989168256521225\n",
      "Iteration 17740, Loss: 0.05417163297533989\n",
      "Iteration 17741, Loss: 0.05398928374052048\n",
      "Iteration 17742, Loss: 0.05417155474424362\n",
      "Iteration 17743, Loss: 0.05398920923471451\n",
      "Iteration 17744, Loss: 0.05417141318321228\n",
      "Iteration 17745, Loss: 0.053989242762327194\n",
      "Iteration 17746, Loss: 0.05417168140411377\n",
      "Iteration 17747, Loss: 0.05398908257484436\n",
      "Iteration 17748, Loss: 0.054171837866306305\n",
      "Iteration 17749, Loss: 0.05398896336555481\n",
      "Iteration 17750, Loss: 0.05417191609740257\n",
      "Iteration 17751, Loss: 0.05398881062865257\n",
      "Iteration 17752, Loss: 0.0541718415915966\n",
      "Iteration 17753, Loss: 0.05398876965045929\n",
      "Iteration 17754, Loss: 0.05417179688811302\n",
      "Iteration 17755, Loss: 0.053989045321941376\n",
      "Iteration 17756, Loss: 0.05417163297533989\n",
      "Iteration 17757, Loss: 0.053989242762327194\n",
      "Iteration 17758, Loss: 0.05417128652334213\n",
      "Iteration 17759, Loss: 0.05398952215909958\n",
      "Iteration 17760, Loss: 0.05417124554514885\n",
      "Iteration 17761, Loss: 0.053989481180906296\n",
      "Iteration 17762, Loss: 0.054171331226825714\n",
      "Iteration 17763, Loss: 0.053989242762327194\n",
      "Iteration 17764, Loss: 0.0541716031730175\n",
      "Iteration 17765, Loss: 0.05398912355303764\n",
      "Iteration 17766, Loss: 0.05417164787650108\n",
      "Iteration 17767, Loss: 0.053988970816135406\n",
      "Iteration 17768, Loss: 0.05417194962501526\n",
      "Iteration 17769, Loss: 0.05398881062865257\n",
      "Iteration 17770, Loss: 0.054171763360500336\n",
      "Iteration 17771, Loss: 0.053988926112651825\n",
      "Iteration 17772, Loss: 0.054171912372112274\n",
      "Iteration 17773, Loss: 0.05398900434374809\n",
      "Iteration 17774, Loss: 0.05417179316282272\n",
      "Iteration 17775, Loss: 0.05398908257484436\n",
      "Iteration 17776, Loss: 0.05417167395353317\n",
      "Iteration 17777, Loss: 0.053989164531230927\n",
      "Iteration 17778, Loss: 0.05417163297533989\n",
      "Iteration 17779, Loss: 0.05398920178413391\n",
      "Iteration 17780, Loss: 0.05417152866721153\n",
      "Iteration 17781, Loss: 0.05398912355303764\n",
      "Iteration 17782, Loss: 0.054171763360500336\n",
      "Iteration 17783, Loss: 0.053988926112651825\n",
      "Iteration 17784, Loss: 0.05417183041572571\n",
      "Iteration 17785, Loss: 0.05398892983794212\n",
      "Iteration 17786, Loss: 0.05417179316282272\n",
      "Iteration 17787, Loss: 0.05398905277252197\n",
      "Iteration 17788, Loss: 0.05417167767882347\n",
      "Iteration 17789, Loss: 0.053989093750715256\n",
      "Iteration 17790, Loss: 0.054171591997146606\n",
      "Iteration 17791, Loss: 0.05398920178413391\n",
      "Iteration 17792, Loss: 0.05417156219482422\n",
      "Iteration 17793, Loss: 0.05398920178413391\n",
      "Iteration 17794, Loss: 0.054171644151210785\n",
      "Iteration 17795, Loss: 0.05398908257484436\n",
      "Iteration 17796, Loss: 0.054171837866306305\n",
      "Iteration 17797, Loss: 0.05398896336555481\n",
      "Iteration 17798, Loss: 0.05417168140411377\n",
      "Iteration 17799, Loss: 0.05398900434374809\n",
      "Iteration 17800, Loss: 0.05417183041572571\n",
      "Iteration 17801, Loss: 0.05398900806903839\n",
      "Iteration 17802, Loss: 0.05417182296514511\n",
      "Iteration 17803, Loss: 0.053989049047231674\n",
      "Iteration 17804, Loss: 0.05417167395353317\n",
      "Iteration 17805, Loss: 0.05398912727832794\n",
      "Iteration 17806, Loss: 0.054171591997146606\n",
      "Iteration 17807, Loss: 0.053989239037036896\n",
      "Iteration 17808, Loss: 0.054171524941921234\n",
      "Iteration 17809, Loss: 0.053989242762327194\n",
      "Iteration 17810, Loss: 0.05417155474424362\n",
      "Iteration 17811, Loss: 0.05398932099342346\n",
      "Iteration 17812, Loss: 0.05417140945792198\n",
      "Iteration 17813, Loss: 0.05398912727832794\n",
      "Iteration 17814, Loss: 0.0541716031730175\n",
      "Iteration 17815, Loss: 0.05398909002542496\n",
      "Iteration 17816, Loss: 0.05417156219482422\n",
      "Iteration 17817, Loss: 0.053989045321941376\n",
      "Iteration 17818, Loss: 0.054171718657016754\n",
      "Iteration 17819, Loss: 0.05398900806903839\n",
      "Iteration 17820, Loss: 0.05417171120643616\n",
      "Iteration 17821, Loss: 0.05398900806903839\n",
      "Iteration 17822, Loss: 0.05417175218462944\n",
      "Iteration 17823, Loss: 0.053988974541425705\n",
      "Iteration 17824, Loss: 0.05417182296514511\n",
      "Iteration 17825, Loss: 0.05398912355303764\n",
      "Iteration 17826, Loss: 0.0541716031730175\n",
      "Iteration 17827, Loss: 0.05398908257484436\n",
      "Iteration 17828, Loss: 0.054171644151210785\n",
      "Iteration 17829, Loss: 0.05398900806903839\n",
      "Iteration 17830, Loss: 0.05417175218462944\n",
      "Iteration 17831, Loss: 0.05398900434374809\n",
      "Iteration 17832, Loss: 0.0541718415915966\n",
      "Iteration 17833, Loss: 0.05398896336555481\n",
      "Iteration 17834, Loss: 0.05417191609740257\n",
      "Iteration 17835, Loss: 0.0539887361228466\n",
      "Iteration 17836, Loss: 0.0541718415915966\n",
      "Iteration 17837, Loss: 0.053988777101039886\n",
      "Iteration 17838, Loss: 0.05417177081108093\n",
      "Iteration 17839, Loss: 0.053988926112651825\n",
      "Iteration 17840, Loss: 0.05417183041572571\n",
      "Iteration 17841, Loss: 0.053989049047231674\n",
      "Iteration 17842, Loss: 0.054171718657016754\n",
      "Iteration 17843, Loss: 0.05398927628993988\n",
      "Iteration 17844, Loss: 0.054171524941921234\n",
      "Iteration 17845, Loss: 0.05398932844400406\n",
      "Iteration 17846, Loss: 0.05417151376605034\n",
      "Iteration 17847, Loss: 0.053989361971616745\n",
      "Iteration 17848, Loss: 0.054171524941921234\n",
      "Iteration 17849, Loss: 0.05398920178413391\n",
      "Iteration 17850, Loss: 0.05417168140411377\n",
      "Iteration 17851, Loss: 0.053988974541425705\n",
      "Iteration 17852, Loss: 0.05417180061340332\n",
      "Iteration 17853, Loss: 0.05398892983794212\n",
      "Iteration 17854, Loss: 0.05417187511920929\n",
      "Iteration 17855, Loss: 0.0539887361228466\n",
      "Iteration 17856, Loss: 0.05417194962501526\n",
      "Iteration 17857, Loss: 0.053988926112651825\n",
      "Iteration 17858, Loss: 0.05417183041572571\n",
      "Iteration 17859, Loss: 0.05398908257484436\n",
      "Iteration 17860, Loss: 0.054171524941921234\n",
      "Iteration 17861, Loss: 0.05398912355303764\n",
      "Iteration 17862, Loss: 0.05417167767882347\n",
      "Iteration 17863, Loss: 0.05398908257484436\n",
      "Iteration 17864, Loss: 0.054171718657016754\n",
      "Iteration 17865, Loss: 0.053989045321941376\n",
      "Iteration 17866, Loss: 0.05417180061340332\n",
      "Iteration 17867, Loss: 0.05398893356323242\n",
      "Iteration 17868, Loss: 0.0541718415915966\n",
      "Iteration 17869, Loss: 0.053988777101039886\n",
      "Iteration 17870, Loss: 0.05417180061340332\n",
      "Iteration 17871, Loss: 0.05398881435394287\n",
      "Iteration 17872, Loss: 0.05417180061340332\n",
      "Iteration 17873, Loss: 0.053988855332136154\n",
      "Iteration 17874, Loss: 0.0541718415915966\n",
      "Iteration 17875, Loss: 0.05398881435394287\n",
      "Iteration 17876, Loss: 0.05417180061340332\n",
      "Iteration 17877, Loss: 0.053988780826330185\n",
      "Iteration 17878, Loss: 0.054171763360500336\n",
      "Iteration 17879, Loss: 0.053988926112651825\n",
      "Iteration 17880, Loss: 0.054171763360500336\n",
      "Iteration 17881, Loss: 0.05398900434374809\n",
      "Iteration 17882, Loss: 0.054171763360500336\n",
      "Iteration 17883, Loss: 0.053988855332136154\n",
      "Iteration 17884, Loss: 0.05417187511920929\n",
      "Iteration 17885, Loss: 0.053988855332136154\n",
      "Iteration 17886, Loss: 0.05417172610759735\n",
      "Iteration 17887, Loss: 0.05398888513445854\n",
      "Iteration 17888, Loss: 0.05417187139391899\n",
      "Iteration 17889, Loss: 0.05398896336555481\n",
      "Iteration 17890, Loss: 0.05417171120643616\n",
      "Iteration 17891, Loss: 0.053989164531230927\n",
      "Iteration 17892, Loss: 0.05417156219482422\n",
      "Iteration 17893, Loss: 0.05398932099342346\n",
      "Iteration 17894, Loss: 0.05417148023843765\n",
      "Iteration 17895, Loss: 0.05398932844400406\n",
      "Iteration 17896, Loss: 0.054171331226825714\n",
      "Iteration 17897, Loss: 0.05398928374052048\n",
      "Iteration 17898, Loss: 0.05417148396372795\n",
      "Iteration 17899, Loss: 0.05398912355303764\n",
      "Iteration 17900, Loss: 0.05417172238230705\n",
      "Iteration 17901, Loss: 0.05398901551961899\n",
      "Iteration 17902, Loss: 0.05417187511920929\n",
      "Iteration 17903, Loss: 0.053988926112651825\n",
      "Iteration 17904, Loss: 0.05417191982269287\n",
      "Iteration 17905, Loss: 0.05398869514465332\n",
      "Iteration 17906, Loss: 0.05417202040553093\n",
      "Iteration 17907, Loss: 0.053988926112651825\n",
      "Iteration 17908, Loss: 0.05417179316282272\n",
      "Iteration 17909, Loss: 0.05398908257484436\n",
      "Iteration 17910, Loss: 0.05417151376605034\n",
      "Iteration 17911, Loss: 0.053989361971616745\n",
      "Iteration 17912, Loss: 0.05417143553495407\n",
      "Iteration 17913, Loss: 0.05398940294981003\n",
      "Iteration 17914, Loss: 0.05417124554514885\n",
      "Iteration 17915, Loss: 0.053989361971616745\n",
      "Iteration 17916, Loss: 0.05417144298553467\n",
      "Iteration 17917, Loss: 0.05398920178413391\n",
      "Iteration 17918, Loss: 0.05417172238230705\n",
      "Iteration 17919, Loss: 0.05398900806903839\n",
      "Iteration 17920, Loss: 0.054171882569789886\n",
      "Iteration 17921, Loss: 0.05398888513445854\n",
      "Iteration 17922, Loss: 0.05417192727327347\n",
      "Iteration 17923, Loss: 0.05398881062865257\n",
      "Iteration 17924, Loss: 0.054171960800886154\n",
      "Iteration 17925, Loss: 0.0539887361228466\n",
      "Iteration 17926, Loss: 0.05417191982269287\n",
      "Iteration 17927, Loss: 0.05398881062865257\n",
      "Iteration 17928, Loss: 0.05417187511920929\n",
      "Iteration 17929, Loss: 0.053989045321941376\n",
      "Iteration 17930, Loss: 0.05417171120643616\n",
      "Iteration 17931, Loss: 0.05398912355303764\n",
      "Iteration 17932, Loss: 0.05417144298553467\n",
      "Iteration 17933, Loss: 0.053989432752132416\n",
      "Iteration 17934, Loss: 0.05417143553495407\n",
      "Iteration 17935, Loss: 0.05398940294981003\n",
      "Iteration 17936, Loss: 0.05417132377624512\n",
      "Iteration 17937, Loss: 0.053989361971616745\n",
      "Iteration 17938, Loss: 0.05417140573263168\n",
      "Iteration 17939, Loss: 0.053989168256521225\n",
      "Iteration 17940, Loss: 0.05417148396372795\n",
      "Iteration 17941, Loss: 0.05398909002542496\n",
      "Iteration 17942, Loss: 0.0541716031730175\n",
      "Iteration 17943, Loss: 0.05398909002542496\n",
      "Iteration 17944, Loss: 0.054171524941921234\n",
      "Iteration 17945, Loss: 0.05398901551961899\n",
      "Iteration 17946, Loss: 0.054171763360500336\n",
      "Iteration 17947, Loss: 0.05398896336555481\n",
      "Iteration 17948, Loss: 0.05417172238230705\n",
      "Iteration 17949, Loss: 0.05398900434374809\n",
      "Iteration 17950, Loss: 0.05417183041572571\n",
      "Iteration 17951, Loss: 0.053988974541425705\n",
      "Iteration 17952, Loss: 0.0541716031730175\n",
      "Iteration 17953, Loss: 0.05398900434374809\n",
      "Iteration 17954, Loss: 0.05417167767882347\n",
      "Iteration 17955, Loss: 0.053989045321941376\n",
      "Iteration 17956, Loss: 0.05417167767882347\n",
      "Iteration 17957, Loss: 0.05398892983794212\n",
      "Iteration 17958, Loss: 0.05417163670063019\n",
      "Iteration 17959, Loss: 0.053989045321941376\n",
      "Iteration 17960, Loss: 0.05417163670063019\n",
      "Iteration 17961, Loss: 0.053989242762327194\n",
      "Iteration 17962, Loss: 0.05417143926024437\n",
      "Iteration 17963, Loss: 0.053989432752132416\n",
      "Iteration 17964, Loss: 0.05417139455676079\n",
      "Iteration 17965, Loss: 0.05398867651820183\n",
      "Iteration 17966, Loss: 0.054171353578567505\n",
      "Iteration 17967, Loss: 0.053988903760910034\n",
      "Iteration 17968, Loss: 0.05417066812515259\n",
      "Iteration 17969, Loss: 0.053988780826330185\n",
      "Iteration 17970, Loss: 0.05417093634605408\n",
      "Iteration 17971, Loss: 0.05398854613304138\n",
      "Iteration 17972, Loss: 0.05417117476463318\n",
      "Iteration 17973, Loss: 0.05398822948336601\n",
      "Iteration 17974, Loss: 0.054171301424503326\n",
      "Iteration 17975, Loss: 0.053988151252269745\n",
      "Iteration 17976, Loss: 0.05417118966579437\n",
      "Iteration 17977, Loss: 0.05398822948336601\n",
      "Iteration 17978, Loss: 0.054171256721019745\n",
      "Iteration 17979, Loss: 0.053988389670848846\n",
      "Iteration 17980, Loss: 0.05417101830244064\n",
      "Iteration 17981, Loss: 0.0539885088801384\n",
      "Iteration 17982, Loss: 0.05417073890566826\n",
      "Iteration 17983, Loss: 0.0539887472987175\n",
      "Iteration 17984, Loss: 0.05417066067457199\n",
      "Iteration 17985, Loss: 0.053988825529813766\n",
      "Iteration 17986, Loss: 0.054170697927474976\n",
      "Iteration 17987, Loss: 0.053988706320524216\n",
      "Iteration 17988, Loss: 0.05417085811495781\n",
      "Iteration 17989, Loss: 0.053988512605428696\n",
      "Iteration 17990, Loss: 0.05417102575302124\n",
      "Iteration 17991, Loss: 0.05398831516504288\n",
      "Iteration 17992, Loss: 0.054171107709407806\n",
      "Iteration 17993, Loss: 0.05398822948336601\n",
      "Iteration 17994, Loss: 0.054171256721019745\n",
      "Iteration 17995, Loss: 0.053988195955753326\n",
      "Iteration 17996, Loss: 0.05417121574282646\n",
      "Iteration 17997, Loss: 0.05398815870285034\n",
      "Iteration 17998, Loss: 0.05417121574282646\n",
      "Iteration 17999, Loss: 0.05398815497756004\n",
      "Iteration 18000, Loss: 0.05417117476463318\n",
      "Iteration 18001, Loss: 0.053988274186849594\n",
      "Iteration 18002, Loss: 0.05417124554514885\n",
      "Iteration 18003, Loss: 0.053988389670848846\n",
      "Iteration 18004, Loss: 0.05417116731405258\n",
      "Iteration 18005, Loss: 0.05398854613304138\n",
      "Iteration 18006, Loss: 0.05417105555534363\n",
      "Iteration 18007, Loss: 0.0539885088801384\n",
      "Iteration 18008, Loss: 0.05417101830244064\n",
      "Iteration 18009, Loss: 0.0539885088801384\n",
      "Iteration 18010, Loss: 0.05417090281844139\n",
      "Iteration 18011, Loss: 0.053988318890333176\n",
      "Iteration 18012, Loss: 0.05417105555534363\n",
      "Iteration 18013, Loss: 0.05398831516504288\n",
      "Iteration 18014, Loss: 0.05417114123702049\n",
      "Iteration 18015, Loss: 0.05398815870285034\n",
      "Iteration 18016, Loss: 0.05417121574282646\n",
      "Iteration 18017, Loss: 0.05398815870285034\n",
      "Iteration 18018, Loss: 0.05417116731405258\n",
      "Iteration 18019, Loss: 0.053988393396139145\n",
      "Iteration 18020, Loss: 0.05417097732424736\n",
      "Iteration 18021, Loss: 0.053988661617040634\n",
      "Iteration 18022, Loss: 0.054170817136764526\n",
      "Iteration 18023, Loss: 0.053988706320524216\n",
      "Iteration 18024, Loss: 0.05417082458734512\n",
      "Iteration 18025, Loss: 0.05398862063884735\n",
      "Iteration 18026, Loss: 0.05417089909315109\n",
      "Iteration 18027, Loss: 0.053988587111234665\n",
      "Iteration 18028, Loss: 0.05417098104953766\n",
      "Iteration 18029, Loss: 0.05398834869265556\n",
      "Iteration 18030, Loss: 0.054171063005924225\n",
      "Iteration 18031, Loss: 0.05398830771446228\n",
      "Iteration 18032, Loss: 0.05417124554514885\n",
      "Iteration 18033, Loss: 0.05398834869265556\n",
      "Iteration 18034, Loss: 0.05417097732424736\n",
      "Iteration 18035, Loss: 0.05398835986852646\n",
      "Iteration 18036, Loss: 0.05417093634605408\n",
      "Iteration 18037, Loss: 0.05398862808942795\n",
      "Iteration 18038, Loss: 0.05417089909315109\n",
      "Iteration 18039, Loss: 0.053988706320524216\n",
      "Iteration 18040, Loss: 0.05417085811495781\n",
      "Iteration 18041, Loss: 0.05398866534233093\n",
      "Iteration 18042, Loss: 0.05417086184024811\n",
      "Iteration 18043, Loss: 0.053988657891750336\n",
      "Iteration 18044, Loss: 0.05417109653353691\n",
      "Iteration 18045, Loss: 0.053988270461559296\n",
      "Iteration 18046, Loss: 0.054171331226825714\n",
      "Iteration 18047, Loss: 0.05398818850517273\n",
      "Iteration 18048, Loss: 0.05417129397392273\n",
      "Iteration 18049, Loss: 0.05398811399936676\n",
      "Iteration 18050, Loss: 0.05417141318321228\n",
      "Iteration 18051, Loss: 0.053988076746463776\n",
      "Iteration 18052, Loss: 0.05417126044631004\n",
      "Iteration 18053, Loss: 0.05398812144994736\n",
      "Iteration 18054, Loss: 0.05417121574282646\n",
      "Iteration 18055, Loss: 0.05398842692375183\n",
      "Iteration 18056, Loss: 0.05417101830244064\n",
      "Iteration 18057, Loss: 0.05398854613304138\n",
      "Iteration 18058, Loss: 0.05417085811495781\n",
      "Iteration 18059, Loss: 0.05398866534233093\n",
      "Iteration 18060, Loss: 0.05417070910334587\n",
      "Iteration 18061, Loss: 0.05398869514465332\n",
      "Iteration 18062, Loss: 0.05417075380682945\n",
      "Iteration 18063, Loss: 0.05398854613304138\n",
      "Iteration 18064, Loss: 0.054170720279216766\n",
      "Iteration 18065, Loss: 0.0539885088801384\n",
      "Iteration 18066, Loss: 0.054170794785022736\n",
      "Iteration 18067, Loss: 0.053988318890333176\n",
      "Iteration 18068, Loss: 0.05417086184024811\n",
      "Iteration 18069, Loss: 0.0539885088801384\n",
      "Iteration 18070, Loss: 0.05417082831263542\n",
      "Iteration 18071, Loss: 0.0539885088801384\n",
      "Iteration 18072, Loss: 0.054170794785022736\n",
      "Iteration 18073, Loss: 0.05398843437433243\n",
      "Iteration 18074, Loss: 0.05417090654373169\n",
      "Iteration 18075, Loss: 0.053988467901945114\n",
      "Iteration 18076, Loss: 0.054171063005924225\n",
      "Iteration 18077, Loss: 0.053988393396139145\n",
      "Iteration 18078, Loss: 0.05417121574282646\n",
      "Iteration 18079, Loss: 0.053988195955753326\n",
      "Iteration 18080, Loss: 0.05417141318321228\n",
      "Iteration 18081, Loss: 0.05398799106478691\n",
      "Iteration 18082, Loss: 0.054171349853277206\n",
      "Iteration 18083, Loss: 0.053987957537174225\n",
      "Iteration 18084, Loss: 0.05417133495211601\n",
      "Iteration 18085, Loss: 0.05398818850517273\n",
      "Iteration 18086, Loss: 0.05417106673121452\n",
      "Iteration 18087, Loss: 0.053988389670848846\n",
      "Iteration 18088, Loss: 0.05417074263095856\n",
      "Iteration 18089, Loss: 0.05398859828710556\n",
      "Iteration 18090, Loss: 0.05417051166296005\n",
      "Iteration 18091, Loss: 0.05398894101381302\n",
      "Iteration 18092, Loss: 0.05417058616876602\n",
      "Iteration 18093, Loss: 0.0539887472987175\n",
      "Iteration 18094, Loss: 0.0541706308722496\n",
      "Iteration 18095, Loss: 0.05398881435394287\n",
      "Iteration 18096, Loss: 0.05417082831263542\n",
      "Iteration 18097, Loss: 0.05398854613304138\n",
      "Iteration 18098, Loss: 0.05417109653353691\n",
      "Iteration 18099, Loss: 0.053988270461559296\n",
      "Iteration 18100, Loss: 0.054171305149793625\n",
      "Iteration 18101, Loss: 0.053988002240657806\n",
      "Iteration 18102, Loss: 0.05417138338088989\n",
      "Iteration 18103, Loss: 0.05398803576827049\n",
      "Iteration 18104, Loss: 0.054171331226825714\n",
      "Iteration 18105, Loss: 0.05398834869265556\n",
      "Iteration 18106, Loss: 0.05417116731405258\n",
      "Iteration 18107, Loss: 0.053988389670848846\n",
      "Iteration 18108, Loss: 0.05417074263095856\n",
      "Iteration 18109, Loss: 0.053988710045814514\n",
      "Iteration 18110, Loss: 0.05417054891586304\n",
      "Iteration 18111, Loss: 0.053988900035619736\n",
      "Iteration 18112, Loss: 0.05417051538825035\n",
      "Iteration 18113, Loss: 0.0539887472987175\n",
      "Iteration 18114, Loss: 0.05417070910334587\n",
      "Iteration 18115, Loss: 0.053988587111234665\n",
      "Iteration 18116, Loss: 0.054170988500118256\n",
      "Iteration 18117, Loss: 0.05398834869265556\n",
      "Iteration 18118, Loss: 0.05417106673121452\n",
      "Iteration 18119, Loss: 0.05398824065923691\n",
      "Iteration 18120, Loss: 0.05417114496231079\n",
      "Iteration 18121, Loss: 0.053988199681043625\n",
      "Iteration 18122, Loss: 0.05417106673121452\n",
      "Iteration 18123, Loss: 0.053988199681043625\n",
      "Iteration 18124, Loss: 0.05417106673121452\n",
      "Iteration 18125, Loss: 0.05398815870285034\n",
      "Iteration 18126, Loss: 0.05417110025882721\n",
      "Iteration 18127, Loss: 0.053988393396139145\n",
      "Iteration 18128, Loss: 0.05417097732424736\n",
      "Iteration 18129, Loss: 0.053988587111234665\n",
      "Iteration 18130, Loss: 0.05417085811495781\n",
      "Iteration 18131, Loss: 0.05398859828710556\n",
      "Iteration 18132, Loss: 0.05417078733444214\n",
      "Iteration 18133, Loss: 0.053988587111234665\n",
      "Iteration 18134, Loss: 0.05417086184024811\n",
      "Iteration 18135, Loss: 0.05398839712142944\n",
      "Iteration 18136, Loss: 0.05417082831263542\n",
      "Iteration 18137, Loss: 0.05398839712142944\n",
      "Iteration 18138, Loss: 0.054170988500118256\n",
      "Iteration 18139, Loss: 0.053988393396139145\n",
      "Iteration 18140, Loss: 0.05417094752192497\n",
      "Iteration 18141, Loss: 0.05398831516504288\n",
      "Iteration 18142, Loss: 0.054170988500118256\n",
      "Iteration 18143, Loss: 0.05398835986852646\n",
      "Iteration 18144, Loss: 0.05417102575302124\n",
      "Iteration 18145, Loss: 0.05398824065923691\n",
      "Iteration 18146, Loss: 0.054171111434698105\n",
      "Iteration 18147, Loss: 0.05398815870285034\n",
      "Iteration 18148, Loss: 0.054171036928892136\n",
      "Iteration 18149, Loss: 0.05398803949356079\n",
      "Iteration 18150, Loss: 0.054171107709407806\n",
      "Iteration 18151, Loss: 0.05398812144994736\n",
      "Iteration 18152, Loss: 0.054171107709407806\n",
      "Iteration 18153, Loss: 0.05398812144994736\n",
      "Iteration 18154, Loss: 0.054171137511730194\n",
      "Iteration 18155, Loss: 0.053988318890333176\n",
      "Iteration 18156, Loss: 0.05417101830244064\n",
      "Iteration 18157, Loss: 0.05398862808942795\n",
      "Iteration 18158, Loss: 0.05417078733444214\n",
      "Iteration 18159, Loss: 0.05398866534233093\n",
      "Iteration 18160, Loss: 0.05417066812515259\n",
      "Iteration 18161, Loss: 0.05398871749639511\n",
      "Iteration 18162, Loss: 0.05417074263095856\n",
      "Iteration 18163, Loss: 0.05398866534233093\n",
      "Iteration 18164, Loss: 0.054170794785022736\n",
      "Iteration 18165, Loss: 0.053988512605428696\n",
      "Iteration 18166, Loss: 0.054170988500118256\n",
      "Iteration 18167, Loss: 0.05398834869265556\n",
      "Iteration 18168, Loss: 0.054171107709407806\n",
      "Iteration 18169, Loss: 0.05398812144994736\n",
      "Iteration 18170, Loss: 0.05417107045650482\n",
      "Iteration 18171, Loss: 0.053988080471754074\n",
      "Iteration 18172, Loss: 0.054171036928892136\n",
      "Iteration 18173, Loss: 0.05398812144994736\n",
      "Iteration 18174, Loss: 0.054171137511730194\n",
      "Iteration 18175, Loss: 0.05398835986852646\n",
      "Iteration 18176, Loss: 0.05417101830244064\n",
      "Iteration 18177, Loss: 0.05398862808942795\n",
      "Iteration 18178, Loss: 0.054170675575733185\n",
      "Iteration 18179, Loss: 0.05398866534233093\n",
      "Iteration 18180, Loss: 0.054170750081539154\n",
      "Iteration 18181, Loss: 0.05398867279291153\n",
      "Iteration 18182, Loss: 0.05417070910334587\n",
      "Iteration 18183, Loss: 0.053988587111234665\n",
      "Iteration 18184, Loss: 0.054170794785022736\n",
      "Iteration 18185, Loss: 0.05398839712142944\n",
      "Iteration 18186, Loss: 0.05417094752192497\n",
      "Iteration 18187, Loss: 0.05398824065923691\n",
      "Iteration 18188, Loss: 0.054171107709407806\n",
      "Iteration 18189, Loss: 0.05398830771446228\n",
      "Iteration 18190, Loss: 0.05417114496231079\n",
      "Iteration 18191, Loss: 0.05398815870285034\n",
      "Iteration 18192, Loss: 0.05417102575302124\n",
      "Iteration 18193, Loss: 0.053988151252269745\n",
      "Iteration 18194, Loss: 0.05417121574282646\n",
      "Iteration 18195, Loss: 0.05398835241794586\n",
      "Iteration 18196, Loss: 0.05417093634605408\n",
      "Iteration 18197, Loss: 0.05398862808942795\n",
      "Iteration 18198, Loss: 0.05417078733444214\n",
      "Iteration 18199, Loss: 0.0539887398481369\n",
      "Iteration 18200, Loss: 0.05417058989405632\n",
      "Iteration 18201, Loss: 0.05398859828710556\n",
      "Iteration 18202, Loss: 0.054170750081539154\n",
      "Iteration 18203, Loss: 0.053988512605428696\n",
      "Iteration 18204, Loss: 0.054170988500118256\n",
      "Iteration 18205, Loss: 0.053988274186849594\n",
      "Iteration 18206, Loss: 0.05417122691869736\n",
      "Iteration 18207, Loss: 0.05398812144994736\n",
      "Iteration 18208, Loss: 0.054171036928892136\n",
      "Iteration 18209, Loss: 0.05398812144994736\n",
      "Iteration 18210, Loss: 0.054171107709407806\n",
      "Iteration 18211, Loss: 0.053988199681043625\n",
      "Iteration 18212, Loss: 0.05417101830244064\n",
      "Iteration 18213, Loss: 0.05398843437433243\n",
      "Iteration 18214, Loss: 0.05417089909315109\n",
      "Iteration 18215, Loss: 0.05398847162723541\n",
      "Iteration 18216, Loss: 0.05417077988386154\n",
      "Iteration 18217, Loss: 0.05398881435394287\n",
      "Iteration 18218, Loss: 0.05417074263095856\n",
      "Iteration 18219, Loss: 0.05398870259523392\n",
      "Iteration 18220, Loss: 0.054170750081539154\n",
      "Iteration 18221, Loss: 0.053988583385944366\n",
      "Iteration 18222, Loss: 0.05417094752192497\n",
      "Iteration 18223, Loss: 0.05398824065923691\n",
      "Iteration 18224, Loss: 0.05417102575302124\n",
      "Iteration 18225, Loss: 0.05398815870285034\n",
      "Iteration 18226, Loss: 0.054171063005924225\n",
      "Iteration 18227, Loss: 0.05398824065923691\n",
      "Iteration 18228, Loss: 0.05417109653353691\n",
      "Iteration 18229, Loss: 0.05398827791213989\n",
      "Iteration 18230, Loss: 0.05417089909315109\n",
      "Iteration 18231, Loss: 0.05398855358362198\n",
      "Iteration 18232, Loss: 0.054170869290828705\n",
      "Iteration 18233, Loss: 0.053988631814718246\n",
      "Iteration 18234, Loss: 0.05417082831263542\n",
      "Iteration 18235, Loss: 0.053988512605428696\n",
      "Iteration 18236, Loss: 0.05417082831263542\n",
      "Iteration 18237, Loss: 0.053988318890333176\n",
      "Iteration 18238, Loss: 0.05417117103934288\n",
      "Iteration 18239, Loss: 0.053988199681043625\n",
      "Iteration 18240, Loss: 0.05417106673121452\n",
      "Iteration 18241, Loss: 0.05398815870285034\n",
      "Iteration 18242, Loss: 0.05417121201753616\n",
      "Iteration 18243, Loss: 0.05398827791213989\n",
      "Iteration 18244, Loss: 0.05417086184024811\n",
      "Iteration 18245, Loss: 0.05398847907781601\n",
      "Iteration 18246, Loss: 0.05417061969637871\n",
      "Iteration 18247, Loss: 0.05398878455162048\n",
      "Iteration 18248, Loss: 0.05417027696967125\n",
      "Iteration 18249, Loss: 0.053988873958587646\n",
      "Iteration 18250, Loss: 0.0541701540350914\n",
      "Iteration 18251, Loss: 0.053989142179489136\n",
      "Iteration 18252, Loss: 0.0541701465845108\n",
      "Iteration 18253, Loss: 0.05398918315768242\n",
      "Iteration 18254, Loss: 0.0541701577603817\n",
      "Iteration 18255, Loss: 0.053989022970199585\n",
      "Iteration 18256, Loss: 0.0541706308722496\n",
      "Iteration 18257, Loss: 0.05398862808942795\n",
      "Iteration 18258, Loss: 0.05417067930102348\n",
      "Iteration 18259, Loss: 0.05398839712142944\n",
      "Iteration 18260, Loss: 0.054170869290828705\n",
      "Iteration 18261, Loss: 0.05398835986852646\n",
      "Iteration 18262, Loss: 0.05417075380682945\n",
      "Iteration 18263, Loss: 0.05398847162723541\n",
      "Iteration 18264, Loss: 0.054170891642570496\n",
      "Iteration 18265, Loss: 0.053988587111234665\n",
      "Iteration 18266, Loss: 0.054170697927474976\n",
      "Iteration 18267, Loss: 0.053988780826330185\n",
      "Iteration 18268, Loss: 0.05417061969637871\n",
      "Iteration 18269, Loss: 0.05398893356323242\n",
      "Iteration 18270, Loss: 0.054170578718185425\n",
      "Iteration 18271, Loss: 0.053988825529813766\n",
      "Iteration 18272, Loss: 0.05417043715715408\n",
      "Iteration 18273, Loss: 0.0539887510240078\n",
      "Iteration 18274, Loss: 0.05417073890566826\n",
      "Iteration 18275, Loss: 0.053988706320524216\n",
      "Iteration 18276, Loss: 0.05417070910334587\n",
      "Iteration 18277, Loss: 0.05398862063884735\n",
      "Iteration 18278, Loss: 0.05417070910334587\n",
      "Iteration 18279, Loss: 0.053988661617040634\n",
      "Iteration 18280, Loss: 0.05417077988386154\n",
      "Iteration 18281, Loss: 0.05398866534233093\n",
      "Iteration 18282, Loss: 0.05417061969637871\n",
      "Iteration 18283, Loss: 0.05398867651820183\n",
      "Iteration 18284, Loss: 0.054170578718185425\n",
      "Iteration 18285, Loss: 0.053988900035619736\n",
      "Iteration 18286, Loss: 0.05417054891586304\n",
      "Iteration 18287, Loss: 0.05398886650800705\n",
      "Iteration 18288, Loss: 0.05417058989405632\n",
      "Iteration 18289, Loss: 0.053988631814718246\n",
      "Iteration 18290, Loss: 0.054170750081539154\n",
      "Iteration 18291, Loss: 0.05398847907781601\n",
      "Iteration 18292, Loss: 0.05417090281844139\n",
      "Iteration 18293, Loss: 0.053988438099622726\n",
      "Iteration 18294, Loss: 0.05417082831263542\n",
      "Iteration 18295, Loss: 0.05398847162723541\n",
      "Iteration 18296, Loss: 0.05417082831263542\n",
      "Iteration 18297, Loss: 0.053988438099622726\n",
      "Iteration 18298, Loss: 0.05417078360915184\n",
      "Iteration 18299, Loss: 0.053988587111234665\n",
      "Iteration 18300, Loss: 0.05417077988386154\n",
      "Iteration 18301, Loss: 0.053988587111234665\n",
      "Iteration 18302, Loss: 0.05417051166296005\n",
      "Iteration 18303, Loss: 0.053988900035619736\n",
      "Iteration 18304, Loss: 0.05417050048708916\n",
      "Iteration 18305, Loss: 0.05398901551961899\n",
      "Iteration 18306, Loss: 0.05417042225599289\n",
      "Iteration 18307, Loss: 0.05398913472890854\n",
      "Iteration 18308, Loss: 0.05417028069496155\n",
      "Iteration 18309, Loss: 0.053988974541425705\n",
      "Iteration 18310, Loss: 0.05417051166296005\n",
      "Iteration 18311, Loss: 0.05398878455162048\n",
      "Iteration 18312, Loss: 0.05417058989405632\n",
      "Iteration 18313, Loss: 0.053988631814718246\n",
      "Iteration 18314, Loss: 0.054170750081539154\n",
      "Iteration 18315, Loss: 0.053988512605428696\n",
      "Iteration 18316, Loss: 0.05417067930102348\n",
      "Iteration 18317, Loss: 0.05398906394839287\n",
      "Iteration 18318, Loss: 0.05417071282863617\n",
      "Iteration 18319, Loss: 0.053989022970199585\n",
      "Iteration 18320, Loss: 0.05417126417160034\n",
      "Iteration 18321, Loss: 0.053989022970199585\n",
      "Iteration 18322, Loss: 0.054171379655599594\n",
      "Iteration 18323, Loss: 0.05398895591497421\n",
      "Iteration 18324, Loss: 0.054171498864889145\n",
      "Iteration 18325, Loss: 0.0539889857172966\n",
      "Iteration 18326, Loss: 0.054171573370695114\n",
      "Iteration 18327, Loss: 0.05398886650800705\n",
      "Iteration 18328, Loss: 0.054171618074178696\n",
      "Iteration 18329, Loss: 0.05398879200220108\n",
      "Iteration 18330, Loss: 0.054171543568372726\n",
      "Iteration 18331, Loss: 0.053988680243492126\n",
      "Iteration 18332, Loss: 0.054171495139598846\n",
      "Iteration 18333, Loss: 0.05398883670568466\n",
      "Iteration 18334, Loss: 0.05417134612798691\n",
      "Iteration 18335, Loss: 0.0539889894425869\n",
      "Iteration 18336, Loss: 0.05417141318321228\n",
      "Iteration 18337, Loss: 0.0539889857172966\n",
      "Iteration 18338, Loss: 0.05417126417160034\n",
      "Iteration 18339, Loss: 0.05398903414607048\n",
      "Iteration 18340, Loss: 0.05417138338088989\n",
      "Iteration 18341, Loss: 0.05398891493678093\n",
      "Iteration 18342, Loss: 0.05417153984308243\n",
      "Iteration 18343, Loss: 0.05398883670568466\n",
      "Iteration 18344, Loss: 0.05417165160179138\n",
      "Iteration 18345, Loss: 0.05398871749639511\n",
      "Iteration 18346, Loss: 0.05417169630527496\n",
      "Iteration 18347, Loss: 0.05398859828710556\n",
      "Iteration 18348, Loss: 0.05417158827185631\n",
      "Iteration 18349, Loss: 0.053988635540008545\n",
      "Iteration 18350, Loss: 0.054171543568372726\n",
      "Iteration 18351, Loss: 0.053988825529813766\n",
      "Iteration 18352, Loss: 0.054171569645404816\n",
      "Iteration 18353, Loss: 0.05398891493678093\n",
      "Iteration 18354, Loss: 0.05417140945792198\n",
      "Iteration 18355, Loss: 0.05398918688297272\n",
      "Iteration 18356, Loss: 0.054171137511730194\n",
      "Iteration 18357, Loss: 0.05398941785097122\n",
      "Iteration 18358, Loss: 0.05417083948850632\n",
      "Iteration 18359, Loss: 0.05398934334516525\n",
      "Iteration 18360, Loss: 0.05417094752192497\n",
      "Iteration 18361, Loss: 0.05398926883935928\n",
      "Iteration 18362, Loss: 0.05417122691869736\n",
      "Iteration 18363, Loss: 0.05398910492658615\n",
      "Iteration 18364, Loss: 0.054171424359083176\n",
      "Iteration 18365, Loss: 0.05398879572749138\n",
      "Iteration 18366, Loss: 0.05417158454656601\n",
      "Iteration 18367, Loss: 0.05398860201239586\n",
      "Iteration 18368, Loss: 0.05417166277766228\n",
      "Iteration 18369, Loss: 0.053988635540008545\n",
      "Iteration 18370, Loss: 0.05417157709598541\n",
      "Iteration 18371, Loss: 0.05398879200220108\n",
      "Iteration 18372, Loss: 0.0541716143488884\n",
      "Iteration 18373, Loss: 0.05398887023329735\n",
      "Iteration 18374, Loss: 0.054171573370695114\n",
      "Iteration 18375, Loss: 0.05398894473910332\n",
      "Iteration 18376, Loss: 0.054171573370695114\n",
      "Iteration 18377, Loss: 0.05398894473910332\n",
      "Iteration 18378, Loss: 0.05417153239250183\n",
      "Iteration 18379, Loss: 0.05398894473910332\n",
      "Iteration 18380, Loss: 0.054171424359083176\n",
      "Iteration 18381, Loss: 0.05398879200220108\n",
      "Iteration 18382, Loss: 0.054171424359083176\n",
      "Iteration 18383, Loss: 0.0539889894425869\n",
      "Iteration 18384, Loss: 0.054171379655599594\n",
      "Iteration 18385, Loss: 0.05398895591497421\n",
      "Iteration 18386, Loss: 0.054171234369277954\n",
      "Iteration 18387, Loss: 0.053988873958587646\n",
      "Iteration 18388, Loss: 0.054171305149793625\n",
      "Iteration 18389, Loss: 0.05398895591497421\n",
      "Iteration 18390, Loss: 0.054171301424503326\n",
      "Iteration 18391, Loss: 0.05398918315768242\n",
      "Iteration 18392, Loss: 0.054171256721019745\n",
      "Iteration 18393, Loss: 0.05398937687277794\n",
      "Iteration 18394, Loss: 0.054170992225408554\n",
      "Iteration 18395, Loss: 0.05398918688297272\n",
      "Iteration 18396, Loss: 0.05417122691869736\n",
      "Iteration 18397, Loss: 0.05398910492658615\n",
      "Iteration 18398, Loss: 0.054171305149793625\n",
      "Iteration 18399, Loss: 0.05398891493678093\n",
      "Iteration 18400, Loss: 0.05417150259017944\n",
      "Iteration 18401, Loss: 0.05398883670568466\n",
      "Iteration 18402, Loss: 0.05417150259017944\n",
      "Iteration 18403, Loss: 0.053988873958587646\n",
      "Iteration 18404, Loss: 0.054171498864889145\n",
      "Iteration 18405, Loss: 0.0539889894425869\n",
      "Iteration 18406, Loss: 0.054171498864889145\n",
      "Iteration 18407, Loss: 0.053988873958587646\n",
      "Iteration 18408, Loss: 0.054171498864889145\n",
      "Iteration 18409, Loss: 0.053988754749298096\n",
      "Iteration 18410, Loss: 0.05417158454656601\n",
      "Iteration 18411, Loss: 0.05398879200220108\n",
      "Iteration 18412, Loss: 0.05417150259017944\n",
      "Iteration 18413, Loss: 0.053988754749298096\n",
      "Iteration 18414, Loss: 0.05417146906256676\n",
      "Iteration 18415, Loss: 0.053988900035619736\n",
      "Iteration 18416, Loss: 0.05417150259017944\n",
      "Iteration 18417, Loss: 0.05398879200220108\n",
      "Iteration 18418, Loss: 0.05417146533727646\n",
      "Iteration 18419, Loss: 0.053988873958587646\n",
      "Iteration 18420, Loss: 0.05417134612798691\n",
      "Iteration 18421, Loss: 0.05398906394839287\n",
      "Iteration 18422, Loss: 0.05417126417160034\n",
      "Iteration 18423, Loss: 0.05398906394839287\n",
      "Iteration 18424, Loss: 0.0541713647544384\n",
      "Iteration 18425, Loss: 0.053989261388778687\n",
      "Iteration 18426, Loss: 0.05417102575302124\n",
      "Iteration 18427, Loss: 0.05398934334516525\n",
      "Iteration 18428, Loss: 0.05417122691869736\n",
      "Iteration 18429, Loss: 0.053989142179489136\n",
      "Iteration 18430, Loss: 0.05417138338088989\n",
      "Iteration 18431, Loss: 0.053988829255104065\n",
      "Iteration 18432, Loss: 0.054171621799468994\n",
      "Iteration 18433, Loss: 0.05398867279291153\n",
      "Iteration 18434, Loss: 0.05417177081108093\n",
      "Iteration 18435, Loss: 0.05398855730891228\n",
      "Iteration 18436, Loss: 0.054171547293663025\n",
      "Iteration 18437, Loss: 0.05398878455162048\n",
      "Iteration 18438, Loss: 0.05417146533727646\n",
      "Iteration 18439, Loss: 0.053988978266716\n",
      "Iteration 18440, Loss: 0.05417145416140556\n",
      "Iteration 18441, Loss: 0.05398895591497421\n",
      "Iteration 18442, Loss: 0.05417121574282646\n",
      "Iteration 18443, Loss: 0.053989261388778687\n",
      "Iteration 18444, Loss: 0.054171137511730194\n",
      "Iteration 18445, Loss: 0.053989261388778687\n",
      "Iteration 18446, Loss: 0.05417114496231079\n",
      "Iteration 18447, Loss: 0.05398918315768242\n",
      "Iteration 18448, Loss: 0.05417126417160034\n",
      "Iteration 18449, Loss: 0.053989022970199585\n",
      "Iteration 18450, Loss: 0.05417157709598541\n",
      "Iteration 18451, Loss: 0.05398879572749138\n",
      "Iteration 18452, Loss: 0.05417170375585556\n",
      "Iteration 18453, Loss: 0.05398844927549362\n",
      "Iteration 18454, Loss: 0.05417189747095108\n",
      "Iteration 18455, Loss: 0.05398836359381676\n",
      "Iteration 18456, Loss: 0.05417170375585556\n",
      "Iteration 18457, Loss: 0.05398867651820183\n",
      "Iteration 18458, Loss: 0.0541716143488884\n",
      "Iteration 18459, Loss: 0.05398872122168541\n",
      "Iteration 18460, Loss: 0.05417141318321228\n",
      "Iteration 18461, Loss: 0.05398895591497421\n",
      "Iteration 18462, Loss: 0.05417114496231079\n",
      "Iteration 18463, Loss: 0.05398934334516525\n",
      "Iteration 18464, Loss: 0.05417105555534363\n",
      "Iteration 18465, Loss: 0.053989529609680176\n",
      "Iteration 18466, Loss: 0.0541711300611496\n",
      "Iteration 18467, Loss: 0.05398930236697197\n",
      "Iteration 18468, Loss: 0.05417121946811676\n",
      "Iteration 18469, Loss: 0.05398915335536003\n",
      "Iteration 18470, Loss: 0.05417134612798691\n",
      "Iteration 18471, Loss: 0.053988903760910034\n",
      "Iteration 18472, Loss: 0.054171621799468994\n",
      "Iteration 18473, Loss: 0.05398855358362198\n",
      "Iteration 18474, Loss: 0.05417197570204735\n",
      "Iteration 18475, Loss: 0.05398839712142944\n",
      "Iteration 18476, Loss: 0.05417190119624138\n",
      "Iteration 18477, Loss: 0.05398835986852646\n",
      "Iteration 18478, Loss: 0.054171860218048096\n",
      "Iteration 18479, Loss: 0.053988516330718994\n",
      "Iteration 18480, Loss: 0.054171688854694366\n",
      "Iteration 18481, Loss: 0.05398876219987869\n",
      "Iteration 18482, Loss: 0.054171569645404816\n",
      "Iteration 18483, Loss: 0.05398906394839287\n",
      "Iteration 18484, Loss: 0.05417126417160034\n",
      "Iteration 18485, Loss: 0.05398918315768242\n",
      "Iteration 18486, Loss: 0.05417133495211601\n",
      "Iteration 18487, Loss: 0.0539889931678772\n",
      "Iteration 18488, Loss: 0.05417145788669586\n",
      "Iteration 18489, Loss: 0.05398879200220108\n",
      "Iteration 18490, Loss: 0.05417164787650108\n",
      "Iteration 18491, Loss: 0.053988754749298096\n",
      "Iteration 18492, Loss: 0.054171688854694366\n",
      "Iteration 18493, Loss: 0.05398894473910332\n",
      "Iteration 18494, Loss: 0.05417145416140556\n",
      "Iteration 18495, Loss: 0.05398906394839287\n",
      "Iteration 18496, Loss: 0.054171495139598846\n",
      "Iteration 18497, Loss: 0.05398895591497421\n",
      "Iteration 18498, Loss: 0.05417133867740631\n",
      "Iteration 18499, Loss: 0.05398906394839287\n",
      "Iteration 18500, Loss: 0.054171305149793625\n",
      "Iteration 18501, Loss: 0.0539889894425869\n",
      "Iteration 18502, Loss: 0.05417138338088989\n",
      "Iteration 18503, Loss: 0.053989022970199585\n",
      "Iteration 18504, Loss: 0.05417153239250183\n",
      "Iteration 18505, Loss: 0.05398883670568466\n",
      "Iteration 18506, Loss: 0.05417153239250183\n",
      "Iteration 18507, Loss: 0.05398883670568466\n",
      "Iteration 18508, Loss: 0.05417145416140556\n",
      "Iteration 18509, Loss: 0.053988903760910034\n",
      "Iteration 18510, Loss: 0.05417133495211601\n",
      "Iteration 18511, Loss: 0.053989142179489136\n",
      "Iteration 18512, Loss: 0.054171375930309296\n",
      "Iteration 18513, Loss: 0.053989022970199585\n",
      "Iteration 18514, Loss: 0.05417133867740631\n",
      "Iteration 18515, Loss: 0.053989067673683167\n",
      "Iteration 18516, Loss: 0.054171495139598846\n",
      "Iteration 18517, Loss: 0.0539889857172966\n",
      "Iteration 18518, Loss: 0.05417153984308243\n",
      "Iteration 18519, Loss: 0.05398879572749138\n",
      "Iteration 18520, Loss: 0.054171573370695114\n",
      "Iteration 18521, Loss: 0.053988829255104065\n",
      "Iteration 18522, Loss: 0.05417153239250183\n",
      "Iteration 18523, Loss: 0.05398894473910332\n",
      "Iteration 18524, Loss: 0.054171305149793625\n",
      "Iteration 18525, Loss: 0.053989022970199585\n",
      "Iteration 18526, Loss: 0.054171379655599594\n",
      "Iteration 18527, Loss: 0.05398917943239212\n",
      "Iteration 18528, Loss: 0.05417133867740631\n",
      "Iteration 18529, Loss: 0.053989142179489136\n",
      "Iteration 18530, Loss: 0.05417129397392273\n",
      "Iteration 18531, Loss: 0.053989261388778687\n",
      "Iteration 18532, Loss: 0.054171185940504074\n",
      "Iteration 18533, Loss: 0.05398907884955406\n",
      "Iteration 18534, Loss: 0.054171182215213776\n",
      "Iteration 18535, Loss: 0.05398914963006973\n",
      "Iteration 18536, Loss: 0.05417122691869736\n",
      "Iteration 18537, Loss: 0.05398918315768242\n",
      "Iteration 18538, Loss: 0.05417127534747124\n",
      "Iteration 18539, Loss: 0.05398879572749138\n",
      "Iteration 18540, Loss: 0.05417158454656601\n",
      "Iteration 18541, Loss: 0.05398867279291153\n",
      "Iteration 18542, Loss: 0.054171737283468246\n",
      "Iteration 18543, Loss: 0.05398867651820183\n",
      "Iteration 18544, Loss: 0.054171573370695114\n",
      "Iteration 18545, Loss: 0.05398879572749138\n",
      "Iteration 18546, Loss: 0.05417148768901825\n",
      "Iteration 18547, Loss: 0.05398910492658615\n",
      "Iteration 18548, Loss: 0.05417124927043915\n",
      "Iteration 18549, Loss: 0.053989227861166\n",
      "Iteration 18550, Loss: 0.05417102202773094\n",
      "Iteration 18551, Loss: 0.05398945137858391\n",
      "Iteration 18552, Loss: 0.054170992225408554\n",
      "Iteration 18553, Loss: 0.05398934334516525\n",
      "Iteration 18554, Loss: 0.05417114496231079\n",
      "Iteration 18555, Loss: 0.053989261388778687\n",
      "Iteration 18556, Loss: 0.05417133867740631\n",
      "Iteration 18557, Loss: 0.0539889894425869\n",
      "Iteration 18558, Loss: 0.05417157709598541\n",
      "Iteration 18559, Loss: 0.0539887510240078\n",
      "Iteration 18560, Loss: 0.05417173355817795\n",
      "Iteration 18561, Loss: 0.053988635540008545\n",
      "Iteration 18562, Loss: 0.05417173355817795\n",
      "Iteration 18563, Loss: 0.053988706320524216\n",
      "Iteration 18564, Loss: 0.05417153984308243\n",
      "Iteration 18565, Loss: 0.05398879572749138\n",
      "Iteration 18566, Loss: 0.05417145416140556\n",
      "Iteration 18567, Loss: 0.053989067673683167\n",
      "Iteration 18568, Loss: 0.05417129024863243\n",
      "Iteration 18569, Loss: 0.053989261388778687\n",
      "Iteration 18570, Loss: 0.05417121201753616\n",
      "Iteration 18571, Loss: 0.05398934334516525\n",
      "Iteration 18572, Loss: 0.054171256721019745\n",
      "Iteration 18573, Loss: 0.05398910492658615\n",
      "Iteration 18574, Loss: 0.05417145416140556\n",
      "Iteration 18575, Loss: 0.0539889857172966\n",
      "Iteration 18576, Loss: 0.05417150259017944\n",
      "Iteration 18577, Loss: 0.053988754749298096\n",
      "Iteration 18578, Loss: 0.0541718453168869\n",
      "Iteration 18579, Loss: 0.05398866534233093\n",
      "Iteration 18580, Loss: 0.05417165905237198\n",
      "Iteration 18581, Loss: 0.05398879200220108\n",
      "Iteration 18582, Loss: 0.054171543568372726\n",
      "Iteration 18583, Loss: 0.053988873958587646\n",
      "Iteration 18584, Loss: 0.05417145416140556\n",
      "Iteration 18585, Loss: 0.05398895591497421\n",
      "Iteration 18586, Loss: 0.05417126417160034\n",
      "Iteration 18587, Loss: 0.05398906394839287\n",
      "Iteration 18588, Loss: 0.05417114496231079\n",
      "Iteration 18589, Loss: 0.05398915335536003\n",
      "Iteration 18590, Loss: 0.05417106673121452\n",
      "Iteration 18591, Loss: 0.053989261388778687\n",
      "Iteration 18592, Loss: 0.05417117476463318\n",
      "Iteration 18593, Loss: 0.053989261388778687\n",
      "Iteration 18594, Loss: 0.05417126417160034\n",
      "Iteration 18595, Loss: 0.05398903042078018\n",
      "Iteration 18596, Loss: 0.054171424359083176\n",
      "Iteration 18597, Loss: 0.05398894473910332\n",
      "Iteration 18598, Loss: 0.05417168140411377\n",
      "Iteration 18599, Loss: 0.05398879572749138\n",
      "Iteration 18600, Loss: 0.054171573370695114\n",
      "Iteration 18601, Loss: 0.053988873958587646\n",
      "Iteration 18602, Loss: 0.05417153239250183\n",
      "Iteration 18603, Loss: 0.053989022970199585\n",
      "Iteration 18604, Loss: 0.05417145416140556\n",
      "Iteration 18605, Loss: 0.053989022970199585\n",
      "Iteration 18606, Loss: 0.05417145788669586\n",
      "Iteration 18607, Loss: 0.05398894473910332\n",
      "Iteration 18608, Loss: 0.054171573370695114\n",
      "Iteration 18609, Loss: 0.05398879572749138\n",
      "Iteration 18610, Loss: 0.05417157709598541\n",
      "Iteration 18611, Loss: 0.05398879200220108\n",
      "Iteration 18612, Loss: 0.054171543568372726\n",
      "Iteration 18613, Loss: 0.053988903760910034\n",
      "Iteration 18614, Loss: 0.05417134612798691\n",
      "Iteration 18615, Loss: 0.0539889857172966\n",
      "Iteration 18616, Loss: 0.05417122691869736\n",
      "Iteration 18617, Loss: 0.053988512605428696\n",
      "Iteration 18618, Loss: 0.05417129397392273\n",
      "Iteration 18619, Loss: 0.05398847162723541\n",
      "Iteration 18620, Loss: 0.054171495139598846\n",
      "Iteration 18621, Loss: 0.053988195955753326\n",
      "Iteration 18622, Loss: 0.054172419011592865\n",
      "Iteration 18623, Loss: 0.05398784205317497\n",
      "Iteration 18624, Loss: 0.05417267978191376\n",
      "Iteration 18625, Loss: 0.053987957537174225\n",
      "Iteration 18626, Loss: 0.0541723370552063\n",
      "Iteration 18627, Loss: 0.05398811399936676\n",
      "Iteration 18628, Loss: 0.0541720911860466\n",
      "Iteration 18629, Loss: 0.05398835986852646\n",
      "Iteration 18630, Loss: 0.05417153239250183\n",
      "Iteration 18631, Loss: 0.05398879572749138\n",
      "Iteration 18632, Loss: 0.05417110025882721\n",
      "Iteration 18633, Loss: 0.053989700973033905\n",
      "Iteration 18634, Loss: 0.05417082831263542\n",
      "Iteration 18635, Loss: 0.05398992821574211\n",
      "Iteration 18636, Loss: 0.05417133867740631\n",
      "Iteration 18637, Loss: 0.053990285843610764\n",
      "Iteration 18638, Loss: 0.05417153239250183\n",
      "Iteration 18639, Loss: 0.05399024486541748\n",
      "Iteration 18640, Loss: 0.05417153239250183\n",
      "Iteration 18641, Loss: 0.053990207612514496\n",
      "Iteration 18642, Loss: 0.05417114123702049\n",
      "Iteration 18643, Loss: 0.05399016663432121\n",
      "Iteration 18644, Loss: 0.054171305149793625\n",
      "Iteration 18645, Loss: 0.05398985743522644\n",
      "Iteration 18646, Loss: 0.05417158454656601\n",
      "Iteration 18647, Loss: 0.05398954451084137\n",
      "Iteration 18648, Loss: 0.05417190119624138\n",
      "Iteration 18649, Loss: 0.0539892315864563\n",
      "Iteration 18650, Loss: 0.05417206138372421\n",
      "Iteration 18651, Loss: 0.053989142179489136\n",
      "Iteration 18652, Loss: 0.05417221039533615\n",
      "Iteration 18653, Loss: 0.05398918315768242\n",
      "Iteration 18654, Loss: 0.054172322154045105\n",
      "Iteration 18655, Loss: 0.0539889931678772\n",
      "Iteration 18656, Loss: 0.0541720949113369\n",
      "Iteration 18657, Loss: 0.053989261388778687\n",
      "Iteration 18658, Loss: 0.054172009229660034\n",
      "Iteration 18659, Loss: 0.0539894625544548\n",
      "Iteration 18660, Loss: 0.05417189002037048\n",
      "Iteration 18661, Loss: 0.0539894700050354\n",
      "Iteration 18662, Loss: 0.054171815514564514\n",
      "Iteration 18663, Loss: 0.05398957431316376\n",
      "Iteration 18664, Loss: 0.054171815514564514\n",
      "Iteration 18665, Loss: 0.0539894625544548\n",
      "Iteration 18666, Loss: 0.05417205020785332\n",
      "Iteration 18667, Loss: 0.05398937687277794\n",
      "Iteration 18668, Loss: 0.054172009229660034\n",
      "Iteration 18669, Loss: 0.05398949980735779\n",
      "Iteration 18670, Loss: 0.054171930998563766\n",
      "Iteration 18671, Loss: 0.05398949980735779\n",
      "Iteration 18672, Loss: 0.0541718527674675\n",
      "Iteration 18673, Loss: 0.05398934334516525\n",
      "Iteration 18674, Loss: 0.054171930998563766\n",
      "Iteration 18675, Loss: 0.05398934707045555\n",
      "Iteration 18676, Loss: 0.05417189747095108\n",
      "Iteration 18677, Loss: 0.0539894625544548\n",
      "Iteration 18678, Loss: 0.05417177826166153\n",
      "Iteration 18679, Loss: 0.05398957058787346\n",
      "Iteration 18680, Loss: 0.054171930998563766\n",
      "Iteration 18681, Loss: 0.05398942157626152\n",
      "Iteration 18682, Loss: 0.05417189002037048\n",
      "Iteration 18683, Loss: 0.05398871749639511\n",
      "Iteration 18684, Loss: 0.054172009229660034\n",
      "Iteration 18685, Loss: 0.05398864671587944\n",
      "Iteration 18686, Loss: 0.054171234369277954\n",
      "Iteration 18687, Loss: 0.05398879572749138\n",
      "Iteration 18688, Loss: 0.054171185940504074\n",
      "Iteration 18689, Loss: 0.0539889931678772\n",
      "Iteration 18690, Loss: 0.0541708841919899\n",
      "Iteration 18691, Loss: 0.05398910865187645\n",
      "Iteration 18692, Loss: 0.05417080223560333\n",
      "Iteration 18693, Loss: 0.05398903414607048\n",
      "Iteration 18694, Loss: 0.05417092889547348\n",
      "Iteration 18695, Loss: 0.05398891493678093\n",
      "Iteration 18696, Loss: 0.054171040654182434\n",
      "Iteration 18697, Loss: 0.05398891493678093\n",
      "Iteration 18698, Loss: 0.05417108163237572\n",
      "Iteration 18699, Loss: 0.05398872494697571\n",
      "Iteration 18700, Loss: 0.054171159863471985\n",
      "Iteration 18701, Loss: 0.05398864299058914\n",
      "Iteration 18702, Loss: 0.05417124181985855\n",
      "Iteration 18703, Loss: 0.053988486528396606\n",
      "Iteration 18704, Loss: 0.054171234369277954\n",
      "Iteration 18705, Loss: 0.05398856848478317\n",
      "Iteration 18706, Loss: 0.054171234369277954\n",
      "Iteration 18707, Loss: 0.05398856848478317\n",
      "Iteration 18708, Loss: 0.0541711151599884\n",
      "Iteration 18709, Loss: 0.05398879572749138\n",
      "Iteration 18710, Loss: 0.054170966148376465\n",
      "Iteration 18711, Loss: 0.05398888513445854\n",
      "Iteration 18712, Loss: 0.054170966148376465\n",
      "Iteration 18713, Loss: 0.053989000618457794\n",
      "Iteration 18714, Loss: 0.05417119711637497\n",
      "Iteration 18715, Loss: 0.05398868769407272\n",
      "Iteration 18716, Loss: 0.05417124554514885\n",
      "Iteration 18717, Loss: 0.05398840829730034\n",
      "Iteration 18718, Loss: 0.05417148396372795\n",
      "Iteration 18719, Loss: 0.05398833006620407\n",
      "Iteration 18720, Loss: 0.05417140573263168\n",
      "Iteration 18721, Loss: 0.05398833006620407\n",
      "Iteration 18722, Loss: 0.054171279072761536\n",
      "Iteration 18723, Loss: 0.05398860573768616\n",
      "Iteration 18724, Loss: 0.054171230643987656\n",
      "Iteration 18725, Loss: 0.05398876592516899\n",
      "Iteration 18726, Loss: 0.05417095869779587\n",
      "Iteration 18727, Loss: 0.053989045321941376\n",
      "Iteration 18728, Loss: 0.05417069047689438\n",
      "Iteration 18729, Loss: 0.053989045321941376\n",
      "Iteration 18730, Loss: 0.0541708841919899\n",
      "Iteration 18731, Loss: 0.0539889931678772\n",
      "Iteration 18732, Loss: 0.05417107790708542\n",
      "Iteration 18733, Loss: 0.05398869514465332\n",
      "Iteration 18734, Loss: 0.05417124554514885\n",
      "Iteration 18735, Loss: 0.05398852750658989\n",
      "Iteration 18736, Loss: 0.05417140573263168\n",
      "Iteration 18737, Loss: 0.05398852378129959\n",
      "Iteration 18738, Loss: 0.0541713647544384\n",
      "Iteration 18739, Loss: 0.05398844927549362\n",
      "Iteration 18740, Loss: 0.054171424359083176\n",
      "Iteration 18741, Loss: 0.05398853123188019\n",
      "Iteration 18742, Loss: 0.054171156138181686\n",
      "Iteration 18743, Loss: 0.05398876965045929\n",
      "Iteration 18744, Loss: 0.05417094752192497\n",
      "Iteration 18745, Loss: 0.05398903414607048\n",
      "Iteration 18746, Loss: 0.054170724004507065\n",
      "Iteration 18747, Loss: 0.05398926883935928\n",
      "Iteration 18748, Loss: 0.05417069047689438\n",
      "Iteration 18749, Loss: 0.05398911237716675\n",
      "Iteration 18750, Loss: 0.0541708841919899\n",
      "Iteration 18751, Loss: 0.05398910492658615\n",
      "Iteration 18752, Loss: 0.05417104810476303\n",
      "Iteration 18753, Loss: 0.05398868769407272\n",
      "Iteration 18754, Loss: 0.054171279072761536\n",
      "Iteration 18755, Loss: 0.05398845300078392\n",
      "Iteration 18756, Loss: 0.05417148396372795\n",
      "Iteration 18757, Loss: 0.05398833006620407\n",
      "Iteration 18758, Loss: 0.05417155474424362\n",
      "Iteration 18759, Loss: 0.05398828908801079\n",
      "Iteration 18760, Loss: 0.05417143553495407\n",
      "Iteration 18761, Loss: 0.053988367319107056\n",
      "Iteration 18762, Loss: 0.05417120084166527\n",
      "Iteration 18763, Loss: 0.05398845672607422\n",
      "Iteration 18764, Loss: 0.054171230643987656\n",
      "Iteration 18765, Loss: 0.05398879572749138\n",
      "Iteration 18766, Loss: 0.05417092889547348\n",
      "Iteration 18767, Loss: 0.05398895591497421\n",
      "Iteration 18768, Loss: 0.054170966148376465\n",
      "Iteration 18769, Loss: 0.05398895591497421\n",
      "Iteration 18770, Loss: 0.054171234369277954\n",
      "Iteration 18771, Loss: 0.05398868769407272\n",
      "Iteration 18772, Loss: 0.05417131632566452\n",
      "Iteration 18773, Loss: 0.053988635540008545\n",
      "Iteration 18774, Loss: 0.054171204566955566\n",
      "Iteration 18775, Loss: 0.05398855730891228\n",
      "Iteration 18776, Loss: 0.05417120084166527\n",
      "Iteration 18777, Loss: 0.05398864671587944\n",
      "Iteration 18778, Loss: 0.054171234369277954\n",
      "Iteration 18779, Loss: 0.05398868769407272\n",
      "Iteration 18780, Loss: 0.05417104810476303\n",
      "Iteration 18781, Loss: 0.05398879572749138\n",
      "Iteration 18782, Loss: 0.05417124554514885\n",
      "Iteration 18783, Loss: 0.05398856848478317\n",
      "Iteration 18784, Loss: 0.054171472787857056\n",
      "Iteration 18785, Loss: 0.053988367319107056\n",
      "Iteration 18786, Loss: 0.05417140573263168\n",
      "Iteration 18787, Loss: 0.05398833006620407\n",
      "Iteration 18788, Loss: 0.05417140573263168\n",
      "Iteration 18789, Loss: 0.05398844927549362\n",
      "Iteration 18790, Loss: 0.054171353578567505\n",
      "Iteration 18791, Loss: 0.05398844927549362\n",
      "Iteration 18792, Loss: 0.05417139083147049\n",
      "Iteration 18793, Loss: 0.05398860573768616\n",
      "Iteration 18794, Loss: 0.0541711151599884\n",
      "Iteration 18795, Loss: 0.05398872494697571\n",
      "Iteration 18796, Loss: 0.05417100340127945\n",
      "Iteration 18797, Loss: 0.053988806903362274\n",
      "Iteration 18798, Loss: 0.0541708879172802\n",
      "Iteration 18799, Loss: 0.0539889931678772\n",
      "Iteration 18800, Loss: 0.054170966148376465\n",
      "Iteration 18801, Loss: 0.05398888513445854\n",
      "Iteration 18802, Loss: 0.05417100712656975\n",
      "Iteration 18803, Loss: 0.05398868769407272\n",
      "Iteration 18804, Loss: 0.054171137511730194\n",
      "Iteration 18805, Loss: 0.05398844927549362\n",
      "Iteration 18806, Loss: 0.054171450436115265\n",
      "Iteration 18807, Loss: 0.05398821085691452\n",
      "Iteration 18808, Loss: 0.05417163670063019\n",
      "Iteration 18809, Loss: 0.05398816987872124\n",
      "Iteration 18810, Loss: 0.05417140573263168\n",
      "Iteration 18811, Loss: 0.05398844927549362\n",
      "Iteration 18812, Loss: 0.054171353578567505\n",
      "Iteration 18813, Loss: 0.05398856848478317\n",
      "Iteration 18814, Loss: 0.054170966148376465\n",
      "Iteration 18815, Loss: 0.05398884043097496\n",
      "Iteration 18816, Loss: 0.0541708879172802\n",
      "Iteration 18817, Loss: 0.05398884415626526\n",
      "Iteration 18818, Loss: 0.05417100340127945\n",
      "Iteration 18819, Loss: 0.05398884043097496\n",
      "Iteration 18820, Loss: 0.0541711151599884\n",
      "Iteration 18821, Loss: 0.05398879572749138\n",
      "Iteration 18822, Loss: 0.05417120084166527\n",
      "Iteration 18823, Loss: 0.05398872122168541\n",
      "Iteration 18824, Loss: 0.05417127534747124\n",
      "Iteration 18825, Loss: 0.05398840829730034\n",
      "Iteration 18826, Loss: 0.054171279072761536\n",
      "Iteration 18827, Loss: 0.05398857221007347\n",
      "Iteration 18828, Loss: 0.0541711151599884\n",
      "Iteration 18829, Loss: 0.053988806903362274\n",
      "Iteration 18830, Loss: 0.05417100712656975\n",
      "Iteration 18831, Loss: 0.053988806903362274\n",
      "Iteration 18832, Loss: 0.054170966148376465\n",
      "Iteration 18833, Loss: 0.05398884043097496\n",
      "Iteration 18834, Loss: 0.054170966148376465\n",
      "Iteration 18835, Loss: 0.05398884043097496\n",
      "Iteration 18836, Loss: 0.05417100712656975\n",
      "Iteration 18837, Loss: 0.05398891121149063\n",
      "Iteration 18838, Loss: 0.054171085357666016\n",
      "Iteration 18839, Loss: 0.053988754749298096\n",
      "Iteration 18840, Loss: 0.05417131632566452\n",
      "Iteration 18841, Loss: 0.05398852750658989\n",
      "Iteration 18842, Loss: 0.05417132377624512\n",
      "Iteration 18843, Loss: 0.05398844927549362\n",
      "Iteration 18844, Loss: 0.054171472787857056\n",
      "Iteration 18845, Loss: 0.05398841202259064\n",
      "Iteration 18846, Loss: 0.054171398282051086\n",
      "Iteration 18847, Loss: 0.05398830026388168\n",
      "Iteration 18848, Loss: 0.054171472787857056\n",
      "Iteration 18849, Loss: 0.05398844927549362\n",
      "Iteration 18850, Loss: 0.05417127534747124\n",
      "Iteration 18851, Loss: 0.0539884939789772\n",
      "Iteration 18852, Loss: 0.054171230643987656\n",
      "Iteration 18853, Loss: 0.05398879572749138\n",
      "Iteration 18854, Loss: 0.0541708879172802\n",
      "Iteration 18855, Loss: 0.0539889931678772\n",
      "Iteration 18856, Loss: 0.0541708879172802\n",
      "Iteration 18857, Loss: 0.05398888513445854\n",
      "Iteration 18858, Loss: 0.05417092889547348\n",
      "Iteration 18859, Loss: 0.053988873958587646\n",
      "Iteration 18860, Loss: 0.054170966148376465\n",
      "Iteration 18861, Loss: 0.05398883670568466\n",
      "Iteration 18862, Loss: 0.054171159863471985\n",
      "Iteration 18863, Loss: 0.05398864671587944\n",
      "Iteration 18864, Loss: 0.054171234369277954\n",
      "Iteration 18865, Loss: 0.05398864671587944\n",
      "Iteration 18866, Loss: 0.05417127534747124\n",
      "Iteration 18867, Loss: 0.05398852750658989\n",
      "Iteration 18868, Loss: 0.05417132005095482\n",
      "Iteration 18869, Loss: 0.05398837849497795\n",
      "Iteration 18870, Loss: 0.05417132005095482\n",
      "Iteration 18871, Loss: 0.05398860573768616\n",
      "Iteration 18872, Loss: 0.054171234369277954\n",
      "Iteration 18873, Loss: 0.05398865044116974\n",
      "Iteration 18874, Loss: 0.0541711263358593\n",
      "Iteration 18875, Loss: 0.053988613188266754\n",
      "Iteration 18876, Loss: 0.054171156138181686\n",
      "Iteration 18877, Loss: 0.05398856848478317\n",
      "Iteration 18878, Loss: 0.054171159863471985\n",
      "Iteration 18879, Loss: 0.05398872494697571\n",
      "Iteration 18880, Loss: 0.054171156138181686\n",
      "Iteration 18881, Loss: 0.05398868769407272\n",
      "Iteration 18882, Loss: 0.0541711151599884\n",
      "Iteration 18883, Loss: 0.05398871749639511\n",
      "Iteration 18884, Loss: 0.05417119711637497\n",
      "Iteration 18885, Loss: 0.053988754749298096\n",
      "Iteration 18886, Loss: 0.054171156138181686\n",
      "Iteration 18887, Loss: 0.05398883670568466\n",
      "Iteration 18888, Loss: 0.05417116731405258\n",
      "Iteration 18889, Loss: 0.053988754749298096\n",
      "Iteration 18890, Loss: 0.054171353578567505\n",
      "Iteration 18891, Loss: 0.053988635540008545\n",
      "Iteration 18892, Loss: 0.05417132377624512\n",
      "Iteration 18893, Loss: 0.05398844927549362\n",
      "Iteration 18894, Loss: 0.054171398282051086\n",
      "Iteration 18895, Loss: 0.05398830026388168\n",
      "Iteration 18896, Loss: 0.0541713647544384\n",
      "Iteration 18897, Loss: 0.05398852750658989\n",
      "Iteration 18898, Loss: 0.05417116731405258\n",
      "Iteration 18899, Loss: 0.053988538682460785\n",
      "Iteration 18900, Loss: 0.05417107790708542\n",
      "Iteration 18901, Loss: 0.05398872494697571\n",
      "Iteration 18902, Loss: 0.054170843213796616\n",
      "Iteration 18903, Loss: 0.053988926112651825\n",
      "Iteration 18904, Loss: 0.05417067930102348\n",
      "Iteration 18905, Loss: 0.05398915335536003\n",
      "Iteration 18906, Loss: 0.054170601069927216\n",
      "Iteration 18907, Loss: 0.0539892315864563\n",
      "Iteration 18908, Loss: 0.05417056009173393\n",
      "Iteration 18909, Loss: 0.053989313542842865\n",
      "Iteration 18910, Loss: 0.054170798510313034\n",
      "Iteration 18911, Loss: 0.05398903787136078\n",
      "Iteration 18912, Loss: 0.0541711151599884\n",
      "Iteration 18913, Loss: 0.05398852750658989\n",
      "Iteration 18914, Loss: 0.05417143553495407\n",
      "Iteration 18915, Loss: 0.05398828908801079\n",
      "Iteration 18916, Loss: 0.05417155846953392\n",
      "Iteration 18917, Loss: 0.053988248109817505\n",
      "Iteration 18918, Loss: 0.05417140573263168\n",
      "Iteration 18919, Loss: 0.05398837476968765\n",
      "Iteration 18920, Loss: 0.054171353578567505\n",
      "Iteration 18921, Loss: 0.05398860573768616\n",
      "Iteration 18922, Loss: 0.05417108163237572\n",
      "Iteration 18923, Loss: 0.053988806903362274\n",
      "Iteration 18924, Loss: 0.0541708879172802\n",
      "Iteration 18925, Loss: 0.05398903414607048\n",
      "Iteration 18926, Loss: 0.0541708767414093\n",
      "Iteration 18927, Loss: 0.05398903787136078\n",
      "Iteration 18928, Loss: 0.054170966148376465\n",
      "Iteration 18929, Loss: 0.05398888513445854\n",
      "Iteration 18930, Loss: 0.05417100712656975\n",
      "Iteration 18931, Loss: 0.053988806903362274\n",
      "Iteration 18932, Loss: 0.05417127534747124\n",
      "Iteration 18933, Loss: 0.05398864671587944\n",
      "Iteration 18934, Loss: 0.054171353578567505\n",
      "Iteration 18935, Loss: 0.05398840829730034\n",
      "Iteration 18936, Loss: 0.05417132377624512\n",
      "Iteration 18937, Loss: 0.053988419473171234\n",
      "Iteration 18938, Loss: 0.05417127534747124\n",
      "Iteration 18939, Loss: 0.05398860573768616\n",
      "Iteration 18940, Loss: 0.0541711263358593\n",
      "Iteration 18941, Loss: 0.05398872494697571\n",
      "Iteration 18942, Loss: 0.05417104810476303\n",
      "Iteration 18943, Loss: 0.05398879572749138\n",
      "Iteration 18944, Loss: 0.05417108163237572\n",
      "Iteration 18945, Loss: 0.05398876592516899\n",
      "Iteration 18946, Loss: 0.05417100712656975\n",
      "Iteration 18947, Loss: 0.05398868769407272\n",
      "Iteration 18948, Loss: 0.05417107790708542\n",
      "Iteration 18949, Loss: 0.05398864671587944\n",
      "Iteration 18950, Loss: 0.05417104810476303\n",
      "Iteration 18951, Loss: 0.05398860573768616\n",
      "Iteration 18952, Loss: 0.054171122610569\n",
      "Iteration 18953, Loss: 0.05398868769407272\n",
      "Iteration 18954, Loss: 0.0541711263358593\n",
      "Iteration 18955, Loss: 0.05398872122168541\n",
      "Iteration 18956, Loss: 0.05417124554514885\n",
      "Iteration 18957, Loss: 0.05398864299058914\n",
      "Iteration 18958, Loss: 0.05417143553495407\n",
      "Iteration 18959, Loss: 0.05398855730891228\n",
      "Iteration 18960, Loss: 0.05417148023843765\n",
      "Iteration 18961, Loss: 0.05398840829730034\n",
      "Iteration 18962, Loss: 0.0541713647544384\n",
      "Iteration 18963, Loss: 0.0539882592856884\n",
      "Iteration 18964, Loss: 0.05417140573263168\n",
      "Iteration 18965, Loss: 0.053988367319107056\n",
      "Iteration 18966, Loss: 0.05417131632566452\n",
      "Iteration 18967, Loss: 0.05398864671587944\n",
      "Iteration 18968, Loss: 0.05417093262076378\n",
      "Iteration 18969, Loss: 0.05398879572749138\n",
      "Iteration 18970, Loss: 0.05417095869779587\n",
      "Iteration 18971, Loss: 0.05398895591497421\n",
      "Iteration 18972, Loss: 0.054170962423086166\n",
      "Iteration 18973, Loss: 0.05398888513445854\n",
      "Iteration 18974, Loss: 0.054170966148376465\n",
      "Iteration 18975, Loss: 0.05398891866207123\n",
      "Iteration 18976, Loss: 0.0541711263358593\n",
      "Iteration 18977, Loss: 0.05398860573768616\n",
      "Iteration 18978, Loss: 0.05417132377624512\n",
      "Iteration 18979, Loss: 0.05398837476968765\n",
      "Iteration 18980, Loss: 0.05417155846953392\n",
      "Iteration 18981, Loss: 0.05398821458220482\n",
      "Iteration 18982, Loss: 0.054171524941921234\n",
      "Iteration 18983, Loss: 0.05398833006620407\n",
      "Iteration 18984, Loss: 0.054171279072761536\n",
      "Iteration 18985, Loss: 0.05398845672607422\n",
      "Iteration 18986, Loss: 0.05417108163237572\n",
      "Iteration 18987, Loss: 0.05398865044116974\n",
      "Iteration 18988, Loss: 0.05417108163237572\n",
      "Iteration 18989, Loss: 0.05398876592516899\n",
      "Iteration 18990, Loss: 0.05417104810476303\n",
      "Iteration 18991, Loss: 0.05398868769407272\n",
      "Iteration 18992, Loss: 0.05417119711637497\n",
      "Iteration 18993, Loss: 0.05398879572749138\n",
      "Iteration 18994, Loss: 0.054170966148376465\n",
      "Iteration 18995, Loss: 0.05398895591497421\n",
      "Iteration 18996, Loss: 0.05417100712656975\n",
      "Iteration 18997, Loss: 0.05398888513445854\n",
      "Iteration 18998, Loss: 0.05417100712656975\n",
      "Iteration 18999, Loss: 0.05398879572749138\n",
      "Iteration 19000, Loss: 0.0541711263358593\n",
      "Iteration 19001, Loss: 0.05398860573768616\n",
      "Iteration 19002, Loss: 0.05417124181985855\n",
      "Iteration 19003, Loss: 0.053988486528396606\n",
      "Iteration 19004, Loss: 0.05417131632566452\n",
      "Iteration 19005, Loss: 0.05398856848478317\n",
      "Iteration 19006, Loss: 0.05417104810476303\n",
      "Iteration 19007, Loss: 0.05398872494697571\n",
      "Iteration 19008, Loss: 0.054171040654182434\n",
      "Iteration 19009, Loss: 0.05398891493678093\n",
      "Iteration 19010, Loss: 0.05417092144489288\n",
      "Iteration 19011, Loss: 0.05398891866207123\n",
      "Iteration 19012, Loss: 0.0541708879172802\n",
      "Iteration 19013, Loss: 0.05398895964026451\n",
      "Iteration 19014, Loss: 0.05417099595069885\n",
      "Iteration 19015, Loss: 0.05398888140916824\n",
      "Iteration 19016, Loss: 0.0541711263358593\n",
      "Iteration 19017, Loss: 0.05398868769407272\n",
      "Iteration 19018, Loss: 0.05417124554514885\n",
      "Iteration 19019, Loss: 0.05398856848478317\n",
      "Iteration 19020, Loss: 0.054171353578567505\n",
      "Iteration 19021, Loss: 0.053988561034202576\n",
      "Iteration 19022, Loss: 0.054171428084373474\n",
      "Iteration 19023, Loss: 0.05398841202259064\n",
      "Iteration 19024, Loss: 0.0541713610291481\n",
      "Iteration 19025, Loss: 0.053988486528396606\n",
      "Iteration 19026, Loss: 0.05417131632566452\n",
      "Iteration 19027, Loss: 0.05398860201239586\n",
      "Iteration 19028, Loss: 0.05417100712656975\n",
      "Iteration 19029, Loss: 0.05398868769407272\n",
      "Iteration 19030, Loss: 0.05417107790708542\n",
      "Iteration 19031, Loss: 0.05398895591497421\n",
      "Iteration 19032, Loss: 0.054170846939086914\n",
      "Iteration 19033, Loss: 0.053988926112651825\n",
      "Iteration 19034, Loss: 0.05417092889547348\n",
      "Iteration 19035, Loss: 0.05398884415626526\n",
      "Iteration 19036, Loss: 0.054171234369277954\n",
      "Iteration 19037, Loss: 0.05398872122168541\n",
      "Iteration 19038, Loss: 0.054171204566955566\n",
      "Iteration 19039, Loss: 0.05398860201239586\n",
      "Iteration 19040, Loss: 0.05417132377624512\n",
      "Iteration 19041, Loss: 0.05398852750658989\n",
      "Iteration 19042, Loss: 0.054171353578567505\n",
      "Iteration 19043, Loss: 0.053988486528396606\n",
      "Iteration 19044, Loss: 0.05417146906256676\n",
      "Iteration 19045, Loss: 0.05398830026388168\n",
      "Iteration 19046, Loss: 0.054171398282051086\n",
      "Iteration 19047, Loss: 0.053988516330718994\n",
      "Iteration 19048, Loss: 0.054171279072761536\n",
      "Iteration 19049, Loss: 0.05398856848478317\n",
      "Iteration 19050, Loss: 0.05417127534747124\n",
      "Iteration 19051, Loss: 0.05398876219987869\n",
      "Iteration 19052, Loss: 0.054171159863471985\n",
      "Iteration 19053, Loss: 0.05398876219987869\n",
      "Iteration 19054, Loss: 0.05417100712656975\n",
      "Iteration 19055, Loss: 0.05398891493678093\n",
      "Iteration 19056, Loss: 0.0541708879172802\n",
      "Iteration 19057, Loss: 0.05398895591497421\n",
      "Iteration 19058, Loss: 0.05417092889547348\n",
      "Iteration 19059, Loss: 0.05398891866207123\n",
      "Iteration 19060, Loss: 0.05417092889547348\n",
      "Iteration 19061, Loss: 0.053988806903362274\n",
      "Iteration 19062, Loss: 0.05417100340127945\n",
      "Iteration 19063, Loss: 0.05398872494697571\n",
      "Iteration 19064, Loss: 0.05417100712656975\n",
      "Iteration 19065, Loss: 0.05398876592516899\n",
      "Iteration 19066, Loss: 0.05417124181985855\n",
      "Iteration 19067, Loss: 0.05398864299058914\n",
      "Iteration 19068, Loss: 0.054171353578567505\n",
      "Iteration 19069, Loss: 0.05398837849497795\n",
      "Iteration 19070, Loss: 0.05417132377624512\n",
      "Iteration 19071, Loss: 0.053988486528396606\n",
      "Iteration 19072, Loss: 0.0541711263358593\n",
      "Iteration 19073, Loss: 0.053988538682460785\n",
      "Iteration 19074, Loss: 0.05417107790708542\n",
      "Iteration 19075, Loss: 0.05398872494697571\n",
      "Iteration 19076, Loss: 0.0541708879172802\n",
      "Iteration 19077, Loss: 0.05398907512426376\n",
      "Iteration 19078, Loss: 0.054170843213796616\n",
      "Iteration 19079, Loss: 0.053989000618457794\n",
      "Iteration 19080, Loss: 0.05417072772979736\n",
      "Iteration 19081, Loss: 0.05398907512426376\n",
      "Iteration 19082, Loss: 0.054170846939086914\n",
      "Iteration 19083, Loss: 0.053988926112651825\n",
      "Iteration 19084, Loss: 0.05417092889547348\n",
      "Iteration 19085, Loss: 0.0539889931678772\n",
      "Iteration 19086, Loss: 0.05417108163237572\n",
      "Iteration 19087, Loss: 0.05398872494697571\n",
      "Iteration 19088, Loss: 0.05417116731405258\n",
      "Iteration 19089, Loss: 0.05398856848478317\n",
      "Iteration 19090, Loss: 0.054171279072761536\n",
      "Iteration 19091, Loss: 0.05398845300078392\n",
      "Iteration 19092, Loss: 0.054171204566955566\n",
      "Iteration 19093, Loss: 0.05398856848478317\n",
      "Iteration 19094, Loss: 0.054171122610569\n",
      "Iteration 19095, Loss: 0.05398868769407272\n",
      "Iteration 19096, Loss: 0.05417100340127945\n",
      "Iteration 19097, Loss: 0.05398884415626526\n",
      "Iteration 19098, Loss: 0.0541708841919899\n",
      "Iteration 19099, Loss: 0.05398903787136078\n",
      "Iteration 19100, Loss: 0.0541708879172802\n",
      "Iteration 19101, Loss: 0.053988926112651825\n",
      "Iteration 19102, Loss: 0.054171036928892136\n",
      "Iteration 19103, Loss: 0.053988806903362274\n",
      "Iteration 19104, Loss: 0.054171156138181686\n",
      "Iteration 19105, Loss: 0.05398872122168541\n",
      "Iteration 19106, Loss: 0.05417119711637497\n",
      "Iteration 19107, Loss: 0.05398856848478317\n",
      "Iteration 19108, Loss: 0.05417124554514885\n",
      "Iteration 19109, Loss: 0.05398856848478317\n",
      "Iteration 19110, Loss: 0.05417124181985855\n",
      "Iteration 19111, Loss: 0.053988680243492126\n",
      "Iteration 19112, Loss: 0.054171085357666016\n",
      "Iteration 19113, Loss: 0.05398864671587944\n",
      "Iteration 19114, Loss: 0.054171085357666016\n",
      "Iteration 19115, Loss: 0.05398869514465332\n",
      "Iteration 19116, Loss: 0.05417100712656975\n",
      "Iteration 19117, Loss: 0.05398876592516899\n",
      "Iteration 19118, Loss: 0.054170966148376465\n",
      "Iteration 19119, Loss: 0.05398876965045929\n",
      "Iteration 19120, Loss: 0.05417104810476303\n",
      "Iteration 19121, Loss: 0.05398872494697571\n",
      "Iteration 19122, Loss: 0.05417131632566452\n",
      "Iteration 19123, Loss: 0.05398860573768616\n",
      "Iteration 19124, Loss: 0.05417132005095482\n",
      "Iteration 19125, Loss: 0.05398852750658989\n",
      "Iteration 19126, Loss: 0.05417143553495407\n",
      "Iteration 19127, Loss: 0.05398840829730034\n",
      "Iteration 19128, Loss: 0.05417143553495407\n",
      "Iteration 19129, Loss: 0.053988486528396606\n",
      "Iteration 19130, Loss: 0.05417127162218094\n",
      "Iteration 19131, Loss: 0.05398861691355705\n",
      "Iteration 19132, Loss: 0.05417100712656975\n",
      "Iteration 19133, Loss: 0.05398872494697571\n",
      "Iteration 19134, Loss: 0.05417073518037796\n",
      "Iteration 19135, Loss: 0.0539889931678772\n",
      "Iteration 19136, Loss: 0.054170843213796616\n",
      "Iteration 19137, Loss: 0.053989000618457794\n",
      "Iteration 19138, Loss: 0.054170846939086914\n",
      "Iteration 19139, Loss: 0.0539889931678772\n",
      "Iteration 19140, Loss: 0.05417104810476303\n",
      "Iteration 19141, Loss: 0.05398883670568466\n",
      "Iteration 19142, Loss: 0.05417124554514885\n",
      "Iteration 19143, Loss: 0.05398845672607422\n",
      "Iteration 19144, Loss: 0.05417124554514885\n",
      "Iteration 19145, Loss: 0.053988486528396606\n",
      "Iteration 19146, Loss: 0.054171349853277206\n",
      "Iteration 19147, Loss: 0.05398856848478317\n",
      "Iteration 19148, Loss: 0.054171085357666016\n",
      "Iteration 19149, Loss: 0.05398872494697571\n",
      "Iteration 19150, Loss: 0.054171040654182434\n",
      "Iteration 19151, Loss: 0.05398891493678093\n",
      "Iteration 19152, Loss: 0.05417099595069885\n",
      "Iteration 19153, Loss: 0.05398891493678093\n",
      "Iteration 19154, Loss: 0.05417100712656975\n",
      "Iteration 19155, Loss: 0.05398876592516899\n",
      "Iteration 19156, Loss: 0.05417124554514885\n",
      "Iteration 19157, Loss: 0.05398856848478317\n",
      "Iteration 19158, Loss: 0.054171279072761536\n",
      "Iteration 19159, Loss: 0.05398833751678467\n",
      "Iteration 19160, Loss: 0.05417128652334213\n",
      "Iteration 19161, Loss: 0.053988486528396606\n",
      "Iteration 19162, Loss: 0.05417117103934288\n",
      "Iteration 19163, Loss: 0.05398864671587944\n",
      "Iteration 19164, Loss: 0.05417104810476303\n",
      "Iteration 19165, Loss: 0.05398879945278168\n",
      "Iteration 19166, Loss: 0.054170891642570496\n",
      "Iteration 19167, Loss: 0.05398883670568466\n",
      "Iteration 19168, Loss: 0.05417100712656975\n",
      "Iteration 19169, Loss: 0.05398891493678093\n",
      "Iteration 19170, Loss: 0.05417092889547348\n",
      "Iteration 19171, Loss: 0.053988806903362274\n",
      "Iteration 19172, Loss: 0.054171085357666016\n",
      "Iteration 19173, Loss: 0.05398872494697571\n",
      "Iteration 19174, Loss: 0.05417116731405258\n",
      "Iteration 19175, Loss: 0.05398852750658989\n",
      "Iteration 19176, Loss: 0.05417121201753616\n",
      "Iteration 19177, Loss: 0.05398845300078392\n",
      "Iteration 19178, Loss: 0.054171204566955566\n",
      "Iteration 19179, Loss: 0.05398841202259064\n",
      "Iteration 19180, Loss: 0.05417128652334213\n",
      "Iteration 19181, Loss: 0.053988419473171234\n",
      "Iteration 19182, Loss: 0.054171279072761536\n",
      "Iteration 19183, Loss: 0.05398856848478317\n",
      "Iteration 19184, Loss: 0.0541711263358593\n",
      "Iteration 19185, Loss: 0.05398868769407272\n",
      "Iteration 19186, Loss: 0.05417108163237572\n",
      "Iteration 19187, Loss: 0.05398876592516899\n",
      "Iteration 19188, Loss: 0.05417100712656975\n",
      "Iteration 19189, Loss: 0.05398883670568466\n",
      "Iteration 19190, Loss: 0.054170891642570496\n",
      "Iteration 19191, Loss: 0.05398895591497421\n",
      "Iteration 19192, Loss: 0.05417093262076378\n",
      "Iteration 19193, Loss: 0.05398876592516899\n",
      "Iteration 19194, Loss: 0.054171204566955566\n",
      "Iteration 19195, Loss: 0.053988754749298096\n",
      "Iteration 19196, Loss: 0.05417132005095482\n",
      "Iteration 19197, Loss: 0.05398837849497795\n",
      "Iteration 19198, Loss: 0.054171137511730194\n",
      "Iteration 19199, Loss: 0.05398852378129959\n",
      "Iteration 19200, Loss: 0.05417120084166527\n",
      "Iteration 19201, Loss: 0.0539884977042675\n",
      "Iteration 19202, Loss: 0.054170966148376465\n",
      "Iteration 19203, Loss: 0.05398876592516899\n",
      "Iteration 19204, Loss: 0.05417077988386154\n",
      "Iteration 19205, Loss: 0.05398895964026451\n",
      "Iteration 19206, Loss: 0.05417073518037796\n",
      "Iteration 19207, Loss: 0.053989000618457794\n",
      "Iteration 19208, Loss: 0.05417073890566826\n",
      "Iteration 19209, Loss: 0.053988926112651825\n",
      "Iteration 19210, Loss: 0.054170966148376465\n",
      "Iteration 19211, Loss: 0.05398883670568466\n",
      "Iteration 19212, Loss: 0.0541711263358593\n",
      "Iteration 19213, Loss: 0.05398867651820183\n",
      "Iteration 19214, Loss: 0.05417139455676079\n",
      "Iteration 19215, Loss: 0.053988561034202576\n",
      "Iteration 19216, Loss: 0.0541713647544384\n",
      "Iteration 19217, Loss: 0.05398833006620407\n",
      "Iteration 19218, Loss: 0.05417129397392273\n",
      "Iteration 19219, Loss: 0.053988292813301086\n",
      "Iteration 19220, Loss: 0.05417128652334213\n",
      "Iteration 19221, Loss: 0.05398852750658989\n",
      "Iteration 19222, Loss: 0.054171085357666016\n",
      "Iteration 19223, Loss: 0.05398861691355705\n",
      "Iteration 19224, Loss: 0.05417107790708542\n",
      "Iteration 19225, Loss: 0.05398868769407272\n",
      "Iteration 19226, Loss: 0.05417100712656975\n",
      "Iteration 19227, Loss: 0.053988538682460785\n",
      "Iteration 19228, Loss: 0.054171085357666016\n",
      "Iteration 19229, Loss: 0.05398856848478317\n",
      "Iteration 19230, Loss: 0.05417116731405258\n",
      "Iteration 19231, Loss: 0.05398845300078392\n",
      "Iteration 19232, Loss: 0.054171137511730194\n",
      "Iteration 19233, Loss: 0.05398853123188019\n",
      "Iteration 19234, Loss: 0.0541711263358593\n",
      "Iteration 19235, Loss: 0.05398857221007347\n",
      "Iteration 19236, Loss: 0.05417108163237572\n",
      "Iteration 19237, Loss: 0.05398869141936302\n",
      "Iteration 19238, Loss: 0.05417107790708542\n",
      "Iteration 19239, Loss: 0.053988732397556305\n",
      "Iteration 19240, Loss: 0.05417100340127945\n",
      "Iteration 19241, Loss: 0.053988806903362274\n",
      "Iteration 19242, Loss: 0.054171040654182434\n",
      "Iteration 19243, Loss: 0.05398884415626526\n",
      "Iteration 19244, Loss: 0.05417100340127945\n",
      "Iteration 19245, Loss: 0.053988657891750336\n",
      "Iteration 19246, Loss: 0.054170891642570496\n",
      "Iteration 19247, Loss: 0.05398861691355705\n",
      "Iteration 19248, Loss: 0.054171010851860046\n",
      "Iteration 19249, Loss: 0.05398860573768616\n",
      "Iteration 19250, Loss: 0.054171137511730194\n",
      "Iteration 19251, Loss: 0.053988419473171234\n",
      "Iteration 19252, Loss: 0.05417116731405258\n",
      "Iteration 19253, Loss: 0.05398856848478317\n",
      "Iteration 19254, Loss: 0.054171204566955566\n",
      "Iteration 19255, Loss: 0.05398864671587944\n",
      "Iteration 19256, Loss: 0.05417109280824661\n",
      "Iteration 19257, Loss: 0.05398868769407272\n",
      "Iteration 19258, Loss: 0.05417108163237572\n",
      "Iteration 19259, Loss: 0.053988732397556305\n",
      "Iteration 19260, Loss: 0.05417107790708542\n",
      "Iteration 19261, Loss: 0.05398868769407272\n",
      "Iteration 19262, Loss: 0.05417100712656975\n",
      "Iteration 19263, Loss: 0.05398857593536377\n",
      "Iteration 19264, Loss: 0.05417120084166527\n",
      "Iteration 19265, Loss: 0.05398857221007347\n",
      "Iteration 19266, Loss: 0.0541711263358593\n",
      "Iteration 19267, Loss: 0.05398856848478317\n",
      "Iteration 19268, Loss: 0.05417116731405258\n",
      "Iteration 19269, Loss: 0.053988613188266754\n",
      "Iteration 19270, Loss: 0.05417108163237572\n",
      "Iteration 19271, Loss: 0.05398864671587944\n",
      "Iteration 19272, Loss: 0.054170966148376465\n",
      "Iteration 19273, Loss: 0.053988806903362274\n",
      "Iteration 19274, Loss: 0.0541708879172802\n",
      "Iteration 19275, Loss: 0.053988777101039886\n",
      "Iteration 19276, Loss: 0.05417085438966751\n",
      "Iteration 19277, Loss: 0.0539889931678772\n",
      "Iteration 19278, Loss: 0.05417077988386154\n",
      "Iteration 19279, Loss: 0.05398888513445854\n",
      "Iteration 19280, Loss: 0.054170891642570496\n",
      "Iteration 19281, Loss: 0.053988806903362274\n",
      "Iteration 19282, Loss: 0.05417104810476303\n",
      "Iteration 19283, Loss: 0.05398864671587944\n",
      "Iteration 19284, Loss: 0.05417116731405258\n",
      "Iteration 19285, Loss: 0.05398837849497795\n",
      "Iteration 19286, Loss: 0.05417116731405258\n",
      "Iteration 19287, Loss: 0.0539884977042675\n",
      "Iteration 19288, Loss: 0.054171156138181686\n",
      "Iteration 19289, Loss: 0.053988613188266754\n",
      "Iteration 19290, Loss: 0.0541711263358593\n",
      "Iteration 19291, Loss: 0.0539884977042675\n",
      "Iteration 19292, Loss: 0.054171085357666016\n",
      "Iteration 19293, Loss: 0.05398860573768616\n",
      "Iteration 19294, Loss: 0.05417100712656975\n",
      "Iteration 19295, Loss: 0.05398868769407272\n",
      "Iteration 19296, Loss: 0.054170966148376465\n",
      "Iteration 19297, Loss: 0.05398876592516899\n",
      "Iteration 19298, Loss: 0.054170966148376465\n",
      "Iteration 19299, Loss: 0.05398884415626526\n",
      "Iteration 19300, Loss: 0.05417092889547348\n",
      "Iteration 19301, Loss: 0.053988732397556305\n",
      "Iteration 19302, Loss: 0.054171040654182434\n",
      "Iteration 19303, Loss: 0.05398872494697571\n",
      "Iteration 19304, Loss: 0.054171040654182434\n",
      "Iteration 19305, Loss: 0.05398868769407272\n",
      "Iteration 19306, Loss: 0.05417108163237572\n",
      "Iteration 19307, Loss: 0.05398868769407272\n",
      "Iteration 19308, Loss: 0.05417105183005333\n",
      "Iteration 19309, Loss: 0.05398868769407272\n",
      "Iteration 19310, Loss: 0.054170966148376465\n",
      "Iteration 19311, Loss: 0.05398876592516899\n",
      "Iteration 19312, Loss: 0.05417077988386154\n",
      "Iteration 19313, Loss: 0.05398888513445854\n",
      "Iteration 19314, Loss: 0.054170846939086914\n",
      "Iteration 19315, Loss: 0.05398900434374809\n",
      "Iteration 19316, Loss: 0.054170846939086914\n",
      "Iteration 19317, Loss: 0.05398888513445854\n",
      "Iteration 19318, Loss: 0.05417093262076378\n",
      "Iteration 19319, Loss: 0.05398865044116974\n",
      "Iteration 19320, Loss: 0.05417104810476303\n",
      "Iteration 19321, Loss: 0.05398852750658989\n",
      "Iteration 19322, Loss: 0.0541711263358593\n",
      "Iteration 19323, Loss: 0.0539884939789772\n",
      "Iteration 19324, Loss: 0.05417127534747124\n",
      "Iteration 19325, Loss: 0.05398856848478317\n",
      "Iteration 19326, Loss: 0.054171085357666016\n",
      "Iteration 19327, Loss: 0.05398864671587944\n",
      "Iteration 19328, Loss: 0.05417108163237572\n",
      "Iteration 19329, Loss: 0.05398857221007347\n",
      "Iteration 19330, Loss: 0.054170891642570496\n",
      "Iteration 19331, Loss: 0.05398884415626526\n",
      "Iteration 19332, Loss: 0.05417081341147423\n",
      "Iteration 19333, Loss: 0.053988926112651825\n",
      "Iteration 19334, Loss: 0.05417073890566826\n",
      "Iteration 19335, Loss: 0.05398895591497421\n",
      "Iteration 19336, Loss: 0.054170846939086914\n",
      "Iteration 19337, Loss: 0.053988732397556305\n",
      "Iteration 19338, Loss: 0.054170817136764526\n",
      "Iteration 19339, Loss: 0.05398876592516899\n",
      "Iteration 19340, Loss: 0.05417092889547348\n",
      "Iteration 19341, Loss: 0.05398876592516899\n",
      "Iteration 19342, Loss: 0.054170768707990646\n",
      "Iteration 19343, Loss: 0.053988926112651825\n",
      "Iteration 19344, Loss: 0.05417050048708916\n",
      "Iteration 19345, Loss: 0.053989045321941376\n",
      "Iteration 19346, Loss: 0.05417057126760483\n",
      "Iteration 19347, Loss: 0.05398919805884361\n",
      "Iteration 19348, Loss: 0.05417060852050781\n",
      "Iteration 19349, Loss: 0.05398908257484436\n",
      "Iteration 19350, Loss: 0.054170772433280945\n",
      "Iteration 19351, Loss: 0.05398888513445854\n",
      "Iteration 19352, Loss: 0.054171085357666016\n",
      "Iteration 19353, Loss: 0.05398860573768616\n",
      "Iteration 19354, Loss: 0.05417124554514885\n",
      "Iteration 19355, Loss: 0.05398844927549362\n",
      "Iteration 19356, Loss: 0.0541711300611496\n",
      "Iteration 19357, Loss: 0.053988419473171234\n",
      "Iteration 19358, Loss: 0.05417104810476303\n",
      "Iteration 19359, Loss: 0.05398861691355705\n",
      "Iteration 19360, Loss: 0.054170817136764526\n",
      "Iteration 19361, Loss: 0.05398869514465332\n",
      "Iteration 19362, Loss: 0.05417076498270035\n",
      "Iteration 19363, Loss: 0.05398900434374809\n",
      "Iteration 19364, Loss: 0.05417034029960632\n",
      "Iteration 19365, Loss: 0.053989164531230927\n",
      "Iteration 19366, Loss: 0.054170411080121994\n",
      "Iteration 19367, Loss: 0.05398912355303764\n",
      "Iteration 19368, Loss: 0.05417050048708916\n",
      "Iteration 19369, Loss: 0.05398901551961899\n",
      "Iteration 19370, Loss: 0.05417073518037796\n",
      "Iteration 19371, Loss: 0.05398884415626526\n",
      "Iteration 19372, Loss: 0.05417100712656975\n",
      "Iteration 19373, Loss: 0.05398845300078392\n",
      "Iteration 19374, Loss: 0.054171279072761536\n",
      "Iteration 19375, Loss: 0.05398841202259064\n",
      "Iteration 19376, Loss: 0.054171204566955566\n",
      "Iteration 19377, Loss: 0.05398853123188019\n",
      "Iteration 19378, Loss: 0.054170966148376465\n",
      "Iteration 19379, Loss: 0.05398876592516899\n",
      "Iteration 19380, Loss: 0.054170917719602585\n",
      "Iteration 19381, Loss: 0.05398900434374809\n",
      "Iteration 19382, Loss: 0.05417057126760483\n",
      "Iteration 19383, Loss: 0.053989313542842865\n",
      "Iteration 19384, Loss: 0.05417030304670334\n",
      "Iteration 19385, Loss: 0.05398931726813316\n",
      "Iteration 19386, Loss: 0.05417037755250931\n",
      "Iteration 19387, Loss: 0.053989194333553314\n",
      "Iteration 19388, Loss: 0.05417072772979736\n",
      "Iteration 19389, Loss: 0.05398888885974884\n",
      "Iteration 19390, Loss: 0.05417081341147423\n",
      "Iteration 19391, Loss: 0.05398884415626526\n",
      "Iteration 19392, Loss: 0.05417085811495781\n",
      "Iteration 19393, Loss: 0.05398861691355705\n",
      "Iteration 19394, Loss: 0.054171040654182434\n",
      "Iteration 19395, Loss: 0.05398857593536377\n",
      "Iteration 19396, Loss: 0.054171036928892136\n",
      "Iteration 19397, Loss: 0.05398872494697571\n",
      "Iteration 19398, Loss: 0.054170768707990646\n",
      "Iteration 19399, Loss: 0.053988926112651825\n",
      "Iteration 19400, Loss: 0.054170768707990646\n",
      "Iteration 19401, Loss: 0.05398896336555481\n",
      "Iteration 19402, Loss: 0.05417060852050781\n",
      "Iteration 19403, Loss: 0.05398907512426376\n",
      "Iteration 19404, Loss: 0.054170649498701096\n",
      "Iteration 19405, Loss: 0.05398915335536003\n",
      "Iteration 19406, Loss: 0.054170653223991394\n",
      "Iteration 19407, Loss: 0.05398896336555481\n",
      "Iteration 19408, Loss: 0.054170817136764526\n",
      "Iteration 19409, Loss: 0.05398876219987869\n",
      "Iteration 19410, Loss: 0.054171085357666016\n",
      "Iteration 19411, Loss: 0.05398852750658989\n",
      "Iteration 19412, Loss: 0.05417124554514885\n",
      "Iteration 19413, Loss: 0.05398859828710556\n",
      "Iteration 19414, Loss: 0.054171204566955566\n",
      "Iteration 19415, Loss: 0.053988538682460785\n",
      "Iteration 19416, Loss: 0.054171010851860046\n",
      "Iteration 19417, Loss: 0.05398864671587944\n",
      "Iteration 19418, Loss: 0.054170966148376465\n",
      "Iteration 19419, Loss: 0.05398872494697571\n",
      "Iteration 19420, Loss: 0.054170966148376465\n",
      "Iteration 19421, Loss: 0.05398876592516899\n",
      "Iteration 19422, Loss: 0.05417089909315109\n",
      "Iteration 19423, Loss: 0.05398861691355705\n",
      "Iteration 19424, Loss: 0.05417109280824661\n",
      "Iteration 19425, Loss: 0.05398845672607422\n",
      "Iteration 19426, Loss: 0.0541711300611496\n",
      "Iteration 19427, Loss: 0.0539884977042675\n",
      "Iteration 19428, Loss: 0.05417109280824661\n",
      "Iteration 19429, Loss: 0.053988538682460785\n",
      "Iteration 19430, Loss: 0.054171234369277954\n",
      "Iteration 19431, Loss: 0.0539884977042675\n",
      "Iteration 19432, Loss: 0.05417104810476303\n",
      "Iteration 19433, Loss: 0.05398861691355705\n",
      "Iteration 19434, Loss: 0.054171036928892136\n",
      "Iteration 19435, Loss: 0.053988806903362274\n",
      "Iteration 19436, Loss: 0.05417089909315109\n",
      "Iteration 19437, Loss: 0.05398888513445854\n",
      "Iteration 19438, Loss: 0.054171040654182434\n",
      "Iteration 19439, Loss: 0.05398868769407272\n",
      "Iteration 19440, Loss: 0.054171010851860046\n",
      "Iteration 19441, Loss: 0.05398857221007347\n",
      "Iteration 19442, Loss: 0.0541711263358593\n",
      "Iteration 19443, Loss: 0.053988635540008545\n",
      "Iteration 19444, Loss: 0.054171085357666016\n",
      "Iteration 19445, Loss: 0.05398857593536377\n",
      "Iteration 19446, Loss: 0.05417089909315109\n",
      "Iteration 19447, Loss: 0.05398872494697571\n",
      "Iteration 19448, Loss: 0.054170966148376465\n",
      "Iteration 19449, Loss: 0.05398876592516899\n",
      "Iteration 19450, Loss: 0.054171036928892136\n",
      "Iteration 19451, Loss: 0.05398865044116974\n",
      "Iteration 19452, Loss: 0.054171122610569\n",
      "Iteration 19453, Loss: 0.05398860573768616\n",
      "Iteration 19454, Loss: 0.05417124181985855\n",
      "Iteration 19455, Loss: 0.053988680243492126\n",
      "Iteration 19456, Loss: 0.054171159863471985\n",
      "Iteration 19457, Loss: 0.05398872122168541\n",
      "Iteration 19458, Loss: 0.05417124554514885\n",
      "Iteration 19459, Loss: 0.05398864671587944\n",
      "Iteration 19460, Loss: 0.054171353578567505\n",
      "Iteration 19461, Loss: 0.05398844927549362\n",
      "Iteration 19462, Loss: 0.05417144298553467\n",
      "Iteration 19463, Loss: 0.053988248109817505\n",
      "Iteration 19464, Loss: 0.05417155846953392\n",
      "Iteration 19465, Loss: 0.05398828908801079\n",
      "Iteration 19466, Loss: 0.0541713610291481\n",
      "Iteration 19467, Loss: 0.05398852750658989\n",
      "Iteration 19468, Loss: 0.05417116731405258\n",
      "Iteration 19469, Loss: 0.05398864671587944\n",
      "Iteration 19470, Loss: 0.05417085438966751\n",
      "Iteration 19471, Loss: 0.05398888140916824\n",
      "Iteration 19472, Loss: 0.05417080968618393\n",
      "Iteration 19473, Loss: 0.05398903414607048\n",
      "Iteration 19474, Loss: 0.054170768707990646\n",
      "Iteration 19475, Loss: 0.05398891866207123\n",
      "Iteration 19476, Loss: 0.05417100712656975\n",
      "Iteration 19477, Loss: 0.05398879945278168\n",
      "Iteration 19478, Loss: 0.0541711263358593\n",
      "Iteration 19479, Loss: 0.05398856848478317\n",
      "Iteration 19480, Loss: 0.05417129024863243\n",
      "Iteration 19481, Loss: 0.053988516330718994\n",
      "Iteration 19482, Loss: 0.05417148396372795\n",
      "Iteration 19483, Loss: 0.05398821085691452\n",
      "Iteration 19484, Loss: 0.054171524941921234\n",
      "Iteration 19485, Loss: 0.0539882592856884\n",
      "Iteration 19486, Loss: 0.0541713647544384\n",
      "Iteration 19487, Loss: 0.053988486528396606\n",
      "Iteration 19488, Loss: 0.05417104810476303\n",
      "Iteration 19489, Loss: 0.05398868769407272\n",
      "Iteration 19490, Loss: 0.054170966148376465\n",
      "Iteration 19491, Loss: 0.05398884415626526\n",
      "Iteration 19492, Loss: 0.05417080968618393\n",
      "Iteration 19493, Loss: 0.053988926112651825\n",
      "Iteration 19494, Loss: 0.0541708879172802\n",
      "Iteration 19495, Loss: 0.05398907512426376\n",
      "Iteration 19496, Loss: 0.0541708879172802\n",
      "Iteration 19497, Loss: 0.05398872494697571\n",
      "Iteration 19498, Loss: 0.05417108163237572\n",
      "Iteration 19499, Loss: 0.05398872494697571\n",
      "Iteration 19500, Loss: 0.05417116731405258\n",
      "Iteration 19501, Loss: 0.05398872122168541\n",
      "Iteration 19502, Loss: 0.054171279072761536\n",
      "Iteration 19503, Loss: 0.053988419473171234\n",
      "Iteration 19504, Loss: 0.05417131632566452\n",
      "Iteration 19505, Loss: 0.05398837849497795\n",
      "Iteration 19506, Loss: 0.05417132005095482\n",
      "Iteration 19507, Loss: 0.053988486528396606\n",
      "Iteration 19508, Loss: 0.05417128652334213\n",
      "Iteration 19509, Loss: 0.0539884939789772\n",
      "Iteration 19510, Loss: 0.054171159863471985\n",
      "Iteration 19511, Loss: 0.05398865044116974\n",
      "Iteration 19512, Loss: 0.05417100712656975\n",
      "Iteration 19513, Loss: 0.05398869141936302\n",
      "Iteration 19514, Loss: 0.054170966148376465\n",
      "Iteration 19515, Loss: 0.05398876592516899\n",
      "Iteration 19516, Loss: 0.05417100340127945\n",
      "Iteration 19517, Loss: 0.05398865044116974\n",
      "Iteration 19518, Loss: 0.054171156138181686\n",
      "Iteration 19519, Loss: 0.053988613188266754\n",
      "Iteration 19520, Loss: 0.05417120084166527\n",
      "Iteration 19521, Loss: 0.05398852750658989\n",
      "Iteration 19522, Loss: 0.05417128652334213\n",
      "Iteration 19523, Loss: 0.05398852750658989\n",
      "Iteration 19524, Loss: 0.05417124554514885\n",
      "Iteration 19525, Loss: 0.05398852750658989\n",
      "Iteration 19526, Loss: 0.05417128652334213\n",
      "Iteration 19527, Loss: 0.05398837476968765\n",
      "Iteration 19528, Loss: 0.05417140573263168\n",
      "Iteration 19529, Loss: 0.05398840829730034\n",
      "Iteration 19530, Loss: 0.0541713647544384\n",
      "Iteration 19531, Loss: 0.053988486528396606\n",
      "Iteration 19532, Loss: 0.05417128652334213\n",
      "Iteration 19533, Loss: 0.053988419473171234\n",
      "Iteration 19534, Loss: 0.0541711263358593\n",
      "Iteration 19535, Loss: 0.05398864671587944\n",
      "Iteration 19536, Loss: 0.05417107790708542\n",
      "Iteration 19537, Loss: 0.053988806903362274\n",
      "Iteration 19538, Loss: 0.05417092889547348\n",
      "Iteration 19539, Loss: 0.05398876592516899\n",
      "Iteration 19540, Loss: 0.05417092144489288\n",
      "Iteration 19541, Loss: 0.05398891493678093\n",
      "Iteration 19542, Loss: 0.054170966148376465\n",
      "Iteration 19543, Loss: 0.053988806903362274\n",
      "Iteration 19544, Loss: 0.054171156138181686\n",
      "Iteration 19545, Loss: 0.05398876219987869\n",
      "Iteration 19546, Loss: 0.054171353578567505\n",
      "Iteration 19547, Loss: 0.053988486528396606\n",
      "Iteration 19548, Loss: 0.054171644151210785\n",
      "Iteration 19549, Loss: 0.05398814007639885\n",
      "Iteration 19550, Loss: 0.05417165160179138\n",
      "Iteration 19551, Loss: 0.05398809164762497\n",
      "Iteration 19552, Loss: 0.0541716031730175\n",
      "Iteration 19553, Loss: 0.0539882592856884\n",
      "Iteration 19554, Loss: 0.05417131632566452\n",
      "Iteration 19555, Loss: 0.05398857221007347\n",
      "Iteration 19556, Loss: 0.0541708767414093\n",
      "Iteration 19557, Loss: 0.05398900434374809\n",
      "Iteration 19558, Loss: 0.05417048558592796\n",
      "Iteration 19559, Loss: 0.05398942157626152\n",
      "Iteration 19560, Loss: 0.054170481860637665\n",
      "Iteration 19561, Loss: 0.05398920178413391\n",
      "Iteration 19562, Loss: 0.05417060852050781\n",
      "Iteration 19563, Loss: 0.05398908257484436\n",
      "Iteration 19564, Loss: 0.054170962423086166\n",
      "Iteration 19565, Loss: 0.05398879945278168\n",
      "Iteration 19566, Loss: 0.054171204566955566\n",
      "Iteration 19567, Loss: 0.05398844927549362\n",
      "Iteration 19568, Loss: 0.05417155846953392\n",
      "Iteration 19569, Loss: 0.05398821085691452\n",
      "Iteration 19570, Loss: 0.05417171120643616\n",
      "Iteration 19571, Loss: 0.053988054394721985\n",
      "Iteration 19572, Loss: 0.05417167395353317\n",
      "Iteration 19573, Loss: 0.053988099098205566\n",
      "Iteration 19574, Loss: 0.05417155474424362\n",
      "Iteration 19575, Loss: 0.05398833751678467\n",
      "Iteration 19576, Loss: 0.054171159863471985\n",
      "Iteration 19577, Loss: 0.05398860573768616\n",
      "Iteration 19578, Loss: 0.054171036928892136\n",
      "Iteration 19579, Loss: 0.05398895591497421\n",
      "Iteration 19580, Loss: 0.054170843213796616\n",
      "Iteration 19581, Loss: 0.05398907512426376\n",
      "Iteration 19582, Loss: 0.0541708879172802\n",
      "Iteration 19583, Loss: 0.05398884415626526\n",
      "Iteration 19584, Loss: 0.054170966148376465\n",
      "Iteration 19585, Loss: 0.053988613188266754\n",
      "Iteration 19586, Loss: 0.054171156138181686\n",
      "Iteration 19587, Loss: 0.05398871749639511\n",
      "Iteration 19588, Loss: 0.0541711263358593\n",
      "Iteration 19589, Loss: 0.05398856848478317\n",
      "Iteration 19590, Loss: 0.054171156138181686\n",
      "Iteration 19591, Loss: 0.05398868769407272\n",
      "Iteration 19592, Loss: 0.05417100712656975\n",
      "Iteration 19593, Loss: 0.05398869514465332\n",
      "Iteration 19594, Loss: 0.05417092889547348\n",
      "Iteration 19595, Loss: 0.05398895591497421\n",
      "Iteration 19596, Loss: 0.054170962423086166\n",
      "Iteration 19597, Loss: 0.053988873958587646\n",
      "Iteration 19598, Loss: 0.0541711151599884\n",
      "Iteration 19599, Loss: 0.05398872494697571\n",
      "Iteration 19600, Loss: 0.0541711263358593\n",
      "Iteration 19601, Loss: 0.05398852750658989\n",
      "Iteration 19602, Loss: 0.054171398282051086\n",
      "Iteration 19603, Loss: 0.05398840829730034\n",
      "Iteration 19604, Loss: 0.05417163670063019\n",
      "Iteration 19605, Loss: 0.05398828908801079\n",
      "Iteration 19606, Loss: 0.05417156219482422\n",
      "Iteration 19607, Loss: 0.05398828908801079\n",
      "Iteration 19608, Loss: 0.0541713610291481\n",
      "Iteration 19609, Loss: 0.05398856848478317\n",
      "Iteration 19610, Loss: 0.0541711263358593\n",
      "Iteration 19611, Loss: 0.05398861691355705\n",
      "Iteration 19612, Loss: 0.05417115241289139\n",
      "Iteration 19613, Loss: 0.05398872494697571\n",
      "Iteration 19614, Loss: 0.05417104810476303\n",
      "Iteration 19615, Loss: 0.05398864671587944\n",
      "Iteration 19616, Loss: 0.054171156138181686\n",
      "Iteration 19617, Loss: 0.05398856848478317\n",
      "Iteration 19618, Loss: 0.05417119711637497\n",
      "Iteration 19619, Loss: 0.05398860573768616\n",
      "Iteration 19620, Loss: 0.054171085357666016\n",
      "Iteration 19621, Loss: 0.0539884977042675\n",
      "Iteration 19622, Loss: 0.05417100712656975\n",
      "Iteration 19623, Loss: 0.05398872494697571\n",
      "Iteration 19624, Loss: 0.05417092144489288\n",
      "Iteration 19625, Loss: 0.05398888513445854\n",
      "Iteration 19626, Loss: 0.05417099595069885\n",
      "Iteration 19627, Loss: 0.05398884415626526\n",
      "Iteration 19628, Loss: 0.054170798510313034\n",
      "Iteration 19629, Loss: 0.053989000618457794\n",
      "Iteration 19630, Loss: 0.05417083948850632\n",
      "Iteration 19631, Loss: 0.05398907884955406\n",
      "Iteration 19632, Loss: 0.05417099595069885\n",
      "Iteration 19633, Loss: 0.0539889931678772\n",
      "Iteration 19634, Loss: 0.054171122610569\n",
      "Iteration 19635, Loss: 0.05398856848478317\n",
      "Iteration 19636, Loss: 0.05417143553495407\n",
      "Iteration 19637, Loss: 0.05398833379149437\n",
      "Iteration 19638, Loss: 0.05417148396372795\n",
      "Iteration 19639, Loss: 0.05398839712142944\n",
      "Iteration 19640, Loss: 0.05417148023843765\n",
      "Iteration 19641, Loss: 0.05398836359381676\n",
      "Iteration 19642, Loss: 0.054171472787857056\n",
      "Iteration 19643, Loss: 0.053988486528396606\n",
      "Iteration 19644, Loss: 0.054171234369277954\n",
      "Iteration 19645, Loss: 0.05398864671587944\n",
      "Iteration 19646, Loss: 0.05417107790708542\n",
      "Iteration 19647, Loss: 0.05398883670568466\n",
      "Iteration 19648, Loss: 0.0541708767414093\n",
      "Iteration 19649, Loss: 0.05398896336555481\n",
      "Iteration 19650, Loss: 0.05417072772979736\n",
      "Iteration 19651, Loss: 0.05398900434374809\n",
      "Iteration 19652, Loss: 0.05417100712656975\n",
      "Iteration 19653, Loss: 0.053988806903362274\n",
      "Iteration 19654, Loss: 0.0541711263358593\n",
      "Iteration 19655, Loss: 0.05398856848478317\n",
      "Iteration 19656, Loss: 0.054171398282051086\n",
      "Iteration 19657, Loss: 0.05398828908801079\n",
      "Iteration 19658, Loss: 0.05417155474424362\n",
      "Iteration 19659, Loss: 0.053988441824913025\n",
      "Iteration 19660, Loss: 0.054171547293663025\n",
      "Iteration 19661, Loss: 0.05398852750658989\n",
      "Iteration 19662, Loss: 0.05417132005095482\n",
      "Iteration 19663, Loss: 0.053988680243492126\n",
      "Iteration 19664, Loss: 0.054171040654182434\n",
      "Iteration 19665, Loss: 0.05398888513445854\n",
      "Iteration 19666, Loss: 0.054170891642570496\n",
      "Iteration 19667, Loss: 0.053988873958587646\n",
      "Iteration 19668, Loss: 0.05417100712656975\n",
      "Iteration 19669, Loss: 0.05398876592516899\n",
      "Iteration 19670, Loss: 0.054171122610569\n",
      "Iteration 19671, Loss: 0.05398876592516899\n",
      "Iteration 19672, Loss: 0.05417120084166527\n",
      "Iteration 19673, Loss: 0.05398864671587944\n",
      "Iteration 19674, Loss: 0.0541711263358593\n",
      "Iteration 19675, Loss: 0.05398868769407272\n",
      "Iteration 19676, Loss: 0.054171085357666016\n",
      "Iteration 19677, Loss: 0.05398871749639511\n",
      "Iteration 19678, Loss: 0.054171156138181686\n",
      "Iteration 19679, Loss: 0.05398864671587944\n",
      "Iteration 19680, Loss: 0.054171159863471985\n",
      "Iteration 19681, Loss: 0.053988613188266754\n",
      "Iteration 19682, Loss: 0.054171156138181686\n",
      "Iteration 19683, Loss: 0.05398879572749138\n",
      "Iteration 19684, Loss: 0.05417104810476303\n",
      "Iteration 19685, Loss: 0.05398879572749138\n",
      "Iteration 19686, Loss: 0.05417115241289139\n",
      "Iteration 19687, Loss: 0.05398876592516899\n",
      "Iteration 19688, Loss: 0.054171159863471985\n",
      "Iteration 19689, Loss: 0.05398879572749138\n",
      "Iteration 19690, Loss: 0.05417116731405258\n",
      "Iteration 19691, Loss: 0.05398860573768616\n",
      "Iteration 19692, Loss: 0.0541711263358593\n",
      "Iteration 19693, Loss: 0.053988538682460785\n",
      "Iteration 19694, Loss: 0.054171159863471985\n",
      "Iteration 19695, Loss: 0.05398860573768616\n",
      "Iteration 19696, Loss: 0.05417100712656975\n",
      "Iteration 19697, Loss: 0.05398879945278168\n",
      "Iteration 19698, Loss: 0.05417089909315109\n",
      "Iteration 19699, Loss: 0.05398884415626526\n",
      "Iteration 19700, Loss: 0.05417099595069885\n",
      "Iteration 19701, Loss: 0.05398888513445854\n",
      "Iteration 19702, Loss: 0.05417099595069885\n",
      "Iteration 19703, Loss: 0.05398888513445854\n",
      "Iteration 19704, Loss: 0.054171036928892136\n",
      "Iteration 19705, Loss: 0.05398879945278168\n",
      "Iteration 19706, Loss: 0.054171085357666016\n",
      "Iteration 19707, Loss: 0.05398872122168541\n",
      "Iteration 19708, Loss: 0.05417128652334213\n",
      "Iteration 19709, Loss: 0.053988486528396606\n",
      "Iteration 19710, Loss: 0.05417128652334213\n",
      "Iteration 19711, Loss: 0.05398844927549362\n",
      "Iteration 19712, Loss: 0.054171543568372726\n",
      "Iteration 19713, Loss: 0.053988441824913025\n",
      "Iteration 19714, Loss: 0.05417131632566452\n",
      "Iteration 19715, Loss: 0.05398860573768616\n",
      "Iteration 19716, Loss: 0.05417116731405258\n",
      "Iteration 19717, Loss: 0.05398860573768616\n",
      "Iteration 19718, Loss: 0.05417100712656975\n",
      "Iteration 19719, Loss: 0.05398879945278168\n",
      "Iteration 19720, Loss: 0.054171040654182434\n",
      "Iteration 19721, Loss: 0.05398814380168915\n",
      "Iteration 19722, Loss: 0.054171085357666016\n",
      "Iteration 19723, Loss: 0.05398806929588318\n",
      "Iteration 19724, Loss: 0.054170578718185425\n",
      "Iteration 19725, Loss: 0.05398779362440109\n",
      "Iteration 19726, Loss: 0.05417066812515259\n",
      "Iteration 19727, Loss: 0.05398779362440109\n",
      "Iteration 19728, Loss: 0.054170697927474976\n",
      "Iteration 19729, Loss: 0.05398775264620781\n",
      "Iteration 19730, Loss: 0.05417061969637871\n",
      "Iteration 19731, Loss: 0.05398798733949661\n",
      "Iteration 19732, Loss: 0.05417042225599289\n",
      "Iteration 19733, Loss: 0.05398811027407646\n",
      "Iteration 19734, Loss: 0.05417030677199364\n",
      "Iteration 19735, Loss: 0.053988225758075714\n",
      "Iteration 19736, Loss: 0.054170262068510056\n",
      "Iteration 19737, Loss: 0.0539882630109787\n",
      "Iteration 19738, Loss: 0.05417030304670334\n",
      "Iteration 19739, Loss: 0.05398830026388168\n",
      "Iteration 19740, Loss: 0.054170385003089905\n",
      "Iteration 19741, Loss: 0.05398811027407646\n",
      "Iteration 19742, Loss: 0.054170459508895874\n",
      "Iteration 19743, Loss: 0.05398810654878616\n",
      "Iteration 19744, Loss: 0.05417054146528244\n",
      "Iteration 19745, Loss: 0.0539880208671093\n",
      "Iteration 19746, Loss: 0.05417054146528244\n",
      "Iteration 19747, Loss: 0.05398791283369064\n",
      "Iteration 19748, Loss: 0.054170459508895874\n",
      "Iteration 19749, Loss: 0.05398814007639885\n",
      "Iteration 19750, Loss: 0.054170381277799606\n",
      "Iteration 19751, Loss: 0.05398821830749512\n",
      "Iteration 19752, Loss: 0.05417030304670334\n",
      "Iteration 19753, Loss: 0.05398818850517273\n",
      "Iteration 19754, Loss: 0.054170381277799606\n",
      "Iteration 19755, Loss: 0.05398818477988243\n",
      "Iteration 19756, Loss: 0.05417030677199364\n",
      "Iteration 19757, Loss: 0.053988032042980194\n",
      "Iteration 19758, Loss: 0.05417042225599289\n",
      "Iteration 19759, Loss: 0.05398799106478691\n",
      "Iteration 19760, Loss: 0.05417050048708916\n",
      "Iteration 19761, Loss: 0.053988032042980194\n",
      "Iteration 19762, Loss: 0.05417050048708916\n",
      "Iteration 19763, Loss: 0.053988032042980194\n",
      "Iteration 19764, Loss: 0.05417042225599289\n",
      "Iteration 19765, Loss: 0.05398806557059288\n",
      "Iteration 19766, Loss: 0.05417050048708916\n",
      "Iteration 19767, Loss: 0.05398806557059288\n",
      "Iteration 19768, Loss: 0.05417050048708916\n",
      "Iteration 19769, Loss: 0.053988032042980194\n",
      "Iteration 19770, Loss: 0.05417054146528244\n",
      "Iteration 19771, Loss: 0.05398806557059288\n",
      "Iteration 19772, Loss: 0.054170459508895874\n",
      "Iteration 19773, Loss: 0.05398810654878616\n",
      "Iteration 19774, Loss: 0.05417050048708916\n",
      "Iteration 19775, Loss: 0.05398798733949661\n",
      "Iteration 19776, Loss: 0.05417073890566826\n",
      "Iteration 19777, Loss: 0.053987786173820496\n",
      "Iteration 19778, Loss: 0.05417078360915184\n",
      "Iteration 19779, Loss: 0.05398767441511154\n",
      "Iteration 19780, Loss: 0.05417089909315109\n",
      "Iteration 19781, Loss: 0.05398763343691826\n",
      "Iteration 19782, Loss: 0.05417073890566826\n",
      "Iteration 19783, Loss: 0.05398783087730408\n",
      "Iteration 19784, Loss: 0.05417053401470184\n",
      "Iteration 19785, Loss: 0.05398799479007721\n",
      "Iteration 19786, Loss: 0.05417030677199364\n",
      "Iteration 19787, Loss: 0.05398830398917198\n",
      "Iteration 19788, Loss: 0.05417030304670334\n",
      "Iteration 19789, Loss: 0.053988225758075714\n",
      "Iteration 19790, Loss: 0.054170455783605576\n",
      "Iteration 19791, Loss: 0.05398818477988243\n",
      "Iteration 19792, Loss: 0.05417050048708916\n",
      "Iteration 19793, Loss: 0.05398814380168915\n",
      "Iteration 19794, Loss: 0.054170653223991394\n",
      "Iteration 19795, Loss: 0.05398794636130333\n",
      "Iteration 19796, Loss: 0.05417077988386154\n",
      "Iteration 19797, Loss: 0.05398763343691826\n",
      "Iteration 19798, Loss: 0.05417097732424736\n",
      "Iteration 19799, Loss: 0.05398763343691826\n",
      "Iteration 19800, Loss: 0.05417089909315109\n",
      "Iteration 19801, Loss: 0.05398767441511154\n",
      "Iteration 19802, Loss: 0.054170697927474976\n",
      "Iteration 19803, Loss: 0.05398787185549736\n",
      "Iteration 19804, Loss: 0.054170459508895874\n",
      "Iteration 19805, Loss: 0.05398799106478691\n",
      "Iteration 19806, Loss: 0.05417034029960632\n",
      "Iteration 19807, Loss: 0.053988419473171234\n",
      "Iteration 19808, Loss: 0.054170262068510056\n",
      "Iteration 19809, Loss: 0.053988419473171234\n",
      "Iteration 19810, Loss: 0.05417023226618767\n",
      "Iteration 19811, Loss: 0.05398811027407646\n",
      "Iteration 19812, Loss: 0.05417049676179886\n",
      "Iteration 19813, Loss: 0.053988032042980194\n",
      "Iteration 19814, Loss: 0.054170653223991394\n",
      "Iteration 19815, Loss: 0.05398779362440109\n",
      "Iteration 19816, Loss: 0.05417066812515259\n",
      "Iteration 19817, Loss: 0.053987860679626465\n",
      "Iteration 19818, Loss: 0.05417080968618393\n",
      "Iteration 19819, Loss: 0.05398797616362572\n",
      "Iteration 19820, Loss: 0.05417061597108841\n",
      "Iteration 19821, Loss: 0.053988032042980194\n",
      "Iteration 19822, Loss: 0.054170459508895874\n",
      "Iteration 19823, Loss: 0.05398811027407646\n",
      "Iteration 19824, Loss: 0.05417042225599289\n",
      "Iteration 19825, Loss: 0.0539882592856884\n",
      "Iteration 19826, Loss: 0.054170459508895874\n",
      "Iteration 19827, Loss: 0.05398811027407646\n",
      "Iteration 19828, Loss: 0.05417042598128319\n",
      "Iteration 19829, Loss: 0.05398810654878616\n",
      "Iteration 19830, Loss: 0.05417047068476677\n",
      "Iteration 19831, Loss: 0.053988032042980194\n",
      "Iteration 19832, Loss: 0.05417047068476677\n",
      "Iteration 19833, Loss: 0.053987979888916016\n",
      "Iteration 19834, Loss: 0.05417061969637871\n",
      "Iteration 19835, Loss: 0.05398795008659363\n",
      "Iteration 19836, Loss: 0.054170578718185425\n",
      "Iteration 19837, Loss: 0.053987883031368256\n",
      "Iteration 19838, Loss: 0.054170459508895874\n",
      "Iteration 19839, Loss: 0.053988032042980194\n",
      "Iteration 19840, Loss: 0.05417042225599289\n",
      "Iteration 19841, Loss: 0.05398803949356079\n",
      "Iteration 19842, Loss: 0.054170381277799606\n",
      "Iteration 19843, Loss: 0.053988657891750336\n",
      "Iteration 19844, Loss: 0.05417030304670334\n",
      "Iteration 19845, Loss: 0.053988777101039886\n",
      "Iteration 19846, Loss: 0.054169707000255585\n",
      "Iteration 19847, Loss: 0.05398889631032944\n",
      "Iteration 19848, Loss: 0.05416978523135185\n",
      "Iteration 19849, Loss: 0.053988661617040634\n",
      "Iteration 19850, Loss: 0.054169993847608566\n",
      "Iteration 19851, Loss: 0.05398834869265556\n",
      "Iteration 19852, Loss: 0.05417042598128319\n",
      "Iteration 19853, Loss: 0.05398811027407646\n",
      "Iteration 19854, Loss: 0.054170578718185425\n",
      "Iteration 19855, Loss: 0.05398787185549736\n",
      "Iteration 19856, Loss: 0.05417078360915184\n",
      "Iteration 19857, Loss: 0.05398782342672348\n",
      "Iteration 19858, Loss: 0.05417093262076378\n",
      "Iteration 19859, Loss: 0.05398768186569214\n",
      "Iteration 19860, Loss: 0.05417070537805557\n",
      "Iteration 19861, Loss: 0.05398794263601303\n",
      "Iteration 19862, Loss: 0.05417061597108841\n",
      "Iteration 19863, Loss: 0.053988151252269745\n",
      "Iteration 19864, Loss: 0.054170459508895874\n",
      "Iteration 19865, Loss: 0.053988151252269745\n",
      "Iteration 19866, Loss: 0.05417042225599289\n",
      "Iteration 19867, Loss: 0.053988151252269745\n",
      "Iteration 19868, Loss: 0.05417034029960632\n",
      "Iteration 19869, Loss: 0.05398822948336601\n",
      "Iteration 19870, Loss: 0.054170262068510056\n",
      "Iteration 19871, Loss: 0.05398887023329735\n",
      "Iteration 19872, Loss: 0.05417018383741379\n",
      "Iteration 19873, Loss: 0.053989022970199585\n",
      "Iteration 19874, Loss: 0.05417023226618767\n",
      "Iteration 19875, Loss: 0.0539887472987175\n",
      "Iteration 19876, Loss: 0.05417085811495781\n",
      "Iteration 19877, Loss: 0.053988512605428696\n",
      "Iteration 19878, Loss: 0.0541711300611496\n",
      "Iteration 19879, Loss: 0.053988512605428696\n",
      "Iteration 19880, Loss: 0.05417109653353691\n",
      "Iteration 19881, Loss: 0.053988438099622726\n",
      "Iteration 19882, Loss: 0.05417117476463318\n",
      "Iteration 19883, Loss: 0.0539885088801384\n",
      "Iteration 19884, Loss: 0.054171063005924225\n",
      "Iteration 19885, Loss: 0.05398839712142944\n",
      "Iteration 19886, Loss: 0.054170988500118256\n",
      "Iteration 19887, Loss: 0.053988318890333176\n",
      "Iteration 19888, Loss: 0.05417094752192497\n",
      "Iteration 19889, Loss: 0.053988438099622726\n",
      "Iteration 19890, Loss: 0.05417078733444214\n",
      "Iteration 19891, Loss: 0.05398867279291153\n",
      "Iteration 19892, Loss: 0.05417066439986229\n",
      "Iteration 19893, Loss: 0.053988754749298096\n",
      "Iteration 19894, Loss: 0.05417058989405632\n",
      "Iteration 19895, Loss: 0.05398883670568466\n",
      "Iteration 19896, Loss: 0.05417066812515259\n",
      "Iteration 19897, Loss: 0.0539887472987175\n",
      "Iteration 19898, Loss: 0.05417086184024811\n",
      "Iteration 19899, Loss: 0.053988512605428696\n",
      "Iteration 19900, Loss: 0.05417114496231079\n",
      "Iteration 19901, Loss: 0.05398830771446228\n",
      "Iteration 19902, Loss: 0.05417134612798691\n",
      "Iteration 19903, Loss: 0.053988080471754074\n",
      "Iteration 19904, Loss: 0.05417146533727646\n",
      "Iteration 19905, Loss: 0.05398803949356079\n",
      "Iteration 19906, Loss: 0.054171375930309296\n",
      "Iteration 19907, Loss: 0.05398859828710556\n",
      "Iteration 19908, Loss: 0.05417117476463318\n",
      "Iteration 19909, Loss: 0.053988903760910034\n",
      "Iteration 19910, Loss: 0.05417141318321228\n",
      "Iteration 19911, Loss: 0.0539892241358757\n",
      "Iteration 19912, Loss: 0.0541708767414093\n",
      "Iteration 19913, Loss: 0.05398938059806824\n",
      "Iteration 19914, Loss: 0.05417083203792572\n",
      "Iteration 19915, Loss: 0.05398938059806824\n",
      "Iteration 19916, Loss: 0.05417102575302124\n",
      "Iteration 19917, Loss: 0.053989529609680176\n",
      "Iteration 19918, Loss: 0.054171185940504074\n",
      "Iteration 19919, Loss: 0.05398913845419884\n",
      "Iteration 19920, Loss: 0.054171424359083176\n",
      "Iteration 19921, Loss: 0.05398871749639511\n",
      "Iteration 19922, Loss: 0.054171621799468994\n",
      "Iteration 19923, Loss: 0.0539887472987175\n",
      "Iteration 19924, Loss: 0.05417177081108093\n",
      "Iteration 19925, Loss: 0.05398859828710556\n",
      "Iteration 19926, Loss: 0.054171692579984665\n",
      "Iteration 19927, Loss: 0.053988754749298096\n",
      "Iteration 19928, Loss: 0.05417153239250183\n",
      "Iteration 19929, Loss: 0.0539889894425869\n",
      "Iteration 19930, Loss: 0.05417121946811676\n",
      "Iteration 19931, Loss: 0.05398911237716675\n",
      "Iteration 19932, Loss: 0.05417094752192497\n",
      "Iteration 19933, Loss: 0.053989604115486145\n",
      "Iteration 19934, Loss: 0.05417102202773094\n",
      "Iteration 19935, Loss: 0.05398934334516525\n",
      "Iteration 19936, Loss: 0.054171185940504074\n",
      "Iteration 19937, Loss: 0.05398910492658615\n",
      "Iteration 19938, Loss: 0.05417146533727646\n",
      "Iteration 19939, Loss: 0.053988829255104065\n",
      "Iteration 19940, Loss: 0.05417170375585556\n",
      "Iteration 19941, Loss: 0.05398855730891228\n",
      "Iteration 19942, Loss: 0.05417189747095108\n",
      "Iteration 19943, Loss: 0.05398854613304138\n",
      "Iteration 19944, Loss: 0.0541718527674675\n",
      "Iteration 19945, Loss: 0.053988516330718994\n",
      "Iteration 19946, Loss: 0.054171621799468994\n",
      "Iteration 19947, Loss: 0.05398879572749138\n",
      "Iteration 19948, Loss: 0.05417133867740631\n",
      "Iteration 19949, Loss: 0.05398903414607048\n",
      "Iteration 19950, Loss: 0.05417102575302124\n",
      "Iteration 19951, Loss: 0.05398926883935928\n",
      "Iteration 19952, Loss: 0.05417109653353691\n",
      "Iteration 19953, Loss: 0.05398934707045555\n",
      "Iteration 19954, Loss: 0.05417102575302124\n",
      "Iteration 19955, Loss: 0.05398934334516525\n",
      "Iteration 19956, Loss: 0.054171111434698105\n",
      "Iteration 19957, Loss: 0.05398914963006973\n",
      "Iteration 19958, Loss: 0.05417119711637497\n",
      "Iteration 19959, Loss: 0.0539889931678772\n",
      "Iteration 19960, Loss: 0.05417141318321228\n",
      "Iteration 19961, Loss: 0.05398910492658615\n",
      "Iteration 19962, Loss: 0.05417138338088989\n",
      "Iteration 19963, Loss: 0.05398883670568466\n",
      "Iteration 19964, Loss: 0.05417150259017944\n",
      "Iteration 19965, Loss: 0.053988825529813766\n",
      "Iteration 19966, Loss: 0.054171692579984665\n",
      "Iteration 19967, Loss: 0.0539887510240078\n",
      "Iteration 19968, Loss: 0.054171543568372726\n",
      "Iteration 19969, Loss: 0.053988829255104065\n",
      "Iteration 19970, Loss: 0.05417150259017944\n",
      "Iteration 19971, Loss: 0.053988903760910034\n",
      "Iteration 19972, Loss: 0.05417153984308243\n",
      "Iteration 19973, Loss: 0.05398879572749138\n",
      "Iteration 19974, Loss: 0.05417146533727646\n",
      "Iteration 19975, Loss: 0.05398891493678093\n",
      "Iteration 19976, Loss: 0.05417145416140556\n",
      "Iteration 19977, Loss: 0.0539889857172966\n",
      "Iteration 19978, Loss: 0.05417141318321228\n",
      "Iteration 19979, Loss: 0.053989022970199585\n",
      "Iteration 19980, Loss: 0.054171495139598846\n",
      "Iteration 19981, Loss: 0.053988948464393616\n",
      "Iteration 19982, Loss: 0.05417146533727646\n",
      "Iteration 19983, Loss: 0.05398891493678093\n",
      "Iteration 19984, Loss: 0.05417158454656601\n",
      "Iteration 19985, Loss: 0.05398894473910332\n",
      "Iteration 19986, Loss: 0.0541716143488884\n",
      "Iteration 19987, Loss: 0.05398879200220108\n",
      "Iteration 19988, Loss: 0.05417153984308243\n",
      "Iteration 19989, Loss: 0.05398894473910332\n",
      "Iteration 19990, Loss: 0.054171498864889145\n",
      "Iteration 19991, Loss: 0.053989019244909286\n",
      "Iteration 19992, Loss: 0.05417141318321228\n",
      "Iteration 19993, Loss: 0.05398906394839287\n",
      "Iteration 19994, Loss: 0.054171182215213776\n",
      "Iteration 19995, Loss: 0.053989335894584656\n",
      "Iteration 19996, Loss: 0.05417102575302124\n",
      "Iteration 19997, Loss: 0.0539892315864563\n",
      "Iteration 19998, Loss: 0.054170988500118256\n",
      "Iteration 19999, Loss: 0.05398930236697197\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.00000001\n",
    "iterations = 20000\n",
    "losses = []\n",
    "determinants = []\n",
    "H_inv = jnp.eye(3)\n",
    "for i in range(iterations):\n",
    "    H_inv = H_inv - alpha * gradient(H_inv, warpedConics)\n",
    "    current_loss = lossConics(H_inv, warpedConics)\n",
    "    det = jnp.linalg.det(H_inv)\n",
    "    determinants.append(det)\n",
    "    losses.append(current_loss)\n",
    "    print(f\"Iteration {i}, Loss: {current_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8232666a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the losses\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel(\"Iteration\")\n",
    "ax1.set_ylabel(\"Loss\", color=color)\n",
    "ax1.plot(losses, color=color, label=\"Loss\")\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:red'\n",
    "ax2.set_ylabel(\"Determinant\", color=color)\n",
    "ax2.plot(determinants, color=color, label=\"Determinant\")\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "plt.title(\"Loss and Determinant over Iterations\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "026ff391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Homography:\n",
      "[[ 1.0000000e+00 -1.4533740e-07  0.0000000e+00]\n",
      " [ 1.0187469e-05  1.0000014e+00  0.0000000e+00]\n",
      " [ 8.7569075e-05 -5.7592955e-03  1.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "H = jnp.linalg.inv(H_inv)\n",
    "print(\"Estimated Homography:\")\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3a5a6b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Conics Warped]\n",
      "Warped Conics:\n",
      "Warped Circle 1: \n",
      "[[ 1.17438731e-04 -4.99526867e-06 -1.04447430e-01]\n",
      " [-4.99526867e-06  1.01833561e-04 -3.92636019e-02]\n",
      " [-1.04447430e-01 -3.92636019e-02  1.11389267e+02]]\n",
      "Det of Warped Circle 1: -3.5999897003256784e-09\n",
      "Warped Circle 2: \n",
      "[[ 1.18595699e-04  7.48967000e-08 -1.12196357e-01]\n",
      " [ 7.48967000e-08  1.18212781e-04 -6.09458670e-02]\n",
      " [-1.12196357e-01 -6.09458670e-02  1.37483475e+02]]\n",
      "Det of Warped Circle 2: -9.999971390144781e-11\n",
      "Warped Circle 3: \n",
      "[[ 1.19780853e-04  5.33190206e-06 -1.20267171e-01]\n",
      " [ 5.33190206e-06  1.35830549e-04 -8.47619059e-02]\n",
      " [-1.20267171e-01 -8.47619059e-02  1.67253753e+02]]\n",
      "Det of Warped Circle 3: -9.999971389058045e-11\n"
     ]
    }
   ],
   "source": [
    "H_reconstructed = Homography(np.array(H))\n",
    "\n",
    "# Warp The Circles\n",
    "warpedConicsRec = ConicWarper().warpConics(img.C_img, H_reconstructed)\n",
    "\n",
    "print(\"[Conics Warped]\")\n",
    "print(\"Warped Conics:\")\n",
    "print(f\"Warped Circle 1: \\n{warpedConicsRec.C1._M}\")\n",
    "print(f\"Det of Warped Circle 1: {np.linalg.det(warpedConicsRec.C1.M)}\")\n",
    "print(f\"Warped Circle 2: \\n{warpedConicsRec.C2._M}\")\n",
    "print(f\"Det of Warped Circle 2: {np.linalg.det(warpedConicsRec.C2.M)}\")\n",
    "print(f\"Warped Circle 3: \\n{warpedConicsRec.C3._M}\")\n",
    "print(f\"Det of Warped Circle 3: {np.linalg.det(warpedConicsRec.C3.M)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "20a4437f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotter = Plotter.Plotter(2, 2, title=\"Warped Scene with Circles\")\n",
    "plotter.plotExperiment(\n",
    "    sceneDescription=sceneDescription,\n",
    "    img=img,\n",
    "    warpedConics=warpedConicsRec,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "17972a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 10.925504684448242\n",
      "Iteration 1, Loss: 10.904354095458984\n",
      "Iteration 2, Loss: 10.874302864074707\n",
      "Iteration 3, Loss: 10.836381912231445\n",
      "Iteration 4, Loss: 10.79152774810791\n",
      "Iteration 5, Loss: 10.740614891052246\n",
      "Iteration 6, Loss: 10.684440612792969\n",
      "Iteration 7, Loss: 10.62372875213623\n",
      "Iteration 8, Loss: 10.559147834777832\n",
      "Iteration 9, Loss: 10.491287231445312\n",
      "Iteration 10, Loss: 10.420689582824707\n",
      "Iteration 11, Loss: 10.347841262817383\n",
      "Iteration 12, Loss: 10.27316951751709\n",
      "Iteration 13, Loss: 10.197059631347656\n",
      "Iteration 14, Loss: 10.119855880737305\n",
      "Iteration 15, Loss: 10.04185962677002\n",
      "Iteration 16, Loss: 9.963339805603027\n",
      "Iteration 17, Loss: 9.884527206420898\n",
      "Iteration 18, Loss: 9.805633544921875\n",
      "Iteration 19, Loss: 9.726828575134277\n",
      "Iteration 20, Loss: 9.648274421691895\n",
      "Iteration 21, Loss: 9.570097923278809\n",
      "Iteration 22, Loss: 9.492417335510254\n",
      "Iteration 23, Loss: 9.415331840515137\n",
      "Iteration 24, Loss: 9.33891773223877\n",
      "Iteration 25, Loss: 9.263242721557617\n",
      "Iteration 26, Loss: 9.188371658325195\n",
      "Iteration 27, Loss: 9.11434555053711\n",
      "Iteration 28, Loss: 9.041199684143066\n",
      "Iteration 29, Loss: 8.968965530395508\n",
      "Iteration 30, Loss: 8.897666931152344\n",
      "Iteration 31, Loss: 8.827313423156738\n",
      "Iteration 32, Loss: 8.75792121887207\n",
      "Iteration 33, Loss: 8.689496040344238\n",
      "Iteration 34, Loss: 8.622034072875977\n",
      "Iteration 35, Loss: 8.55553913116455\n",
      "Iteration 36, Loss: 8.490002632141113\n",
      "Iteration 37, Loss: 8.425421714782715\n",
      "Iteration 38, Loss: 8.36177921295166\n",
      "Iteration 39, Loss: 8.299074172973633\n",
      "Iteration 40, Loss: 8.237286567687988\n",
      "Iteration 41, Loss: 8.176405906677246\n",
      "Iteration 42, Loss: 8.116412162780762\n",
      "Iteration 43, Loss: 8.057300567626953\n",
      "Iteration 44, Loss: 7.99904203414917\n",
      "Iteration 45, Loss: 7.941629409790039\n",
      "Iteration 46, Loss: 7.885043621063232\n",
      "Iteration 47, Loss: 7.829264163970947\n",
      "Iteration 48, Loss: 7.774276256561279\n",
      "Iteration 49, Loss: 7.720066070556641\n",
      "Iteration 50, Loss: 7.666609764099121\n",
      "Iteration 51, Loss: 7.613893508911133\n",
      "Iteration 52, Loss: 7.561900615692139\n",
      "Iteration 53, Loss: 7.510611534118652\n",
      "Iteration 54, Loss: 7.46001672744751\n",
      "Iteration 55, Loss: 7.410096645355225\n",
      "Iteration 56, Loss: 7.3608245849609375\n",
      "Iteration 57, Loss: 7.312204360961914\n",
      "Iteration 58, Loss: 7.264208793640137\n",
      "Iteration 59, Loss: 7.216822147369385\n",
      "Iteration 60, Loss: 7.170039176940918\n",
      "Iteration 61, Loss: 7.123833179473877\n",
      "Iteration 62, Loss: 7.078200340270996\n",
      "Iteration 63, Loss: 7.033121585845947\n",
      "Iteration 64, Loss: 6.988587856292725\n",
      "Iteration 65, Loss: 6.944584369659424\n",
      "Iteration 66, Loss: 6.901095867156982\n",
      "Iteration 67, Loss: 6.85811710357666\n",
      "Iteration 68, Loss: 6.815632343292236\n",
      "Iteration 69, Loss: 6.773629188537598\n",
      "Iteration 70, Loss: 6.73209810256958\n",
      "Iteration 71, Loss: 6.691028118133545\n",
      "Iteration 72, Loss: 6.650407314300537\n",
      "Iteration 73, Loss: 6.610229969024658\n",
      "Iteration 74, Loss: 6.570479869842529\n",
      "Iteration 75, Loss: 6.531154632568359\n",
      "Iteration 76, Loss: 6.4922404289245605\n",
      "Iteration 77, Loss: 6.453728675842285\n",
      "Iteration 78, Loss: 6.415610313415527\n",
      "Iteration 79, Loss: 6.377874851226807\n",
      "Iteration 80, Loss: 6.340521812438965\n",
      "Iteration 81, Loss: 6.303535461425781\n",
      "Iteration 82, Loss: 6.266911029815674\n",
      "Iteration 83, Loss: 6.2306389808654785\n",
      "Iteration 84, Loss: 6.1947174072265625\n",
      "Iteration 85, Loss: 6.159135341644287\n",
      "Iteration 86, Loss: 6.123885631561279\n",
      "Iteration 87, Loss: 6.088961601257324\n",
      "Iteration 88, Loss: 6.0543599128723145\n",
      "Iteration 89, Loss: 6.020068168640137\n",
      "Iteration 90, Loss: 5.986081600189209\n",
      "Iteration 91, Loss: 5.95239782333374\n",
      "Iteration 92, Loss: 5.919013023376465\n",
      "Iteration 93, Loss: 5.885916233062744\n",
      "Iteration 94, Loss: 5.853105068206787\n",
      "Iteration 95, Loss: 5.820572853088379\n",
      "Iteration 96, Loss: 5.788308620452881\n",
      "Iteration 97, Loss: 5.756320953369141\n",
      "Iteration 98, Loss: 5.724587917327881\n",
      "Iteration 99, Loss: 5.693122386932373\n",
      "Iteration 100, Loss: 5.661907196044922\n",
      "Iteration 101, Loss: 5.63094425201416\n",
      "Iteration 102, Loss: 5.60022497177124\n",
      "Iteration 103, Loss: 5.569745063781738\n",
      "Iteration 104, Loss: 5.5395050048828125\n",
      "Iteration 105, Loss: 5.509493350982666\n",
      "Iteration 106, Loss: 5.479715824127197\n",
      "Iteration 107, Loss: 5.450161457061768\n",
      "Iteration 108, Loss: 5.4208269119262695\n",
      "Iteration 109, Loss: 5.391708850860596\n",
      "Iteration 110, Loss: 5.362804889678955\n",
      "Iteration 111, Loss: 5.334110260009766\n",
      "Iteration 112, Loss: 5.305624485015869\n",
      "Iteration 113, Loss: 5.277340888977051\n",
      "Iteration 114, Loss: 5.249261856079102\n",
      "Iteration 115, Loss: 5.221367835998535\n",
      "Iteration 116, Loss: 5.193680286407471\n",
      "Iteration 117, Loss: 5.166175365447998\n",
      "Iteration 118, Loss: 5.138862609863281\n",
      "Iteration 119, Loss: 5.111732006072998\n",
      "Iteration 120, Loss: 5.084781646728516\n",
      "Iteration 121, Loss: 5.058017253875732\n",
      "Iteration 122, Loss: 5.031423091888428\n",
      "Iteration 123, Loss: 5.005004405975342\n",
      "Iteration 124, Loss: 4.978753089904785\n",
      "Iteration 125, Loss: 4.95267391204834\n",
      "Iteration 126, Loss: 4.926758289337158\n",
      "Iteration 127, Loss: 4.901008129119873\n",
      "Iteration 128, Loss: 4.875420570373535\n",
      "Iteration 129, Loss: 4.8499884605407715\n",
      "Iteration 130, Loss: 4.824714660644531\n",
      "Iteration 131, Loss: 4.799594879150391\n",
      "Iteration 132, Loss: 4.774625778198242\n",
      "Iteration 133, Loss: 4.749805927276611\n",
      "Iteration 134, Loss: 4.725132465362549\n",
      "Iteration 135, Loss: 4.7006096839904785\n",
      "Iteration 136, Loss: 4.67622709274292\n",
      "Iteration 137, Loss: 4.651988506317139\n",
      "Iteration 138, Loss: 4.6278862953186035\n",
      "Iteration 139, Loss: 4.6039252281188965\n",
      "Iteration 140, Loss: 4.58009672164917\n",
      "Iteration 141, Loss: 4.556407928466797\n",
      "Iteration 142, Loss: 4.532843589782715\n",
      "Iteration 143, Loss: 4.509414196014404\n",
      "Iteration 144, Loss: 4.48611307144165\n",
      "Iteration 145, Loss: 4.4629340171813965\n",
      "Iteration 146, Loss: 4.439889907836914\n",
      "Iteration 147, Loss: 4.416964530944824\n",
      "Iteration 148, Loss: 4.394160747528076\n",
      "Iteration 149, Loss: 4.37147855758667\n",
      "Iteration 150, Loss: 4.348917484283447\n",
      "Iteration 151, Loss: 4.326467514038086\n",
      "Iteration 152, Loss: 4.304136753082275\n",
      "Iteration 153, Loss: 4.281921863555908\n",
      "Iteration 154, Loss: 4.25982141494751\n",
      "Iteration 155, Loss: 4.237828254699707\n",
      "Iteration 156, Loss: 4.215949535369873\n",
      "Iteration 157, Loss: 4.19417667388916\n",
      "Iteration 158, Loss: 4.172510623931885\n",
      "Iteration 159, Loss: 4.1509552001953125\n",
      "Iteration 160, Loss: 4.129502296447754\n",
      "Iteration 161, Loss: 4.108154773712158\n",
      "Iteration 162, Loss: 4.086904525756836\n",
      "Iteration 163, Loss: 4.065760612487793\n",
      "Iteration 164, Loss: 4.044714450836182\n",
      "Iteration 165, Loss: 4.023767948150635\n",
      "Iteration 166, Loss: 4.002920150756836\n",
      "Iteration 167, Loss: 3.9821648597717285\n",
      "Iteration 168, Loss: 3.961508274078369\n",
      "Iteration 169, Loss: 3.940943717956543\n",
      "Iteration 170, Loss: 3.92047381401062\n",
      "Iteration 171, Loss: 3.9000954627990723\n",
      "Iteration 172, Loss: 3.879809617996216\n",
      "Iteration 173, Loss: 3.8596110343933105\n",
      "Iteration 174, Loss: 3.8395047187805176\n",
      "Iteration 175, Loss: 3.819486141204834\n",
      "Iteration 176, Loss: 3.7995500564575195\n",
      "Iteration 177, Loss: 3.7797036170959473\n",
      "Iteration 178, Loss: 3.7599403858184814\n",
      "Iteration 179, Loss: 3.740262269973755\n",
      "Iteration 180, Loss: 3.720665693283081\n",
      "Iteration 181, Loss: 3.7011539936065674\n",
      "Iteration 182, Loss: 3.68172287940979\n",
      "Iteration 183, Loss: 3.662369966506958\n",
      "Iteration 184, Loss: 3.6431002616882324\n",
      "Iteration 185, Loss: 3.623906135559082\n",
      "Iteration 186, Loss: 3.6047911643981934\n",
      "Iteration 187, Loss: 3.5857534408569336\n",
      "Iteration 188, Loss: 3.5667905807495117\n",
      "Iteration 189, Loss: 3.547903060913086\n",
      "Iteration 190, Loss: 3.529090166091919\n",
      "Iteration 191, Loss: 3.5103516578674316\n",
      "Iteration 192, Loss: 3.491687297821045\n",
      "Iteration 193, Loss: 3.4730920791625977\n",
      "Iteration 194, Loss: 3.4545702934265137\n",
      "Iteration 195, Loss: 3.4361202716827393\n",
      "Iteration 196, Loss: 3.4177398681640625\n",
      "Iteration 197, Loss: 3.399423360824585\n",
      "Iteration 198, Loss: 3.38118314743042\n",
      "Iteration 199, Loss: 3.363004684448242\n",
      "Iteration 200, Loss: 3.3448965549468994\n",
      "Iteration 201, Loss: 3.326852321624756\n",
      "Iteration 202, Loss: 3.308875799179077\n",
      "Iteration 203, Loss: 3.290966272354126\n",
      "Iteration 204, Loss: 3.2731165885925293\n",
      "Iteration 205, Loss: 3.2553348541259766\n",
      "Iteration 206, Loss: 3.237612009048462\n",
      "Iteration 207, Loss: 3.2199547290802\n",
      "Iteration 208, Loss: 3.202357292175293\n",
      "Iteration 209, Loss: 3.1848232746124268\n",
      "Iteration 210, Loss: 3.1673500537872314\n",
      "Iteration 211, Loss: 3.1499311923980713\n",
      "Iteration 212, Loss: 3.1325788497924805\n",
      "Iteration 213, Loss: 3.115281343460083\n",
      "Iteration 214, Loss: 3.098045587539673\n",
      "Iteration 215, Loss: 3.080864429473877\n",
      "Iteration 216, Loss: 3.063741683959961\n",
      "Iteration 217, Loss: 3.046677589416504\n",
      "Iteration 218, Loss: 3.029665946960449\n",
      "Iteration 219, Loss: 3.0127105712890625\n",
      "Iteration 220, Loss: 2.9958112239837646\n",
      "Iteration 221, Loss: 2.978965997695923\n",
      "Iteration 222, Loss: 2.9621729850769043\n",
      "Iteration 223, Loss: 2.945436477661133\n",
      "Iteration 224, Loss: 2.928750514984131\n",
      "Iteration 225, Loss: 2.912119150161743\n",
      "Iteration 226, Loss: 2.8955366611480713\n",
      "Iteration 227, Loss: 2.879007577896118\n",
      "Iteration 228, Loss: 2.8625264167785645\n",
      "Iteration 229, Loss: 2.846099853515625\n",
      "Iteration 230, Loss: 2.8297204971313477\n",
      "Iteration 231, Loss: 2.813394069671631\n",
      "Iteration 232, Loss: 2.797114372253418\n",
      "Iteration 233, Loss: 2.7808821201324463\n",
      "Iteration 234, Loss: 2.7646992206573486\n",
      "Iteration 235, Loss: 2.7485647201538086\n",
      "Iteration 236, Loss: 2.7324752807617188\n",
      "Iteration 237, Loss: 2.7164342403411865\n",
      "Iteration 238, Loss: 2.7004401683807373\n",
      "Iteration 239, Loss: 2.6844916343688965\n",
      "Iteration 240, Loss: 2.6685893535614014\n",
      "Iteration 241, Loss: 2.6527295112609863\n",
      "Iteration 242, Loss: 2.636918783187866\n",
      "Iteration 243, Loss: 2.6211483478546143\n",
      "Iteration 244, Loss: 2.605424404144287\n",
      "Iteration 245, Loss: 2.5897412300109863\n",
      "Iteration 246, Loss: 2.574101209640503\n",
      "Iteration 247, Loss: 2.558507204055786\n",
      "Iteration 248, Loss: 2.5429539680480957\n",
      "Iteration 249, Loss: 2.5274429321289062\n",
      "Iteration 250, Loss: 2.5119731426239014\n",
      "Iteration 251, Loss: 2.4965450763702393\n",
      "Iteration 252, Loss: 2.4811582565307617\n",
      "Iteration 253, Loss: 2.4658102989196777\n",
      "Iteration 254, Loss: 2.4505019187927246\n",
      "Iteration 255, Loss: 2.4352362155914307\n",
      "Iteration 256, Loss: 2.420008420944214\n",
      "Iteration 257, Loss: 2.4048213958740234\n",
      "Iteration 258, Loss: 2.389672040939331\n",
      "Iteration 259, Loss: 2.3745598793029785\n",
      "Iteration 260, Loss: 2.3594858646392822\n",
      "Iteration 261, Loss: 2.3444504737854004\n",
      "Iteration 262, Loss: 2.3294527530670166\n",
      "Iteration 263, Loss: 2.314492702484131\n",
      "Iteration 264, Loss: 2.299567937850952\n",
      "Iteration 265, Loss: 2.2846810817718506\n",
      "Iteration 266, Loss: 2.2698309421539307\n",
      "Iteration 267, Loss: 2.255014181137085\n",
      "Iteration 268, Loss: 2.2402350902557373\n",
      "Iteration 269, Loss: 2.2254881858825684\n",
      "Iteration 270, Loss: 2.2107787132263184\n",
      "Iteration 271, Loss: 2.196103572845459\n",
      "Iteration 272, Loss: 2.1814627647399902\n",
      "Iteration 273, Loss: 2.166856527328491\n",
      "Iteration 274, Loss: 2.152282953262329\n",
      "Iteration 275, Loss: 2.1377437114715576\n",
      "Iteration 276, Loss: 2.123236894607544\n",
      "Iteration 277, Loss: 2.108764410018921\n",
      "Iteration 278, Loss: 2.0943214893341064\n",
      "Iteration 279, Loss: 2.079914093017578\n",
      "Iteration 280, Loss: 2.065537691116333\n",
      "Iteration 281, Loss: 2.05119252204895\n",
      "Iteration 282, Loss: 2.036878824234009\n",
      "Iteration 283, Loss: 2.022596597671509\n",
      "Iteration 284, Loss: 2.008347511291504\n",
      "Iteration 285, Loss: 1.9941245317459106\n",
      "Iteration 286, Loss: 1.9799363613128662\n",
      "Iteration 287, Loss: 1.9657752513885498\n",
      "Iteration 288, Loss: 1.9516465663909912\n",
      "Iteration 289, Loss: 1.9375454187393188\n",
      "Iteration 290, Loss: 1.9234771728515625\n",
      "Iteration 291, Loss: 1.9094338417053223\n",
      "Iteration 292, Loss: 1.8954226970672607\n",
      "Iteration 293, Loss: 1.88143789768219\n",
      "Iteration 294, Loss: 1.8674840927124023\n",
      "Iteration 295, Loss: 1.8535563945770264\n",
      "Iteration 296, Loss: 1.839656114578247\n",
      "Iteration 297, Loss: 1.82578706741333\n",
      "Iteration 298, Loss: 1.81194269657135\n",
      "Iteration 299, Loss: 1.7981274127960205\n",
      "Iteration 300, Loss: 1.7843389511108398\n",
      "Iteration 301, Loss: 1.7705761194229126\n",
      "Iteration 302, Loss: 1.7568410634994507\n",
      "Iteration 303, Loss: 1.7431316375732422\n",
      "Iteration 304, Loss: 1.7294487953186035\n",
      "Iteration 305, Loss: 1.715792179107666\n",
      "Iteration 306, Loss: 1.702161192893982\n",
      "Iteration 307, Loss: 1.6885559558868408\n",
      "Iteration 308, Loss: 1.6749769449234009\n",
      "Iteration 309, Loss: 1.6614214181900024\n",
      "Iteration 310, Loss: 1.6478921175003052\n",
      "Iteration 311, Loss: 1.6343873739242554\n",
      "Iteration 312, Loss: 1.6209068298339844\n",
      "Iteration 313, Loss: 1.6074504852294922\n",
      "Iteration 314, Loss: 1.5940186977386475\n",
      "Iteration 315, Loss: 1.5806106328964233\n",
      "Iteration 316, Loss: 1.5672274827957153\n",
      "Iteration 317, Loss: 1.5538655519485474\n",
      "Iteration 318, Loss: 1.5405302047729492\n",
      "Iteration 319, Loss: 1.527214527130127\n",
      "Iteration 320, Loss: 1.5139254331588745\n",
      "Iteration 321, Loss: 1.5006555318832397\n",
      "Iteration 322, Loss: 1.4874093532562256\n",
      "Iteration 323, Loss: 1.474186658859253\n",
      "Iteration 324, Loss: 1.4609856605529785\n",
      "Iteration 325, Loss: 1.4478083848953247\n",
      "Iteration 326, Loss: 1.43464994430542\n",
      "Iteration 327, Loss: 1.421517014503479\n",
      "Iteration 328, Loss: 1.4084030389785767\n",
      "Iteration 329, Loss: 1.3953112363815308\n",
      "Iteration 330, Loss: 1.3822402954101562\n",
      "Iteration 331, Loss: 1.3691918849945068\n",
      "Iteration 332, Loss: 1.3561629056930542\n",
      "Iteration 333, Loss: 1.3431549072265625\n",
      "Iteration 334, Loss: 1.3301669359207153\n",
      "Iteration 335, Loss: 1.317199468612671\n",
      "Iteration 336, Loss: 1.3042525053024292\n",
      "Iteration 337, Loss: 1.2913249731063843\n",
      "Iteration 338, Loss: 1.2784202098846436\n",
      "Iteration 339, Loss: 1.2655320167541504\n",
      "Iteration 340, Loss: 1.252665638923645\n",
      "Iteration 341, Loss: 1.2398182153701782\n",
      "Iteration 342, Loss: 1.2269909381866455\n",
      "Iteration 343, Loss: 1.2141807079315186\n",
      "Iteration 344, Loss: 1.2013909816741943\n",
      "Iteration 345, Loss: 1.18861985206604\n",
      "Iteration 346, Loss: 1.1758677959442139\n",
      "Iteration 347, Loss: 1.1631332635879517\n",
      "Iteration 348, Loss: 1.1504193544387817\n",
      "Iteration 349, Loss: 1.1377208232879639\n",
      "Iteration 350, Loss: 1.1250419616699219\n",
      "Iteration 351, Loss: 1.1123815774917603\n",
      "Iteration 352, Loss: 1.0997390747070312\n",
      "Iteration 353, Loss: 1.0871134996414185\n",
      "Iteration 354, Loss: 1.0745062828063965\n",
      "Iteration 355, Loss: 1.0619170665740967\n",
      "Iteration 356, Loss: 1.0493437051773071\n",
      "Iteration 357, Loss: 1.036789059638977\n",
      "Iteration 358, Loss: 1.0242512226104736\n",
      "Iteration 359, Loss: 1.0117313861846924\n",
      "Iteration 360, Loss: 0.9992266893386841\n",
      "Iteration 361, Loss: 0.986739993095398\n",
      "Iteration 362, Loss: 0.9742708802223206\n",
      "Iteration 363, Loss: 0.9618169665336609\n",
      "Iteration 364, Loss: 0.9493796825408936\n",
      "Iteration 365, Loss: 0.9369602203369141\n",
      "Iteration 366, Loss: 0.924557089805603\n",
      "Iteration 367, Loss: 0.9121688008308411\n",
      "Iteration 368, Loss: 0.8997983932495117\n",
      "Iteration 369, Loss: 0.8874425888061523\n",
      "Iteration 370, Loss: 0.8751033544540405\n",
      "Iteration 371, Loss: 0.8627794981002808\n",
      "Iteration 372, Loss: 0.8504727482795715\n",
      "Iteration 373, Loss: 0.8381803631782532\n",
      "Iteration 374, Loss: 0.8259055018424988\n",
      "Iteration 375, Loss: 0.8136462569236755\n",
      "Iteration 376, Loss: 0.8014003038406372\n",
      "Iteration 377, Loss: 0.7891719937324524\n",
      "Iteration 378, Loss: 0.7769577503204346\n",
      "Iteration 379, Loss: 0.764761209487915\n",
      "Iteration 380, Loss: 0.7525770664215088\n",
      "Iteration 381, Loss: 0.740410566329956\n",
      "Iteration 382, Loss: 0.7282597422599792\n",
      "Iteration 383, Loss: 0.716123104095459\n",
      "Iteration 384, Loss: 0.7040026783943176\n",
      "Iteration 385, Loss: 0.691897988319397\n",
      "Iteration 386, Loss: 0.6798080205917358\n",
      "Iteration 387, Loss: 0.6677325367927551\n",
      "Iteration 388, Loss: 0.6556739211082458\n",
      "Iteration 389, Loss: 0.6436301469802856\n",
      "Iteration 390, Loss: 0.6316033601760864\n",
      "Iteration 391, Loss: 0.6195915937423706\n",
      "Iteration 392, Loss: 0.6075956225395203\n",
      "Iteration 393, Loss: 0.595615029335022\n",
      "Iteration 394, Loss: 0.5836509466171265\n",
      "Iteration 395, Loss: 0.5717028975486755\n",
      "Iteration 396, Loss: 0.5597718358039856\n",
      "Iteration 397, Loss: 0.5478564500808716\n",
      "Iteration 398, Loss: 0.5359581708908081\n",
      "Iteration 399, Loss: 0.5240777730941772\n",
      "Iteration 400, Loss: 0.512214183807373\n",
      "Iteration 401, Loss: 0.5003669857978821\n",
      "Iteration 402, Loss: 0.48854029178619385\n",
      "Iteration 403, Loss: 0.4767296314239502\n",
      "Iteration 404, Loss: 0.46493834257125854\n",
      "Iteration 405, Loss: 0.4531676471233368\n",
      "Iteration 406, Loss: 0.44141602516174316\n",
      "Iteration 407, Loss: 0.4296843409538269\n",
      "Iteration 408, Loss: 0.41797539591789246\n",
      "Iteration 409, Loss: 0.4062892198562622\n",
      "Iteration 410, Loss: 0.39462384581565857\n",
      "Iteration 411, Loss: 0.3829842805862427\n",
      "Iteration 412, Loss: 0.3713700473308563\n",
      "Iteration 413, Loss: 0.3597829043865204\n",
      "Iteration 414, Loss: 0.34822165966033936\n",
      "Iteration 415, Loss: 0.3366928696632385\n",
      "Iteration 416, Loss: 0.32519397139549255\n",
      "Iteration 417, Loss: 0.3137299716472626\n",
      "Iteration 418, Loss: 0.3023022413253784\n",
      "Iteration 419, Loss: 0.29091379046440125\n",
      "Iteration 420, Loss: 0.2795683741569519\n",
      "Iteration 421, Loss: 0.26826930046081543\n",
      "Iteration 422, Loss: 0.2570224702358246\n",
      "Iteration 423, Loss: 0.24583017826080322\n",
      "Iteration 424, Loss: 0.2347012460231781\n",
      "Iteration 425, Loss: 0.22364096343517303\n",
      "Iteration 426, Loss: 0.2126590460538864\n",
      "Iteration 427, Loss: 0.201764315366745\n",
      "Iteration 428, Loss: 0.19096946716308594\n",
      "Iteration 429, Loss: 0.18028795719146729\n",
      "Iteration 430, Loss: 0.16973590850830078\n",
      "Iteration 431, Loss: 0.15933525562286377\n",
      "Iteration 432, Loss: 0.14910893142223358\n",
      "Iteration 433, Loss: 0.13908728957176208\n",
      "Iteration 434, Loss: 0.12930385768413544\n",
      "Iteration 435, Loss: 0.11980458348989487\n",
      "Iteration 436, Loss: 0.11063743382692337\n",
      "Iteration 437, Loss: 0.10186120122671127\n",
      "Iteration 438, Loss: 0.09354778379201889\n",
      "Iteration 439, Loss: 0.08576985448598862\n",
      "Iteration 440, Loss: 0.0786113366484642\n",
      "Iteration 441, Loss: 0.0721505880355835\n",
      "Iteration 442, Loss: 0.06645794957876205\n",
      "Iteration 443, Loss: 0.06158447265625\n",
      "Iteration 444, Loss: 0.05755237862467766\n",
      "Iteration 445, Loss: 0.056775372475385666\n",
      "Iteration 446, Loss: 0.05946890637278557\n",
      "Iteration 447, Loss: 0.062119726091623306\n",
      "Iteration 448, Loss: 0.06453744322061539\n",
      "Iteration 449, Loss: 0.06658542156219482\n",
      "Iteration 450, Loss: 0.06817702949047089\n",
      "Iteration 451, Loss: 0.06927160173654556\n",
      "Iteration 452, Loss: 0.0698632001876831\n",
      "Iteration 453, Loss: 0.0699748620390892\n",
      "Iteration 454, Loss: 0.06964989751577377\n",
      "Iteration 455, Loss: 0.06894683837890625\n",
      "Iteration 456, Loss: 0.06793411821126938\n",
      "Iteration 457, Loss: 0.06668464839458466\n",
      "Iteration 458, Loss: 0.06527344137430191\n",
      "Iteration 459, Loss: 0.06377403438091278\n",
      "Iteration 460, Loss: 0.062256138771772385\n",
      "Iteration 461, Loss: 0.06078493967652321\n",
      "Iteration 462, Loss: 0.05942269414663315\n",
      "Iteration 463, Loss: 0.05823998153209686\n",
      "Iteration 464, Loss: 0.05735047906637192\n",
      "Iteration 465, Loss: 0.05698895454406738\n",
      "Iteration 466, Loss: 0.05731841176748276\n",
      "Iteration 467, Loss: 0.05796993151307106\n",
      "Iteration 468, Loss: 0.058574359863996506\n",
      "Iteration 469, Loss: 0.058993302285671234\n",
      "Iteration 470, Loss: 0.05918912589550018\n",
      "Iteration 471, Loss: 0.05916297808289528\n",
      "Iteration 472, Loss: 0.05893389508128166\n",
      "Iteration 473, Loss: 0.05853164196014404\n",
      "Iteration 474, Loss: 0.05799039453268051\n",
      "Iteration 475, Loss: 0.05735345929861069\n",
      "Iteration 476, Loss: 0.056686125695705414\n",
      "Iteration 477, Loss: 0.05620646849274635\n",
      "Iteration 478, Loss: 0.05634145066142082\n",
      "Iteration 479, Loss: 0.05669069290161133\n",
      "Iteration 480, Loss: 0.05696523189544678\n",
      "Iteration 481, Loss: 0.05712171643972397\n",
      "Iteration 482, Loss: 0.05716097727417946\n",
      "Iteration 483, Loss: 0.057096682488918304\n",
      "Iteration 484, Loss: 0.056947194039821625\n",
      "Iteration 485, Loss: 0.05673392862081528\n",
      "Iteration 486, Loss: 0.05648208037018776\n",
      "Iteration 487, Loss: 0.05622685328125954\n",
      "Iteration 488, Loss: 0.05605121701955795\n",
      "Iteration 489, Loss: 0.05610581487417221\n",
      "Iteration 490, Loss: 0.056276123970746994\n",
      "Iteration 491, Loss: 0.05636405944824219\n",
      "Iteration 492, Loss: 0.05632726475596428\n",
      "Iteration 493, Loss: 0.0561826229095459\n",
      "Iteration 494, Loss: 0.055992644280195236\n",
      "Iteration 495, Loss: 0.055914442986249924\n",
      "Iteration 496, Loss: 0.05599344149231911\n",
      "Iteration 497, Loss: 0.05607903376221657\n",
      "Iteration 498, Loss: 0.05610780045390129\n",
      "Iteration 499, Loss: 0.056065164506435394\n",
      "Iteration 500, Loss: 0.05595088005065918\n",
      "Iteration 501, Loss: 0.055790506303310394\n",
      "Iteration 502, Loss: 0.05583461374044418\n",
      "Iteration 503, Loss: 0.05595497414469719\n",
      "Iteration 504, Loss: 0.05594813823699951\n",
      "Iteration 505, Loss: 0.05583791062235832\n",
      "Iteration 506, Loss: 0.05575212091207504\n",
      "Iteration 507, Loss: 0.055825911462306976\n",
      "Iteration 508, Loss: 0.055875860154628754\n",
      "Iteration 509, Loss: 0.05582587048411369\n",
      "Iteration 510, Loss: 0.055685997009277344\n",
      "Iteration 511, Loss: 0.05580711364746094\n",
      "Iteration 512, Loss: 0.05589199438691139\n",
      "Iteration 513, Loss: 0.055853646248579025\n",
      "Iteration 514, Loss: 0.05573086068034172\n",
      "Iteration 515, Loss: 0.05575478449463844\n",
      "Iteration 516, Loss: 0.05583568662405014\n",
      "Iteration 517, Loss: 0.055777791887521744\n",
      "Iteration 518, Loss: 0.055664777755737305\n",
      "Iteration 519, Loss: 0.05572597309947014\n",
      "Iteration 520, Loss: 0.055712103843688965\n",
      "Iteration 521, Loss: 0.055665891617536545\n",
      "Iteration 522, Loss: 0.05571727082133293\n",
      "Iteration 523, Loss: 0.055681269615888596\n",
      "Iteration 524, Loss: 0.055704277008771896\n",
      "Iteration 525, Loss: 0.0557330846786499\n",
      "Iteration 526, Loss: 0.05567082017660141\n",
      "Iteration 527, Loss: 0.05574024096131325\n",
      "Iteration 528, Loss: 0.05575370788574219\n",
      "Iteration 529, Loss: 0.05563954636454582\n",
      "Iteration 530, Loss: 0.05573149770498276\n",
      "Iteration 531, Loss: 0.05573407933115959\n",
      "Iteration 532, Loss: 0.055624764412641525\n",
      "Iteration 533, Loss: 0.05581331253051758\n",
      "Iteration 534, Loss: 0.05584665387868881\n",
      "Iteration 535, Loss: 0.05571933835744858\n",
      "Iteration 536, Loss: 0.05574345588684082\n",
      "Iteration 537, Loss: 0.05583755299448967\n",
      "Iteration 538, Loss: 0.05581843852996826\n",
      "Iteration 539, Loss: 0.055699706077575684\n",
      "Iteration 540, Loss: 0.05576213449239731\n",
      "Iteration 541, Loss: 0.05583655834197998\n",
      "Iteration 542, Loss: 0.05574401468038559\n",
      "Iteration 543, Loss: 0.05570018291473389\n",
      "Iteration 544, Loss: 0.05577298253774643\n",
      "Iteration 545, Loss: 0.055736225098371506\n",
      "Iteration 546, Loss: 0.05563442036509514\n",
      "Iteration 547, Loss: 0.05572807788848877\n",
      "Iteration 548, Loss: 0.055670224130153656\n",
      "Iteration 549, Loss: 0.055732570588588715\n",
      "Iteration 550, Loss: 0.05578422546386719\n",
      "Iteration 551, Loss: 0.05573562905192375\n",
      "Iteration 552, Loss: 0.05563287064433098\n",
      "Iteration 553, Loss: 0.055634379386901855\n",
      "Iteration 554, Loss: 0.05571878328919411\n",
      "Iteration 555, Loss: 0.05573515221476555\n",
      "Iteration 556, Loss: 0.05565611645579338\n",
      "Iteration 557, Loss: 0.05577588081359863\n",
      "Iteration 558, Loss: 0.05580437555909157\n",
      "Iteration 559, Loss: 0.05565854161977768\n",
      "Iteration 560, Loss: 0.055801987648010254\n",
      "Iteration 561, Loss: 0.05590923875570297\n",
      "Iteration 562, Loss: 0.055911026895046234\n",
      "Iteration 563, Loss: 0.055818043649196625\n",
      "Iteration 564, Loss: 0.05564228817820549\n",
      "Iteration 565, Loss: 0.055910348892211914\n",
      "Iteration 566, Loss: 0.05604120343923569\n",
      "Iteration 567, Loss: 0.055984895676374435\n",
      "Iteration 568, Loss: 0.0557607039809227\n",
      "Iteration 569, Loss: 0.05577842518687248\n",
      "Iteration 570, Loss: 0.055933158844709396\n",
      "Iteration 571, Loss: 0.05597774311900139\n",
      "Iteration 572, Loss: 0.05592314526438713\n",
      "Iteration 573, Loss: 0.05577965825796127\n",
      "Iteration 574, Loss: 0.055687230080366135\n",
      "Iteration 575, Loss: 0.05578676983714104\n",
      "Iteration 576, Loss: 0.055703841149806976\n",
      "Iteration 577, Loss: 0.055726807564496994\n",
      "Iteration 578, Loss: 0.05579710379242897\n",
      "Iteration 579, Loss: 0.05576638504862785\n",
      "Iteration 580, Loss: 0.05564530938863754\n",
      "Iteration 581, Loss: 0.055842798203229904\n",
      "Iteration 582, Loss: 0.05591730400919914\n",
      "Iteration 583, Loss: 0.055811285972595215\n",
      "Iteration 584, Loss: 0.05566366761922836\n",
      "Iteration 585, Loss: 0.05574846640229225\n",
      "Iteration 586, Loss: 0.05573225021362305\n",
      "Iteration 587, Loss: 0.05562448874115944\n",
      "Iteration 588, Loss: 0.05585233494639397\n",
      "Iteration 589, Loss: 0.05591221898794174\n",
      "Iteration 590, Loss: 0.05579690262675285\n",
      "Iteration 591, Loss: 0.05568035691976547\n",
      "Iteration 592, Loss: 0.05577143281698227\n",
      "Iteration 593, Loss: 0.055761221796274185\n",
      "Iteration 594, Loss: 0.05565575882792473\n",
      "Iteration 595, Loss: 0.05580826848745346\n",
      "Iteration 596, Loss: 0.05586802959442139\n",
      "Iteration 597, Loss: 0.055757444351911545\n",
      "Iteration 598, Loss: 0.055704716593027115\n",
      "Iteration 599, Loss: 0.05579070374369621\n",
      "Iteration 600, Loss: 0.05577202886343002\n",
      "Iteration 601, Loss: 0.05565663427114487\n",
      "Iteration 602, Loss: 0.0558168925344944\n",
      "Iteration 603, Loss: 0.055887144058942795\n",
      "Iteration 604, Loss: 0.0557868517935276\n",
      "Iteration 605, Loss: 0.05567622557282448\n",
      "Iteration 606, Loss: 0.05575951188802719\n",
      "Iteration 607, Loss: 0.05573996156454086\n",
      "Iteration 608, Loss: 0.05561995878815651\n",
      "Iteration 609, Loss: 0.055864810943603516\n",
      "Iteration 610, Loss: 0.05594182386994362\n",
      "Iteration 611, Loss: 0.055854640901088715\n",
      "Iteration 612, Loss: 0.055656593292951584\n",
      "Iteration 613, Loss: 0.05582229420542717\n",
      "Iteration 614, Loss: 0.055918775498867035\n",
      "Iteration 615, Loss: 0.05588666722178459\n",
      "Iteration 616, Loss: 0.05574222654104233\n",
      "Iteration 617, Loss: 0.05573872849345207\n",
      "Iteration 618, Loss: 0.055842045694589615\n",
      "Iteration 619, Loss: 0.05579245463013649\n",
      "Iteration 620, Loss: 0.05562448501586914\n",
      "Iteration 621, Loss: 0.05573630332946777\n",
      "Iteration 622, Loss: 0.05570411682128906\n",
      "Iteration 623, Loss: 0.05568321794271469\n",
      "Iteration 624, Loss: 0.0557076558470726\n",
      "Iteration 625, Loss: 0.05562059208750725\n",
      "Iteration 626, Loss: 0.05576833337545395\n",
      "Iteration 627, Loss: 0.055745601654052734\n",
      "Iteration 628, Loss: 0.05565603822469711\n",
      "Iteration 629, Loss: 0.055690448731184006\n",
      "Iteration 630, Loss: 0.05562552064657211\n",
      "Iteration 631, Loss: 0.055800240486860275\n",
      "Iteration 632, Loss: 0.0558137521147728\n",
      "Iteration 633, Loss: 0.055657509714365005\n",
      "Iteration 634, Loss: 0.05580989643931389\n",
      "Iteration 635, Loss: 0.055920325219631195\n",
      "Iteration 636, Loss: 0.05591770261526108\n",
      "Iteration 637, Loss: 0.055814824998378754\n",
      "Iteration 638, Loss: 0.055647771805524826\n",
      "Iteration 639, Loss: 0.05586918443441391\n",
      "Iteration 640, Loss: 0.055954813957214355\n",
      "Iteration 641, Loss: 0.05585845559835434\n",
      "Iteration 642, Loss: 0.05563469976186752\n",
      "Iteration 643, Loss: 0.055772505700588226\n",
      "Iteration 644, Loss: 0.05582082271575928\n",
      "Iteration 645, Loss: 0.05576741695404053\n",
      "Iteration 646, Loss: 0.05562460795044899\n",
      "Iteration 647, Loss: 0.055885475128889084\n",
      "Iteration 648, Loss: 0.055974047631025314\n",
      "Iteration 649, Loss: 0.055882297456264496\n",
      "Iteration 650, Loss: 0.0556364469230175\n",
      "Iteration 651, Loss: 0.05586882680654526\n",
      "Iteration 652, Loss: 0.05601620674133301\n",
      "Iteration 653, Loss: 0.05605026334524155\n",
      "Iteration 654, Loss: 0.05598271265625954\n",
      "Iteration 655, Loss: 0.05582614988088608\n",
      "Iteration 656, Loss: 0.055664777755737305\n",
      "Iteration 657, Loss: 0.05583580583333969\n",
      "Iteration 658, Loss: 0.05585448071360588\n",
      "Iteration 659, Loss: 0.055706582963466644\n",
      "Iteration 660, Loss: 0.055769167840480804\n",
      "Iteration 661, Loss: 0.05587558075785637\n",
      "Iteration 662, Loss: 0.0558781623840332\n",
      "Iteration 663, Loss: 0.055786292999982834\n",
      "Iteration 664, Loss: 0.055626511573791504\n",
      "Iteration 665, Loss: 0.05581172555685043\n",
      "Iteration 666, Loss: 0.05583222955465317\n",
      "Iteration 667, Loss: 0.055696409195661545\n",
      "Iteration 668, Loss: 0.05576638504862785\n",
      "Iteration 669, Loss: 0.05586067959666252\n",
      "Iteration 670, Loss: 0.05583664029836655\n",
      "Iteration 671, Loss: 0.05570749565958977\n",
      "Iteration 672, Loss: 0.055763840675354004\n",
      "Iteration 673, Loss: 0.05584847927093506\n",
      "Iteration 674, Loss: 0.0557638444006443\n",
      "Iteration 675, Loss: 0.055679481476545334\n",
      "Iteration 676, Loss: 0.055748939514160156\n",
      "Iteration 677, Loss: 0.05570507049560547\n",
      "Iteration 678, Loss: 0.055676739662885666\n",
      "Iteration 679, Loss: 0.05568190664052963\n",
      "Iteration 680, Loss: 0.05567967891693115\n",
      "Iteration 681, Loss: 0.05568560212850571\n",
      "Iteration 682, Loss: 0.0556466206908226\n",
      "Iteration 683, Loss: 0.05564026162028313\n",
      "Iteration 684, Loss: 0.055692438036203384\n",
      "Iteration 685, Loss: 0.05568806454539299\n",
      "Iteration 686, Loss: 0.05564304441213608\n",
      "Iteration 687, Loss: 0.055628299713134766\n",
      "Iteration 688, Loss: 0.05567161366343498\n",
      "Iteration 689, Loss: 0.05564296245574951\n",
      "Iteration 690, Loss: 0.05561733618378639\n",
      "Iteration 691, Loss: 0.05575808137655258\n",
      "Iteration 692, Loss: 0.05572863668203354\n",
      "Iteration 693, Loss: 0.055670540779829025\n",
      "Iteration 694, Loss: 0.055707693099975586\n",
      "Iteration 695, Loss: 0.05564463511109352\n",
      "Iteration 696, Loss: 0.05577218905091286\n",
      "Iteration 697, Loss: 0.055789630860090256\n",
      "Iteration 698, Loss: 0.05565237998962402\n",
      "Iteration 699, Loss: 0.055795591324567795\n",
      "Iteration 700, Loss: 0.055882178246974945\n",
      "Iteration 701, Loss: 0.05585285276174545\n",
      "Iteration 702, Loss: 0.05572402477264404\n",
      "Iteration 703, Loss: 0.05574476718902588\n",
      "Iteration 704, Loss: 0.055831193923950195\n",
      "Iteration 705, Loss: 0.055755261331796646\n",
      "Iteration 706, Loss: 0.055680714547634125\n",
      "Iteration 707, Loss: 0.05574135109782219\n",
      "Iteration 708, Loss: 0.05569478124380112\n",
      "Iteration 709, Loss: 0.05568850412964821\n",
      "Iteration 710, Loss: 0.05569501966238022\n",
      "Iteration 711, Loss: 0.055670540779829025\n",
      "Iteration 712, Loss: 0.055677931755781174\n",
      "Iteration 713, Loss: 0.055651307106018066\n",
      "Iteration 714, Loss: 0.055657509714365005\n",
      "Iteration 715, Loss: 0.05567014217376709\n",
      "Iteration 716, Loss: 0.05565798655152321\n",
      "Iteration 717, Loss: 0.05569247528910637\n",
      "Iteration 718, Loss: 0.05564785376191139\n",
      "Iteration 719, Loss: 0.055738966912031174\n",
      "Iteration 720, Loss: 0.05578025430440903\n",
      "Iteration 721, Loss: 0.0557202510535717\n",
      "Iteration 722, Loss: 0.05566879361867905\n",
      "Iteration 723, Loss: 0.055683813989162445\n",
      "Iteration 724, Loss: 0.055676501244306564\n",
      "Iteration 725, Loss: 0.05568584054708481\n",
      "Iteration 726, Loss: 0.055636368691921234\n",
      "Iteration 727, Loss: 0.05568937584757805\n",
      "Iteration 728, Loss: 0.05564475432038307\n",
      "Iteration 729, Loss: 0.055720411241054535\n",
      "Iteration 730, Loss: 0.05571174994111061\n",
      "Iteration 731, Loss: 0.05565321445465088\n",
      "Iteration 732, Loss: 0.0556594543159008\n",
      "Iteration 733, Loss: 0.05568882077932358\n",
      "Iteration 734, Loss: 0.05567086115479469\n",
      "Iteration 735, Loss: 0.0556923970580101\n",
      "Iteration 736, Loss: 0.05568842217326164\n",
      "Iteration 737, Loss: 0.05566982552409172\n",
      "Iteration 738, Loss: 0.055662792176008224\n",
      "Iteration 739, Loss: 0.05569363012909889\n",
      "Iteration 740, Loss: 0.055681709200143814\n",
      "Iteration 741, Loss: 0.055682264268398285\n",
      "Iteration 742, Loss: 0.05568154901266098\n",
      "Iteration 743, Loss: 0.055669866502285004\n",
      "Iteration 744, Loss: 0.05565544217824936\n",
      "Iteration 745, Loss: 0.05571095272898674\n",
      "Iteration 746, Loss: 0.055709682404994965\n",
      "Iteration 747, Loss: 0.05564789101481438\n",
      "Iteration 748, Loss: 0.0556764230132103\n",
      "Iteration 749, Loss: 0.05565401166677475\n",
      "Iteration 750, Loss: 0.05564912408590317\n",
      "Iteration 751, Loss: 0.05566469952464104\n",
      "Iteration 752, Loss: 0.05564530938863754\n",
      "Iteration 753, Loss: 0.05565126985311508\n",
      "Iteration 754, Loss: 0.05564066022634506\n",
      "Iteration 755, Loss: 0.055675309151411057\n",
      "Iteration 756, Loss: 0.055656831711530685\n",
      "Iteration 757, Loss: 0.05570177361369133\n",
      "Iteration 758, Loss: 0.05566330999135971\n",
      "Iteration 759, Loss: 0.0557272844016552\n",
      "Iteration 760, Loss: 0.0557708777487278\n",
      "Iteration 761, Loss: 0.055716197937726974\n",
      "Iteration 762, Loss: 0.05566505715250969\n",
      "Iteration 763, Loss: 0.055667124688625336\n",
      "Iteration 764, Loss: 0.05569791793823242\n",
      "Iteration 765, Loss: 0.05571747198700905\n",
      "Iteration 766, Loss: 0.05564204975962639\n",
      "Iteration 767, Loss: 0.055791061371564865\n",
      "Iteration 768, Loss: 0.055814944207668304\n",
      "Iteration 769, Loss: 0.05566330999135971\n",
      "Iteration 770, Loss: 0.05580195039510727\n",
      "Iteration 771, Loss: 0.055913131684064865\n",
      "Iteration 772, Loss: 0.05591857433319092\n",
      "Iteration 773, Loss: 0.055828891694545746\n",
      "Iteration 774, Loss: 0.05565476417541504\n",
      "Iteration 775, Loss: 0.05589481443166733\n",
      "Iteration 776, Loss: 0.056027851998806\n",
      "Iteration 777, Loss: 0.055973052978515625\n",
      "Iteration 778, Loss: 0.055750612169504166\n",
      "Iteration 779, Loss: 0.055784743279218674\n",
      "Iteration 780, Loss: 0.05593820661306381\n",
      "Iteration 781, Loss: 0.05598195642232895\n",
      "Iteration 782, Loss: 0.055926363915205\n",
      "Iteration 783, Loss: 0.055782001465559006\n",
      "Iteration 784, Loss: 0.055685363709926605\n",
      "Iteration 785, Loss: 0.05578581616282463\n",
      "Iteration 786, Loss: 0.05570447817444801\n",
      "Iteration 787, Loss: 0.05572501942515373\n",
      "Iteration 788, Loss: 0.055793724954128265\n",
      "Iteration 789, Loss: 0.055761855095624924\n",
      "Iteration 790, Loss: 0.05563966557383537\n",
      "Iteration 791, Loss: 0.05585182085633278\n",
      "Iteration 792, Loss: 0.055928152054548264\n",
      "Iteration 793, Loss: 0.055824361741542816\n",
      "Iteration 794, Loss: 0.05565337464213371\n",
      "Iteration 795, Loss: 0.055740080773830414\n",
      "Iteration 796, Loss: 0.05572700500488281\n",
      "Iteration 797, Loss: 0.05562083050608635\n",
      "Iteration 798, Loss: 0.055849235504865646\n",
      "Iteration 799, Loss: 0.05590729042887688\n",
      "Iteration 800, Loss: 0.05579746142029762\n",
      "Iteration 801, Loss: 0.055676620453596115\n",
      "Iteration 802, Loss: 0.055768173187971115\n",
      "Iteration 803, Loss: 0.055758796632289886\n",
      "Iteration 804, Loss: 0.055646300315856934\n",
      "Iteration 805, Loss: 0.055824004113674164\n",
      "Iteration 806, Loss: 0.055894456803798676\n",
      "Iteration 807, Loss: 0.05580484867095947\n",
      "Iteration 808, Loss: 0.05566176027059555\n",
      "Iteration 809, Loss: 0.055764198303222656\n",
      "Iteration 810, Loss: 0.05577000230550766\n",
      "Iteration 811, Loss: 0.055654048919677734\n",
      "Iteration 812, Loss: 0.05580965802073479\n",
      "Iteration 813, Loss: 0.055887941271066666\n",
      "Iteration 814, Loss: 0.05582205578684807\n",
      "Iteration 815, Loss: 0.05566497892141342\n",
      "Iteration 816, Loss: 0.055809617042541504\n",
      "Iteration 817, Loss: 0.05588507652282715\n",
      "Iteration 818, Loss: 0.055811405181884766\n",
      "Iteration 819, Loss: 0.05563219636678696\n",
      "Iteration 820, Loss: 0.055795907974243164\n",
      "Iteration 821, Loss: 0.05582503601908684\n",
      "Iteration 822, Loss: 0.05570849031209946\n",
      "Iteration 823, Loss: 0.05574556440114975\n",
      "Iteration 824, Loss: 0.05582841485738754\n",
      "Iteration 825, Loss: 0.055784743279218674\n",
      "Iteration 826, Loss: 0.05564693734049797\n",
      "Iteration 827, Loss: 0.055819395929574966\n",
      "Iteration 828, Loss: 0.05588122457265854\n",
      "Iteration 829, Loss: 0.055780135095119476\n",
      "Iteration 830, Loss: 0.055677056312561035\n",
      "Iteration 831, Loss: 0.05575752258300781\n",
      "Iteration 832, Loss: 0.0557253398001194\n",
      "Iteration 833, Loss: 0.05564117431640625\n",
      "Iteration 834, Loss: 0.05566060543060303\n",
      "Iteration 835, Loss: 0.055682938545942307\n",
      "Iteration 836, Loss: 0.0556819848716259\n",
      "Iteration 837, Loss: 0.05564948171377182\n",
      "Iteration 838, Loss: 0.05564836785197258\n",
      "Iteration 839, Loss: 0.055673759430646896\n",
      "Iteration 840, Loss: 0.055651865899562836\n",
      "Iteration 841, Loss: 0.055716756731271744\n",
      "Iteration 842, Loss: 0.055693745613098145\n",
      "Iteration 843, Loss: 0.05569025129079819\n",
      "Iteration 844, Loss: 0.05571699142456055\n",
      "Iteration 845, Loss: 0.05563811585307121\n",
      "Iteration 846, Loss: 0.05579587072134018\n",
      "Iteration 847, Loss: 0.05582606792449951\n",
      "Iteration 848, Loss: 0.05569092556834221\n",
      "Iteration 849, Loss: 0.05577155202627182\n",
      "Iteration 850, Loss: 0.05586886778473854\n",
      "Iteration 851, Loss: 0.05584975332021713\n",
      "Iteration 852, Loss: 0.05572911351919174\n",
      "Iteration 853, Loss: 0.05572950839996338\n",
      "Iteration 854, Loss: 0.05580723658204079\n",
      "Iteration 855, Loss: 0.0557151660323143\n",
      "Iteration 856, Loss: 0.05572625249624252\n",
      "Iteration 857, Loss: 0.0557989701628685\n",
      "Iteration 858, Loss: 0.055762410163879395\n",
      "Iteration 859, Loss: 0.05564713850617409\n",
      "Iteration 860, Loss: 0.05581081286072731\n",
      "Iteration 861, Loss: 0.0558503083884716\n",
      "Iteration 862, Loss: 0.055713772773742676\n",
      "Iteration 863, Loss: 0.05575617402791977\n",
      "Iteration 864, Loss: 0.055858056992292404\n",
      "Iteration 865, Loss: 0.055854443460702896\n",
      "Iteration 866, Loss: 0.05575696751475334\n",
      "Iteration 867, Loss: 0.05566680431365967\n",
      "Iteration 868, Loss: 0.05573093891143799\n",
      "Iteration 869, Loss: 0.05564936250448227\n",
      "Iteration 870, Loss: 0.05573972314596176\n",
      "Iteration 871, Loss: 0.05577743053436279\n",
      "Iteration 872, Loss: 0.055711984634399414\n",
      "Iteration 873, Loss: 0.05568727105855942\n",
      "Iteration 874, Loss: 0.05570809170603752\n",
      "Iteration 875, Loss: 0.05565011873841286\n",
      "Iteration 876, Loss: 0.05565258115530014\n",
      "Iteration 877, Loss: 0.05568528175354004\n",
      "Iteration 878, Loss: 0.05563601106405258\n",
      "Iteration 879, Loss: 0.05575331300497055\n",
      "Iteration 880, Loss: 0.05579177662730217\n",
      "Iteration 881, Loss: 0.05572386831045151\n",
      "Iteration 882, Loss: 0.05568039417266846\n",
      "Iteration 883, Loss: 0.055718421936035156\n",
      "Iteration 884, Loss: 0.055638670921325684\n",
      "Iteration 885, Loss: 0.05570137873291969\n",
      "Iteration 886, Loss: 0.05569728463888168\n",
      "Iteration 887, Loss: 0.05563585087656975\n",
      "Iteration 888, Loss: 0.05564872547984123\n",
      "Iteration 889, Loss: 0.055649399757385254\n",
      "Iteration 890, Loss: 0.05565166473388672\n",
      "Iteration 891, Loss: 0.05564657971262932\n",
      "Iteration 892, Loss: 0.0556364469230175\n",
      "Iteration 893, Loss: 0.05568341538310051\n",
      "Iteration 894, Loss: 0.055668752640485764\n",
      "Iteration 895, Loss: 0.055681586265563965\n",
      "Iteration 896, Loss: 0.05563918873667717\n",
      "Iteration 897, Loss: 0.0557483471930027\n",
      "Iteration 898, Loss: 0.0557941235601902\n",
      "Iteration 899, Loss: 0.05574091523885727\n",
      "Iteration 900, Loss: 0.05563144013285637\n",
      "Iteration 901, Loss: 0.055651307106018066\n",
      "Iteration 902, Loss: 0.05568766966462135\n",
      "Iteration 903, Loss: 0.05568254366517067\n",
      "Iteration 904, Loss: 0.05565619841217995\n",
      "Iteration 905, Loss: 0.05561820790171623\n",
      "Iteration 906, Loss: 0.05574139207601547\n",
      "Iteration 907, Loss: 0.05576499551534653\n",
      "Iteration 908, Loss: 0.05569124221801758\n",
      "Iteration 909, Loss: 0.05572386831045151\n",
      "Iteration 910, Loss: 0.055746834725141525\n",
      "Iteration 911, Loss: 0.05562623590230942\n",
      "Iteration 912, Loss: 0.0556459054350853\n",
      "Iteration 913, Loss: 0.05566676706075668\n",
      "Iteration 914, Loss: 0.05563418194651604\n",
      "Iteration 915, Loss: 0.055646419525146484\n",
      "Iteration 916, Loss: 0.055676501244306564\n",
      "Iteration 917, Loss: 0.055657271295785904\n",
      "Iteration 918, Loss: 0.0557028092443943\n",
      "Iteration 919, Loss: 0.05566974729299545\n",
      "Iteration 920, Loss: 0.055719971656799316\n",
      "Iteration 921, Loss: 0.05575605481863022\n",
      "Iteration 922, Loss: 0.05568993091583252\n",
      "Iteration 923, Loss: 0.05571647733449936\n",
      "Iteration 924, Loss: 0.055734001100063324\n",
      "Iteration 925, Loss: 0.05563664436340332\n",
      "Iteration 926, Loss: 0.05564594268798828\n",
      "Iteration 927, Loss: 0.05567964166402817\n",
      "Iteration 928, Loss: 0.05561773106455803\n",
      "Iteration 929, Loss: 0.055689018219709396\n",
      "Iteration 930, Loss: 0.05563267320394516\n",
      "Iteration 931, Loss: 0.055751048028469086\n",
      "Iteration 932, Loss: 0.05575045198202133\n",
      "Iteration 933, Loss: 0.05563557147979736\n",
      "Iteration 934, Loss: 0.05575438588857651\n",
      "Iteration 935, Loss: 0.055750928819179535\n",
      "Iteration 936, Loss: 0.05562945455312729\n",
      "Iteration 937, Loss: 0.05573292821645737\n",
      "Iteration 938, Loss: 0.055700384080410004\n",
      "Iteration 939, Loss: 0.05568993091583252\n",
      "Iteration 940, Loss: 0.05571722984313965\n",
      "Iteration 941, Loss: 0.05563613027334213\n",
      "Iteration 942, Loss: 0.05578514188528061\n",
      "Iteration 943, Loss: 0.055805884301662445\n",
      "Iteration 944, Loss: 0.05566680431365967\n",
      "Iteration 945, Loss: 0.05579245463013649\n",
      "Iteration 946, Loss: 0.0558902844786644\n",
      "Iteration 947, Loss: 0.055873554199934006\n",
      "Iteration 948, Loss: 0.05575772374868393\n",
      "Iteration 949, Loss: 0.05569390580058098\n",
      "Iteration 950, Loss: 0.055779580026865005\n",
      "Iteration 951, Loss: 0.05571095272898674\n",
      "Iteration 952, Loss: 0.055715642869472504\n",
      "Iteration 953, Loss: 0.055772941559553146\n",
      "Iteration 954, Loss: 0.05573181435465813\n",
      "Iteration 955, Loss: 0.05565357208251953\n",
      "Iteration 956, Loss: 0.05573614686727524\n",
      "Iteration 957, Loss: 0.055695097893476486\n",
      "Iteration 958, Loss: 0.05570388212800026\n",
      "Iteration 959, Loss: 0.05574413388967514\n",
      "Iteration 960, Loss: 0.055688779801130295\n",
      "Iteration 961, Loss: 0.05570705980062485\n",
      "Iteration 962, Loss: 0.05571218580007553\n",
      "Iteration 963, Loss: 0.05566255375742912\n",
      "Iteration 964, Loss: 0.055682700127363205\n",
      "Iteration 965, Loss: 0.05562921613454819\n",
      "Iteration 966, Loss: 0.0557096004486084\n",
      "Iteration 967, Loss: 0.05569148063659668\n",
      "Iteration 968, Loss: 0.05567014217376709\n",
      "Iteration 969, Loss: 0.055672287940979004\n",
      "Iteration 970, Loss: 0.05568552017211914\n",
      "Iteration 971, Loss: 0.05568147078156471\n",
      "Iteration 972, Loss: 0.055665016174316406\n",
      "Iteration 973, Loss: 0.0556589774787426\n",
      "Iteration 974, Loss: 0.05569744482636452\n",
      "Iteration 975, Loss: 0.055698275566101074\n",
      "Iteration 976, Loss: 0.05564884468913078\n",
      "Iteration 977, Loss: 0.05569732189178467\n",
      "Iteration 978, Loss: 0.055644553154706955\n",
      "Iteration 979, Loss: 0.05571639910340309\n",
      "Iteration 980, Loss: 0.05572891607880592\n",
      "Iteration 981, Loss: 0.05563807487487793\n",
      "Iteration 982, Loss: 0.05580965802073479\n",
      "Iteration 983, Loss: 0.05585086718201637\n",
      "Iteration 984, Loss: 0.055721960961818695\n",
      "Iteration 985, Loss: 0.055743418633937836\n",
      "Iteration 986, Loss: 0.055839262902736664\n",
      "Iteration 987, Loss: 0.05582265183329582\n",
      "Iteration 988, Loss: 0.05570618435740471\n",
      "Iteration 989, Loss: 0.05575207993388176\n",
      "Iteration 990, Loss: 0.05582229420542717\n",
      "Iteration 991, Loss: 0.05571905896067619\n",
      "Iteration 992, Loss: 0.05572915077209473\n",
      "Iteration 993, Loss: 0.055809617042541504\n",
      "Iteration 994, Loss: 0.05578037351369858\n",
      "Iteration 995, Loss: 0.05565989017486572\n",
      "Iteration 996, Loss: 0.05581458657979965\n",
      "Iteration 997, Loss: 0.05588146299123764\n",
      "Iteration 998, Loss: 0.05577000230550766\n",
      "Iteration 999, Loss: 0.0556967668235302\n",
      "Iteration 1000, Loss: 0.05578378960490227\n",
      "Iteration 1001, Loss: 0.05576435849070549\n",
      "Iteration 1002, Loss: 0.05565313622355461\n",
      "Iteration 1003, Loss: 0.055815696716308594\n",
      "Iteration 1004, Loss: 0.055875103920698166\n",
      "Iteration 1005, Loss: 0.05575510114431381\n",
      "Iteration 1006, Loss: 0.055713772773742676\n",
      "Iteration 1007, Loss: 0.05580596253275871\n",
      "Iteration 1008, Loss: 0.055793486535549164\n",
      "Iteration 1009, Loss: 0.05568786710500717\n",
      "Iteration 1010, Loss: 0.05576503649353981\n",
      "Iteration 1011, Loss: 0.05582205578684807\n",
      "Iteration 1012, Loss: 0.05570030212402344\n",
      "Iteration 1013, Loss: 0.0557553805410862\n",
      "Iteration 1014, Loss: 0.055848680436611176\n",
      "Iteration 1015, Loss: 0.0558369979262352\n",
      "Iteration 1016, Loss: 0.05573252961039543\n",
      "Iteration 1017, Loss: 0.05570463463664055\n",
      "Iteration 1018, Loss: 0.05576249212026596\n",
      "Iteration 1019, Loss: 0.05564677715301514\n",
      "Iteration 1020, Loss: 0.055784743279218674\n",
      "Iteration 1021, Loss: 0.055868785828351974\n",
      "Iteration 1022, Loss: 0.05585026741027832\n",
      "Iteration 1023, Loss: 0.05573880672454834\n",
      "Iteration 1024, Loss: 0.055700384080410004\n",
      "Iteration 1025, Loss: 0.05576388165354729\n",
      "Iteration 1026, Loss: 0.0556495226919651\n",
      "Iteration 1027, Loss: 0.055785100907087326\n",
      "Iteration 1028, Loss: 0.0558728389441967\n",
      "Iteration 1029, Loss: 0.05585726350545883\n",
      "Iteration 1030, Loss: 0.05574886128306389\n",
      "Iteration 1031, Loss: 0.0556844100356102\n",
      "Iteration 1032, Loss: 0.05574687570333481\n",
      "Iteration 1033, Loss: 0.05563243478536606\n",
      "Iteration 1034, Loss: 0.055797021836042404\n",
      "Iteration 1035, Loss: 0.055884480476379395\n",
      "Iteration 1036, Loss: 0.05586842820048332\n",
      "Iteration 1037, Loss: 0.055759869515895844\n",
      "Iteration 1038, Loss: 0.055669307708740234\n",
      "Iteration 1039, Loss: 0.05573050305247307\n",
      "Iteration 1040, Loss: 0.05561435595154762\n",
      "Iteration 1041, Loss: 0.05578402802348137\n",
      "Iteration 1042, Loss: 0.055839698761701584\n",
      "Iteration 1043, Loss: 0.055790506303310394\n",
      "Iteration 1044, Loss: 0.0556592158973217\n",
      "Iteration 1045, Loss: 0.05582404136657715\n",
      "Iteration 1046, Loss: 0.055897992104291916\n",
      "Iteration 1047, Loss: 0.05579610913991928\n",
      "Iteration 1048, Loss: 0.055668674409389496\n",
      "Iteration 1049, Loss: 0.05574953928589821\n",
      "Iteration 1050, Loss: 0.05572712793946266\n",
      "Iteration 1051, Loss: 0.055624090135097504\n",
      "Iteration 1052, Loss: 0.055783551186323166\n",
      "Iteration 1053, Loss: 0.055777668952941895\n",
      "Iteration 1054, Loss: 0.05563267320394516\n",
      "Iteration 1055, Loss: 0.05574802681803703\n",
      "Iteration 1056, Loss: 0.05576539412140846\n",
      "Iteration 1057, Loss: 0.05566791817545891\n",
      "Iteration 1058, Loss: 0.055772267282009125\n",
      "Iteration 1059, Loss: 0.05582539364695549\n",
      "Iteration 1060, Loss: 0.05572120472788811\n",
      "Iteration 1061, Loss: 0.05572386831045151\n",
      "Iteration 1062, Loss: 0.05580051988363266\n",
      "Iteration 1063, Loss: 0.055756013840436935\n",
      "Iteration 1064, Loss: 0.0556313619017601\n",
      "Iteration 1065, Loss: 0.05575947090983391\n",
      "Iteration 1066, Loss: 0.055731020867824554\n",
      "Iteration 1067, Loss: 0.055669985711574554\n",
      "Iteration 1068, Loss: 0.05570698156952858\n",
      "Iteration 1069, Loss: 0.05564900487661362\n",
      "Iteration 1070, Loss: 0.05575517937541008\n",
      "Iteration 1071, Loss: 0.05575617402791977\n",
      "Iteration 1072, Loss: 0.05563263222575188\n",
      "Iteration 1073, Loss: 0.05566044896841049\n",
      "Iteration 1074, Loss: 0.05563553422689438\n",
      "Iteration 1075, Loss: 0.055667005479335785\n",
      "Iteration 1076, Loss: 0.05563895031809807\n",
      "Iteration 1077, Loss: 0.055732570588588715\n",
      "Iteration 1078, Loss: 0.05570157617330551\n",
      "Iteration 1079, Loss: 0.055692993104457855\n",
      "Iteration 1080, Loss: 0.05573229119181633\n",
      "Iteration 1081, Loss: 0.05567411705851555\n",
      "Iteration 1082, Loss: 0.05572442337870598\n",
      "Iteration 1083, Loss: 0.05572954937815666\n",
      "Iteration 1084, Loss: 0.0556485652923584\n",
      "Iteration 1085, Loss: 0.055667005479335785\n",
      "Iteration 1086, Loss: 0.05564066022634506\n",
      "Iteration 1087, Loss: 0.05565071105957031\n",
      "Iteration 1088, Loss: 0.0556257963180542\n",
      "Iteration 1089, Loss: 0.05570678040385246\n",
      "Iteration 1090, Loss: 0.055660687386989594\n",
      "Iteration 1091, Loss: 0.055725257843732834\n",
      "Iteration 1092, Loss: 0.05575140565633774\n",
      "Iteration 1093, Loss: 0.05566160008311272\n",
      "Iteration 1094, Loss: 0.055768292397260666\n",
      "Iteration 1095, Loss: 0.05581367388367653\n",
      "Iteration 1096, Loss: 0.05570753663778305\n",
      "Iteration 1097, Loss: 0.055736105889081955\n",
      "Iteration 1098, Loss: 0.05581267923116684\n",
      "Iteration 1099, Loss: 0.055768053978681564\n",
      "Iteration 1100, Loss: 0.0556383952498436\n",
      "Iteration 1101, Loss: 0.05580373853445053\n",
      "Iteration 1102, Loss: 0.0558343343436718\n",
      "Iteration 1103, Loss: 0.05569767951965332\n",
      "Iteration 1104, Loss: 0.05576682090759277\n",
      "Iteration 1105, Loss: 0.055867910385131836\n",
      "Iteration 1106, Loss: 0.05585996434092522\n",
      "Iteration 1107, Loss: 0.055755697190761566\n",
      "Iteration 1108, Loss: 0.055678367614746094\n",
      "Iteration 1109, Loss: 0.05575331300497055\n",
      "Iteration 1110, Loss: 0.05567225068807602\n",
      "Iteration 1111, Loss: 0.055745404213666916\n",
      "Iteration 1112, Loss: 0.05580922216176987\n",
      "Iteration 1113, Loss: 0.05577417463064194\n",
      "Iteration 1114, Loss: 0.05565345659852028\n",
      "Iteration 1115, Loss: 0.05582785606384277\n",
      "Iteration 1116, Loss: 0.055897992104291916\n",
      "Iteration 1117, Loss: 0.055787645280361176\n",
      "Iteration 1118, Loss: 0.05568289756774902\n",
      "Iteration 1119, Loss: 0.055770158767700195\n",
      "Iteration 1120, Loss: 0.05575573444366455\n",
      "Iteration 1121, Loss: 0.05564912408590317\n",
      "Iteration 1122, Loss: 0.055818043649196625\n",
      "Iteration 1123, Loss: 0.05587645620107651\n",
      "Iteration 1124, Loss: 0.055755339562892914\n",
      "Iteration 1125, Loss: 0.05571313947439194\n",
      "Iteration 1126, Loss: 0.05580655857920647\n",
      "Iteration 1127, Loss: 0.055795710533857346\n",
      "Iteration 1128, Loss: 0.05569148436188698\n",
      "Iteration 1129, Loss: 0.05575708672404289\n",
      "Iteration 1130, Loss: 0.05581303685903549\n",
      "Iteration 1131, Loss: 0.055690012872219086\n",
      "Iteration 1132, Loss: 0.05576225370168686\n",
      "Iteration 1133, Loss: 0.055856507271528244\n",
      "Iteration 1134, Loss: 0.055845897644758224\n",
      "Iteration 1135, Loss: 0.05574158951640129\n",
      "Iteration 1136, Loss: 0.05569009110331535\n",
      "Iteration 1137, Loss: 0.05574790760874748\n",
      "Iteration 1138, Loss: 0.05563430115580559\n",
      "Iteration 1139, Loss: 0.05578434467315674\n",
      "Iteration 1140, Loss: 0.05585996434092522\n",
      "Iteration 1141, Loss: 0.05583349987864494\n",
      "Iteration 1142, Loss: 0.0557149276137352\n",
      "Iteration 1143, Loss: 0.05574214458465576\n",
      "Iteration 1144, Loss: 0.0558147057890892\n",
      "Iteration 1145, Loss: 0.05570896714925766\n",
      "Iteration 1146, Loss: 0.055736422538757324\n",
      "Iteration 1147, Loss: 0.05581987276673317\n",
      "Iteration 1148, Loss: 0.055800676345825195\n",
      "Iteration 1149, Loss: 0.055689454078674316\n",
      "Iteration 1150, Loss: 0.05576781556010246\n",
      "Iteration 1151, Loss: 0.0558321513235569\n",
      "Iteration 1152, Loss: 0.05571854114532471\n",
      "Iteration 1153, Loss: 0.05573487654328346\n",
      "Iteration 1154, Loss: 0.055823128670454025\n",
      "Iteration 1155, Loss: 0.055808547884225845\n",
      "Iteration 1156, Loss: 0.05570129677653313\n",
      "Iteration 1157, Loss: 0.05574711412191391\n",
      "Iteration 1158, Loss: 0.0558069571852684\n",
      "Iteration 1159, Loss: 0.05568957328796387\n",
      "Iteration 1160, Loss: 0.05575883388519287\n",
      "Iteration 1161, Loss: 0.055849671363830566\n",
      "Iteration 1162, Loss: 0.05583691969513893\n",
      "Iteration 1163, Loss: 0.05573153868317604\n",
      "Iteration 1164, Loss: 0.055704593658447266\n",
      "Iteration 1165, Loss: 0.05576324462890625\n",
      "Iteration 1166, Loss: 0.05564475059509277\n",
      "Iteration 1167, Loss: 0.05579225346446037\n",
      "Iteration 1168, Loss: 0.05588356778025627\n",
      "Iteration 1169, Loss: 0.05587128922343254\n",
      "Iteration 1170, Loss: 0.055765990167856216\n",
      "Iteration 1171, Loss: 0.055657386779785156\n",
      "Iteration 1172, Loss: 0.055717311799526215\n",
      "Iteration 1173, Loss: 0.055624090135097504\n",
      "Iteration 1174, Loss: 0.055684369057416916\n",
      "Iteration 1175, Loss: 0.05563819780945778\n",
      "Iteration 1176, Loss: 0.055754028260707855\n",
      "Iteration 1177, Loss: 0.05576157569885254\n",
      "Iteration 1178, Loss: 0.05564137548208237\n",
      "Iteration 1179, Loss: 0.05577957630157471\n",
      "Iteration 1180, Loss: 0.05582153797149658\n",
      "Iteration 1181, Loss: 0.05572625249624252\n",
      "Iteration 1182, Loss: 0.05570932477712631\n",
      "Iteration 1183, Loss: 0.055775485932826996\n",
      "Iteration 1184, Loss: 0.0557069405913353\n",
      "Iteration 1185, Loss: 0.05570455640554428\n",
      "Iteration 1186, Loss: 0.05574623867869377\n",
      "Iteration 1187, Loss: 0.05565508455038071\n",
      "Iteration 1188, Loss: 0.05577397719025612\n",
      "Iteration 1189, Loss: 0.05582594871520996\n",
      "Iteration 1190, Loss: 0.055739644914865494\n",
      "Iteration 1191, Loss: 0.05569152161478996\n",
      "Iteration 1192, Loss: 0.055756013840436935\n",
      "Iteration 1193, Loss: 0.05569136515259743\n",
      "Iteration 1194, Loss: 0.05572199821472168\n",
      "Iteration 1195, Loss: 0.05576173588633537\n",
      "Iteration 1196, Loss: 0.0556797981262207\n",
      "Iteration 1197, Loss: 0.05573717877268791\n",
      "Iteration 1198, Loss: 0.05577830597758293\n",
      "Iteration 1199, Loss: 0.05567745491862297\n",
      "Iteration 1200, Loss: 0.05576292797923088\n",
      "Iteration 1201, Loss: 0.05583246797323227\n",
      "Iteration 1202, Loss: 0.055773936212062836\n",
      "Iteration 1203, Loss: 0.05565500259399414\n",
      "Iteration 1204, Loss: 0.05577421188354492\n",
      "Iteration 1205, Loss: 0.05578327178955078\n",
      "Iteration 1206, Loss: 0.05563688278198242\n",
      "Iteration 1207, Loss: 0.05581911653280258\n",
      "Iteration 1208, Loss: 0.055922865867614746\n",
      "Iteration 1209, Loss: 0.05591794103384018\n",
      "Iteration 1210, Loss: 0.055817246437072754\n",
      "Iteration 1211, Loss: 0.05565734952688217\n",
      "Iteration 1212, Loss: 0.05585746094584465\n",
      "Iteration 1213, Loss: 0.055944085121154785\n",
      "Iteration 1214, Loss: 0.05585094541311264\n",
      "Iteration 1215, Loss: 0.055632077157497406\n",
      "Iteration 1216, Loss: 0.055752240121364594\n",
      "Iteration 1217, Loss: 0.05577941983938217\n",
      "Iteration 1218, Loss: 0.05570666119456291\n",
      "Iteration 1219, Loss: 0.0557001456618309\n",
      "Iteration 1220, Loss: 0.055724065750837326\n",
      "Iteration 1221, Loss: 0.05563851445913315\n",
      "Iteration 1222, Loss: 0.05564860627055168\n",
      "Iteration 1223, Loss: 0.05567920580506325\n",
      "Iteration 1224, Loss: 0.055626314133405685\n",
      "Iteration 1225, Loss: 0.05574465170502663\n",
      "Iteration 1226, Loss: 0.05575398728251457\n",
      "Iteration 1227, Loss: 0.05565202236175537\n",
      "Iteration 1228, Loss: 0.0557832345366478\n",
      "Iteration 1229, Loss: 0.0558268241584301\n",
      "Iteration 1230, Loss: 0.05571440979838371\n",
      "Iteration 1231, Loss: 0.05573614686727524\n",
      "Iteration 1232, Loss: 0.05581820011138916\n",
      "Iteration 1233, Loss: 0.055777233093976974\n",
      "Iteration 1234, Loss: 0.05564149469137192\n",
      "Iteration 1235, Loss: 0.055828094482421875\n",
      "Iteration 1236, Loss: 0.05588877573609352\n",
      "Iteration 1237, Loss: 0.05577763170003891\n",
      "Iteration 1238, Loss: 0.055686913430690765\n",
      "Iteration 1239, Loss: 0.05577389523386955\n",
      "Iteration 1240, Loss: 0.05574914067983627\n",
      "Iteration 1241, Loss: 0.0556282214820385\n",
      "Iteration 1242, Loss: 0.05584057420492172\n",
      "Iteration 1243, Loss: 0.055894456803798676\n",
      "Iteration 1244, Loss: 0.05577079579234123\n",
      "Iteration 1245, Loss: 0.05570264905691147\n",
      "Iteration 1246, Loss: 0.05579721927642822\n",
      "Iteration 1247, Loss: 0.05578490346670151\n",
      "Iteration 1248, Loss: 0.055676817893981934\n",
      "Iteration 1249, Loss: 0.05578025430440903\n",
      "Iteration 1250, Loss: 0.0558401383459568\n",
      "Iteration 1251, Loss: 0.05572263523936272\n",
      "Iteration 1252, Loss: 0.05573471635580063\n",
      "Iteration 1253, Loss: 0.05582519620656967\n",
      "Iteration 1254, Loss: 0.05580870434641838\n",
      "Iteration 1255, Loss: 0.05569816008210182\n",
      "Iteration 1256, Loss: 0.0557558573782444\n",
      "Iteration 1257, Loss: 0.05581863969564438\n",
      "Iteration 1258, Loss: 0.05570380017161369\n",
      "Iteration 1259, Loss: 0.05574754998087883\n",
      "Iteration 1260, Loss: 0.05583648011088371\n",
      "Iteration 1261, Loss: 0.05581935495138168\n",
      "Iteration 1262, Loss: 0.055709801614284515\n",
      "Iteration 1263, Loss: 0.05573984235525131\n",
      "Iteration 1264, Loss: 0.05580206960439682\n",
      "Iteration 1265, Loss: 0.05568739026784897\n",
      "Iteration 1266, Loss: 0.05575931444764137\n",
      "Iteration 1267, Loss: 0.05584784597158432\n",
      "Iteration 1268, Loss: 0.05583222955465317\n",
      "Iteration 1269, Loss: 0.05572434514760971\n",
      "Iteration 1270, Loss: 0.05571846291422844\n",
      "Iteration 1271, Loss: 0.055779457092285156\n",
      "Iteration 1272, Loss: 0.05566593259572983\n",
      "Iteration 1273, Loss: 0.055771589279174805\n",
      "Iteration 1274, Loss: 0.05585738271474838\n",
      "Iteration 1275, Loss: 0.05584053322672844\n",
      "Iteration 1276, Loss: 0.05573185533285141\n",
      "Iteration 1277, Loss: 0.055708371102809906\n",
      "Iteration 1278, Loss: 0.05576996132731438\n",
      "Iteration 1279, Loss: 0.05565544217824936\n",
      "Iteration 1280, Loss: 0.055779218673706055\n",
      "Iteration 1281, Loss: 0.05586544796824455\n",
      "Iteration 1282, Loss: 0.05584836006164551\n",
      "Iteration 1283, Loss: 0.05573892593383789\n",
      "Iteration 1284, Loss: 0.055699072778224945\n",
      "Iteration 1285, Loss: 0.05576193332672119\n",
      "Iteration 1286, Loss: 0.05564657971262932\n",
      "Iteration 1287, Loss: 0.055788878351449966\n",
      "Iteration 1288, Loss: 0.0558781623840332\n",
      "Iteration 1289, Loss: 0.055863820016384125\n",
      "Iteration 1290, Loss: 0.05575680732727051\n",
      "Iteration 1291, Loss: 0.05567284673452377\n",
      "Iteration 1292, Loss: 0.0557352714240551\n",
      "Iteration 1293, Loss: 0.055626630783081055\n",
      "Iteration 1294, Loss: 0.05578148365020752\n",
      "Iteration 1295, Loss: 0.055848121643066406\n",
      "Iteration 1296, Loss: 0.055809974670410156\n",
      "Iteration 1297, Loss: 0.05568158999085426\n",
      "Iteration 1298, Loss: 0.055799126625061035\n",
      "Iteration 1299, Loss: 0.05587991327047348\n",
      "Iteration 1300, Loss: 0.055779457092285156\n",
      "Iteration 1301, Loss: 0.05568142980337143\n",
      "Iteration 1302, Loss: 0.05576181411743164\n",
      "Iteration 1303, Loss: 0.05573924630880356\n",
      "Iteration 1304, Loss: 0.05562981218099594\n",
      "Iteration 1305, Loss: 0.05583393946290016\n",
      "Iteration 1306, Loss: 0.05588114634156227\n",
      "Iteration 1307, Loss: 0.055751364678144455\n",
      "Iteration 1308, Loss: 0.055721841752529144\n",
      "Iteration 1309, Loss: 0.05581995099782944\n",
      "Iteration 1310, Loss: 0.05581466481089592\n",
      "Iteration 1311, Loss: 0.05571655556559563\n",
      "Iteration 1312, Loss: 0.05571647733449936\n",
      "Iteration 1313, Loss: 0.05576658248901367\n",
      "Iteration 1314, Loss: 0.05564236640930176\n",
      "Iteration 1315, Loss: 0.05579475685954094\n",
      "Iteration 1316, Loss: 0.0558849573135376\n",
      "Iteration 1317, Loss: 0.05587053298950195\n",
      "Iteration 1318, Loss: 0.05576268956065178\n",
      "Iteration 1319, Loss: 0.05566763877868652\n",
      "Iteration 1320, Loss: 0.055737774819135666\n",
      "Iteration 1321, Loss: 0.055642127990722656\n",
      "Iteration 1322, Loss: 0.0557686910033226\n",
      "Iteration 1323, Loss: 0.05583532899618149\n",
      "Iteration 1324, Loss: 0.05579984188079834\n",
      "Iteration 1325, Loss: 0.05567359924316406\n",
      "Iteration 1326, Loss: 0.05580814927816391\n",
      "Iteration 1327, Loss: 0.05588821694254875\n",
      "Iteration 1328, Loss: 0.05578601360321045\n",
      "Iteration 1329, Loss: 0.055677931755781174\n",
      "Iteration 1330, Loss: 0.055759988725185394\n",
      "Iteration 1331, Loss: 0.05573928356170654\n",
      "Iteration 1332, Loss: 0.055628061294555664\n",
      "Iteration 1333, Loss: 0.05584907904267311\n",
      "Iteration 1334, Loss: 0.055909037590026855\n",
      "Iteration 1335, Loss: 0.05578915402293205\n",
      "Iteration 1336, Loss: 0.05568758770823479\n",
      "Iteration 1337, Loss: 0.05577997490763664\n",
      "Iteration 1338, Loss: 0.055769722908735275\n",
      "Iteration 1339, Loss: 0.05566760152578354\n",
      "Iteration 1340, Loss: 0.05578748509287834\n",
      "Iteration 1341, Loss: 0.055841367691755295\n",
      "Iteration 1342, Loss: 0.05571695417165756\n",
      "Iteration 1343, Loss: 0.055743180215358734\n",
      "Iteration 1344, Loss: 0.05583787336945534\n",
      "Iteration 1345, Loss: 0.055829405784606934\n",
      "Iteration 1346, Loss: 0.05572783946990967\n",
      "Iteration 1347, Loss: 0.05570467561483383\n",
      "Iteration 1348, Loss: 0.05575808137655258\n",
      "Iteration 1349, Loss: 0.05563501641154289\n",
      "Iteration 1350, Loss: 0.05580008029937744\n",
      "Iteration 1351, Loss: 0.055891357362270355\n",
      "Iteration 1352, Loss: 0.0558784045279026\n",
      "Iteration 1353, Loss: 0.05577230453491211\n",
      "Iteration 1354, Loss: 0.05565210431814194\n",
      "Iteration 1355, Loss: 0.05572501942515373\n",
      "Iteration 1356, Loss: 0.05563656613230705\n",
      "Iteration 1357, Loss: 0.05575696751475334\n",
      "Iteration 1358, Loss: 0.055804770439863205\n",
      "Iteration 1359, Loss: 0.05574750900268555\n",
      "Iteration 1360, Loss: 0.0556366853415966\n",
      "Iteration 1361, Loss: 0.05572625249624252\n",
      "Iteration 1362, Loss: 0.05566728115081787\n",
      "Iteration 1363, Loss: 0.0557299479842186\n",
      "Iteration 1364, Loss: 0.055777668952941895\n",
      "Iteration 1365, Loss: 0.05572470277547836\n",
      "Iteration 1366, Loss: 0.05565158650279045\n",
      "Iteration 1367, Loss: 0.0556563138961792\n",
      "Iteration 1368, Loss: 0.05570145696401596\n",
      "Iteration 1369, Loss: 0.055717550218105316\n",
      "Iteration 1370, Loss: 0.055639028549194336\n",
      "Iteration 1371, Loss: 0.055793046951293945\n",
      "Iteration 1372, Loss: 0.05581740662455559\n",
      "Iteration 1373, Loss: 0.055667124688625336\n",
      "Iteration 1374, Loss: 0.05579690262675285\n",
      "Iteration 1375, Loss: 0.05590725317597389\n",
      "Iteration 1376, Loss: 0.05591190233826637\n",
      "Iteration 1377, Loss: 0.05582161992788315\n",
      "Iteration 1378, Loss: 0.055647533386945724\n",
      "Iteration 1379, Loss: 0.05590132996439934\n",
      "Iteration 1380, Loss: 0.05603313818573952\n",
      "Iteration 1381, Loss: 0.05597742646932602\n",
      "Iteration 1382, Loss: 0.05575442686676979\n",
      "Iteration 1383, Loss: 0.05578049272298813\n",
      "Iteration 1384, Loss: 0.05593474954366684\n",
      "Iteration 1385, Loss: 0.05597889795899391\n",
      "Iteration 1386, Loss: 0.0559237003326416\n",
      "Iteration 1387, Loss: 0.05577981472015381\n",
      "Iteration 1388, Loss: 0.055685680359601974\n",
      "Iteration 1389, Loss: 0.05578545853495598\n",
      "Iteration 1390, Loss: 0.055703043937683105\n",
      "Iteration 1391, Loss: 0.055725496262311935\n",
      "Iteration 1392, Loss: 0.055795036256313324\n",
      "Iteration 1393, Loss: 0.05576388165354729\n",
      "Iteration 1394, Loss: 0.05564264580607414\n",
      "Iteration 1395, Loss: 0.055844664573669434\n",
      "Iteration 1396, Loss: 0.055919647216796875\n",
      "Iteration 1397, Loss: 0.055813949555158615\n",
      "Iteration 1398, Loss: 0.055659692734479904\n",
      "Iteration 1399, Loss: 0.05574465170502663\n",
      "Iteration 1400, Loss: 0.055728595703840256\n",
      "Iteration 1401, Loss: 0.05562090873718262\n",
      "Iteration 1402, Loss: 0.055852893739938736\n",
      "Iteration 1403, Loss: 0.05591209977865219\n",
      "Iteration 1404, Loss: 0.05579773709177971\n",
      "Iteration 1405, Loss: 0.05567765235900879\n",
      "Iteration 1406, Loss: 0.055769048631191254\n",
      "Iteration 1407, Loss: 0.05575935170054436\n",
      "Iteration 1408, Loss: 0.05565289780497551\n",
      "Iteration 1409, Loss: 0.055809974670410156\n",
      "Iteration 1410, Loss: 0.05587184801697731\n",
      "Iteration 1411, Loss: 0.055765509605407715\n",
      "Iteration 1412, Loss: 0.05569406598806381\n",
      "Iteration 1413, Loss: 0.0557783842086792\n",
      "Iteration 1414, Loss: 0.055757761001586914\n",
      "Iteration 1415, Loss: 0.055637478828430176\n",
      "Iteration 1416, Loss: 0.05584283918142319\n",
      "Iteration 1417, Loss: 0.055919013917446136\n",
      "Iteration 1418, Loss: 0.05582805722951889\n",
      "Iteration 1419, Loss: 0.05564983934164047\n",
      "Iteration 1420, Loss: 0.055776119232177734\n",
      "Iteration 1421, Loss: 0.05580969899892807\n",
      "Iteration 1422, Loss: 0.05572287365794182\n",
      "Iteration 1423, Loss: 0.05570220947265625\n",
      "Iteration 1424, Loss: 0.05575462430715561\n",
      "Iteration 1425, Loss: 0.05566096678376198\n",
      "Iteration 1426, Loss: 0.055767856538295746\n",
      "Iteration 1427, Loss: 0.05582885071635246\n",
      "Iteration 1428, Loss: 0.05576348677277565\n",
      "Iteration 1429, Loss: 0.05565599724650383\n",
      "Iteration 1430, Loss: 0.05575072765350342\n",
      "Iteration 1431, Loss: 0.05572223663330078\n",
      "Iteration 1432, Loss: 0.055675629526376724\n",
      "Iteration 1433, Loss: 0.055706024169921875\n",
      "Iteration 1434, Loss: 0.05565377324819565\n",
      "Iteration 1435, Loss: 0.0557326078414917\n",
      "Iteration 1436, Loss: 0.05571715161204338\n",
      "Iteration 1437, Loss: 0.055671535432338715\n",
      "Iteration 1438, Loss: 0.05570065975189209\n",
      "Iteration 1439, Loss: 0.05564061924815178\n",
      "Iteration 1440, Loss: 0.05576511472463608\n",
      "Iteration 1441, Loss: 0.055763524025678635\n",
      "Iteration 1442, Loss: 0.0556306466460228\n",
      "Iteration 1443, Loss: 0.05567852780222893\n",
      "Iteration 1444, Loss: 0.05563024803996086\n",
      "Iteration 1445, Loss: 0.05577055737376213\n",
      "Iteration 1446, Loss: 0.05577655881643295\n",
      "Iteration 1447, Loss: 0.05564519017934799\n",
      "Iteration 1448, Loss: 0.05578581616282463\n",
      "Iteration 1449, Loss: 0.05584585666656494\n",
      "Iteration 1450, Loss: 0.05577588081359863\n",
      "Iteration 1451, Loss: 0.055645864456892014\n",
      "Iteration 1452, Loss: 0.05575891584157944\n",
      "Iteration 1453, Loss: 0.05573539063334465\n",
      "Iteration 1454, Loss: 0.055660367012023926\n",
      "Iteration 1455, Loss: 0.05568897724151611\n",
      "Iteration 1456, Loss: 0.05563652515411377\n",
      "Iteration 1457, Loss: 0.05573546886444092\n",
      "Iteration 1458, Loss: 0.055699944496154785\n",
      "Iteration 1459, Loss: 0.055696647614240646\n",
      "Iteration 1460, Loss: 0.0557379350066185\n",
      "Iteration 1461, Loss: 0.055682819336652756\n",
      "Iteration 1462, Loss: 0.0557096004486084\n",
      "Iteration 1463, Loss: 0.05571246147155762\n",
      "Iteration 1464, Loss: 0.05566239356994629\n",
      "Iteration 1465, Loss: 0.055682700127363205\n",
      "Iteration 1466, Loss: 0.05562258139252663\n",
      "Iteration 1467, Loss: 0.05571548268198967\n",
      "Iteration 1468, Loss: 0.05569140240550041\n",
      "Iteration 1469, Loss: 0.05567741394042969\n",
      "Iteration 1470, Loss: 0.05568484589457512\n",
      "Iteration 1471, Loss: 0.05566553398966789\n",
      "Iteration 1472, Loss: 0.05565337464213371\n",
      "Iteration 1473, Loss: 0.05569537729024887\n",
      "Iteration 1474, Loss: 0.05567225068807602\n",
      "Iteration 1475, Loss: 0.055702924728393555\n",
      "Iteration 1476, Loss: 0.0557154044508934\n",
      "Iteration 1477, Loss: 0.055633507668972015\n",
      "Iteration 1478, Loss: 0.055738210678100586\n",
      "Iteration 1479, Loss: 0.05570157617330551\n",
      "Iteration 1480, Loss: 0.05569637194275856\n",
      "Iteration 1481, Loss: 0.05573638528585434\n",
      "Iteration 1482, Loss: 0.05567654222249985\n",
      "Iteration 1483, Loss: 0.055722713470458984\n",
      "Iteration 1484, Loss: 0.05573105812072754\n",
      "Iteration 1485, Loss: 0.055643998086452484\n",
      "Iteration 1486, Loss: 0.0556587390601635\n",
      "Iteration 1487, Loss: 0.0556565523147583\n",
      "Iteration 1488, Loss: 0.05563453957438469\n",
      "Iteration 1489, Loss: 0.05565420910716057\n",
      "Iteration 1490, Loss: 0.05564431473612785\n",
      "Iteration 1491, Loss: 0.05563632771372795\n",
      "Iteration 1492, Loss: 0.05566263198852539\n",
      "Iteration 1493, Loss: 0.0556308850646019\n",
      "Iteration 1494, Loss: 0.055667757987976074\n",
      "Iteration 1495, Loss: 0.055646657943725586\n",
      "Iteration 1496, Loss: 0.05562901496887207\n",
      "Iteration 1497, Loss: 0.05572303384542465\n",
      "Iteration 1498, Loss: 0.05567920580506325\n",
      "Iteration 1499, Loss: 0.05571397393941879\n",
      "Iteration 1500, Loss: 0.05575498193502426\n",
      "Iteration 1501, Loss: 0.055692434310913086\n",
      "Iteration 1502, Loss: 0.05570554733276367\n",
      "Iteration 1503, Loss: 0.055721960961818695\n",
      "Iteration 1504, Loss: 0.05564093589782715\n",
      "Iteration 1505, Loss: 0.05564785376191139\n",
      "Iteration 1506, Loss: 0.05568532273173332\n",
      "Iteration 1507, Loss: 0.055635493248701096\n",
      "Iteration 1508, Loss: 0.05575045198202133\n",
      "Iteration 1509, Loss: 0.05578339472413063\n",
      "Iteration 1510, Loss: 0.05570673942565918\n",
      "Iteration 1511, Loss: 0.05570662394165993\n",
      "Iteration 1512, Loss: 0.05574524402618408\n",
      "Iteration 1513, Loss: 0.055631283670663834\n",
      "Iteration 1514, Loss: 0.05580342188477516\n",
      "Iteration 1515, Loss: 0.05588690564036369\n",
      "Iteration 1516, Loss: 0.05586099624633789\n",
      "Iteration 1517, Loss: 0.05574258416891098\n",
      "Iteration 1518, Loss: 0.05571397393941879\n",
      "Iteration 1519, Loss: 0.05579551309347153\n",
      "Iteration 1520, Loss: 0.05571739003062248\n",
      "Iteration 1521, Loss: 0.055716078728437424\n",
      "Iteration 1522, Loss: 0.05578005686402321\n",
      "Iteration 1523, Loss: 0.05574576184153557\n",
      "Iteration 1524, Loss: 0.05564848706126213\n",
      "Iteration 1525, Loss: 0.055783867835998535\n",
      "Iteration 1526, Loss: 0.05579551309347153\n",
      "Iteration 1527, Loss: 0.05564836785197258\n",
      "Iteration 1528, Loss: 0.05579173564910889\n",
      "Iteration 1529, Loss: 0.05587856099009514\n",
      "Iteration 1530, Loss: 0.05585559457540512\n",
      "Iteration 1531, Loss: 0.055735670030117035\n",
      "Iteration 1532, Loss: 0.05571766942739487\n",
      "Iteration 1533, Loss: 0.055793169885873795\n",
      "Iteration 1534, Loss: 0.05569382756948471\n",
      "Iteration 1535, Loss: 0.05574536323547363\n",
      "Iteration 1536, Loss: 0.0558241605758667\n",
      "Iteration 1537, Loss: 0.05579710006713867\n",
      "Iteration 1538, Loss: 0.05568007752299309\n",
      "Iteration 1539, Loss: 0.055786971002817154\n",
      "Iteration 1540, Loss: 0.055853962898254395\n",
      "Iteration 1541, Loss: 0.05574214458465576\n",
      "Iteration 1542, Loss: 0.055717192590236664\n",
      "Iteration 1543, Loss: 0.05580425262451172\n",
      "Iteration 1544, Loss: 0.05578688904643059\n",
      "Iteration 1545, Loss: 0.055679164826869965\n",
      "Iteration 1546, Loss: 0.05577806755900383\n",
      "Iteration 1547, Loss: 0.05583620443940163\n",
      "Iteration 1548, Loss: 0.055716000497341156\n",
      "Iteration 1549, Loss: 0.055741749703884125\n",
      "Iteration 1550, Loss: 0.055833857506513596\n",
      "Iteration 1551, Loss: 0.05582249164581299\n",
      "Iteration 1552, Loss: 0.05571834370493889\n",
      "Iteration 1553, Loss: 0.05572132393717766\n",
      "Iteration 1554, Loss: 0.05577751249074936\n",
      "Iteration 1555, Loss: 0.05565842241048813\n",
      "Iteration 1556, Loss: 0.0557808093726635\n",
      "Iteration 1557, Loss: 0.055870335549116135\n",
      "Iteration 1558, Loss: 0.05585658550262451\n",
      "Iteration 1559, Loss: 0.05575033277273178\n",
      "Iteration 1560, Loss: 0.055679839104413986\n",
      "Iteration 1561, Loss: 0.055738452821969986\n",
      "Iteration 1562, Loss: 0.05562591552734375\n",
      "Iteration 1563, Loss: 0.05578196048736572\n",
      "Iteration 1564, Loss: 0.055845700204372406\n",
      "Iteration 1565, Loss: 0.05580437555909157\n",
      "Iteration 1566, Loss: 0.05567391961812973\n",
      "Iteration 1567, Loss: 0.05580794811248779\n",
      "Iteration 1568, Loss: 0.05588976666331291\n",
      "Iteration 1569, Loss: 0.0557963065803051\n",
      "Iteration 1570, Loss: 0.05566203594207764\n",
      "Iteration 1571, Loss: 0.05573849007487297\n",
      "Iteration 1572, Loss: 0.05571059510111809\n",
      "Iteration 1573, Loss: 0.05564204975962639\n",
      "Iteration 1574, Loss: 0.055642805993556976\n",
      "Iteration 1575, Loss: 0.05570065975189209\n",
      "Iteration 1576, Loss: 0.05570697784423828\n",
      "Iteration 1577, Loss: 0.055618565529584885\n",
      "Iteration 1578, Loss: 0.05583481118083\n",
      "Iteration 1579, Loss: 0.05587077513337135\n",
      "Iteration 1580, Loss: 0.05572938919067383\n",
      "Iteration 1581, Loss: 0.05574552342295647\n",
      "Iteration 1582, Loss: 0.05585082620382309\n",
      "Iteration 1583, Loss: 0.05585074424743652\n",
      "Iteration 1584, Loss: 0.05575593560934067\n",
      "Iteration 1585, Loss: 0.055658578872680664\n",
      "Iteration 1586, Loss: 0.05570865049958229\n",
      "Iteration 1587, Loss: 0.05563676357269287\n",
      "Iteration 1588, Loss: 0.055661361664533615\n",
      "Iteration 1589, Loss: 0.05563346669077873\n",
      "Iteration 1590, Loss: 0.05567566677927971\n",
      "Iteration 1591, Loss: 0.05565397068858147\n",
      "Iteration 1592, Loss: 0.05570709705352783\n",
      "Iteration 1593, Loss: 0.055672209709882736\n",
      "Iteration 1594, Loss: 0.055716197937726974\n",
      "Iteration 1595, Loss: 0.05575760453939438\n",
      "Iteration 1596, Loss: 0.05570098012685776\n",
      "Iteration 1597, Loss: 0.055685680359601974\n",
      "Iteration 1598, Loss: 0.05568961426615715\n",
      "Iteration 1599, Loss: 0.05567812919616699\n",
      "Iteration 1600, Loss: 0.05569649115204811\n",
      "Iteration 1601, Loss: 0.055619917809963226\n",
      "Iteration 1602, Loss: 0.05582078546285629\n",
      "Iteration 1603, Loss: 0.055846136063337326\n",
      "Iteration 1604, Loss: 0.055696289986371994\n",
      "Iteration 1605, Loss: 0.05577496811747551\n",
      "Iteration 1606, Loss: 0.05588523671030998\n",
      "Iteration 1607, Loss: 0.0558900460600853\n",
      "Iteration 1608, Loss: 0.05579984188079834\n",
      "Iteration 1609, Loss: 0.05562571808695793\n",
      "Iteration 1610, Loss: 0.055932365357875824\n",
      "Iteration 1611, Loss: 0.056065283715724945\n",
      "Iteration 1612, Loss: 0.056010764092206955\n",
      "Iteration 1613, Loss: 0.05578828230500221\n",
      "Iteration 1614, Loss: 0.05575565621256828\n",
      "Iteration 1615, Loss: 0.05590919777750969\n",
      "Iteration 1616, Loss: 0.055953383445739746\n",
      "Iteration 1617, Loss: 0.05589815229177475\n",
      "Iteration 1618, Loss: 0.055754464119672775\n",
      "Iteration 1619, Loss: 0.05571949481964111\n",
      "Iteration 1620, Loss: 0.055819157510995865\n",
      "Iteration 1621, Loss: 0.05573658272624016\n",
      "Iteration 1622, Loss: 0.055700939148664474\n",
      "Iteration 1623, Loss: 0.055770717561244965\n",
      "Iteration 1624, Loss: 0.055740199983119965\n",
      "Iteration 1625, Loss: 0.055619679391384125\n",
      "Iteration 1626, Loss: 0.05587097257375717\n",
      "Iteration 1627, Loss: 0.0559438094496727\n",
      "Iteration 1628, Loss: 0.0558396615087986\n",
      "Iteration 1629, Loss: 0.055644553154706955\n",
      "Iteration 1630, Loss: 0.05575207993388176\n",
      "Iteration 1631, Loss: 0.0557633675634861\n",
      "Iteration 1632, Loss: 0.05567129701375961\n",
      "Iteration 1633, Loss: 0.055767737329006195\n",
      "Iteration 1634, Loss: 0.05581629276275635\n",
      "Iteration 1635, Loss: 0.055703919380903244\n",
      "Iteration 1636, Loss: 0.0557430200278759\n",
      "Iteration 1637, Loss: 0.05582583323121071\n",
      "Iteration 1638, Loss: 0.055796146392822266\n",
      "Iteration 1639, Loss: 0.055668316781520844\n",
      "Iteration 1640, Loss: 0.05580723658204079\n",
      "Iteration 1641, Loss: 0.055886946618556976\n",
      "Iteration 1642, Loss: 0.05579817295074463\n",
      "Iteration 1643, Loss: 0.05565556138753891\n",
      "Iteration 1644, Loss: 0.055736422538757324\n",
      "Iteration 1645, Loss: 0.05570944398641586\n",
      "Iteration 1646, Loss: 0.05565277859568596\n",
      "Iteration 1647, Loss: 0.05564364045858383\n",
      "Iteration 1648, Loss: 0.05571802705526352\n",
      "Iteration 1649, Loss: 0.05572875589132309\n",
      "Iteration 1650, Loss: 0.05564185231924057\n",
      "Iteration 1651, Loss: 0.055778466165065765\n",
      "Iteration 1652, Loss: 0.05579710006713867\n",
      "Iteration 1653, Loss: 0.05565047636628151\n",
      "Iteration 1654, Loss: 0.055809617042541504\n",
      "Iteration 1655, Loss: 0.05591496080160141\n",
      "Iteration 1656, Loss: 0.0559086799621582\n",
      "Iteration 1657, Loss: 0.055804770439863205\n",
      "Iteration 1658, Loss: 0.055652499198913574\n",
      "Iteration 1659, Loss: 0.05583532899618149\n",
      "Iteration 1660, Loss: 0.055886946618556976\n",
      "Iteration 1661, Loss: 0.055763524025678635\n",
      "Iteration 1662, Loss: 0.05571063607931137\n",
      "Iteration 1663, Loss: 0.055804770439863205\n",
      "Iteration 1664, Loss: 0.05579781532287598\n",
      "Iteration 1665, Loss: 0.05569998547434807\n",
      "Iteration 1666, Loss: 0.05573980137705803\n",
      "Iteration 1667, Loss: 0.0557892732322216\n",
      "Iteration 1668, Loss: 0.055663466453552246\n",
      "Iteration 1669, Loss: 0.055782001465559006\n",
      "Iteration 1670, Loss: 0.055876296013593674\n",
      "Iteration 1671, Loss: 0.05586600303649902\n",
      "Iteration 1672, Loss: 0.05576189607381821\n",
      "Iteration 1673, Loss: 0.0556614026427269\n",
      "Iteration 1674, Loss: 0.05571790784597397\n",
      "Iteration 1675, Loss: 0.055624090135097504\n",
      "Iteration 1676, Loss: 0.055654410272836685\n",
      "Iteration 1677, Loss: 0.05565047264099121\n",
      "Iteration 1678, Loss: 0.05563179776072502\n",
      "Iteration 1679, Loss: 0.055663030594587326\n",
      "Iteration 1680, Loss: 0.055643677711486816\n",
      "Iteration 1681, Loss: 0.05562325567007065\n",
      "Iteration 1682, Loss: 0.055719535797834396\n",
      "Iteration 1683, Loss: 0.055710554122924805\n",
      "Iteration 1684, Loss: 0.05564618110656738\n",
      "Iteration 1685, Loss: 0.05569469928741455\n",
      "Iteration 1686, Loss: 0.055639151483774185\n",
      "Iteration 1687, Loss: 0.05570483207702637\n",
      "Iteration 1688, Loss: 0.05570602789521217\n",
      "Iteration 1689, Loss: 0.05561677739024162\n",
      "Iteration 1690, Loss: 0.05567920580506325\n",
      "Iteration 1691, Loss: 0.05563267320394516\n",
      "Iteration 1692, Loss: 0.05567558854818344\n",
      "Iteration 1693, Loss: 0.055631041526794434\n",
      "Iteration 1694, Loss: 0.05564801022410393\n",
      "Iteration 1695, Loss: 0.055654287338256836\n",
      "Iteration 1696, Loss: 0.055631838738918304\n",
      "Iteration 1697, Loss: 0.055664580315351486\n",
      "Iteration 1698, Loss: 0.05564693734049797\n",
      "Iteration 1699, Loss: 0.05562516301870346\n",
      "Iteration 1700, Loss: 0.0557379350066185\n",
      "Iteration 1701, Loss: 0.05571059510111809\n",
      "Iteration 1702, Loss: 0.055676620453596115\n",
      "Iteration 1703, Loss: 0.055705826729536057\n",
      "Iteration 1704, Loss: 0.05562547966837883\n",
      "Iteration 1705, Loss: 0.0558069571852684\n",
      "Iteration 1706, Loss: 0.05584836006164551\n",
      "Iteration 1707, Loss: 0.05574357509613037\n",
      "Iteration 1708, Loss: 0.05570698156952858\n",
      "Iteration 1709, Loss: 0.05578581616282463\n",
      "Iteration 1710, Loss: 0.05574393272399902\n",
      "Iteration 1711, Loss: 0.05564181134104729\n",
      "Iteration 1712, Loss: 0.05565901845693588\n",
      "Iteration 1713, Loss: 0.05567272752523422\n",
      "Iteration 1714, Loss: 0.05564669892191887\n",
      "Iteration 1715, Loss: 0.055728595703840256\n",
      "Iteration 1716, Loss: 0.05573038384318352\n",
      "Iteration 1717, Loss: 0.05564141273498535\n",
      "Iteration 1718, Loss: 0.05572303384542465\n",
      "Iteration 1719, Loss: 0.055686913430690765\n",
      "Iteration 1720, Loss: 0.05570530891418457\n",
      "Iteration 1721, Loss: 0.05573726072907448\n",
      "Iteration 1722, Loss: 0.05566684529185295\n",
      "Iteration 1723, Loss: 0.05574246495962143\n",
      "Iteration 1724, Loss: 0.05576257035136223\n",
      "Iteration 1725, Loss: 0.055622100830078125\n",
      "Iteration 1726, Loss: 0.05583079904317856\n",
      "Iteration 1727, Loss: 0.05592775344848633\n",
      "Iteration 1728, Loss: 0.055907368659973145\n",
      "Iteration 1729, Loss: 0.0557883195579052\n",
      "Iteration 1730, Loss: 0.05568063631653786\n",
      "Iteration 1731, Loss: 0.05580019950866699\n",
      "Iteration 1732, Loss: 0.05579209700226784\n",
      "Iteration 1733, Loss: 0.05565277859568596\n",
      "Iteration 1734, Loss: 0.05576757714152336\n",
      "Iteration 1735, Loss: 0.055832069367170334\n",
      "Iteration 1736, Loss: 0.05579706281423569\n",
      "Iteration 1737, Loss: 0.05567328259348869\n",
      "Iteration 1738, Loss: 0.055807631462812424\n",
      "Iteration 1739, Loss: 0.05588483810424805\n",
      "Iteration 1740, Loss: 0.0557810477912426\n",
      "Iteration 1741, Loss: 0.055683258920907974\n",
      "Iteration 1742, Loss: 0.05576694384217262\n",
      "Iteration 1743, Loss: 0.05574890226125717\n",
      "Iteration 1744, Loss: 0.05563763901591301\n",
      "Iteration 1745, Loss: 0.05583854764699936\n",
      "Iteration 1746, Loss: 0.055902086198329926\n",
      "Iteration 1747, Loss: 0.05578633397817612\n",
      "Iteration 1748, Loss: 0.05568663403391838\n",
      "Iteration 1749, Loss: 0.05577739328145981\n",
      "Iteration 1750, Loss: 0.05576348304748535\n",
      "Iteration 1751, Loss: 0.055654168128967285\n",
      "Iteration 1752, Loss: 0.05581267923116684\n",
      "Iteration 1753, Loss: 0.05587395280599594\n",
      "Iteration 1754, Loss: 0.05575728788971901\n",
      "Iteration 1755, Loss: 0.05570821091532707\n",
      "Iteration 1756, Loss: 0.055798571556806564\n",
      "Iteration 1757, Loss: 0.055783551186323166\n",
      "Iteration 1758, Loss: 0.055672645568847656\n",
      "Iteration 1759, Loss: 0.05579066276550293\n",
      "Iteration 1760, Loss: 0.05585336685180664\n",
      "Iteration 1761, Loss: 0.05573789402842522\n",
      "Iteration 1762, Loss: 0.055722951889038086\n",
      "Iteration 1763, Loss: 0.055812399834394455\n",
      "Iteration 1764, Loss: 0.05579567328095436\n",
      "Iteration 1765, Loss: 0.0556846484541893\n",
      "Iteration 1766, Loss: 0.05577453225851059\n",
      "Iteration 1767, Loss: 0.05583711713552475\n",
      "Iteration 1768, Loss: 0.05572112649679184\n",
      "Iteration 1769, Loss: 0.055735986679792404\n",
      "Iteration 1770, Loss: 0.055825792253017426\n",
      "Iteration 1771, Loss: 0.05580981820821762\n",
      "Iteration 1772, Loss: 0.055700939148664474\n",
      "Iteration 1773, Loss: 0.055751048028469086\n",
      "Iteration 1774, Loss: 0.055811844766139984\n",
      "Iteration 1775, Loss: 0.05569501966238022\n",
      "Iteration 1776, Loss: 0.05575625225901604\n",
      "Iteration 1777, Loss: 0.055846452713012695\n",
      "Iteration 1778, Loss: 0.05583202838897705\n",
      "Iteration 1779, Loss: 0.05572497844696045\n",
      "Iteration 1780, Loss: 0.05571766942739487\n",
      "Iteration 1781, Loss: 0.055777911096811295\n",
      "Iteration 1782, Loss: 0.05566323176026344\n",
      "Iteration 1783, Loss: 0.05577540397644043\n",
      "Iteration 1784, Loss: 0.05586234852671623\n",
      "Iteration 1785, Loss: 0.05584593862295151\n",
      "Iteration 1786, Loss: 0.05573773384094238\n",
      "Iteration 1787, Loss: 0.05570121854543686\n",
      "Iteration 1788, Loss: 0.05576257035136223\n",
      "Iteration 1789, Loss: 0.05564912408590317\n",
      "Iteration 1790, Loss: 0.05578223988413811\n",
      "Iteration 1791, Loss: 0.05586572736501694\n",
      "Iteration 1792, Loss: 0.05584597960114479\n",
      "Iteration 1793, Loss: 0.05573419854044914\n",
      "Iteration 1794, Loss: 0.05570987984538078\n",
      "Iteration 1795, Loss: 0.05577544495463371\n",
      "Iteration 1796, Loss: 0.055663228034973145\n",
      "Iteration 1797, Loss: 0.05577560514211655\n",
      "Iteration 1798, Loss: 0.05586322396993637\n",
      "Iteration 1799, Loss: 0.055847447365522385\n",
      "Iteration 1800, Loss: 0.055739086121320724\n",
      "Iteration 1801, Loss: 0.05569899082183838\n",
      "Iteration 1802, Loss: 0.05576125904917717\n",
      "Iteration 1803, Loss: 0.055646978318691254\n",
      "Iteration 1804, Loss: 0.05578824132680893\n",
      "Iteration 1805, Loss: 0.05587633699178696\n",
      "Iteration 1806, Loss: 0.05586151406168938\n",
      "Iteration 1807, Loss: 0.05575398728251457\n",
      "Iteration 1808, Loss: 0.05567709729075432\n",
      "Iteration 1809, Loss: 0.0557374581694603\n",
      "Iteration 1810, Loss: 0.055621188133955\n",
      "Iteration 1811, Loss: 0.055801115930080414\n",
      "Iteration 1812, Loss: 0.05588177964091301\n",
      "Iteration 1813, Loss: 0.05585746094584465\n",
      "Iteration 1814, Loss: 0.05574027821421623\n",
      "Iteration 1815, Loss: 0.05570964142680168\n",
      "Iteration 1816, Loss: 0.05578204244375229\n",
      "Iteration 1817, Loss: 0.05567892640829086\n",
      "Iteration 1818, Loss: 0.055757880210876465\n",
      "Iteration 1819, Loss: 0.05583842843770981\n",
      "Iteration 1820, Loss: 0.05581657215952873\n",
      "Iteration 1821, Loss: 0.05570435896515846\n",
      "Iteration 1822, Loss: 0.05575196072459221\n",
      "Iteration 1823, Loss: 0.05581653490662575\n",
      "Iteration 1824, Loss: 0.05570288747549057\n",
      "Iteration 1825, Loss: 0.055747389793395996\n",
      "Iteration 1826, Loss: 0.05583524703979492\n",
      "Iteration 1827, Loss: 0.05582066625356674\n",
      "Iteration 1828, Loss: 0.05571397393941879\n",
      "Iteration 1829, Loss: 0.055731020867824554\n",
      "Iteration 1830, Loss: 0.05578979104757309\n",
      "Iteration 1831, Loss: 0.055671218782663345\n",
      "Iteration 1832, Loss: 0.05577266216278076\n",
      "Iteration 1833, Loss: 0.055862944573163986\n",
      "Iteration 1834, Loss: 0.05584995076060295\n",
      "Iteration 1835, Loss: 0.05574409291148186\n",
      "Iteration 1836, Loss: 0.05568850040435791\n",
      "Iteration 1837, Loss: 0.05574647709727287\n",
      "Iteration 1838, Loss: 0.055627744644880295\n",
      "Iteration 1839, Loss: 0.05580286309123039\n",
      "Iteration 1840, Loss: 0.055890005081892014\n",
      "Iteration 1841, Loss: 0.055873434990644455\n",
      "Iteration 1842, Loss: 0.05576388165354729\n",
      "Iteration 1843, Loss: 0.05566982552409172\n",
      "Iteration 1844, Loss: 0.05574238300323486\n",
      "Iteration 1845, Loss: 0.055648766458034515\n",
      "Iteration 1846, Loss: 0.055765632539987564\n",
      "Iteration 1847, Loss: 0.055833183228969574\n",
      "Iteration 1848, Loss: 0.05579876899719238\n",
      "Iteration 1849, Loss: 0.055673521012067795\n",
      "Iteration 1850, Loss: 0.05580858513712883\n",
      "Iteration 1851, Loss: 0.05588750168681145\n",
      "Iteration 1852, Loss: 0.05578450486063957\n",
      "Iteration 1853, Loss: 0.05568043515086174\n",
      "Iteration 1854, Loss: 0.055762968957424164\n",
      "Iteration 1855, Loss: 0.055742740631103516\n",
      "Iteration 1856, Loss: 0.055631399154663086\n",
      "Iteration 1857, Loss: 0.05584708973765373\n",
      "Iteration 1858, Loss: 0.05590876191854477\n",
      "Iteration 1859, Loss: 0.05579002946615219\n",
      "Iteration 1860, Loss: 0.055686913430690765\n",
      "Iteration 1861, Loss: 0.055778663605451584\n",
      "Iteration 1862, Loss: 0.05576781556010246\n",
      "Iteration 1863, Loss: 0.055664580315351486\n",
      "Iteration 1864, Loss: 0.05579396337270737\n",
      "Iteration 1865, Loss: 0.055848799645900726\n",
      "Iteration 1866, Loss: 0.055724382400512695\n",
      "Iteration 1867, Loss: 0.055738452821969986\n",
      "Iteration 1868, Loss: 0.05583333969116211\n",
      "Iteration 1869, Loss: 0.055824559181928635\n",
      "Iteration 1870, Loss: 0.05572303384542465\n",
      "Iteration 1871, Loss: 0.05571206659078598\n",
      "Iteration 1872, Loss: 0.055765390396118164\n",
      "Iteration 1873, Loss: 0.055641770362854004\n",
      "Iteration 1874, Loss: 0.05579690262675285\n",
      "Iteration 1875, Loss: 0.05588928982615471\n",
      "Iteration 1876, Loss: 0.05587800592184067\n",
      "Iteration 1877, Loss: 0.055773500353097916\n",
      "Iteration 1878, Loss: 0.05564817041158676\n",
      "Iteration 1879, Loss: 0.0557096004486084\n",
      "Iteration 1880, Loss: 0.055628418922424316\n",
      "Iteration 1881, Loss: 0.055666208267211914\n",
      "Iteration 1882, Loss: 0.05562404915690422\n",
      "Iteration 1883, Loss: 0.05567014589905739\n",
      "Iteration 1884, Loss: 0.05562702938914299\n",
      "Iteration 1885, Loss: 0.055716317147016525\n",
      "Iteration 1886, Loss: 0.05565448850393295\n",
      "Iteration 1887, Loss: 0.05574917793273926\n",
      "Iteration 1888, Loss: 0.055806081742048264\n",
      "Iteration 1889, Loss: 0.05576213449239731\n",
      "Iteration 1890, Loss: 0.05563334748148918\n",
      "Iteration 1891, Loss: 0.05585523694753647\n",
      "Iteration 1892, Loss: 0.05592449754476547\n",
      "Iteration 1893, Loss: 0.05581343546509743\n",
      "Iteration 1894, Loss: 0.05566581338644028\n",
      "Iteration 1895, Loss: 0.055754661560058594\n",
      "Iteration 1896, Loss: 0.05574405565857887\n",
      "Iteration 1897, Loss: 0.055642370134592056\n",
      "Iteration 1898, Loss: 0.05582086369395256\n",
      "Iteration 1899, Loss: 0.05587407201528549\n",
      "Iteration 1900, Loss: 0.05575462430715561\n",
      "Iteration 1901, Loss: 0.05571293830871582\n",
      "Iteration 1902, Loss: 0.05580441281199455\n",
      "Iteration 1903, Loss: 0.05579241365194321\n",
      "Iteration 1904, Loss: 0.05568544194102287\n",
      "Iteration 1905, Loss: 0.055769048631191254\n",
      "Iteration 1906, Loss: 0.055830083787441254\n",
      "Iteration 1907, Loss: 0.05571834370493889\n",
      "Iteration 1908, Loss: 0.05573336407542229\n",
      "Iteration 1909, Loss: 0.055819276720285416\n",
      "Iteration 1910, Loss: 0.055799923837184906\n",
      "Iteration 1911, Loss: 0.055686235427856445\n",
      "Iteration 1912, Loss: 0.05577516555786133\n",
      "Iteration 1913, Loss: 0.05584283918142319\n",
      "Iteration 1914, Loss: 0.05573638528585434\n",
      "Iteration 1915, Loss: 0.055716633796691895\n",
      "Iteration 1916, Loss: 0.055799804627895355\n",
      "Iteration 1917, Loss: 0.05577826499938965\n",
      "Iteration 1918, Loss: 0.05566338822245598\n",
      "Iteration 1919, Loss: 0.055806081742048264\n",
      "Iteration 1920, Loss: 0.055873673409223557\n",
      "Iteration 1921, Loss: 0.05576547235250473\n",
      "Iteration 1922, Loss: 0.05569617077708244\n",
      "Iteration 1923, Loss: 0.05578085035085678\n",
      "Iteration 1924, Loss: 0.055761776864528656\n",
      "Iteration 1925, Loss: 0.0556490421295166\n",
      "Iteration 1926, Loss: 0.05582360550761223\n",
      "Iteration 1927, Loss: 0.05588905140757561\n",
      "Iteration 1928, Loss: 0.05577906221151352\n",
      "Iteration 1929, Loss: 0.05568802356719971\n",
      "Iteration 1930, Loss: 0.05577465146780014\n",
      "Iteration 1931, Loss: 0.05575820058584213\n",
      "Iteration 1932, Loss: 0.055647216737270355\n",
      "Iteration 1933, Loss: 0.05582432076334953\n",
      "Iteration 1934, Loss: 0.05588897317647934\n",
      "Iteration 1935, Loss: 0.05577997490763664\n",
      "Iteration 1936, Loss: 0.05568671226501465\n",
      "Iteration 1937, Loss: 0.05577317997813225\n",
      "Iteration 1938, Loss: 0.05575696751475334\n",
      "Iteration 1939, Loss: 0.05564431473612785\n",
      "Iteration 1940, Loss: 0.05582881346344948\n",
      "Iteration 1941, Loss: 0.05589580908417702\n",
      "Iteration 1942, Loss: 0.055791616439819336\n",
      "Iteration 1943, Loss: 0.055675189942121506\n",
      "Iteration 1944, Loss: 0.055761538445949554\n",
      "Iteration 1945, Loss: 0.055746160447597504\n",
      "Iteration 1946, Loss: 0.05563080310821533\n",
      "Iteration 1947, Loss: 0.05584605783224106\n",
      "Iteration 1948, Loss: 0.05591829866170883\n",
      "Iteration 1949, Loss: 0.05582690238952637\n",
      "Iteration 1950, Loss: 0.05565381422638893\n",
      "Iteration 1951, Loss: 0.05578136816620827\n",
      "Iteration 1952, Loss: 0.05581895634531975\n",
      "Iteration 1953, Loss: 0.055732570588588715\n",
      "Iteration 1954, Loss: 0.05569363012909889\n",
      "Iteration 1955, Loss: 0.05574754998087883\n",
      "Iteration 1956, Loss: 0.05565834045410156\n",
      "Iteration 1957, Loss: 0.05576801300048828\n",
      "Iteration 1958, Loss: 0.055824004113674164\n",
      "Iteration 1959, Loss: 0.05575084686279297\n",
      "Iteration 1960, Loss: 0.055670540779829025\n",
      "Iteration 1961, Loss: 0.05573773384094238\n",
      "Iteration 1962, Loss: 0.055678848177194595\n",
      "Iteration 1963, Loss: 0.055732253938913345\n",
      "Iteration 1964, Loss: 0.05577659606933594\n",
      "Iteration 1965, Loss: 0.05571552366018295\n",
      "Iteration 1966, Loss: 0.055687032639980316\n",
      "Iteration 1967, Loss: 0.055722951889038086\n",
      "Iteration 1968, Loss: 0.05564717575907707\n",
      "Iteration 1969, Loss: 0.05572033300995827\n",
      "Iteration 1970, Loss: 0.05574047937989235\n",
      "Iteration 1971, Loss: 0.05566851422190666\n",
      "Iteration 1972, Loss: 0.05575164407491684\n",
      "Iteration 1973, Loss: 0.05577151104807854\n",
      "Iteration 1974, Loss: 0.05562862008810043\n",
      "Iteration 1975, Loss: 0.05578700825572014\n",
      "Iteration 1976, Loss: 0.05585114285349846\n",
      "Iteration 1977, Loss: 0.05580361932516098\n",
      "Iteration 1978, Loss: 0.05566605180501938\n",
      "Iteration 1979, Loss: 0.055823568254709244\n",
      "Iteration 1980, Loss: 0.055906932801008224\n",
      "Iteration 1981, Loss: 0.055812083184719086\n",
      "Iteration 1982, Loss: 0.05565246194601059\n",
      "Iteration 1983, Loss: 0.055730585008859634\n",
      "Iteration 1984, Loss: 0.05570133775472641\n",
      "Iteration 1985, Loss: 0.05565889924764633\n",
      "Iteration 1986, Loss: 0.05564657971262932\n",
      "Iteration 1987, Loss: 0.05571902170777321\n",
      "Iteration 1988, Loss: 0.055741868913173676\n",
      "Iteration 1989, Loss: 0.05566839501261711\n",
      "Iteration 1990, Loss: 0.05575041100382805\n",
      "Iteration 1991, Loss: 0.055771153420209885\n",
      "Iteration 1992, Loss: 0.05561916157603264\n",
      "Iteration 1993, Loss: 0.05582773685455322\n",
      "Iteration 1994, Loss: 0.05593319982290268\n",
      "Iteration 1995, Loss: 0.055933039635419846\n",
      "Iteration 1996, Loss: 0.05583791062235832\n",
      "Iteration 1997, Loss: 0.055659931153059006\n",
      "Iteration 1998, Loss: 0.05589175596833229\n",
      "Iteration 1999, Loss: 0.05602840706706047\n",
      "Iteration 2000, Loss: 0.05597678944468498\n",
      "Iteration 2001, Loss: 0.05575708672404289\n",
      "Iteration 2002, Loss: 0.055777549743652344\n",
      "Iteration 2003, Loss: 0.05592930316925049\n",
      "Iteration 2004, Loss: 0.05597178265452385\n",
      "Iteration 2005, Loss: 0.055914998054504395\n",
      "Iteration 2006, Loss: 0.05576980113983154\n",
      "Iteration 2007, Loss: 0.055702053010463715\n",
      "Iteration 2008, Loss: 0.05580341815948486\n",
      "Iteration 2009, Loss: 0.05572335049510002\n",
      "Iteration 2010, Loss: 0.0557098425924778\n",
      "Iteration 2011, Loss: 0.05577806755900383\n",
      "Iteration 2012, Loss: 0.05574623867869377\n",
      "Iteration 2013, Loss: 0.05562512204051018\n",
      "Iteration 2014, Loss: 0.05586457625031471\n",
      "Iteration 2015, Loss: 0.05593705177307129\n",
      "Iteration 2016, Loss: 0.05583298206329346\n",
      "Iteration 2017, Loss: 0.05564944073557854\n",
      "Iteration 2018, Loss: 0.055751245468854904\n",
      "Iteration 2019, Loss: 0.055757444351911545\n",
      "Iteration 2020, Loss: 0.05566132441163063\n",
      "Iteration 2021, Loss: 0.055786531418561935\n",
      "Iteration 2022, Loss: 0.05583922192454338\n",
      "Iteration 2023, Loss: 0.05573034659028053\n",
      "Iteration 2024, Loss: 0.055721085518598557\n",
      "Iteration 2025, Loss: 0.05580341815948486\n",
      "Iteration 2026, Loss: 0.05577465146780014\n",
      "Iteration 2027, Loss: 0.05564602464437485\n",
      "Iteration 2028, Loss: 0.055836837738752365\n",
      "Iteration 2029, Loss: 0.055916666984558105\n",
      "Iteration 2030, Loss: 0.055827658623456955\n",
      "Iteration 2031, Loss: 0.055644433945417404\n",
      "Iteration 2032, Loss: 0.05576825514435768\n",
      "Iteration 2033, Loss: 0.055792372673749924\n",
      "Iteration 2034, Loss: 0.05569680780172348\n",
      "Iteration 2035, Loss: 0.05573936551809311\n",
      "Iteration 2036, Loss: 0.055797379463911057\n",
      "Iteration 2037, Loss: 0.055707257241010666\n",
      "Iteration 2038, Loss: 0.05572390556335449\n",
      "Iteration 2039, Loss: 0.055787842720746994\n",
      "Iteration 2040, Loss: 0.05572712421417236\n",
      "Iteration 2041, Loss: 0.055673085153102875\n",
      "Iteration 2042, Loss: 0.055709123611450195\n",
      "Iteration 2043, Loss: 0.055625323206186295\n",
      "Iteration 2044, Loss: 0.055660370737314224\n",
      "Iteration 2045, Loss: 0.0556134395301342\n",
      "Iteration 2046, Loss: 0.055704038590192795\n",
      "Iteration 2047, Loss: 0.055693190544843674\n",
      "Iteration 2048, Loss: 0.05564308539032936\n",
      "Iteration 2049, Loss: 0.05563688650727272\n",
      "Iteration 2050, Loss: 0.0556865930557251\n",
      "Iteration 2051, Loss: 0.055661242455244064\n",
      "Iteration 2052, Loss: 0.05570797249674797\n",
      "Iteration 2053, Loss: 0.055696289986371994\n",
      "Iteration 2054, Loss: 0.05567574501037598\n",
      "Iteration 2055, Loss: 0.05568965524435043\n",
      "Iteration 2056, Loss: 0.055636487901210785\n",
      "Iteration 2057, Loss: 0.055635176599025726\n",
      "Iteration 2058, Loss: 0.05568600073456764\n",
      "Iteration 2059, Loss: 0.055662158876657486\n",
      "Iteration 2060, Loss: 0.05570363998413086\n",
      "Iteration 2061, Loss: 0.055686794221401215\n",
      "Iteration 2062, Loss: 0.055689774453639984\n",
      "Iteration 2063, Loss: 0.05570900812745094\n",
      "Iteration 2064, Loss: 0.05562889575958252\n",
      "Iteration 2065, Loss: 0.055782001465559006\n",
      "Iteration 2066, Loss: 0.05578303709626198\n",
      "Iteration 2067, Loss: 0.05562019348144531\n",
      "Iteration 2068, Loss: 0.05580560490489006\n",
      "Iteration 2069, Loss: 0.055876534432172775\n",
      "Iteration 2070, Loss: 0.05583413690328598\n",
      "Iteration 2071, Loss: 0.05570065975189209\n",
      "Iteration 2072, Loss: 0.05577874556183815\n",
      "Iteration 2073, Loss: 0.05586513131856918\n",
      "Iteration 2074, Loss: 0.055785857141017914\n",
      "Iteration 2075, Loss: 0.05565941333770752\n",
      "Iteration 2076, Loss: 0.05572358891367912\n",
      "Iteration 2077, Loss: 0.055681388825178146\n",
      "Iteration 2078, Loss: 0.05569843575358391\n",
      "Iteration 2079, Loss: 0.055694740265607834\n",
      "Iteration 2080, Loss: 0.055677931755781174\n",
      "Iteration 2081, Loss: 0.05569378659129143\n",
      "Iteration 2082, Loss: 0.05563163757324219\n",
      "Iteration 2083, Loss: 0.055728040635585785\n",
      "Iteration 2084, Loss: 0.055679164826869965\n",
      "Iteration 2085, Loss: 0.05572045221924782\n",
      "Iteration 2086, Loss: 0.05576638504862785\n",
      "Iteration 2087, Loss: 0.05570995807647705\n",
      "Iteration 2088, Loss: 0.05567789077758789\n",
      "Iteration 2089, Loss: 0.05568540468811989\n",
      "Iteration 2090, Loss: 0.05568007752299309\n",
      "Iteration 2091, Loss: 0.055693864822387695\n",
      "Iteration 2092, Loss: 0.055624961853027344\n",
      "Iteration 2093, Loss: 0.055738210678100586\n",
      "Iteration 2094, Loss: 0.05570113658905029\n",
      "Iteration 2095, Loss: 0.055693864822387695\n",
      "Iteration 2096, Loss: 0.05573038384318352\n",
      "Iteration 2097, Loss: 0.05566271394491196\n",
      "Iteration 2098, Loss: 0.05575370788574219\n",
      "Iteration 2099, Loss: 0.05577707663178444\n",
      "Iteration 2100, Loss: 0.05564717575907707\n",
      "Iteration 2101, Loss: 0.05579439923167229\n",
      "Iteration 2102, Loss: 0.055872999131679535\n",
      "Iteration 2103, Loss: 0.05583302304148674\n",
      "Iteration 2104, Loss: 0.05569593235850334\n",
      "Iteration 2105, Loss: 0.055784545838832855\n",
      "Iteration 2106, Loss: 0.05587426945567131\n",
      "Iteration 2107, Loss: 0.0558011569082737\n",
      "Iteration 2108, Loss: 0.055639468133449554\n",
      "Iteration 2109, Loss: 0.055707573890686035\n",
      "Iteration 2110, Loss: 0.05565798282623291\n",
      "Iteration 2111, Loss: 0.055737655609846115\n",
      "Iteration 2112, Loss: 0.05575072765350342\n",
      "Iteration 2113, Loss: 0.055635809898376465\n",
      "Iteration 2114, Loss: 0.05577445030212402\n",
      "Iteration 2115, Loss: 0.05580143257975578\n",
      "Iteration 2116, Loss: 0.055691760033369064\n",
      "Iteration 2117, Loss: 0.05575470253825188\n",
      "Iteration 2118, Loss: 0.055827341973781586\n",
      "Iteration 2119, Loss: 0.05576423928141594\n",
      "Iteration 2120, Loss: 0.05564972013235092\n",
      "Iteration 2121, Loss: 0.05572644993662834\n",
      "Iteration 2122, Loss: 0.05566692352294922\n",
      "Iteration 2123, Loss: 0.0557425431907177\n",
      "Iteration 2124, Loss: 0.055789828300476074\n",
      "Iteration 2125, Loss: 0.05572744458913803\n",
      "Iteration 2126, Loss: 0.05567602440714836\n",
      "Iteration 2127, Loss: 0.05571945756673813\n",
      "Iteration 2128, Loss: 0.05564757436513901\n",
      "Iteration 2129, Loss: 0.05571889877319336\n",
      "Iteration 2130, Loss: 0.0557379350066185\n",
      "Iteration 2131, Loss: 0.055664144456386566\n",
      "Iteration 2132, Loss: 0.05576050281524658\n",
      "Iteration 2133, Loss: 0.05578390881419182\n",
      "Iteration 2134, Loss: 0.05564292520284653\n",
      "Iteration 2135, Loss: 0.055798888206481934\n",
      "Iteration 2136, Loss: 0.055887702852487564\n",
      "Iteration 2137, Loss: 0.05586818978190422\n",
      "Iteration 2138, Loss: 0.055753033608198166\n",
      "Iteration 2139, Loss: 0.05569450184702873\n",
      "Iteration 2140, Loss: 0.055772583931684494\n",
      "Iteration 2141, Loss: 0.055685680359601974\n",
      "Iteration 2142, Loss: 0.05574452877044678\n",
      "Iteration 2143, Loss: 0.0558147057890892\n",
      "Iteration 2144, Loss: 0.05578378960490227\n",
      "Iteration 2145, Loss: 0.055667243897914886\n",
      "Iteration 2146, Loss: 0.05580548569560051\n",
      "Iteration 2147, Loss: 0.05587093159556389\n",
      "Iteration 2148, Loss: 0.05575808137655258\n",
      "Iteration 2149, Loss: 0.055707138031721115\n",
      "Iteration 2150, Loss: 0.05579519271850586\n",
      "Iteration 2151, Loss: 0.0557808093726635\n",
      "Iteration 2152, Loss: 0.05567499250173569\n",
      "Iteration 2153, Loss: 0.0557839497923851\n",
      "Iteration 2154, Loss: 0.055841367691755295\n",
      "Iteration 2155, Loss: 0.05572044849395752\n",
      "Iteration 2156, Loss: 0.05574003979563713\n",
      "Iteration 2157, Loss: 0.05583306401968002\n",
      "Iteration 2158, Loss: 0.05582285299897194\n",
      "Iteration 2159, Loss: 0.05571981519460678\n",
      "Iteration 2160, Loss: 0.055719178169965744\n",
      "Iteration 2161, Loss: 0.055774491280317307\n",
      "Iteration 2162, Loss: 0.05565138906240463\n",
      "Iteration 2163, Loss: 0.055792175233364105\n",
      "Iteration 2164, Loss: 0.05588646978139877\n",
      "Iteration 2165, Loss: 0.05587681382894516\n",
      "Iteration 2166, Loss: 0.05577389523386955\n",
      "Iteration 2167, Loss: 0.05564574524760246\n",
      "Iteration 2168, Loss: 0.055701617151498795\n",
      "Iteration 2169, Loss: 0.05563700571656227\n",
      "Iteration 2170, Loss: 0.05562901496887207\n",
      "Iteration 2171, Loss: 0.05572191998362541\n",
      "Iteration 2172, Loss: 0.05567268654704094\n",
      "Iteration 2173, Loss: 0.05572640895843506\n",
      "Iteration 2174, Loss: 0.05577174946665764\n",
      "Iteration 2175, Loss: 0.05571254342794418\n",
      "Iteration 2176, Loss: 0.055679600685834885\n",
      "Iteration 2177, Loss: 0.055694542825222015\n",
      "Iteration 2178, Loss: 0.05566763877868652\n",
      "Iteration 2179, Loss: 0.05567502975463867\n",
      "Iteration 2180, Loss: 0.0556509904563427\n",
      "Iteration 2181, Loss: 0.055650196969509125\n",
      "Iteration 2182, Loss: 0.05566763877868652\n",
      "Iteration 2183, Loss: 0.05564383789896965\n",
      "Iteration 2184, Loss: 0.05572796240448952\n",
      "Iteration 2185, Loss: 0.0557076558470726\n",
      "Iteration 2186, Loss: 0.055676817893981934\n",
      "Iteration 2187, Loss: 0.05570375919342041\n",
      "Iteration 2188, Loss: 0.055625997483730316\n",
      "Iteration 2189, Loss: 0.05581005662679672\n",
      "Iteration 2190, Loss: 0.05584637448191643\n",
      "Iteration 2191, Loss: 0.05572843924164772\n",
      "Iteration 2192, Loss: 0.05572954937815666\n",
      "Iteration 2193, Loss: 0.05581657215952873\n",
      "Iteration 2194, Loss: 0.055787961930036545\n",
      "Iteration 2195, Loss: 0.055655281990766525\n",
      "Iteration 2196, Loss: 0.055828213691711426\n",
      "Iteration 2197, Loss: 0.05591348931193352\n",
      "Iteration 2198, Loss: 0.0558343343436718\n",
      "Iteration 2199, Loss: 0.05564502999186516\n",
      "Iteration 2200, Loss: 0.05580691620707512\n",
      "Iteration 2201, Loss: 0.05587482824921608\n",
      "Iteration 2202, Loss: 0.05581223964691162\n",
      "Iteration 2203, Loss: 0.05565142631530762\n",
      "Iteration 2204, Loss: 0.055836956948041916\n",
      "Iteration 2205, Loss: 0.05592505261301994\n",
      "Iteration 2206, Loss: 0.055852971971035004\n",
      "Iteration 2207, Loss: 0.05565047264099121\n",
      "Iteration 2208, Loss: 0.055851541459560394\n",
      "Iteration 2209, Loss: 0.055976033210754395\n",
      "Iteration 2210, Loss: 0.05596482753753662\n",
      "Iteration 2211, Loss: 0.05583703890442848\n",
      "Iteration 2212, Loss: 0.05567316338419914\n",
      "Iteration 2213, Loss: 0.055834852159023285\n",
      "Iteration 2214, Loss: 0.05590132996439934\n",
      "Iteration 2215, Loss: 0.0558091402053833\n",
      "Iteration 2216, Loss: 0.055650871247053146\n",
      "Iteration 2217, Loss: 0.055723391473293304\n",
      "Iteration 2218, Loss: 0.0556941032409668\n",
      "Iteration 2219, Loss: 0.055671535432338715\n",
      "Iteration 2220, Loss: 0.055671576410532\n",
      "Iteration 2221, Loss: 0.05568957328796387\n",
      "Iteration 2222, Loss: 0.05569835752248764\n",
      "Iteration 2223, Loss: 0.05564165115356445\n",
      "Iteration 2224, Loss: 0.05573181435465813\n",
      "Iteration 2225, Loss: 0.055694580078125\n",
      "Iteration 2226, Loss: 0.05570129677653313\n",
      "Iteration 2227, Loss: 0.05574023723602295\n",
      "Iteration 2228, Loss: 0.05567924305796623\n",
      "Iteration 2229, Loss: 0.05572541803121567\n",
      "Iteration 2230, Loss: 0.05573702231049538\n",
      "Iteration 2231, Loss: 0.055640142410993576\n",
      "Iteration 2232, Loss: 0.05567169561982155\n",
      "Iteration 2233, Loss: 0.055629294365644455\n",
      "Iteration 2234, Loss: 0.05568190664052963\n",
      "Iteration 2235, Loss: 0.05566132068634033\n",
      "Iteration 2236, Loss: 0.05569780245423317\n",
      "Iteration 2237, Loss: 0.05566171929240227\n",
      "Iteration 2238, Loss: 0.055727165192365646\n",
      "Iteration 2239, Loss: 0.05576932430267334\n",
      "Iteration 2240, Loss: 0.05571385473012924\n",
      "Iteration 2241, Loss: 0.055669546127319336\n",
      "Iteration 2242, Loss: 0.055672407150268555\n",
      "Iteration 2243, Loss: 0.05569350719451904\n",
      "Iteration 2244, Loss: 0.055712781846523285\n",
      "Iteration 2245, Loss: 0.05563684552907944\n",
      "Iteration 2246, Loss: 0.05579904839396477\n",
      "Iteration 2247, Loss: 0.05582372471690178\n",
      "Iteration 2248, Loss: 0.05567256733775139\n",
      "Iteration 2249, Loss: 0.05579535290598869\n",
      "Iteration 2250, Loss: 0.05590641498565674\n",
      "Iteration 2251, Loss: 0.05591177940368652\n",
      "Iteration 2252, Loss: 0.055822014808654785\n",
      "Iteration 2253, Loss: 0.055648092180490494\n",
      "Iteration 2254, Loss: 0.055904071778059006\n",
      "Iteration 2255, Loss: 0.05603659152984619\n",
      "Iteration 2256, Loss: 0.05598131939768791\n",
      "Iteration 2257, Loss: 0.055758558213710785\n",
      "Iteration 2258, Loss: 0.05577949807047844\n",
      "Iteration 2259, Loss: 0.05593311786651611\n",
      "Iteration 2260, Loss: 0.055977147072553635\n",
      "Iteration 2261, Loss: 0.055921874940395355\n",
      "Iteration 2262, Loss: 0.05577798932790756\n",
      "Iteration 2263, Loss: 0.05569060891866684\n",
      "Iteration 2264, Loss: 0.05579046532511711\n",
      "Iteration 2265, Loss: 0.055709004402160645\n",
      "Iteration 2266, Loss: 0.055721960961818695\n",
      "Iteration 2267, Loss: 0.0557909831404686\n",
      "Iteration 2268, Loss: 0.05575931444764137\n",
      "Iteration 2269, Loss: 0.05563728138804436\n",
      "Iteration 2270, Loss: 0.055854879319667816\n",
      "Iteration 2271, Loss: 0.05593085661530495\n",
      "Iteration 2272, Loss: 0.05582718178629875\n",
      "Iteration 2273, Loss: 0.05565170571208\n",
      "Iteration 2274, Loss: 0.0557401217520237\n",
      "Iteration 2275, Loss: 0.055729590356349945\n",
      "Iteration 2276, Loss: 0.05562468618154526\n",
      "Iteration 2277, Loss: 0.055845778435468674\n",
      "Iteration 2278, Loss: 0.05590454861521721\n",
      "Iteration 2279, Loss: 0.05579499527812004\n",
      "Iteration 2280, Loss: 0.05567800998687744\n",
      "Iteration 2281, Loss: 0.05576876923441887\n",
      "Iteration 2282, Loss: 0.05575808137655258\n",
      "Iteration 2283, Loss: 0.05564514920115471\n",
      "Iteration 2284, Loss: 0.05582626909017563\n",
      "Iteration 2285, Loss: 0.05589668080210686\n",
      "Iteration 2286, Loss: 0.0558062419295311\n",
      "Iteration 2287, Loss: 0.05566152185201645\n",
      "Iteration 2288, Loss: 0.05576400086283684\n",
      "Iteration 2289, Loss: 0.05577043816447258\n",
      "Iteration 2290, Loss: 0.05565623566508293\n",
      "Iteration 2291, Loss: 0.055806081742048264\n",
      "Iteration 2292, Loss: 0.05588265508413315\n",
      "Iteration 2293, Loss: 0.055814228951931\n",
      "Iteration 2294, Loss: 0.055661123245954514\n",
      "Iteration 2295, Loss: 0.05580262467265129\n",
      "Iteration 2296, Loss: 0.05586330220103264\n",
      "Iteration 2297, Loss: 0.055776119232177734\n",
      "Iteration 2298, Loss: 0.05566307157278061\n",
      "Iteration 2299, Loss: 0.05573153868317604\n",
      "Iteration 2300, Loss: 0.05567089840769768\n",
      "Iteration 2301, Loss: 0.05573658272624016\n",
      "Iteration 2302, Loss: 0.05576745793223381\n",
      "Iteration 2303, Loss: 0.05566692352294922\n",
      "Iteration 2304, Loss: 0.05576710030436516\n",
      "Iteration 2305, Loss: 0.05582428351044655\n",
      "Iteration 2306, Loss: 0.05574309825897217\n",
      "Iteration 2307, Loss: 0.05568353459239006\n",
      "Iteration 2308, Loss: 0.055742304772138596\n",
      "Iteration 2309, Loss: 0.055667124688625336\n",
      "Iteration 2310, Loss: 0.05575486272573471\n",
      "Iteration 2311, Loss: 0.05580242723226547\n",
      "Iteration 2312, Loss: 0.05571981519460678\n",
      "Iteration 2313, Loss: 0.05570578575134277\n",
      "Iteration 2314, Loss: 0.05576034635305405\n",
      "Iteration 2315, Loss: 0.05567809194326401\n",
      "Iteration 2316, Loss: 0.05575152486562729\n",
      "Iteration 2317, Loss: 0.055807434022426605\n",
      "Iteration 2318, Loss: 0.055735789239406586\n",
      "Iteration 2319, Loss: 0.055682819336652756\n",
      "Iteration 2320, Loss: 0.05573773756623268\n",
      "Iteration 2321, Loss: 0.055661797523498535\n",
      "Iteration 2322, Loss: 0.055759672075510025\n",
      "Iteration 2323, Loss: 0.05581434816122055\n",
      "Iteration 2324, Loss: 0.05575529858469963\n",
      "Iteration 2325, Loss: 0.05566271394491196\n",
      "Iteration 2326, Loss: 0.055756330490112305\n",
      "Iteration 2327, Loss: 0.05573078244924545\n",
      "Iteration 2328, Loss: 0.05567701905965805\n",
      "Iteration 2329, Loss: 0.0557122640311718\n",
      "Iteration 2330, Loss: 0.05567284673452377\n",
      "Iteration 2331, Loss: 0.055708013474941254\n",
      "Iteration 2332, Loss: 0.0556897334754467\n",
      "Iteration 2333, Loss: 0.05569426342844963\n",
      "Iteration 2334, Loss: 0.055722277611494064\n",
      "Iteration 2335, Loss: 0.0556614026427269\n",
      "Iteration 2336, Loss: 0.05574719235301018\n",
      "Iteration 2337, Loss: 0.05575275793671608\n",
      "Iteration 2338, Loss: 0.055638592690229416\n",
      "Iteration 2339, Loss: 0.055693309754133224\n",
      "Iteration 2340, Loss: 0.05566016957163811\n",
      "Iteration 2341, Loss: 0.05572124570608139\n",
      "Iteration 2342, Loss: 0.055716514587402344\n",
      "Iteration 2343, Loss: 0.055658064782619476\n",
      "Iteration 2344, Loss: 0.055681705474853516\n",
      "Iteration 2345, Loss: 0.05563763901591301\n",
      "Iteration 2346, Loss: 0.05566982552409172\n",
      "Iteration 2347, Loss: 0.05564403533935547\n",
      "Iteration 2348, Loss: 0.05572410672903061\n",
      "Iteration 2349, Loss: 0.0556894950568676\n",
      "Iteration 2350, Loss: 0.055705033242702484\n",
      "Iteration 2351, Loss: 0.055745724588632584\n",
      "Iteration 2352, Loss: 0.0556892566382885\n",
      "Iteration 2353, Loss: 0.055704716593027115\n",
      "Iteration 2354, Loss: 0.05570904538035393\n",
      "Iteration 2355, Loss: 0.055665258318185806\n",
      "Iteration 2356, Loss: 0.05568349361419678\n",
      "Iteration 2357, Loss: 0.05562127009034157\n",
      "Iteration 2358, Loss: 0.05567685887217522\n",
      "Iteration 2359, Loss: 0.055641334503889084\n",
      "Iteration 2360, Loss: 0.05573701858520508\n",
      "Iteration 2361, Loss: 0.05571436882019043\n",
      "Iteration 2362, Loss: 0.05567849054932594\n",
      "Iteration 2363, Loss: 0.05571071431040764\n",
      "Iteration 2364, Loss: 0.05564320087432861\n",
      "Iteration 2365, Loss: 0.05577782914042473\n",
      "Iteration 2366, Loss: 0.055792372673749924\n",
      "Iteration 2367, Loss: 0.055636048316955566\n",
      "Iteration 2368, Loss: 0.05582563206553459\n",
      "Iteration 2369, Loss: 0.05593498796224594\n",
      "Iteration 2370, Loss: 0.05593101307749748\n",
      "Iteration 2371, Loss: 0.05582769960165024\n",
      "Iteration 2372, Loss: 0.05565675348043442\n",
      "Iteration 2373, Loss: 0.05587522312998772\n",
      "Iteration 2374, Loss: 0.05597973242402077\n",
      "Iteration 2375, Loss: 0.055900100618600845\n",
      "Iteration 2376, Loss: 0.055658936500549316\n",
      "Iteration 2377, Loss: 0.055861473083496094\n",
      "Iteration 2378, Loss: 0.056021928787231445\n",
      "Iteration 2379, Loss: 0.056070927530527115\n",
      "Iteration 2380, Loss: 0.05601875111460686\n",
      "Iteration 2381, Loss: 0.05587729066610336\n",
      "Iteration 2382, Loss: 0.055659931153059006\n",
      "Iteration 2383, Loss: 0.05594063177704811\n",
      "Iteration 2384, Loss: 0.05611836910247803\n",
      "Iteration 2385, Loss: 0.05610422417521477\n",
      "Iteration 2386, Loss: 0.05591897293925285\n",
      "Iteration 2387, Loss: 0.055648088455200195\n",
      "Iteration 2388, Loss: 0.05580810829997063\n",
      "Iteration 2389, Loss: 0.05587482452392578\n",
      "Iteration 2390, Loss: 0.05583822727203369\n",
      "Iteration 2391, Loss: 0.05570749565958977\n",
      "Iteration 2392, Loss: 0.05576853081583977\n",
      "Iteration 2393, Loss: 0.05585523694753647\n",
      "Iteration 2394, Loss: 0.055767737329006195\n",
      "Iteration 2395, Loss: 0.055681031197309494\n",
      "Iteration 2396, Loss: 0.05575426667928696\n",
      "Iteration 2397, Loss: 0.055723629891872406\n",
      "Iteration 2398, Loss: 0.05563291162252426\n",
      "Iteration 2399, Loss: 0.05562027543783188\n",
      "Iteration 2400, Loss: 0.05573701858520508\n",
      "Iteration 2401, Loss: 0.05576058477163315\n",
      "Iteration 2402, Loss: 0.05568806454539299\n",
      "Iteration 2403, Loss: 0.05572529882192612\n",
      "Iteration 2404, Loss: 0.055746518075466156\n",
      "Iteration 2405, Loss: 0.055627547204494476\n",
      "Iteration 2406, Loss: 0.055639706552028656\n",
      "Iteration 2407, Loss: 0.055685799568891525\n",
      "Iteration 2408, Loss: 0.05562039464712143\n",
      "Iteration 2409, Loss: 0.055749617516994476\n",
      "Iteration 2410, Loss: 0.05576249212026596\n",
      "Iteration 2411, Loss: 0.055668238550424576\n",
      "Iteration 2412, Loss: 0.05576499551534653\n",
      "Iteration 2413, Loss: 0.0558091402053833\n",
      "Iteration 2414, Loss: 0.05569756403565407\n",
      "Iteration 2415, Loss: 0.05575212091207504\n",
      "Iteration 2416, Loss: 0.05583393573760986\n",
      "Iteration 2417, Loss: 0.05579805374145508\n",
      "Iteration 2418, Loss: 0.05566931143403053\n",
      "Iteration 2419, Loss: 0.05580393597483635\n",
      "Iteration 2420, Loss: 0.05587780475616455\n",
      "Iteration 2421, Loss: 0.055785857141017914\n",
      "Iteration 2422, Loss: 0.05566684529185295\n",
      "Iteration 2423, Loss: 0.05574135109782219\n",
      "Iteration 2424, Loss: 0.05570487305521965\n",
      "Iteration 2425, Loss: 0.05566652864217758\n",
      "Iteration 2426, Loss: 0.055665694177150726\n",
      "Iteration 2427, Loss: 0.05569760128855705\n",
      "Iteration 2428, Loss: 0.05570908635854721\n",
      "Iteration 2429, Loss: 0.055635176599025726\n",
      "Iteration 2430, Loss: 0.055762212723493576\n",
      "Iteration 2431, Loss: 0.055746596306562424\n",
      "Iteration 2432, Loss: 0.05565210431814194\n",
      "Iteration 2433, Loss: 0.055684130638837814\n",
      "Iteration 2434, Loss: 0.05562707036733627\n",
      "Iteration 2435, Loss: 0.055775485932826996\n",
      "Iteration 2436, Loss: 0.05577047914266586\n",
      "Iteration 2437, Loss: 0.05563589185476303\n",
      "Iteration 2438, Loss: 0.05574091523885727\n",
      "Iteration 2439, Loss: 0.05574532598257065\n",
      "Iteration 2440, Loss: 0.05563315004110336\n",
      "Iteration 2441, Loss: 0.055818598717451096\n",
      "Iteration 2442, Loss: 0.05587112903594971\n",
      "Iteration 2443, Loss: 0.05576503276824951\n",
      "Iteration 2444, Loss: 0.05569370836019516\n",
      "Iteration 2445, Loss: 0.05577763170003891\n",
      "Iteration 2446, Loss: 0.05574556440114975\n",
      "Iteration 2447, Loss: 0.055624645203351974\n",
      "Iteration 2448, Loss: 0.05562921613454819\n",
      "Iteration 2449, Loss: 0.05572311207652092\n",
      "Iteration 2450, Loss: 0.05573217198252678\n",
      "Iteration 2451, Loss: 0.055646102875471115\n",
      "Iteration 2452, Loss: 0.05578756704926491\n",
      "Iteration 2453, Loss: 0.055813275277614594\n",
      "Iteration 2454, Loss: 0.05566529557108879\n",
      "Iteration 2455, Loss: 0.05580071732401848\n",
      "Iteration 2456, Loss: 0.05590943619608879\n",
      "Iteration 2457, Loss: 0.05590784549713135\n",
      "Iteration 2458, Loss: 0.055808745324611664\n",
      "Iteration 2459, Loss: 0.055644989013671875\n",
      "Iteration 2460, Loss: 0.05586842820048332\n",
      "Iteration 2461, Loss: 0.05595100298523903\n",
      "Iteration 2462, Loss: 0.055852971971035004\n",
      "Iteration 2463, Loss: 0.05564173310995102\n",
      "Iteration 2464, Loss: 0.05576964467763901\n",
      "Iteration 2465, Loss: 0.0558115653693676\n",
      "Iteration 2466, Loss: 0.05575132369995117\n",
      "Iteration 2467, Loss: 0.05563132092356682\n",
      "Iteration 2468, Loss: 0.05566176027059555\n",
      "Iteration 2469, Loss: 0.05567924305796623\n",
      "Iteration 2470, Loss: 0.05567856878042221\n",
      "Iteration 2471, Loss: 0.05565035715699196\n",
      "Iteration 2472, Loss: 0.055631957948207855\n",
      "Iteration 2473, Loss: 0.05567403882741928\n",
      "Iteration 2474, Loss: 0.055620353668928146\n",
      "Iteration 2475, Loss: 0.05572589486837387\n",
      "Iteration 2476, Loss: 0.05566863343119621\n",
      "Iteration 2477, Loss: 0.05573559179902077\n",
      "Iteration 2478, Loss: 0.05578676983714104\n",
      "Iteration 2479, Loss: 0.05573328584432602\n",
      "Iteration 2480, Loss: 0.05564991757273674\n",
      "Iteration 2481, Loss: 0.05569545552134514\n",
      "Iteration 2482, Loss: 0.055648207664489746\n",
      "Iteration 2483, Loss: 0.05567435547709465\n",
      "Iteration 2484, Loss: 0.055641453713178635\n",
      "Iteration 2485, Loss: 0.055736783891916275\n",
      "Iteration 2486, Loss: 0.05573614686727524\n",
      "Iteration 2487, Loss: 0.055647335946559906\n",
      "Iteration 2488, Loss: 0.05573193356394768\n",
      "Iteration 2489, Loss: 0.05570928379893303\n",
      "Iteration 2490, Loss: 0.055677495896816254\n",
      "Iteration 2491, Loss: 0.05569791793823242\n",
      "Iteration 2492, Loss: 0.05563744157552719\n",
      "Iteration 2493, Loss: 0.0557204894721508\n",
      "Iteration 2494, Loss: 0.055667441338300705\n",
      "Iteration 2495, Loss: 0.05573249235749245\n",
      "Iteration 2496, Loss: 0.05578291788697243\n",
      "Iteration 2497, Loss: 0.055734872817993164\n",
      "Iteration 2498, Loss: 0.055634379386901855\n",
      "Iteration 2499, Loss: 0.05567701905965805\n",
      "Iteration 2500, Loss: 0.05564868450164795\n",
      "Iteration 2501, Loss: 0.05565448850393295\n",
      "Iteration 2502, Loss: 0.05567256733775139\n",
      "Iteration 2503, Loss: 0.05564097687602043\n",
      "Iteration 2504, Loss: 0.055706702172756195\n",
      "Iteration 2505, Loss: 0.05567292496562004\n",
      "Iteration 2506, Loss: 0.05571365728974342\n",
      "Iteration 2507, Loss: 0.0557376965880394\n",
      "Iteration 2508, Loss: 0.05565055459737778\n",
      "Iteration 2509, Loss: 0.05577639862895012\n",
      "Iteration 2510, Loss: 0.055807989090681076\n",
      "Iteration 2511, Loss: 0.05568122863769531\n",
      "Iteration 2512, Loss: 0.05577608197927475\n",
      "Iteration 2513, Loss: 0.05586652085185051\n",
      "Iteration 2514, Loss: 0.05583437532186508\n",
      "Iteration 2515, Loss: 0.05570018291473389\n",
      "Iteration 2516, Loss: 0.055777035653591156\n",
      "Iteration 2517, Loss: 0.05586409568786621\n",
      "Iteration 2518, Loss: 0.055784229189157486\n",
      "Iteration 2519, Loss: 0.05565917491912842\n",
      "Iteration 2520, Loss: 0.05572621151804924\n",
      "Iteration 2521, Loss: 0.05567801371216774\n",
      "Iteration 2522, Loss: 0.055713098496198654\n",
      "Iteration 2523, Loss: 0.05572144314646721\n",
      "Iteration 2524, Loss: 0.05564415454864502\n",
      "Iteration 2525, Loss: 0.055662792176008224\n",
      "Iteration 2526, Loss: 0.055665019899606705\n",
      "Iteration 2527, Loss: 0.05563430115580559\n",
      "Iteration 2528, Loss: 0.05571095272898674\n",
      "Iteration 2529, Loss: 0.05567169189453125\n",
      "Iteration 2530, Loss: 0.05571846291422844\n",
      "Iteration 2531, Loss: 0.05574917793273926\n",
      "Iteration 2532, Loss: 0.05567014217376709\n",
      "Iteration 2533, Loss: 0.05575033277273178\n",
      "Iteration 2534, Loss: 0.05578367039561272\n",
      "Iteration 2535, Loss: 0.055661242455244064\n",
      "Iteration 2536, Loss: 0.05578967183828354\n",
      "Iteration 2537, Loss: 0.05587677285075188\n",
      "Iteration 2538, Loss: 0.055844902992248535\n",
      "Iteration 2539, Loss: 0.055715445429086685\n",
      "Iteration 2540, Loss: 0.05575581640005112\n",
      "Iteration 2541, Loss: 0.05584144592285156\n",
      "Iteration 2542, Loss: 0.05576721951365471\n",
      "Iteration 2543, Loss: 0.05566950887441635\n",
      "Iteration 2544, Loss: 0.05572843551635742\n",
      "Iteration 2545, Loss: 0.05568051338195801\n",
      "Iteration 2546, Loss: 0.05570555105805397\n",
      "Iteration 2547, Loss: 0.05570896714925766\n",
      "Iteration 2548, Loss: 0.055660687386989594\n",
      "Iteration 2549, Loss: 0.055670224130153656\n",
      "Iteration 2550, Loss: 0.05565575882792473\n",
      "Iteration 2551, Loss: 0.05563358590006828\n",
      "Iteration 2552, Loss: 0.05569557473063469\n",
      "Iteration 2553, Loss: 0.05568615719676018\n",
      "Iteration 2554, Loss: 0.05565468594431877\n",
      "Iteration 2555, Loss: 0.05562059208750725\n",
      "Iteration 2556, Loss: 0.05573495477437973\n",
      "Iteration 2557, Loss: 0.055754903703927994\n",
      "Iteration 2558, Loss: 0.055678170174360275\n",
      "Iteration 2559, Loss: 0.055742740631103516\n",
      "Iteration 2560, Loss: 0.05576980113983154\n",
      "Iteration 2561, Loss: 0.05562528222799301\n",
      "Iteration 2562, Loss: 0.05582631006836891\n",
      "Iteration 2563, Loss: 0.05593021959066391\n",
      "Iteration 2564, Loss: 0.055926721543073654\n",
      "Iteration 2565, Loss: 0.05582761764526367\n",
      "Iteration 2566, Loss: 0.05565520375967026\n",
      "Iteration 2567, Loss: 0.055882375687360764\n",
      "Iteration 2568, Loss: 0.055998366326093674\n",
      "Iteration 2569, Loss: 0.05593033879995346\n",
      "Iteration 2570, Loss: 0.055696845054626465\n",
      "Iteration 2571, Loss: 0.055832069367170334\n",
      "Iteration 2572, Loss: 0.055992644280195236\n",
      "Iteration 2573, Loss: 0.056042276322841644\n",
      "Iteration 2574, Loss: 0.055991850793361664\n",
      "Iteration 2575, Loss: 0.05585161969065666\n",
      "Iteration 2576, Loss: 0.05563453957438469\n",
      "Iteration 2577, Loss: 0.05596967786550522\n",
      "Iteration 2578, Loss: 0.05614360421895981\n",
      "Iteration 2579, Loss: 0.05612441152334213\n",
      "Iteration 2580, Loss: 0.05593212693929672\n",
      "Iteration 2581, Loss: 0.05563235282897949\n",
      "Iteration 2582, Loss: 0.055769406259059906\n",
      "Iteration 2583, Loss: 0.055799923837184906\n",
      "Iteration 2584, Loss: 0.055733565241098404\n",
      "Iteration 2585, Loss: 0.05565575882792473\n",
      "Iteration 2586, Loss: 0.055670659989118576\n",
      "Iteration 2587, Loss: 0.05568631738424301\n",
      "Iteration 2588, Loss: 0.05569799989461899\n",
      "Iteration 2589, Loss: 0.05561550706624985\n",
      "Iteration 2590, Loss: 0.055831752717494965\n",
      "Iteration 2591, Loss: 0.055859170854091644\n",
      "Iteration 2592, Loss: 0.055709999054670334\n",
      "Iteration 2593, Loss: 0.05576634407043457\n",
      "Iteration 2594, Loss: 0.05587601661682129\n",
      "Iteration 2595, Loss: 0.05587979406118393\n",
      "Iteration 2596, Loss: 0.05578780174255371\n",
      "Iteration 2597, Loss: 0.0556233748793602\n",
      "Iteration 2598, Loss: 0.05583222955465317\n",
      "Iteration 2599, Loss: 0.055868230760097504\n",
      "Iteration 2600, Loss: 0.05573872849345207\n",
      "Iteration 2601, Loss: 0.05573141574859619\n",
      "Iteration 2602, Loss: 0.05582781881093979\n",
      "Iteration 2603, Loss: 0.055817604064941406\n",
      "Iteration 2604, Loss: 0.055709563195705414\n",
      "Iteration 2605, Loss: 0.055739205330610275\n",
      "Iteration 2606, Loss: 0.055802587419748306\n",
      "Iteration 2607, Loss: 0.05569660663604736\n",
      "Iteration 2608, Loss: 0.055746398866176605\n",
      "Iteration 2609, Loss: 0.055827222764492035\n",
      "Iteration 2610, Loss: 0.055800240486860275\n",
      "Iteration 2611, Loss: 0.055679403245449066\n",
      "Iteration 2612, Loss: 0.0557912215590477\n",
      "Iteration 2613, Loss: 0.0558648519217968\n",
      "Iteration 2614, Loss: 0.0557660274207592\n",
      "Iteration 2615, Loss: 0.05568937584757805\n",
      "Iteration 2616, Loss: 0.05576789379119873\n",
      "Iteration 2617, Loss: 0.055740874260663986\n",
      "Iteration 2618, Loss: 0.05562559887766838\n",
      "Iteration 2619, Loss: 0.05582749843597412\n",
      "Iteration 2620, Loss: 0.055861473083496094\n",
      "Iteration 2621, Loss: 0.05571866035461426\n",
      "Iteration 2622, Loss: 0.05575541779398918\n",
      "Iteration 2623, Loss: 0.055861394852399826\n",
      "Iteration 2624, Loss: 0.05586286634206772\n",
      "Iteration 2625, Loss: 0.05576988309621811\n",
      "Iteration 2626, Loss: 0.055639348924160004\n",
      "Iteration 2627, Loss: 0.05568385496735573\n",
      "Iteration 2628, Loss: 0.05565750598907471\n",
      "Iteration 2629, Loss: 0.05565202236175537\n",
      "Iteration 2630, Loss: 0.05569171905517578\n",
      "Iteration 2631, Loss: 0.05563899129629135\n",
      "Iteration 2632, Loss: 0.05575482174754143\n",
      "Iteration 2633, Loss: 0.05580707639455795\n",
      "Iteration 2634, Loss: 0.055760107934474945\n",
      "Iteration 2635, Loss: 0.05562397092580795\n",
      "Iteration 2636, Loss: 0.05588730424642563\n",
      "Iteration 2637, Loss: 0.055976711213588715\n",
      "Iteration 2638, Loss: 0.055883292108774185\n",
      "Iteration 2639, Loss: 0.055629853159189224\n",
      "Iteration 2640, Loss: 0.05588182061910629\n",
      "Iteration 2641, Loss: 0.05604195594787598\n",
      "Iteration 2642, Loss: 0.056090354919433594\n",
      "Iteration 2643, Loss: 0.056038420647382736\n",
      "Iteration 2644, Loss: 0.05589636415243149\n",
      "Iteration 2645, Loss: 0.05567678064107895\n",
      "Iteration 2646, Loss: 0.05591738596558571\n",
      "Iteration 2647, Loss: 0.05609830468893051\n",
      "Iteration 2648, Loss: 0.056088488548994064\n",
      "Iteration 2649, Loss: 0.05590697377920151\n",
      "Iteration 2650, Loss: 0.055642567574977875\n",
      "Iteration 2651, Loss: 0.05577775090932846\n",
      "Iteration 2652, Loss: 0.055807314813137054\n",
      "Iteration 2653, Loss: 0.05573837086558342\n",
      "Iteration 2654, Loss: 0.055652979761362076\n",
      "Iteration 2655, Loss: 0.055676382035017014\n",
      "Iteration 2656, Loss: 0.05567427724599838\n",
      "Iteration 2657, Loss: 0.055679284036159515\n",
      "Iteration 2658, Loss: 0.055642370134592056\n",
      "Iteration 2659, Loss: 0.05564570799469948\n",
      "Iteration 2660, Loss: 0.05565746873617172\n",
      "Iteration 2661, Loss: 0.055619124323129654\n",
      "Iteration 2662, Loss: 0.05569935217499733\n",
      "Iteration 2663, Loss: 0.05568814277648926\n",
      "Iteration 2664, Loss: 0.0556495226919651\n",
      "Iteration 2665, Loss: 0.05562436580657959\n",
      "Iteration 2666, Loss: 0.05570566654205322\n",
      "Iteration 2667, Loss: 0.055683016777038574\n",
      "Iteration 2668, Loss: 0.05568110942840576\n",
      "Iteration 2669, Loss: 0.0556766614317894\n",
      "Iteration 2670, Loss: 0.05568532273173332\n",
      "Iteration 2671, Loss: 0.05568663403391838\n",
      "Iteration 2672, Loss: 0.0556560754776001\n",
      "Iteration 2673, Loss: 0.05564983934164047\n",
      "Iteration 2674, Loss: 0.05570407956838608\n",
      "Iteration 2675, Loss: 0.05570721626281738\n",
      "Iteration 2676, Loss: 0.05564117804169655\n",
      "Iteration 2677, Loss: 0.05572923272848129\n",
      "Iteration 2678, Loss: 0.05568631738424301\n",
      "Iteration 2679, Loss: 0.05571151152253151\n",
      "Iteration 2680, Loss: 0.05575641244649887\n",
      "Iteration 2681, Loss: 0.05570399761199951\n",
      "Iteration 2682, Loss: 0.05567809194326401\n",
      "Iteration 2683, Loss: 0.05567876622080803\n",
      "Iteration 2684, Loss: 0.05568814277648926\n",
      "Iteration 2685, Loss: 0.05570761486887932\n",
      "Iteration 2686, Loss: 0.05563279241323471\n",
      "Iteration 2687, Loss: 0.05580131337046623\n",
      "Iteration 2688, Loss: 0.05582420155405998\n",
      "Iteration 2689, Loss: 0.05567248910665512\n",
      "Iteration 2690, Loss: 0.055794477462768555\n",
      "Iteration 2691, Loss: 0.05590522289276123\n",
      "Iteration 2692, Loss: 0.05590745061635971\n",
      "Iteration 2693, Loss: 0.055812519043684006\n",
      "Iteration 2694, Loss: 0.05563998222351074\n",
      "Iteration 2695, Loss: 0.055899184197187424\n",
      "Iteration 2696, Loss: 0.056014738976955414\n",
      "Iteration 2697, Loss: 0.055944204330444336\n",
      "Iteration 2698, Loss: 0.05570833012461662\n",
      "Iteration 2699, Loss: 0.05582340806722641\n",
      "Iteration 2700, Loss: 0.05598441883921623\n",
      "Iteration 2701, Loss: 0.05603440850973129\n",
      "Iteration 2702, Loss: 0.05598413944244385\n",
      "Iteration 2703, Loss: 0.0558442696928978\n",
      "Iteration 2704, Loss: 0.055628418922424316\n",
      "Iteration 2705, Loss: 0.05596943944692612\n",
      "Iteration 2706, Loss: 0.05613772198557854\n",
      "Iteration 2707, Loss: 0.05611785501241684\n",
      "Iteration 2708, Loss: 0.05592966079711914\n",
      "Iteration 2709, Loss: 0.0556488037109375\n",
      "Iteration 2710, Loss: 0.055832743644714355\n",
      "Iteration 2711, Loss: 0.05592989921569824\n",
      "Iteration 2712, Loss: 0.0559164322912693\n",
      "Iteration 2713, Loss: 0.05580294132232666\n",
      "Iteration 2714, Loss: 0.05563664808869362\n",
      "Iteration 2715, Loss: 0.055800002068281174\n",
      "Iteration 2716, Loss: 0.05579984188079834\n",
      "Iteration 2717, Loss: 0.055633705109357834\n",
      "Iteration 2718, Loss: 0.05581720918416977\n",
      "Iteration 2719, Loss: 0.055921558290719986\n",
      "Iteration 2720, Loss: 0.05591853708028793\n",
      "Iteration 2721, Loss: 0.055819395929574966\n",
      "Iteration 2722, Loss: 0.055640339851379395\n",
      "Iteration 2723, Loss: 0.05591229721903801\n",
      "Iteration 2724, Loss: 0.05604179948568344\n",
      "Iteration 2725, Loss: 0.055983226746320724\n",
      "Iteration 2726, Loss: 0.055757008492946625\n",
      "Iteration 2727, Loss: 0.05578164383769035\n",
      "Iteration 2728, Loss: 0.05593749135732651\n",
      "Iteration 2729, Loss: 0.05598307028412819\n",
      "Iteration 2730, Loss: 0.0559290274977684\n",
      "Iteration 2731, Loss: 0.0557861328125\n",
      "Iteration 2732, Loss: 0.055677175521850586\n",
      "Iteration 2733, Loss: 0.05577826499938965\n",
      "Iteration 2734, Loss: 0.055702053010463715\n",
      "Iteration 2735, Loss: 0.055721960961818695\n",
      "Iteration 2736, Loss: 0.05578681081533432\n",
      "Iteration 2737, Loss: 0.05575176328420639\n",
      "Iteration 2738, Loss: 0.05562738701701164\n",
      "Iteration 2739, Loss: 0.055865369737148285\n",
      "Iteration 2740, Loss: 0.055942337960004807\n",
      "Iteration 2741, Loss: 0.05584311485290527\n",
      "Iteration 2742, Loss: 0.05564292520284653\n",
      "Iteration 2743, Loss: 0.05576566979289055\n",
      "Iteration 2744, Loss: 0.055795036256313324\n",
      "Iteration 2745, Loss: 0.05571572110056877\n",
      "Iteration 2746, Loss: 0.0556994304060936\n",
      "Iteration 2747, Loss: 0.055738650262355804\n",
      "Iteration 2748, Loss: 0.055625319480895996\n",
      "Iteration 2749, Loss: 0.0557941235601902\n",
      "Iteration 2750, Loss: 0.05584486573934555\n",
      "Iteration 2751, Loss: 0.055769287049770355\n",
      "Iteration 2752, Loss: 0.0556643009185791\n",
      "Iteration 2753, Loss: 0.05575939267873764\n",
      "Iteration 2754, Loss: 0.05573849007487297\n",
      "Iteration 2755, Loss: 0.055659256875514984\n",
      "Iteration 2756, Loss: 0.05568965524435043\n",
      "Iteration 2757, Loss: 0.055655717849731445\n",
      "Iteration 2758, Loss: 0.05570928379893303\n",
      "Iteration 2759, Loss: 0.055672209709882736\n",
      "Iteration 2760, Loss: 0.05571635812520981\n",
      "Iteration 2761, Loss: 0.05575605481863022\n",
      "Iteration 2762, Loss: 0.055698834359645844\n",
      "Iteration 2763, Loss: 0.05569104477763176\n",
      "Iteration 2764, Loss: 0.055696289986371994\n",
      "Iteration 2765, Loss: 0.05567304417490959\n",
      "Iteration 2766, Loss: 0.05569104477763176\n",
      "Iteration 2767, Loss: 0.05562063306570053\n",
      "Iteration 2768, Loss: 0.055776916444301605\n",
      "Iteration 2769, Loss: 0.05577389523386955\n",
      "Iteration 2770, Loss: 0.05564232915639877\n",
      "Iteration 2771, Loss: 0.0557708740234375\n",
      "Iteration 2772, Loss: 0.0558096207678318\n",
      "Iteration 2773, Loss: 0.05571926012635231\n",
      "Iteration 2774, Loss: 0.05571047589182854\n",
      "Iteration 2775, Loss: 0.05576833337545395\n",
      "Iteration 2776, Loss: 0.055683255195617676\n",
      "Iteration 2777, Loss: 0.05574282258749008\n",
      "Iteration 2778, Loss: 0.0557992085814476\n",
      "Iteration 2779, Loss: 0.05572553724050522\n",
      "Iteration 2780, Loss: 0.05568937584757805\n",
      "Iteration 2781, Loss: 0.05573761463165283\n",
      "Iteration 2782, Loss: 0.05564530938863754\n",
      "Iteration 2783, Loss: 0.05578816309571266\n",
      "Iteration 2784, Loss: 0.05585328862071037\n",
      "Iteration 2785, Loss: 0.05579225346446037\n",
      "Iteration 2786, Loss: 0.055659931153059006\n",
      "Iteration 2787, Loss: 0.05580075830221176\n",
      "Iteration 2788, Loss: 0.05584573745727539\n",
      "Iteration 2789, Loss: 0.055725812911987305\n",
      "Iteration 2790, Loss: 0.05573662370443344\n",
      "Iteration 2791, Loss: 0.055826585739851\n",
      "Iteration 2792, Loss: 0.05580171197652817\n",
      "Iteration 2793, Loss: 0.05568373203277588\n",
      "Iteration 2794, Loss: 0.05578009411692619\n",
      "Iteration 2795, Loss: 0.05584661290049553\n",
      "Iteration 2796, Loss: 0.05574055761098862\n",
      "Iteration 2797, Loss: 0.05571460723876953\n",
      "Iteration 2798, Loss: 0.05579690262675285\n",
      "Iteration 2799, Loss: 0.05576996132731438\n",
      "Iteration 2800, Loss: 0.05565563961863518\n",
      "Iteration 2801, Loss: 0.05581009387969971\n",
      "Iteration 2802, Loss: 0.05586620420217514\n",
      "Iteration 2803, Loss: 0.05574456974864006\n",
      "Iteration 2804, Loss: 0.0557221993803978\n",
      "Iteration 2805, Loss: 0.05581530183553696\n",
      "Iteration 2806, Loss: 0.05580270290374756\n",
      "Iteration 2807, Loss: 0.0556972436606884\n",
      "Iteration 2808, Loss: 0.05575180426239967\n",
      "Iteration 2809, Loss: 0.05580878257751465\n",
      "Iteration 2810, Loss: 0.055688463151454926\n",
      "Iteration 2811, Loss: 0.05576268956065178\n",
      "Iteration 2812, Loss: 0.0558549165725708\n",
      "Iteration 2813, Loss: 0.055842284113168716\n",
      "Iteration 2814, Loss: 0.05573678016662598\n",
      "Iteration 2815, Loss: 0.05569986626505852\n",
      "Iteration 2816, Loss: 0.055759988725185394\n",
      "Iteration 2817, Loss: 0.05565015599131584\n",
      "Iteration 2818, Loss: 0.05577540397644043\n",
      "Iteration 2819, Loss: 0.05585316941142082\n",
      "Iteration 2820, Loss: 0.0558292493224144\n",
      "Iteration 2821, Loss: 0.055713534355163574\n",
      "Iteration 2822, Loss: 0.05574151128530502\n",
      "Iteration 2823, Loss: 0.05581017583608627\n",
      "Iteration 2824, Loss: 0.055700741708278656\n",
      "Iteration 2825, Loss: 0.05574536323547363\n",
      "Iteration 2826, Loss: 0.05583103746175766\n",
      "Iteration 2827, Loss: 0.055813949555158615\n",
      "Iteration 2828, Loss: 0.05570455640554428\n",
      "Iteration 2829, Loss: 0.05574595928192139\n",
      "Iteration 2830, Loss: 0.05580834671854973\n",
      "Iteration 2831, Loss: 0.05569267272949219\n",
      "Iteration 2832, Loss: 0.05575522035360336\n",
      "Iteration 2833, Loss: 0.05584470555186272\n",
      "Iteration 2834, Loss: 0.05583084002137184\n",
      "Iteration 2835, Loss: 0.05572418496012688\n",
      "Iteration 2836, Loss: 0.055716078728437424\n",
      "Iteration 2837, Loss: 0.0557757243514061\n",
      "Iteration 2838, Loss: 0.055657945573329926\n",
      "Iteration 2839, Loss: 0.05578283593058586\n",
      "Iteration 2840, Loss: 0.05587379261851311\n",
      "Iteration 2841, Loss: 0.05586119741201401\n",
      "Iteration 2842, Loss: 0.055755697190761566\n",
      "Iteration 2843, Loss: 0.055672649294137955\n",
      "Iteration 2844, Loss: 0.05573320388793945\n",
      "Iteration 2845, Loss: 0.05562381073832512\n",
      "Iteration 2846, Loss: 0.05577310174703598\n",
      "Iteration 2847, Loss: 0.0558270625770092\n",
      "Iteration 2848, Loss: 0.05577584356069565\n",
      "Iteration 2849, Loss: 0.055642008781433105\n",
      "Iteration 2850, Loss: 0.05584442988038063\n",
      "Iteration 2851, Loss: 0.055912695825099945\n",
      "Iteration 2852, Loss: 0.05580028146505356\n",
      "Iteration 2853, Loss: 0.05567554756999016\n",
      "Iteration 2854, Loss: 0.05576356500387192\n",
      "Iteration 2855, Loss: 0.05574953556060791\n",
      "Iteration 2856, Loss: 0.05564602464437485\n",
      "Iteration 2857, Loss: 0.055816493928432465\n",
      "Iteration 2858, Loss: 0.05586814880371094\n",
      "Iteration 2859, Loss: 0.055742185562849045\n",
      "Iteration 2860, Loss: 0.05572621151804924\n",
      "Iteration 2861, Loss: 0.055822014808654785\n",
      "Iteration 2862, Loss: 0.05581458657979965\n",
      "Iteration 2863, Loss: 0.05571460723876953\n",
      "Iteration 2864, Loss: 0.05572160333395004\n",
      "Iteration 2865, Loss: 0.05577349662780762\n",
      "Iteration 2866, Loss: 0.05564972013235092\n",
      "Iteration 2867, Loss: 0.05579046532511711\n",
      "Iteration 2868, Loss: 0.055882375687360764\n",
      "Iteration 2869, Loss: 0.055870216339826584\n",
      "Iteration 2870, Loss: 0.055764954537153244\n",
      "Iteration 2871, Loss: 0.05566064640879631\n",
      "Iteration 2872, Loss: 0.05572418496012688\n",
      "Iteration 2873, Loss: 0.05562456697225571\n",
      "Iteration 2874, Loss: 0.05574564263224602\n",
      "Iteration 2875, Loss: 0.05577031895518303\n",
      "Iteration 2876, Loss: 0.05569016933441162\n",
      "Iteration 2877, Loss: 0.055729351937770844\n",
      "Iteration 2878, Loss: 0.055762968957424164\n",
      "Iteration 2879, Loss: 0.055630091577768326\n",
      "Iteration 2880, Loss: 0.05582094565033913\n",
      "Iteration 2881, Loss: 0.05591785907745361\n",
      "Iteration 2882, Loss: 0.05589751526713371\n",
      "Iteration 2883, Loss: 0.05577739328145981\n",
      "Iteration 2884, Loss: 0.05568309873342514\n",
      "Iteration 2885, Loss: 0.05578915402293205\n",
      "Iteration 2886, Loss: 0.05575017258524895\n",
      "Iteration 2887, Loss: 0.05567574501037598\n",
      "Iteration 2888, Loss: 0.05572275444865227\n",
      "Iteration 2889, Loss: 0.055697839707136154\n",
      "Iteration 2890, Loss: 0.05567117780447006\n",
      "Iteration 2891, Loss: 0.055686235427856445\n",
      "Iteration 2892, Loss: 0.055667441338300705\n",
      "Iteration 2893, Loss: 0.05567622557282448\n",
      "Iteration 2894, Loss: 0.05565758794546127\n",
      "Iteration 2895, Loss: 0.055697642266750336\n",
      "Iteration 2896, Loss: 0.055680397897958755\n",
      "Iteration 2897, Loss: 0.05568063259124756\n",
      "Iteration 2898, Loss: 0.05568023771047592\n",
      "Iteration 2899, Loss: 0.055676620453596115\n",
      "Iteration 2900, Loss: 0.05566617101430893\n",
      "Iteration 2901, Loss: 0.0556844100356102\n",
      "Iteration 2902, Loss: 0.05567046254873276\n",
      "Iteration 2903, Loss: 0.055697087198495865\n",
      "Iteration 2904, Loss: 0.05569716542959213\n",
      "Iteration 2905, Loss: 0.05565488710999489\n",
      "Iteration 2906, Loss: 0.0556643046438694\n",
      "Iteration 2907, Loss: 0.055679801851511\n",
      "Iteration 2908, Loss: 0.055664777755737305\n",
      "Iteration 2909, Loss: 0.0556870736181736\n",
      "Iteration 2910, Loss: 0.05566608905792236\n",
      "Iteration 2911, Loss: 0.05571015924215317\n",
      "Iteration 2912, Loss: 0.055724743753671646\n",
      "Iteration 2913, Loss: 0.055642448365688324\n",
      "Iteration 2914, Loss: 0.05576280876994133\n",
      "Iteration 2915, Loss: 0.05576423928141594\n",
      "Iteration 2916, Loss: 0.05562055483460426\n",
      "Iteration 2917, Loss: 0.05566323176026344\n",
      "Iteration 2918, Loss: 0.05564431473612785\n",
      "Iteration 2919, Loss: 0.05562969297170639\n",
      "Iteration 2920, Loss: 0.05567753687500954\n",
      "Iteration 2921, Loss: 0.05563315004110336\n",
      "Iteration 2922, Loss: 0.05574421212077141\n",
      "Iteration 2923, Loss: 0.05573419854044914\n",
      "Iteration 2924, Loss: 0.055645864456892014\n",
      "Iteration 2925, Loss: 0.055680952966213226\n",
      "Iteration 2926, Loss: 0.05562834069132805\n",
      "Iteration 2927, Loss: 0.05567936226725578\n",
      "Iteration 2928, Loss: 0.055656276643276215\n",
      "Iteration 2929, Loss: 0.05570650473237038\n",
      "Iteration 2930, Loss: 0.055673997849226\n",
      "Iteration 2931, Loss: 0.05571353808045387\n",
      "Iteration 2932, Loss: 0.055753033608198166\n",
      "Iteration 2933, Loss: 0.055694542825222015\n",
      "Iteration 2934, Loss: 0.05569767951965332\n",
      "Iteration 2935, Loss: 0.055704355239868164\n",
      "Iteration 2936, Loss: 0.055665772408246994\n",
      "Iteration 2937, Loss: 0.05568218231201172\n",
      "Iteration 2938, Loss: 0.05562325567007065\n",
      "Iteration 2939, Loss: 0.05566171929240227\n",
      "Iteration 2940, Loss: 0.05562381073832512\n",
      "Iteration 2941, Loss: 0.0557379350066185\n",
      "Iteration 2942, Loss: 0.05569382756948471\n",
      "Iteration 2943, Loss: 0.05570678040385246\n",
      "Iteration 2944, Loss: 0.05575255677103996\n",
      "Iteration 2945, Loss: 0.05569788068532944\n",
      "Iteration 2946, Loss: 0.05568981170654297\n",
      "Iteration 2947, Loss: 0.055693745613098145\n",
      "Iteration 2948, Loss: 0.055675309151411057\n",
      "Iteration 2949, Loss: 0.055692315101623535\n",
      "Iteration 2950, Loss: 0.05562055483460426\n",
      "Iteration 2951, Loss: 0.055764518678188324\n",
      "Iteration 2952, Loss: 0.055745404213666916\n",
      "Iteration 2953, Loss: 0.055651746690273285\n",
      "Iteration 2954, Loss: 0.0556945838034153\n",
      "Iteration 2955, Loss: 0.05564248561859131\n",
      "Iteration 2956, Loss: 0.05575959011912346\n",
      "Iteration 2957, Loss: 0.05577588453888893\n",
      "Iteration 2958, Loss: 0.05566263198852539\n",
      "Iteration 2959, Loss: 0.055772703140974045\n",
      "Iteration 2960, Loss: 0.055833976715803146\n",
      "Iteration 2961, Loss: 0.05576169863343239\n",
      "Iteration 2962, Loss: 0.05565690994262695\n",
      "Iteration 2963, Loss: 0.0557277612388134\n",
      "Iteration 2964, Loss: 0.055664580315351486\n",
      "Iteration 2965, Loss: 0.05574508756399155\n",
      "Iteration 2966, Loss: 0.05579320713877678\n",
      "Iteration 2967, Loss: 0.05573225021362305\n",
      "Iteration 2968, Loss: 0.0556718111038208\n",
      "Iteration 2969, Loss: 0.0557253360748291\n",
      "Iteration 2970, Loss: 0.0556592158973217\n",
      "Iteration 2971, Loss: 0.05573471635580063\n",
      "Iteration 2972, Loss: 0.05577731132507324\n",
      "Iteration 2973, Loss: 0.05572466179728508\n",
      "Iteration 2974, Loss: 0.05565635487437248\n",
      "Iteration 2975, Loss: 0.05568953603506088\n",
      "Iteration 2976, Loss: 0.05565699189901352\n",
      "Iteration 2977, Loss: 0.055669110268354416\n",
      "Iteration 2978, Loss: 0.05564789101481438\n",
      "Iteration 2979, Loss: 0.05568830296397209\n",
      "Iteration 2980, Loss: 0.05567248910665512\n",
      "Iteration 2981, Loss: 0.055679045617580414\n",
      "Iteration 2982, Loss: 0.05565885826945305\n",
      "Iteration 2983, Loss: 0.055714648216962814\n",
      "Iteration 2984, Loss: 0.055733680725097656\n",
      "Iteration 2985, Loss: 0.0556589774787426\n",
      "Iteration 2986, Loss: 0.055755458772182465\n",
      "Iteration 2987, Loss: 0.05577242374420166\n",
      "Iteration 2988, Loss: 0.0556233748793602\n",
      "Iteration 2989, Loss: 0.055833619087934494\n",
      "Iteration 2990, Loss: 0.05594261735677719\n",
      "Iteration 2991, Loss: 0.05594261735677719\n",
      "Iteration 2992, Loss: 0.05584581941366196\n",
      "Iteration 2993, Loss: 0.05567411705851555\n",
      "Iteration 2994, Loss: 0.05586099997162819\n",
      "Iteration 2995, Loss: 0.05598294734954834\n",
      "Iteration 2996, Loss: 0.05592334270477295\n",
      "Iteration 2997, Loss: 0.0556994304060936\n",
      "Iteration 2998, Loss: 0.05582388490438461\n",
      "Iteration 2999, Loss: 0.05597889423370361\n",
      "Iteration 3000, Loss: 0.056023240089416504\n",
      "Iteration 3001, Loss: 0.05596772953867912\n",
      "Iteration 3002, Loss: 0.05582328885793686\n",
      "Iteration 3003, Loss: 0.05564185231924057\n",
      "Iteration 3004, Loss: 0.055821776390075684\n",
      "Iteration 3005, Loss: 0.05583544820547104\n",
      "Iteration 3006, Loss: 0.05568365380167961\n",
      "Iteration 3007, Loss: 0.055783711373806\n",
      "Iteration 3008, Loss: 0.05588988587260246\n",
      "Iteration 3009, Loss: 0.05588698387145996\n",
      "Iteration 3010, Loss: 0.055785536766052246\n",
      "Iteration 3011, Loss: 0.05563458055257797\n",
      "Iteration 3012, Loss: 0.05574997514486313\n",
      "Iteration 3013, Loss: 0.05570078268647194\n",
      "Iteration 3014, Loss: 0.055704515427351\n",
      "Iteration 3015, Loss: 0.055752478539943695\n",
      "Iteration 3016, Loss: 0.05570284649729729\n",
      "Iteration 3017, Loss: 0.05567769333720207\n",
      "Iteration 3018, Loss: 0.05567682161927223\n",
      "Iteration 3019, Loss: 0.055688779801130295\n",
      "Iteration 3020, Loss: 0.055707257241010666\n",
      "Iteration 3021, Loss: 0.05562921613454819\n",
      "Iteration 3022, Loss: 0.0558089055120945\n",
      "Iteration 3023, Loss: 0.055838070809841156\n",
      "Iteration 3024, Loss: 0.055697325617074966\n",
      "Iteration 3025, Loss: 0.05576825141906738\n",
      "Iteration 3026, Loss: 0.05587136745452881\n",
      "Iteration 3027, Loss: 0.055867914110422134\n",
      "Iteration 3028, Loss: 0.05576853081583977\n",
      "Iteration 3029, Loss: 0.05565210431814194\n",
      "Iteration 3030, Loss: 0.05572644993662834\n",
      "Iteration 3031, Loss: 0.05564240738749504\n",
      "Iteration 3032, Loss: 0.055756211280822754\n",
      "Iteration 3033, Loss: 0.055810097604990005\n",
      "Iteration 3034, Loss: 0.05576121807098389\n",
      "Iteration 3035, Loss: 0.055625639855861664\n",
      "Iteration 3036, Loss: 0.05586802959442139\n",
      "Iteration 3037, Loss: 0.055939916521310806\n",
      "Iteration 3038, Loss: 0.05583067983388901\n",
      "Iteration 3039, Loss: 0.05564936250448227\n",
      "Iteration 3040, Loss: 0.05573670193552971\n",
      "Iteration 3041, Loss: 0.05572386831045151\n",
      "Iteration 3042, Loss: 0.05562106892466545\n",
      "Iteration 3043, Loss: 0.05583564564585686\n",
      "Iteration 3044, Loss: 0.055881302803754807\n",
      "Iteration 3045, Loss: 0.05575736612081528\n",
      "Iteration 3046, Loss: 0.05571115016937256\n",
      "Iteration 3047, Loss: 0.05580544471740723\n",
      "Iteration 3048, Loss: 0.05579432100057602\n",
      "Iteration 3049, Loss: 0.05568615719676018\n",
      "Iteration 3050, Loss: 0.05576638504862785\n",
      "Iteration 3051, Loss: 0.05582936853170395\n",
      "Iteration 3052, Loss: 0.055722158402204514\n",
      "Iteration 3053, Loss: 0.05572513863444328\n",
      "Iteration 3054, Loss: 0.05580771341919899\n",
      "Iteration 3055, Loss: 0.055783551186323166\n",
      "Iteration 3056, Loss: 0.05566410347819328\n",
      "Iteration 3057, Loss: 0.05580652132630348\n",
      "Iteration 3058, Loss: 0.05587899684906006\n",
      "Iteration 3059, Loss: 0.055778663605451584\n",
      "Iteration 3060, Loss: 0.055678728967905045\n",
      "Iteration 3061, Loss: 0.05575990676879883\n",
      "Iteration 3062, Loss: 0.05573606491088867\n",
      "Iteration 3063, Loss: 0.05561741441488266\n",
      "Iteration 3064, Loss: 0.055851422250270844\n",
      "Iteration 3065, Loss: 0.05590387433767319\n",
      "Iteration 3066, Loss: 0.05577925965189934\n",
      "Iteration 3067, Loss: 0.055696092545986176\n",
      "Iteration 3068, Loss: 0.055791061371564865\n",
      "Iteration 3069, Loss: 0.055782996118068695\n",
      "Iteration 3070, Loss: 0.05568110942840576\n",
      "Iteration 3071, Loss: 0.05576658248901367\n",
      "Iteration 3072, Loss: 0.055821023881435394\n",
      "Iteration 3073, Loss: 0.0557001456618309\n",
      "Iteration 3074, Loss: 0.055751603096723557\n",
      "Iteration 3075, Loss: 0.05584363266825676\n",
      "Iteration 3076, Loss: 0.05583127588033676\n",
      "Iteration 3077, Loss: 0.055725179612636566\n",
      "Iteration 3078, Loss: 0.0557122640311718\n",
      "Iteration 3079, Loss: 0.05577186867594719\n",
      "Iteration 3080, Loss: 0.05565560236573219\n",
      "Iteration 3081, Loss: 0.05578247830271721\n",
      "Iteration 3082, Loss: 0.05587224289774895\n",
      "Iteration 3083, Loss: 0.055857859551906586\n",
      "Iteration 3084, Loss: 0.05575069040060043\n",
      "Iteration 3085, Loss: 0.05568067356944084\n",
      "Iteration 3086, Loss: 0.055745601654052734\n",
      "Iteration 3087, Loss: 0.055641058832407\n",
      "Iteration 3088, Loss: 0.05577695369720459\n",
      "Iteration 3089, Loss: 0.05585193634033203\n",
      "Iteration 3090, Loss: 0.05582428351044655\n",
      "Iteration 3091, Loss: 0.05570483207702637\n",
      "Iteration 3092, Loss: 0.05575573444366455\n",
      "Iteration 3093, Loss: 0.05582825466990471\n",
      "Iteration 3094, Loss: 0.055719856172800064\n",
      "Iteration 3095, Loss: 0.05572966858744621\n",
      "Iteration 3096, Loss: 0.05581545829772949\n",
      "Iteration 3097, Loss: 0.05579785630106926\n",
      "Iteration 3098, Loss: 0.05568806454539299\n",
      "Iteration 3099, Loss: 0.05576682090759277\n",
      "Iteration 3100, Loss: 0.05582849308848381\n",
      "Iteration 3101, Loss: 0.055710356682538986\n",
      "Iteration 3102, Loss: 0.05574294179677963\n",
      "Iteration 3103, Loss: 0.055834297090768814\n",
      "Iteration 3104, Loss: 0.0558219775557518\n",
      "Iteration 3105, Loss: 0.05571699142456055\n",
      "Iteration 3106, Loss: 0.05572200194001198\n",
      "Iteration 3107, Loss: 0.055778902024030685\n",
      "Iteration 3108, Loss: 0.055658143013715744\n",
      "Iteration 3109, Loss: 0.05578216165304184\n",
      "Iteration 3110, Loss: 0.05587383359670639\n",
      "Iteration 3111, Loss: 0.0558621920645237\n",
      "Iteration 3112, Loss: 0.05575760453939438\n",
      "Iteration 3113, Loss: 0.05566629022359848\n",
      "Iteration 3114, Loss: 0.05572330951690674\n",
      "Iteration 3115, Loss: 0.05562099069356918\n",
      "Iteration 3116, Loss: 0.05569720268249512\n",
      "Iteration 3117, Loss: 0.05566000938415527\n",
      "Iteration 3118, Loss: 0.05571890249848366\n",
      "Iteration 3119, Loss: 0.05572422593832016\n",
      "Iteration 3120, Loss: 0.05563553422689438\n",
      "Iteration 3121, Loss: 0.05568508431315422\n",
      "Iteration 3122, Loss: 0.05562349408864975\n",
      "Iteration 3123, Loss: 0.05568715184926987\n",
      "Iteration 3124, Loss: 0.055663466453552246\n",
      "Iteration 3125, Loss: 0.05569728463888168\n",
      "Iteration 3126, Loss: 0.05567316338419914\n",
      "Iteration 3127, Loss: 0.05570574849843979\n",
      "Iteration 3128, Loss: 0.055735111236572266\n",
      "Iteration 3129, Loss: 0.0556635856628418\n",
      "Iteration 3130, Loss: 0.05575069040060043\n",
      "Iteration 3131, Loss: 0.05577290430665016\n",
      "Iteration 3132, Loss: 0.05563048645853996\n",
      "Iteration 3133, Loss: 0.05581744760274887\n",
      "Iteration 3134, Loss: 0.05591337010264397\n",
      "Iteration 3135, Loss: 0.05589616671204567\n",
      "Iteration 3136, Loss: 0.055781684815883636\n",
      "Iteration 3137, Loss: 0.05567225068807602\n",
      "Iteration 3138, Loss: 0.05578657239675522\n",
      "Iteration 3139, Loss: 0.05576181411743164\n",
      "Iteration 3140, Loss: 0.055658381432294846\n",
      "Iteration 3141, Loss: 0.055713020265102386\n",
      "Iteration 3142, Loss: 0.0557025708258152\n",
      "Iteration 3143, Loss: 0.05564844608306885\n",
      "Iteration 3144, Loss: 0.05570455640554428\n",
      "Iteration 3145, Loss: 0.05567523092031479\n",
      "Iteration 3146, Loss: 0.05569271370768547\n",
      "Iteration 3147, Loss: 0.05569756031036377\n",
      "Iteration 3148, Loss: 0.05564936250448227\n",
      "Iteration 3149, Loss: 0.05564693734049797\n",
      "Iteration 3150, Loss: 0.05567129701375961\n",
      "Iteration 3151, Loss: 0.05562083050608635\n",
      "Iteration 3152, Loss: 0.05574576184153557\n",
      "Iteration 3153, Loss: 0.05577727407217026\n",
      "Iteration 3154, Loss: 0.05571107193827629\n",
      "Iteration 3155, Loss: 0.05568492412567139\n",
      "Iteration 3156, Loss: 0.05570578575134277\n",
      "Iteration 3157, Loss: 0.05565440654754639\n",
      "Iteration 3158, Loss: 0.05566374585032463\n",
      "Iteration 3159, Loss: 0.055653415620326996\n",
      "Iteration 3160, Loss: 0.05564781278371811\n",
      "Iteration 3161, Loss: 0.05565476417541504\n",
      "Iteration 3162, Loss: 0.055627547204494476\n",
      "Iteration 3163, Loss: 0.055712342262268066\n",
      "Iteration 3164, Loss: 0.05570948123931885\n",
      "Iteration 3165, Loss: 0.05563807487487793\n",
      "Iteration 3166, Loss: 0.05572951212525368\n",
      "Iteration 3167, Loss: 0.055684369057416916\n",
      "Iteration 3168, Loss: 0.05571305751800537\n",
      "Iteration 3169, Loss: 0.05575907602906227\n",
      "Iteration 3170, Loss: 0.05570737645030022\n",
      "Iteration 3171, Loss: 0.05567117780447006\n",
      "Iteration 3172, Loss: 0.05567117780447006\n",
      "Iteration 3173, Loss: 0.055692993104457855\n",
      "Iteration 3174, Loss: 0.05571281909942627\n",
      "Iteration 3175, Loss: 0.055637042969465256\n",
      "Iteration 3176, Loss: 0.05579543113708496\n",
      "Iteration 3177, Loss: 0.05582018941640854\n",
      "Iteration 3178, Loss: 0.05566950887441635\n",
      "Iteration 3179, Loss: 0.055795036256313324\n",
      "Iteration 3180, Loss: 0.05590566247701645\n",
      "Iteration 3181, Loss: 0.055908799171447754\n",
      "Iteration 3182, Loss: 0.055815499275922775\n",
      "Iteration 3183, Loss: 0.05564220994710922\n",
      "Iteration 3184, Loss: 0.05590212345123291\n",
      "Iteration 3185, Loss: 0.05602530762553215\n",
      "Iteration 3186, Loss: 0.05596184730529785\n",
      "Iteration 3187, Loss: 0.05573161691427231\n",
      "Iteration 3188, Loss: 0.05580246448516846\n",
      "Iteration 3189, Loss: 0.05596026033163071\n",
      "Iteration 3190, Loss: 0.05600786209106445\n",
      "Iteration 3191, Loss: 0.05595557019114494\n",
      "Iteration 3192, Loss: 0.05581387132406235\n",
      "Iteration 3193, Loss: 0.055639032274484634\n",
      "Iteration 3194, Loss: 0.05575704574584961\n",
      "Iteration 3195, Loss: 0.05570586770772934\n",
      "Iteration 3196, Loss: 0.05570086091756821\n",
      "Iteration 3197, Loss: 0.05574870482087135\n",
      "Iteration 3198, Loss: 0.055696845054626465\n",
      "Iteration 3199, Loss: 0.05568957328796387\n",
      "Iteration 3200, Loss: 0.055693309754133224\n",
      "Iteration 3201, Loss: 0.05567232891917229\n",
      "Iteration 3202, Loss: 0.055686913430690765\n",
      "Iteration 3203, Loss: 0.0556231364607811\n",
      "Iteration 3204, Loss: 0.0556563176214695\n",
      "Iteration 3205, Loss: 0.05561300367116928\n",
      "Iteration 3206, Loss: 0.05569680780172348\n",
      "Iteration 3207, Loss: 0.055679164826869965\n",
      "Iteration 3208, Loss: 0.05566931143403053\n",
      "Iteration 3209, Loss: 0.05563589185476303\n",
      "Iteration 3210, Loss: 0.05574099346995354\n",
      "Iteration 3211, Loss: 0.05577930063009262\n",
      "Iteration 3212, Loss: 0.05572013184428215\n",
      "Iteration 3213, Loss: 0.05566438287496567\n",
      "Iteration 3214, Loss: 0.05567701905965805\n",
      "Iteration 3215, Loss: 0.055680833756923676\n",
      "Iteration 3216, Loss: 0.05569235607981682\n",
      "Iteration 3217, Loss: 0.055624525994062424\n",
      "Iteration 3218, Loss: 0.055740319192409515\n",
      "Iteration 3219, Loss: 0.05571313947439194\n",
      "Iteration 3220, Loss: 0.05567443370819092\n",
      "Iteration 3221, Loss: 0.055705033242702484\n",
      "Iteration 3222, Loss: 0.05562881752848625\n",
      "Iteration 3223, Loss: 0.055790387094020844\n",
      "Iteration 3224, Loss: 0.055828772485256195\n",
      "Iteration 3225, Loss: 0.0557355098426342\n",
      "Iteration 3226, Loss: 0.05570463463664055\n",
      "Iteration 3227, Loss: 0.055772941559553146\n",
      "Iteration 3228, Loss: 0.05571536347270012\n",
      "Iteration 3229, Loss: 0.055696092545986176\n",
      "Iteration 3230, Loss: 0.05573081970214844\n",
      "Iteration 3231, Loss: 0.05565842241048813\n",
      "Iteration 3232, Loss: 0.05574151128530502\n",
      "Iteration 3233, Loss: 0.05575506016612053\n",
      "Iteration 3234, Loss: 0.05562114715576172\n",
      "Iteration 3235, Loss: 0.055836599320173264\n",
      "Iteration 3236, Loss: 0.055922865867614746\n",
      "Iteration 3237, Loss: 0.05587900057435036\n",
      "Iteration 3238, Loss: 0.05573403835296631\n",
      "Iteration 3239, Loss: 0.05575462430715561\n",
      "Iteration 3240, Loss: 0.05585634708404541\n",
      "Iteration 3241, Loss: 0.05581216141581535\n",
      "Iteration 3242, Loss: 0.055628977715969086\n",
      "Iteration 3243, Loss: 0.05585587024688721\n",
      "Iteration 3244, Loss: 0.05597877502441406\n",
      "Iteration 3245, Loss: 0.05598342418670654\n",
      "Iteration 3246, Loss: 0.05588646978139877\n",
      "Iteration 3247, Loss: 0.05571655556559563\n",
      "Iteration 3248, Loss: 0.055812638252973557\n",
      "Iteration 3249, Loss: 0.05593597888946533\n",
      "Iteration 3250, Loss: 0.05589064210653305\n",
      "Iteration 3251, Loss: 0.05568842217326164\n",
      "Iteration 3252, Loss: 0.05582305043935776\n",
      "Iteration 3253, Loss: 0.055965106934309006\n",
      "Iteration 3254, Loss: 0.05599459260702133\n",
      "Iteration 3255, Loss: 0.05592449754476547\n",
      "Iteration 3256, Loss: 0.05576964467763901\n",
      "Iteration 3257, Loss: 0.05572641268372536\n",
      "Iteration 3258, Loss: 0.05584597587585449\n",
      "Iteration 3259, Loss: 0.05579936504364014\n",
      "Iteration 3260, Loss: 0.055649518966674805\n",
      "Iteration 3261, Loss: 0.05573682114481926\n",
      "Iteration 3262, Loss: 0.05575386807322502\n",
      "Iteration 3263, Loss: 0.05567777529358864\n",
      "Iteration 3264, Loss: 0.055746715515851974\n",
      "Iteration 3265, Loss: 0.05577341839671135\n",
      "Iteration 3266, Loss: 0.05564173310995102\n",
      "Iteration 3267, Loss: 0.055785417556762695\n",
      "Iteration 3268, Loss: 0.055855631828308105\n",
      "Iteration 3269, Loss: 0.055810652673244476\n",
      "Iteration 3270, Loss: 0.055671095848083496\n",
      "Iteration 3271, Loss: 0.05581855773925781\n",
      "Iteration 3272, Loss: 0.05590609833598137\n",
      "Iteration 3273, Loss: 0.05581796541810036\n",
      "Iteration 3274, Loss: 0.055642884224653244\n",
      "Iteration 3275, Loss: 0.05572700873017311\n",
      "Iteration 3276, Loss: 0.05570010468363762\n",
      "Iteration 3277, Loss: 0.05566485971212387\n",
      "Iteration 3278, Loss: 0.05565563961863518\n",
      "Iteration 3279, Loss: 0.05571083351969719\n",
      "Iteration 3280, Loss: 0.05572319030761719\n",
      "Iteration 3281, Loss: 0.05563851445913315\n",
      "Iteration 3282, Loss: 0.055779896676540375\n",
      "Iteration 3283, Loss: 0.055790387094020844\n",
      "Iteration 3284, Loss: 0.055631719529628754\n",
      "Iteration 3285, Loss: 0.055833540856838226\n",
      "Iteration 3286, Loss: 0.05594615265727043\n",
      "Iteration 3287, Loss: 0.05594317242503166\n",
      "Iteration 3288, Loss: 0.05583862587809563\n",
      "Iteration 3289, Loss: 0.05566728487610817\n",
      "Iteration 3290, Loss: 0.05586715787649155\n",
      "Iteration 3291, Loss: 0.055977385491132736\n",
      "Iteration 3292, Loss: 0.055903635919094086\n",
      "Iteration 3293, Loss: 0.0556669645011425\n",
      "Iteration 3294, Loss: 0.05585559457540512\n",
      "Iteration 3295, Loss: 0.05601624771952629\n",
      "Iteration 3296, Loss: 0.056064050644636154\n",
      "Iteration 3297, Loss: 0.05600925534963608\n",
      "Iteration 3298, Loss: 0.055865250527858734\n",
      "Iteration 3299, Loss: 0.05565603822469711\n",
      "Iteration 3300, Loss: 0.055930498987436295\n",
      "Iteration 3301, Loss: 0.05608586594462395\n",
      "Iteration 3302, Loss: 0.05605284497141838\n",
      "Iteration 3303, Loss: 0.05585181713104248\n",
      "Iteration 3304, Loss: 0.055701058357954025\n",
      "Iteration 3305, Loss: 0.05584263801574707\n",
      "Iteration 3306, Loss: 0.055882494896650314\n",
      "Iteration 3307, Loss: 0.05582515522837639\n",
      "Iteration 3308, Loss: 0.05568087473511696\n",
      "Iteration 3309, Loss: 0.05582328885793686\n",
      "Iteration 3310, Loss: 0.05592381954193115\n",
      "Iteration 3311, Loss: 0.05584574118256569\n",
      "Iteration 3312, Loss: 0.055643480271101\n",
      "Iteration 3313, Loss: 0.0557989701628685\n",
      "Iteration 3314, Loss: 0.055873434990644455\n",
      "Iteration 3315, Loss: 0.0558345727622509\n",
      "Iteration 3316, Loss: 0.05569656938314438\n",
      "Iteration 3317, Loss: 0.05578514188528061\n",
      "Iteration 3318, Loss: 0.055877964943647385\n",
      "Iteration 3319, Loss: 0.05580409616231918\n",
      "Iteration 3320, Loss: 0.05564109608530998\n",
      "Iteration 3321, Loss: 0.05572768300771713\n",
      "Iteration 3322, Loss: 0.055701058357954025\n",
      "Iteration 3323, Loss: 0.0556696280837059\n",
      "Iteration 3324, Loss: 0.055669549852609634\n",
      "Iteration 3325, Loss: 0.05568631738424301\n",
      "Iteration 3326, Loss: 0.05568345636129379\n",
      "Iteration 3327, Loss: 0.0556635856628418\n",
      "Iteration 3328, Loss: 0.05564522743225098\n",
      "Iteration 3329, Loss: 0.05572565644979477\n",
      "Iteration 3330, Loss: 0.0557376965880394\n",
      "Iteration 3331, Loss: 0.055650435388088226\n",
      "Iteration 3332, Loss: 0.05576841160655022\n",
      "Iteration 3333, Loss: 0.05578991025686264\n",
      "Iteration 3334, Loss: 0.055652063339948654\n",
      "Iteration 3335, Loss: 0.05580707639455795\n",
      "Iteration 3336, Loss: 0.055906377732753754\n",
      "Iteration 3337, Loss: 0.0558900460600853\n",
      "Iteration 3338, Loss: 0.055775128304958344\n",
      "Iteration 3339, Loss: 0.05568190664052963\n",
      "Iteration 3340, Loss: 0.05578506365418434\n",
      "Iteration 3341, Loss: 0.05574902147054672\n",
      "Iteration 3342, Loss: 0.05567268654704094\n",
      "Iteration 3343, Loss: 0.05571746826171875\n",
      "Iteration 3344, Loss: 0.05569025129079819\n",
      "Iteration 3345, Loss: 0.05567840859293938\n",
      "Iteration 3346, Loss: 0.05567614361643791\n",
      "Iteration 3347, Loss: 0.055682223290205\n",
      "Iteration 3348, Loss: 0.05568913742899895\n",
      "Iteration 3349, Loss: 0.055644553154706955\n",
      "Iteration 3350, Loss: 0.055710237473249435\n",
      "Iteration 3351, Loss: 0.055679403245449066\n",
      "Iteration 3352, Loss: 0.05569601058959961\n",
      "Iteration 3353, Loss: 0.055710118263959885\n",
      "Iteration 3354, Loss: 0.055622100830078125\n",
      "Iteration 3355, Loss: 0.05566764250397682\n",
      "Iteration 3356, Loss: 0.05562993139028549\n",
      "Iteration 3357, Loss: 0.05575831979513168\n",
      "Iteration 3358, Loss: 0.05573884770274162\n",
      "Iteration 3359, Loss: 0.05565878003835678\n",
      "Iteration 3360, Loss: 0.055691879242658615\n",
      "Iteration 3361, Loss: 0.055628422647714615\n",
      "Iteration 3362, Loss: 0.05579376220703125\n",
      "Iteration 3363, Loss: 0.05580540746450424\n",
      "Iteration 3364, Loss: 0.05564729496836662\n",
      "Iteration 3365, Loss: 0.05581657215952873\n",
      "Iteration 3366, Loss: 0.05592691898345947\n",
      "Iteration 3367, Loss: 0.055924057960510254\n",
      "Iteration 3368, Loss: 0.05582161992788315\n",
      "Iteration 3369, Loss: 0.05565103143453598\n",
      "Iteration 3370, Loss: 0.05587788671255112\n",
      "Iteration 3371, Loss: 0.05597905442118645\n",
      "Iteration 3372, Loss: 0.055896203964948654\n",
      "Iteration 3373, Loss: 0.05565258115530014\n",
      "Iteration 3374, Loss: 0.055865608155727386\n",
      "Iteration 3375, Loss: 0.05602622404694557\n",
      "Iteration 3376, Loss: 0.056075576692819595\n",
      "Iteration 3377, Loss: 0.05602423474192619\n",
      "Iteration 3378, Loss: 0.05588321015238762\n",
      "Iteration 3379, Loss: 0.055664341896772385\n",
      "Iteration 3380, Loss: 0.055935584008693695\n",
      "Iteration 3381, Loss: 0.0561165027320385\n",
      "Iteration 3382, Loss: 0.05610545724630356\n",
      "Iteration 3383, Loss: 0.05592226982116699\n",
      "Iteration 3384, Loss: 0.055642805993556976\n",
      "Iteration 3385, Loss: 0.05580779165029526\n",
      "Iteration 3386, Loss: 0.05587804690003395\n",
      "Iteration 3387, Loss: 0.05584383010864258\n",
      "Iteration 3388, Loss: 0.0557151660323143\n",
      "Iteration 3389, Loss: 0.05575482174754143\n",
      "Iteration 3390, Loss: 0.05583922192454338\n",
      "Iteration 3391, Loss: 0.05574973672628403\n",
      "Iteration 3392, Loss: 0.055694423615932465\n",
      "Iteration 3393, Loss: 0.05576733872294426\n",
      "Iteration 3394, Loss: 0.055735353380441666\n",
      "Iteration 3395, Loss: 0.055621981620788574\n",
      "Iteration 3396, Loss: 0.05575358867645264\n",
      "Iteration 3397, Loss: 0.055716514587402344\n",
      "Iteration 3398, Loss: 0.05568603798747063\n",
      "Iteration 3399, Loss: 0.055727921426296234\n",
      "Iteration 3400, Loss: 0.05567034333944321\n",
      "Iteration 3401, Loss: 0.05573181435465813\n",
      "Iteration 3402, Loss: 0.055739324539899826\n",
      "Iteration 3403, Loss: 0.05564038082957268\n",
      "Iteration 3404, Loss: 0.055668871849775314\n",
      "Iteration 3405, Loss: 0.0556333065032959\n",
      "Iteration 3406, Loss: 0.05566883459687233\n",
      "Iteration 3407, Loss: 0.05564030259847641\n",
      "Iteration 3408, Loss: 0.05573483556509018\n",
      "Iteration 3409, Loss: 0.05570666119456291\n",
      "Iteration 3410, Loss: 0.05568794533610344\n",
      "Iteration 3411, Loss: 0.055725377053022385\n",
      "Iteration 3412, Loss: 0.055665336549282074\n",
      "Iteration 3413, Loss: 0.05573984235525131\n",
      "Iteration 3414, Loss: 0.05574759095907211\n",
      "Iteration 3415, Loss: 0.05563465878367424\n",
      "Iteration 3416, Loss: 0.05565258115530014\n",
      "Iteration 3417, Loss: 0.055661797523498535\n",
      "Iteration 3418, Loss: 0.055632710456848145\n",
      "Iteration 3419, Loss: 0.05564364045858383\n",
      "Iteration 3420, Loss: 0.055677060037851334\n",
      "Iteration 3421, Loss: 0.0556621178984642\n",
      "Iteration 3422, Loss: 0.05568893998861313\n",
      "Iteration 3423, Loss: 0.05564836785197258\n",
      "Iteration 3424, Loss: 0.055738769471645355\n",
      "Iteration 3425, Loss: 0.05578271672129631\n",
      "Iteration 3426, Loss: 0.055727843195199966\n",
      "Iteration 3427, Loss: 0.05565079301595688\n",
      "Iteration 3428, Loss: 0.055669985711574554\n",
      "Iteration 3429, Loss: 0.055678606033325195\n",
      "Iteration 3430, Loss: 0.05568286031484604\n",
      "Iteration 3431, Loss: 0.05563962832093239\n",
      "Iteration 3432, Loss: 0.055659692734479904\n",
      "Iteration 3433, Loss: 0.05564228817820549\n",
      "Iteration 3434, Loss: 0.0556664876639843\n",
      "Iteration 3435, Loss: 0.055647652596235275\n",
      "Iteration 3436, Loss: 0.05563950538635254\n",
      "Iteration 3437, Loss: 0.05568838492035866\n",
      "Iteration 3438, Loss: 0.05563267320394516\n",
      "Iteration 3439, Loss: 0.055737100541591644\n",
      "Iteration 3440, Loss: 0.05574699491262436\n",
      "Iteration 3441, Loss: 0.0556466206908226\n",
      "Iteration 3442, Loss: 0.055790744721889496\n",
      "Iteration 3443, Loss: 0.055832188576459885\n",
      "Iteration 3444, Loss: 0.05571397393941879\n",
      "Iteration 3445, Loss: 0.05574166774749756\n",
      "Iteration 3446, Loss: 0.05582845211029053\n",
      "Iteration 3447, Loss: 0.05579439923167229\n",
      "Iteration 3448, Loss: 0.055660128593444824\n",
      "Iteration 3449, Loss: 0.05582118406891823\n",
      "Iteration 3450, Loss: 0.055901527404785156\n",
      "Iteration 3451, Loss: 0.05581101030111313\n",
      "Iteration 3452, Loss: 0.05565011501312256\n",
      "Iteration 3453, Loss: 0.055741868913173676\n",
      "Iteration 3454, Loss: 0.05572335049510002\n",
      "Iteration 3455, Loss: 0.055635690689086914\n",
      "Iteration 3456, Loss: 0.05562790483236313\n",
      "Iteration 3457, Loss: 0.055732451379299164\n",
      "Iteration 3458, Loss: 0.055745404213666916\n",
      "Iteration 3459, Loss: 0.055658143013715744\n",
      "Iteration 3460, Loss: 0.055772267282009125\n",
      "Iteration 3461, Loss: 0.05580361932516098\n",
      "Iteration 3462, Loss: 0.0556662492454052\n",
      "Iteration 3463, Loss: 0.05579408258199692\n",
      "Iteration 3464, Loss: 0.055894892662763596\n",
      "Iteration 3465, Loss: 0.05588027089834213\n",
      "Iteration 3466, Loss: 0.05576638504862785\n",
      "Iteration 3467, Loss: 0.05568345636129379\n",
      "Iteration 3468, Loss: 0.055774133652448654\n",
      "Iteration 3469, Loss: 0.05571206659078598\n",
      "Iteration 3470, Loss: 0.05571126937866211\n",
      "Iteration 3471, Loss: 0.055764876306056976\n",
      "Iteration 3472, Loss: 0.05572478100657463\n",
      "Iteration 3473, Loss: 0.05565663427114487\n",
      "Iteration 3474, Loss: 0.05572120472788811\n",
      "Iteration 3475, Loss: 0.055677615106105804\n",
      "Iteration 3476, Loss: 0.05570721998810768\n",
      "Iteration 3477, Loss: 0.055735550820827484\n",
      "Iteration 3478, Loss: 0.055662475526332855\n",
      "Iteration 3479, Loss: 0.05576121807098389\n",
      "Iteration 3480, Loss: 0.05579189583659172\n",
      "Iteration 3481, Loss: 0.055672407150268555\n",
      "Iteration 3482, Loss: 0.0557711124420166\n",
      "Iteration 3483, Loss: 0.05585090443491936\n",
      "Iteration 3484, Loss: 0.05581244081258774\n",
      "Iteration 3485, Loss: 0.05567483231425285\n",
      "Iteration 3486, Loss: 0.05580727383494377\n",
      "Iteration 3487, Loss: 0.055895447731018066\n",
      "Iteration 3488, Loss: 0.055819034576416016\n",
      "Iteration 3489, Loss: 0.05563497543334961\n",
      "Iteration 3490, Loss: 0.05576249212026596\n",
      "Iteration 3491, Loss: 0.05577373504638672\n",
      "Iteration 3492, Loss: 0.05566096305847168\n",
      "Iteration 3493, Loss: 0.05578986927866936\n",
      "Iteration 3494, Loss: 0.05585865303874016\n",
      "Iteration 3495, Loss: 0.05578077211976051\n",
      "Iteration 3496, Loss: 0.05565718933939934\n",
      "Iteration 3497, Loss: 0.05574731156229973\n",
      "Iteration 3498, Loss: 0.05571858212351799\n",
      "Iteration 3499, Loss: 0.05566847324371338\n",
      "Iteration 3500, Loss: 0.055685799568891525\n",
      "Iteration 3501, Loss: 0.055648528039455414\n",
      "Iteration 3502, Loss: 0.05566314980387688\n",
      "Iteration 3503, Loss: 0.05566970631480217\n",
      "Iteration 3504, Loss: 0.05565643683075905\n",
      "Iteration 3505, Loss: 0.05568826571106911\n",
      "Iteration 3506, Loss: 0.05564427375793457\n",
      "Iteration 3507, Loss: 0.05574357509613037\n",
      "Iteration 3508, Loss: 0.05578545853495598\n",
      "Iteration 3509, Loss: 0.055726051330566406\n",
      "Iteration 3510, Loss: 0.05566342920064926\n",
      "Iteration 3511, Loss: 0.055696528404951096\n",
      "Iteration 3512, Loss: 0.05565619468688965\n",
      "Iteration 3513, Loss: 0.0556691512465477\n",
      "Iteration 3514, Loss: 0.055647850036621094\n",
      "Iteration 3515, Loss: 0.05569779872894287\n",
      "Iteration 3516, Loss: 0.055672407150268555\n",
      "Iteration 3517, Loss: 0.05569366738200188\n",
      "Iteration 3518, Loss: 0.055694423615932465\n",
      "Iteration 3519, Loss: 0.05565878003835678\n",
      "Iteration 3520, Loss: 0.05564681813120842\n",
      "Iteration 3521, Loss: 0.055708013474941254\n",
      "Iteration 3522, Loss: 0.05569136515259743\n",
      "Iteration 3523, Loss: 0.05567765608429909\n",
      "Iteration 3524, Loss: 0.055685244500637054\n",
      "Iteration 3525, Loss: 0.055656157433986664\n",
      "Iteration 3526, Loss: 0.055631641298532486\n",
      "Iteration 3527, Loss: 0.055741194635629654\n",
      "Iteration 3528, Loss: 0.055744171142578125\n",
      "Iteration 3529, Loss: 0.05564181134104729\n",
      "Iteration 3530, Loss: 0.05576841160655022\n",
      "Iteration 3531, Loss: 0.05578184127807617\n",
      "Iteration 3532, Loss: 0.05564336106181145\n",
      "Iteration 3533, Loss: 0.055816613137722015\n",
      "Iteration 3534, Loss: 0.05590888112783432\n",
      "Iteration 3535, Loss: 0.05587160587310791\n",
      "Iteration 3536, Loss: 0.055728159844875336\n",
      "Iteration 3537, Loss: 0.05575478449463844\n",
      "Iteration 3538, Loss: 0.055854879319667816\n",
      "Iteration 3539, Loss: 0.055797379463911057\n",
      "Iteration 3540, Loss: 0.05562714859843254\n",
      "Iteration 3541, Loss: 0.0556773766875267\n",
      "Iteration 3542, Loss: 0.05562524124979973\n",
      "Iteration 3543, Loss: 0.05570065975189209\n",
      "Iteration 3544, Loss: 0.05562667176127434\n",
      "Iteration 3545, Loss: 0.0557611808180809\n",
      "Iteration 3546, Loss: 0.05580504983663559\n",
      "Iteration 3547, Loss: 0.05574492737650871\n",
      "Iteration 3548, Loss: 0.05564785376191139\n",
      "Iteration 3549, Loss: 0.05572720617055893\n",
      "Iteration 3550, Loss: 0.05566660687327385\n",
      "Iteration 3551, Loss: 0.05573185533285141\n",
      "Iteration 3552, Loss: 0.055780015885829926\n",
      "Iteration 3553, Loss: 0.05572879686951637\n",
      "Iteration 3554, Loss: 0.055644869804382324\n",
      "Iteration 3555, Loss: 0.05564673990011215\n",
      "Iteration 3556, Loss: 0.05570916458964348\n",
      "Iteration 3557, Loss: 0.055723194032907486\n",
      "Iteration 3558, Loss: 0.05564042180776596\n",
      "Iteration 3559, Loss: 0.05579646676778793\n",
      "Iteration 3560, Loss: 0.05582483857870102\n",
      "Iteration 3561, Loss: 0.05567781254649162\n",
      "Iteration 3562, Loss: 0.055788200348615646\n",
      "Iteration 3563, Loss: 0.05589672178030014\n",
      "Iteration 3564, Loss: 0.05589640513062477\n",
      "Iteration 3565, Loss: 0.0557994470000267\n",
      "Iteration 3566, Loss: 0.0556357316672802\n",
      "Iteration 3567, Loss: 0.05586389824748039\n",
      "Iteration 3568, Loss: 0.05593554303050041\n",
      "Iteration 3569, Loss: 0.05582873150706291\n",
      "Iteration 3570, Loss: 0.05565718933939934\n",
      "Iteration 3571, Loss: 0.0557553805410862\n",
      "Iteration 3572, Loss: 0.055763088166713715\n",
      "Iteration 3573, Loss: 0.055673956871032715\n",
      "Iteration 3574, Loss: 0.05576511472463608\n",
      "Iteration 3575, Loss: 0.0558091402053833\n",
      "Iteration 3576, Loss: 0.05569136142730713\n",
      "Iteration 3577, Loss: 0.055756133049726486\n",
      "Iteration 3578, Loss: 0.05584144964814186\n",
      "Iteration 3579, Loss: 0.055816374719142914\n",
      "Iteration 3580, Loss: 0.05569346994161606\n",
      "Iteration 3581, Loss: 0.055774133652448654\n",
      "Iteration 3582, Loss: 0.05585205554962158\n",
      "Iteration 3583, Loss: 0.055760860443115234\n",
      "Iteration 3584, Loss: 0.05568572133779526\n",
      "Iteration 3585, Loss: 0.05575907230377197\n",
      "Iteration 3586, Loss: 0.05572430416941643\n",
      "Iteration 3587, Loss: 0.055638913065195084\n",
      "Iteration 3588, Loss: 0.055667996406555176\n",
      "Iteration 3589, Loss: 0.055668238550424576\n",
      "Iteration 3590, Loss: 0.05566243454813957\n",
      "Iteration 3591, Loss: 0.055676184594631195\n",
      "Iteration 3592, Loss: 0.05562889575958252\n",
      "Iteration 3593, Loss: 0.055741190910339355\n",
      "Iteration 3594, Loss: 0.05577091500163078\n",
      "Iteration 3595, Loss: 0.055699944496154785\n",
      "Iteration 3596, Loss: 0.05570697784423828\n",
      "Iteration 3597, Loss: 0.05572943016886711\n",
      "Iteration 3598, Loss: 0.055636048316955566\n",
      "Iteration 3599, Loss: 0.05564546585083008\n",
      "Iteration 3600, Loss: 0.05567626282572746\n",
      "Iteration 3601, Loss: 0.055621784180402756\n",
      "Iteration 3602, Loss: 0.055679045617580414\n",
      "Iteration 3603, Loss: 0.05562392994761467\n",
      "Iteration 3604, Loss: 0.055717192590236664\n",
      "Iteration 3605, Loss: 0.05566021054983139\n",
      "Iteration 3606, Loss: 0.05574039742350578\n",
      "Iteration 3607, Loss: 0.05578991025686264\n",
      "Iteration 3608, Loss: 0.05573546886444092\n",
      "Iteration 3609, Loss: 0.05565027520060539\n",
      "Iteration 3610, Loss: 0.05570678040385246\n",
      "Iteration 3611, Loss: 0.05564352124929428\n",
      "Iteration 3612, Loss: 0.055713336914777756\n",
      "Iteration 3613, Loss: 0.055720292031764984\n",
      "Iteration 3614, Loss: 0.05562397092580795\n",
      "Iteration 3615, Loss: 0.05583203211426735\n",
      "Iteration 3616, Loss: 0.05587705224752426\n",
      "Iteration 3617, Loss: 0.05575084686279297\n",
      "Iteration 3618, Loss: 0.055719416588544846\n",
      "Iteration 3619, Loss: 0.05581498146057129\n",
      "Iteration 3620, Loss: 0.05579972267150879\n",
      "Iteration 3621, Loss: 0.05568405240774155\n",
      "Iteration 3622, Loss: 0.05577894300222397\n",
      "Iteration 3623, Loss: 0.0558478869497776\n",
      "Iteration 3624, Loss: 0.05574290081858635\n",
      "Iteration 3625, Loss: 0.0557105578482151\n",
      "Iteration 3626, Loss: 0.055792730301618576\n",
      "Iteration 3627, Loss: 0.05576471611857414\n",
      "Iteration 3628, Loss: 0.05564204975962639\n",
      "Iteration 3629, Loss: 0.055838070809841156\n",
      "Iteration 3630, Loss: 0.055906377732753754\n",
      "Iteration 3631, Loss: 0.055796068161726\n",
      "Iteration 3632, Loss: 0.05567590519785881\n",
      "Iteration 3633, Loss: 0.05576348304748535\n",
      "Iteration 3634, Loss: 0.05574492737650871\n",
      "Iteration 3635, Loss: 0.0556308850646019\n",
      "Iteration 3636, Loss: 0.05584625527262688\n",
      "Iteration 3637, Loss: 0.0559077262878418\n",
      "Iteration 3638, Loss: 0.05579026788473129\n",
      "Iteration 3639, Loss: 0.055685363709926605\n",
      "Iteration 3640, Loss: 0.05577695369720459\n",
      "Iteration 3641, Loss: 0.055763207376003265\n",
      "Iteration 3642, Loss: 0.055654529482126236\n",
      "Iteration 3643, Loss: 0.055812280625104904\n",
      "Iteration 3644, Loss: 0.05587279796600342\n",
      "Iteration 3645, Loss: 0.055754661560058594\n",
      "Iteration 3646, Loss: 0.055711787194013596\n",
      "Iteration 3647, Loss: 0.055803101509809494\n",
      "Iteration 3648, Loss: 0.05578836053609848\n",
      "Iteration 3649, Loss: 0.05567892640829086\n",
      "Iteration 3650, Loss: 0.05578025430440903\n",
      "Iteration 3651, Loss: 0.055841606110334396\n",
      "Iteration 3652, Loss: 0.055724065750837326\n",
      "Iteration 3653, Loss: 0.05573431774973869\n",
      "Iteration 3654, Loss: 0.05582507699728012\n",
      "Iteration 3655, Loss: 0.05580969899892807\n",
      "Iteration 3656, Loss: 0.05570058152079582\n",
      "Iteration 3657, Loss: 0.05575128644704819\n",
      "Iteration 3658, Loss: 0.055812519043684006\n",
      "Iteration 3659, Loss: 0.05569565296173096\n",
      "Iteration 3660, Loss: 0.055755697190761566\n",
      "Iteration 3661, Loss: 0.055846016854047775\n",
      "Iteration 3662, Loss: 0.05583123490214348\n",
      "Iteration 3663, Loss: 0.05572346970438957\n",
      "Iteration 3664, Loss: 0.05572021007537842\n",
      "Iteration 3665, Loss: 0.05578096956014633\n",
      "Iteration 3666, Loss: 0.05566652864217758\n",
      "Iteration 3667, Loss: 0.055773500353097916\n",
      "Iteration 3668, Loss: 0.05586063861846924\n",
      "Iteration 3669, Loss: 0.055844467133283615\n",
      "Iteration 3670, Loss: 0.05573658272624016\n",
      "Iteration 3671, Loss: 0.055702727288007736\n",
      "Iteration 3672, Loss: 0.055763959884643555\n",
      "Iteration 3673, Loss: 0.05565134808421135\n",
      "Iteration 3674, Loss: 0.055779341608285904\n",
      "Iteration 3675, Loss: 0.05586167424917221\n",
      "Iteration 3676, Loss: 0.055841367691755295\n",
      "Iteration 3677, Loss: 0.05572883412241936\n",
      "Iteration 3678, Loss: 0.0557171106338501\n",
      "Iteration 3679, Loss: 0.05578279495239258\n",
      "Iteration 3680, Loss: 0.05567058175802231\n",
      "Iteration 3681, Loss: 0.05576968193054199\n",
      "Iteration 3682, Loss: 0.055857062339782715\n",
      "Iteration 3683, Loss: 0.055841367691755295\n",
      "Iteration 3684, Loss: 0.05573276802897453\n",
      "Iteration 3685, Loss: 0.05570737645030022\n",
      "Iteration 3686, Loss: 0.055769287049770355\n",
      "Iteration 3687, Loss: 0.055654048919677734\n",
      "Iteration 3688, Loss: 0.055784307420253754\n",
      "Iteration 3689, Loss: 0.05587379261851311\n",
      "Iteration 3690, Loss: 0.055859845131635666\n",
      "Iteration 3691, Loss: 0.05575331300497055\n",
      "Iteration 3692, Loss: 0.05567721650004387\n",
      "Iteration 3693, Loss: 0.055737774819135666\n",
      "Iteration 3694, Loss: 0.055624645203351974\n",
      "Iteration 3695, Loss: 0.05578986927866936\n",
      "Iteration 3696, Loss: 0.055862508714199066\n",
      "Iteration 3697, Loss: 0.05583016201853752\n",
      "Iteration 3698, Loss: 0.05570606514811516\n",
      "Iteration 3699, Loss: 0.055762410163879395\n",
      "Iteration 3700, Loss: 0.05583978071808815\n",
      "Iteration 3701, Loss: 0.05573725700378418\n",
      "Iteration 3702, Loss: 0.05571536347270012\n",
      "Iteration 3703, Loss: 0.05579690262675285\n",
      "Iteration 3704, Loss: 0.05577520653605461\n",
      "Iteration 3705, Loss: 0.05566354840993881\n",
      "Iteration 3706, Loss: 0.0558038167655468\n",
      "Iteration 3707, Loss: 0.0558650903403759\n",
      "Iteration 3708, Loss: 0.05574663728475571\n",
      "Iteration 3709, Loss: 0.055718664079904556\n",
      "Iteration 3710, Loss: 0.055809974670410156\n",
      "Iteration 3711, Loss: 0.055797938257455826\n",
      "Iteration 3712, Loss: 0.05569426342844963\n",
      "Iteration 3713, Loss: 0.055754583328962326\n",
      "Iteration 3714, Loss: 0.05580965802073479\n",
      "Iteration 3715, Loss: 0.0556870736181736\n",
      "Iteration 3716, Loss: 0.05576471611857414\n",
      "Iteration 3717, Loss: 0.05585825443267822\n",
      "Iteration 3718, Loss: 0.05584828183054924\n",
      "Iteration 3719, Loss: 0.055745404213666916\n",
      "Iteration 3720, Loss: 0.05568353459239006\n",
      "Iteration 3721, Loss: 0.055739205330610275\n",
      "Iteration 3722, Loss: 0.05562615767121315\n",
      "Iteration 3723, Loss: 0.05577202886343002\n",
      "Iteration 3724, Loss: 0.05582460016012192\n",
      "Iteration 3725, Loss: 0.05577179044485092\n",
      "Iteration 3726, Loss: 0.05564117804169655\n",
      "Iteration 3727, Loss: 0.05583016201853752\n",
      "Iteration 3728, Loss: 0.055883049964904785\n",
      "Iteration 3729, Loss: 0.05575927346944809\n",
      "Iteration 3730, Loss: 0.0557125024497509\n",
      "Iteration 3731, Loss: 0.05580703541636467\n",
      "Iteration 3732, Loss: 0.055798135697841644\n",
      "Iteration 3733, Loss: 0.055696528404951096\n",
      "Iteration 3734, Loss: 0.0557481087744236\n",
      "Iteration 3735, Loss: 0.0558016300201416\n",
      "Iteration 3736, Loss: 0.055677853524684906\n",
      "Iteration 3737, Loss: 0.05577242374420166\n",
      "Iteration 3738, Loss: 0.05586695671081543\n",
      "Iteration 3739, Loss: 0.055857740342617035\n",
      "Iteration 3740, Loss: 0.05575549602508545\n",
      "Iteration 3741, Loss: 0.055669110268354416\n",
      "Iteration 3742, Loss: 0.05572597309947014\n",
      "Iteration 3743, Loss: 0.055622342973947525\n",
      "Iteration 3744, Loss: 0.05571127310395241\n",
      "Iteration 3745, Loss: 0.055694423615932465\n",
      "Iteration 3746, Loss: 0.055662237107753754\n",
      "Iteration 3747, Loss: 0.055653415620326996\n",
      "Iteration 3748, Loss: 0.055711351335048676\n",
      "Iteration 3749, Loss: 0.05572235584259033\n",
      "Iteration 3750, Loss: 0.05564284697175026\n",
      "Iteration 3751, Loss: 0.0557682141661644\n",
      "Iteration 3752, Loss: 0.0557703971862793\n",
      "Iteration 3753, Loss: 0.05561971664428711\n",
      "Iteration 3754, Loss: 0.05565810576081276\n",
      "Iteration 3755, Loss: 0.05562710762023926\n",
      "Iteration 3756, Loss: 0.05567558854818344\n",
      "Iteration 3757, Loss: 0.05564713478088379\n",
      "Iteration 3758, Loss: 0.05572430416941643\n",
      "Iteration 3759, Loss: 0.05569915100932121\n",
      "Iteration 3760, Loss: 0.0556899718940258\n",
      "Iteration 3761, Loss: 0.05572414770722389\n",
      "Iteration 3762, Loss: 0.055659692734479904\n",
      "Iteration 3763, Loss: 0.05575180426239967\n",
      "Iteration 3764, Loss: 0.055765632539987564\n",
      "Iteration 3765, Loss: 0.05562039464712143\n",
      "Iteration 3766, Loss: 0.05575331300497055\n",
      "Iteration 3767, Loss: 0.0557689294219017\n",
      "Iteration 3768, Loss: 0.055674634873867035\n",
      "Iteration 3769, Loss: 0.05575863644480705\n",
      "Iteration 3770, Loss: 0.05580683797597885\n",
      "Iteration 3771, Loss: 0.055700741708278656\n",
      "Iteration 3772, Loss: 0.05574433133006096\n",
      "Iteration 3773, Loss: 0.0558222159743309\n",
      "Iteration 3774, Loss: 0.05578140541911125\n",
      "Iteration 3775, Loss: 0.055653177201747894\n",
      "Iteration 3776, Loss: 0.05581256002187729\n",
      "Iteration 3777, Loss: 0.05587252229452133\n",
      "Iteration 3778, Loss: 0.05576511472463608\n",
      "Iteration 3779, Loss: 0.0556941032409668\n",
      "Iteration 3780, Loss: 0.05577731132507324\n",
      "Iteration 3781, Loss: 0.055751245468854904\n",
      "Iteration 3782, Loss: 0.05563589185476303\n",
      "Iteration 3783, Loss: 0.055825673043727875\n",
      "Iteration 3784, Loss: 0.055873990058898926\n",
      "Iteration 3785, Loss: 0.05574743077158928\n",
      "Iteration 3786, Loss: 0.05572223663330078\n",
      "Iteration 3787, Loss: 0.055818282067775726\n",
      "Iteration 3788, Loss: 0.05580989643931389\n",
      "Iteration 3789, Loss: 0.05570749565958977\n",
      "Iteration 3790, Loss: 0.055733922868967056\n",
      "Iteration 3791, Loss: 0.0557892732322216\n",
      "Iteration 3792, Loss: 0.0556691512465477\n",
      "Iteration 3793, Loss: 0.055776797235012054\n",
      "Iteration 3794, Loss: 0.055868666619062424\n",
      "Iteration 3795, Loss: 0.05585610866546631\n",
      "Iteration 3796, Loss: 0.05575060844421387\n",
      "Iteration 3797, Loss: 0.055681467056274414\n",
      "Iteration 3798, Loss: 0.05574468895792961\n",
      "Iteration 3799, Loss: 0.055639348924160004\n",
      "Iteration 3800, Loss: 0.055779021233320236\n",
      "Iteration 3801, Loss: 0.05585305020213127\n",
      "Iteration 3802, Loss: 0.05582451820373535\n",
      "Iteration 3803, Loss: 0.05570435896515846\n",
      "Iteration 3804, Loss: 0.05575990676879883\n",
      "Iteration 3805, Loss: 0.05583314225077629\n",
      "Iteration 3806, Loss: 0.0557255744934082\n",
      "Iteration 3807, Loss: 0.05572664737701416\n",
      "Iteration 3808, Loss: 0.055811647325754166\n",
      "Iteration 3809, Loss: 0.055793724954128265\n",
      "Iteration 3810, Loss: 0.055683813989162445\n",
      "Iteration 3811, Loss: 0.05577552691102028\n",
      "Iteration 3812, Loss: 0.055837392807006836\n",
      "Iteration 3813, Loss: 0.05571925640106201\n",
      "Iteration 3814, Loss: 0.05573785677552223\n",
      "Iteration 3815, Loss: 0.0558290109038353\n",
      "Iteration 3816, Loss: 0.055816493928432465\n",
      "Iteration 3817, Loss: 0.055711667984724045\n",
      "Iteration 3818, Loss: 0.055731456726789474\n",
      "Iteration 3819, Loss: 0.055788278579711914\n",
      "Iteration 3820, Loss: 0.05566692352294922\n",
      "Iteration 3821, Loss: 0.05577802658081055\n",
      "Iteration 3822, Loss: 0.05587053671479225\n",
      "Iteration 3823, Loss: 0.05585968866944313\n",
      "Iteration 3824, Loss: 0.05575553700327873\n",
      "Iteration 3825, Loss: 0.055670980364084244\n",
      "Iteration 3826, Loss: 0.05572724714875221\n",
      "Iteration 3827, Loss: 0.05562245845794678\n",
      "Iteration 3828, Loss: 0.05572100728750229\n",
      "Iteration 3829, Loss: 0.05571071431040764\n",
      "Iteration 3830, Loss: 0.055643241852521896\n",
      "Iteration 3831, Loss: 0.05567976087331772\n",
      "Iteration 3832, Loss: 0.055650196969509125\n",
      "Iteration 3833, Loss: 0.055651627480983734\n",
      "Iteration 3834, Loss: 0.05565977096557617\n",
      "Iteration 3835, Loss: 0.055652301758527756\n",
      "Iteration 3836, Loss: 0.05564077943563461\n",
      "Iteration 3837, Loss: 0.05568563938140869\n",
      "Iteration 3838, Loss: 0.05562881752848625\n",
      "Iteration 3839, Loss: 0.05572156235575676\n",
      "Iteration 3840, Loss: 0.05571456998586655\n",
      "Iteration 3841, Loss: 0.05564038082957268\n",
      "Iteration 3842, Loss: 0.055685561150312424\n",
      "Iteration 3843, Loss: 0.05564066022634506\n",
      "Iteration 3844, Loss: 0.05566386505961418\n",
      "Iteration 3845, Loss: 0.0556255578994751\n",
      "Iteration 3846, Loss: 0.05574103444814682\n",
      "Iteration 3847, Loss: 0.055745404213666916\n",
      "Iteration 3848, Loss: 0.05565071478486061\n",
      "Iteration 3849, Loss: 0.055770598351955414\n",
      "Iteration 3850, Loss: 0.05579324811697006\n",
      "Iteration 3851, Loss: 0.05565575882792473\n",
      "Iteration 3852, Loss: 0.055806081742048264\n",
      "Iteration 3853, Loss: 0.05590486526489258\n",
      "Iteration 3854, Loss: 0.055881183594465256\n",
      "Iteration 3855, Loss: 0.055754464119672775\n",
      "Iteration 3856, Loss: 0.055712103843688965\n",
      "Iteration 3857, Loss: 0.05580449476838112\n",
      "Iteration 3858, Loss: 0.05574147030711174\n",
      "Iteration 3859, Loss: 0.055689454078674316\n",
      "Iteration 3860, Loss: 0.05574266240000725\n",
      "Iteration 3861, Loss: 0.05569911375641823\n",
      "Iteration 3862, Loss: 0.055684249848127365\n",
      "Iteration 3863, Loss: 0.05569478124380112\n",
      "Iteration 3864, Loss: 0.05567232891917229\n",
      "Iteration 3865, Loss: 0.05568035691976547\n",
      "Iteration 3866, Loss: 0.055656593292951584\n",
      "Iteration 3867, Loss: 0.05568655580282211\n",
      "Iteration 3868, Loss: 0.0556512288749218\n",
      "Iteration 3869, Loss: 0.05569283291697502\n",
      "Iteration 3870, Loss: 0.0556797981262207\n",
      "Iteration 3871, Loss: 0.055678367614746094\n",
      "Iteration 3872, Loss: 0.05566160008311272\n",
      "Iteration 3873, Loss: 0.055703602731227875\n",
      "Iteration 3874, Loss: 0.05571107193827629\n",
      "Iteration 3875, Loss: 0.05562993139028549\n",
      "Iteration 3876, Loss: 0.055710673332214355\n",
      "Iteration 3877, Loss: 0.0556415356695652\n",
      "Iteration 3878, Loss: 0.05575963109731674\n",
      "Iteration 3879, Loss: 0.055818162858486176\n",
      "Iteration 3880, Loss: 0.055776599794626236\n",
      "Iteration 3881, Loss: 0.05564550682902336\n",
      "Iteration 3882, Loss: 0.05585360527038574\n",
      "Iteration 3883, Loss: 0.05593876168131828\n",
      "Iteration 3884, Loss: 0.05584089085459709\n",
      "Iteration 3885, Loss: 0.055635929107666016\n",
      "Iteration 3886, Loss: 0.05571568012237549\n",
      "Iteration 3887, Loss: 0.055694542825222015\n",
      "Iteration 3888, Loss: 0.05565258115530014\n",
      "Iteration 3889, Loss: 0.055621188133955\n",
      "Iteration 3890, Loss: 0.05574238672852516\n",
      "Iteration 3891, Loss: 0.055764954537153244\n",
      "Iteration 3892, Loss: 0.05568766966462135\n",
      "Iteration 3893, Loss: 0.055729351937770844\n",
      "Iteration 3894, Loss: 0.05575982853770256\n",
      "Iteration 3895, Loss: 0.055624328553676605\n",
      "Iteration 3896, Loss: 0.0558244027197361\n",
      "Iteration 3897, Loss: 0.05592501163482666\n",
      "Iteration 3898, Loss: 0.05591698735952377\n",
      "Iteration 3899, Loss: 0.05581315606832504\n",
      "Iteration 3900, Loss: 0.05565222352743149\n",
      "Iteration 3901, Loss: 0.055857103317976\n",
      "Iteration 3902, Loss: 0.055934589356184006\n",
      "Iteration 3903, Loss: 0.05583266541361809\n",
      "Iteration 3904, Loss: 0.05564828962087631\n",
      "Iteration 3905, Loss: 0.05573872849345207\n",
      "Iteration 3906, Loss: 0.05573483556509018\n",
      "Iteration 3907, Loss: 0.05564026162028313\n",
      "Iteration 3908, Loss: 0.05581367388367653\n",
      "Iteration 3909, Loss: 0.055856745690107346\n",
      "Iteration 3910, Loss: 0.055726371705532074\n",
      "Iteration 3911, Loss: 0.05574099346995354\n",
      "Iteration 3912, Loss: 0.05583830922842026\n",
      "Iteration 3913, Loss: 0.0558270625770092\n",
      "Iteration 3914, Loss: 0.05571766942739487\n",
      "Iteration 3915, Loss: 0.05572887510061264\n",
      "Iteration 3916, Loss: 0.05579177662730217\n",
      "Iteration 3917, Loss: 0.0556790865957737\n",
      "Iteration 3918, Loss: 0.05576566979289055\n",
      "Iteration 3919, Loss: 0.055852655321359634\n",
      "Iteration 3920, Loss: 0.05583111569285393\n",
      "Iteration 3921, Loss: 0.05571580305695534\n",
      "Iteration 3922, Loss: 0.05573960393667221\n",
      "Iteration 3923, Loss: 0.05580854415893555\n",
      "Iteration 3924, Loss: 0.05570181459188461\n",
      "Iteration 3925, Loss: 0.055744968354701996\n",
      "Iteration 3926, Loss: 0.05582825466990471\n",
      "Iteration 3927, Loss: 0.05580655857920647\n",
      "Iteration 3928, Loss: 0.055694740265607834\n",
      "Iteration 3929, Loss: 0.05576416105031967\n",
      "Iteration 3930, Loss: 0.055827539414167404\n",
      "Iteration 3931, Loss: 0.055713098496198654\n",
      "Iteration 3932, Loss: 0.05574127286672592\n",
      "Iteration 3933, Loss: 0.05582968518137932\n",
      "Iteration 3934, Loss: 0.055814228951931\n",
      "Iteration 3935, Loss: 0.05570753663778305\n",
      "Iteration 3936, Loss: 0.05574099346995354\n",
      "Iteration 3937, Loss: 0.055799566209316254\n",
      "Iteration 3938, Loss: 0.055681947618722916\n",
      "Iteration 3939, Loss: 0.055765073746442795\n",
      "Iteration 3940, Loss: 0.05585499852895737\n",
      "Iteration 3941, Loss: 0.05584152787923813\n",
      "Iteration 3942, Loss: 0.05573586747050285\n",
      "Iteration 3943, Loss: 0.05570070073008537\n",
      "Iteration 3944, Loss: 0.05575915426015854\n",
      "Iteration 3945, Loss: 0.055645108222961426\n",
      "Iteration 3946, Loss: 0.055782441049814224\n",
      "Iteration 3947, Loss: 0.05586318299174309\n",
      "Iteration 3948, Loss: 0.055840931832790375\n",
      "Iteration 3949, Loss: 0.05572652816772461\n",
      "Iteration 3950, Loss: 0.05572303384542465\n",
      "Iteration 3951, Loss: 0.0557912215590477\n",
      "Iteration 3952, Loss: 0.055682223290205\n",
      "Iteration 3953, Loss: 0.05575947090983391\n",
      "Iteration 3954, Loss: 0.05584518238902092\n",
      "Iteration 3955, Loss: 0.05582726001739502\n",
      "Iteration 3956, Loss: 0.055717431008815765\n",
      "Iteration 3957, Loss: 0.05572986602783203\n",
      "Iteration 3958, Loss: 0.0557934045791626\n",
      "Iteration 3959, Loss: 0.055679403245449066\n",
      "Iteration 3960, Loss: 0.05576479807496071\n",
      "Iteration 3961, Loss: 0.055853329598903656\n",
      "Iteration 3962, Loss: 0.05583874508738518\n",
      "Iteration 3963, Loss: 0.055731575936079025\n",
      "Iteration 3964, Loss: 0.05570753663778305\n",
      "Iteration 3965, Loss: 0.055768173187971115\n",
      "Iteration 3966, Loss: 0.055651746690273285\n",
      "Iteration 3967, Loss: 0.055786214768886566\n",
      "Iteration 3968, Loss: 0.05587590113282204\n",
      "Iteration 3969, Loss: 0.05586231127381325\n",
      "Iteration 3970, Loss: 0.05575605481863022\n",
      "Iteration 3971, Loss: 0.05567304417490959\n",
      "Iteration 3972, Loss: 0.055732373148202896\n",
      "Iteration 3973, Loss: 0.05561832711100578\n",
      "Iteration 3974, Loss: 0.05577266588807106\n",
      "Iteration 3975, Loss: 0.05581887811422348\n",
      "Iteration 3976, Loss: 0.055758118629455566\n",
      "Iteration 3977, Loss: 0.055644117295742035\n",
      "Iteration 3978, Loss: 0.05577019974589348\n",
      "Iteration 3979, Loss: 0.055753353983163834\n",
      "Iteration 3980, Loss: 0.05565440654754639\n",
      "Iteration 3981, Loss: 0.05570109933614731\n",
      "Iteration 3982, Loss: 0.055671535432338715\n",
      "Iteration 3983, Loss: 0.05570058152079582\n",
      "Iteration 3984, Loss: 0.055682223290205\n",
      "Iteration 3985, Loss: 0.055693428963422775\n",
      "Iteration 3986, Loss: 0.055717192590236664\n",
      "Iteration 3987, Loss: 0.05564066022634506\n",
      "Iteration 3988, Loss: 0.055791180580854416\n",
      "Iteration 3989, Loss: 0.05582261085510254\n",
      "Iteration 3990, Loss: 0.055693112313747406\n",
      "Iteration 3991, Loss: 0.0557636059820652\n",
      "Iteration 3992, Loss: 0.055856429040431976\n",
      "Iteration 3993, Loss: 0.0558372363448143\n",
      "Iteration 3994, Loss: 0.055719297379255295\n",
      "Iteration 3995, Loss: 0.055736664682626724\n",
      "Iteration 3996, Loss: 0.0558120422065258\n",
      "Iteration 3997, Loss: 0.05572021007537842\n",
      "Iteration 3998, Loss: 0.055718980729579926\n",
      "Iteration 3999, Loss: 0.055791180580854416\n",
      "Iteration 4000, Loss: 0.05575541779398918\n",
      "Iteration 4001, Loss: 0.055640339851379395\n",
      "Iteration 4002, Loss: 0.05581124871969223\n",
      "Iteration 4003, Loss: 0.055846016854047775\n",
      "Iteration 4004, Loss: 0.055706143379211426\n",
      "Iteration 4005, Loss: 0.055762648582458496\n",
      "Iteration 4006, Loss: 0.05586675927042961\n",
      "Iteration 4007, Loss: 0.055866241455078125\n",
      "Iteration 4008, Loss: 0.055771827697753906\n",
      "Iteration 4009, Loss: 0.055640220642089844\n",
      "Iteration 4010, Loss: 0.055716995149850845\n",
      "Iteration 4011, Loss: 0.05564204975962639\n",
      "Iteration 4012, Loss: 0.055744171142578125\n",
      "Iteration 4013, Loss: 0.055777907371520996\n",
      "Iteration 4014, Loss: 0.055701933801174164\n",
      "Iteration 4015, Loss: 0.055711232125759125\n",
      "Iteration 4016, Loss: 0.05574417486786842\n",
      "Iteration 4017, Loss: 0.05561693757772446\n",
      "Iteration 4018, Loss: 0.055808428674936295\n",
      "Iteration 4019, Loss: 0.0558648519217968\n",
      "Iteration 4020, Loss: 0.055793486535549164\n",
      "Iteration 4021, Loss: 0.055658936500549316\n",
      "Iteration 4022, Loss: 0.05579622834920883\n",
      "Iteration 4023, Loss: 0.05582944676280022\n",
      "Iteration 4024, Loss: 0.05570113658905029\n",
      "Iteration 4025, Loss: 0.05576392263174057\n",
      "Iteration 4026, Loss: 0.05585924908518791\n",
      "Iteration 4027, Loss: 0.055837713181972504\n",
      "Iteration 4028, Loss: 0.05571945756673813\n",
      "Iteration 4029, Loss: 0.0557405948638916\n",
      "Iteration 4030, Loss: 0.05581478402018547\n",
      "Iteration 4031, Loss: 0.05572319030761719\n",
      "Iteration 4032, Loss: 0.05572104454040527\n",
      "Iteration 4033, Loss: 0.05579312890768051\n",
      "Iteration 4034, Loss: 0.05575847625732422\n",
      "Iteration 4035, Loss: 0.055652301758527756\n",
      "Iteration 4036, Loss: 0.05579563230276108\n",
      "Iteration 4037, Loss: 0.055824004113674164\n",
      "Iteration 4038, Loss: 0.05568067356944084\n",
      "Iteration 4039, Loss: 0.05578279495239258\n",
      "Iteration 4040, Loss: 0.05588690564036369\n",
      "Iteration 4041, Loss: 0.05588630959391594\n",
      "Iteration 4042, Loss: 0.055791616439819336\n",
      "Iteration 4043, Loss: 0.05562905594706535\n",
      "Iteration 4044, Loss: 0.05584565922617912\n",
      "Iteration 4045, Loss: 0.05589906498789787\n",
      "Iteration 4046, Loss: 0.055785421282052994\n",
      "Iteration 4047, Loss: 0.055688779801130295\n",
      "Iteration 4048, Loss: 0.05578009411692619\n",
      "Iteration 4049, Loss: 0.05577234551310539\n",
      "Iteration 4050, Loss: 0.05566442385315895\n",
      "Iteration 4051, Loss: 0.055797576904296875\n",
      "Iteration 4052, Loss: 0.05586310476064682\n",
      "Iteration 4053, Loss: 0.05576726049184799\n",
      "Iteration 4054, Loss: 0.055686116218566895\n",
      "Iteration 4055, Loss: 0.05576571077108383\n",
      "Iteration 4056, Loss: 0.05573984235525131\n",
      "Iteration 4057, Loss: 0.05562099069356918\n",
      "Iteration 4058, Loss: 0.055673640221357346\n",
      "Iteration 4059, Loss: 0.055635176599025726\n",
      "Iteration 4060, Loss: 0.055737219750881195\n",
      "Iteration 4061, Loss: 0.055704712867736816\n",
      "Iteration 4062, Loss: 0.05569235607981682\n",
      "Iteration 4063, Loss: 0.055731695145368576\n",
      "Iteration 4064, Loss: 0.05567292496562004\n",
      "Iteration 4065, Loss: 0.0557277612388134\n",
      "Iteration 4066, Loss: 0.05573396012187004\n",
      "Iteration 4067, Loss: 0.05564514920115471\n",
      "Iteration 4068, Loss: 0.05566243454813957\n",
      "Iteration 4069, Loss: 0.05564944073557854\n",
      "Iteration 4070, Loss: 0.05564224720001221\n",
      "Iteration 4071, Loss: 0.055636290460824966\n",
      "Iteration 4072, Loss: 0.05567781254649162\n",
      "Iteration 4073, Loss: 0.0556412935256958\n",
      "Iteration 4074, Loss: 0.05573205277323723\n",
      "Iteration 4075, Loss: 0.05571826547384262\n",
      "Iteration 4076, Loss: 0.05566120147705078\n",
      "Iteration 4077, Loss: 0.055682223290205\n",
      "Iteration 4078, Loss: 0.055637042969465256\n",
      "Iteration 4079, Loss: 0.05563879385590553\n",
      "Iteration 4080, Loss: 0.05565321445465088\n",
      "Iteration 4081, Loss: 0.05565556138753891\n",
      "Iteration 4082, Loss: 0.055626433342695236\n",
      "Iteration 4083, Loss: 0.05575454607605934\n",
      "Iteration 4084, Loss: 0.05572986602783203\n",
      "Iteration 4085, Loss: 0.055667757987976074\n",
      "Iteration 4086, Loss: 0.05570312589406967\n",
      "Iteration 4087, Loss: 0.05563895031809807\n",
      "Iteration 4088, Loss: 0.05578049272298813\n",
      "Iteration 4089, Loss: 0.05579467862844467\n",
      "Iteration 4090, Loss: 0.05564260855317116\n",
      "Iteration 4091, Loss: 0.05581355094909668\n",
      "Iteration 4092, Loss: 0.05591432377696037\n",
      "Iteration 4093, Loss: 0.055898748338222504\n",
      "Iteration 4094, Loss: 0.05578271672129631\n",
      "Iteration 4095, Loss: 0.05566934868693352\n",
      "Iteration 4096, Loss: 0.0557808093726635\n",
      "Iteration 4097, Loss: 0.055744051933288574\n",
      "Iteration 4098, Loss: 0.05567610263824463\n",
      "Iteration 4099, Loss: 0.05572124570608139\n",
      "Iteration 4100, Loss: 0.0556899718940258\n",
      "Iteration 4101, Loss: 0.05568154901266098\n",
      "Iteration 4102, Loss: 0.05567554756999016\n",
      "Iteration 4103, Loss: 0.05568563938140869\n",
      "Iteration 4104, Loss: 0.055695973336696625\n",
      "Iteration 4105, Loss: 0.055638235062360764\n",
      "Iteration 4106, Loss: 0.0557410754263401\n",
      "Iteration 4107, Loss: 0.055728793144226074\n",
      "Iteration 4108, Loss: 0.055657628923654556\n",
      "Iteration 4109, Loss: 0.05570356175303459\n",
      "Iteration 4110, Loss: 0.05565468594431877\n",
      "Iteration 4111, Loss: 0.05574282258749008\n",
      "Iteration 4112, Loss: 0.05577047914266586\n",
      "Iteration 4113, Loss: 0.05568699166178703\n",
      "Iteration 4114, Loss: 0.05573499575257301\n",
      "Iteration 4115, Loss: 0.05577516555786133\n",
      "Iteration 4116, Loss: 0.05566513538360596\n",
      "Iteration 4117, Loss: 0.055782876908779144\n",
      "Iteration 4118, Loss: 0.05586286634206772\n",
      "Iteration 4119, Loss: 0.05581895634531975\n",
      "Iteration 4120, Loss: 0.0556819848716259\n",
      "Iteration 4121, Loss: 0.05579809471964836\n",
      "Iteration 4122, Loss: 0.05587967485189438\n",
      "Iteration 4123, Loss: 0.055795036256313324\n",
      "Iteration 4124, Loss: 0.055653057992458344\n",
      "Iteration 4125, Loss: 0.0557224377989769\n",
      "Iteration 4126, Loss: 0.05567626282572746\n",
      "Iteration 4127, Loss: 0.055711112916469574\n",
      "Iteration 4128, Loss: 0.055716753005981445\n",
      "Iteration 4129, Loss: 0.05564860627055168\n",
      "Iteration 4130, Loss: 0.05565929412841797\n",
      "Iteration 4131, Loss: 0.0556747131049633\n",
      "Iteration 4132, Loss: 0.055636487901210785\n",
      "Iteration 4133, Loss: 0.0557357482612133\n",
      "Iteration 4134, Loss: 0.055740837007761\n",
      "Iteration 4135, Loss: 0.05563712120056152\n",
      "Iteration 4136, Loss: 0.05577671900391579\n",
      "Iteration 4137, Loss: 0.055791858583688736\n",
      "Iteration 4138, Loss: 0.055652737617492676\n",
      "Iteration 4139, Loss: 0.055807195603847504\n",
      "Iteration 4140, Loss: 0.05590132996439934\n",
      "Iteration 4141, Loss: 0.05586854740977287\n",
      "Iteration 4142, Loss: 0.05573026463389397\n",
      "Iteration 4143, Loss: 0.05574715510010719\n",
      "Iteration 4144, Loss: 0.05584311485290527\n",
      "Iteration 4145, Loss: 0.0557788610458374\n",
      "Iteration 4146, Loss: 0.05565003678202629\n",
      "Iteration 4147, Loss: 0.05570169538259506\n",
      "Iteration 4148, Loss: 0.05564463511109352\n",
      "Iteration 4149, Loss: 0.0557536706328392\n",
      "Iteration 4150, Loss: 0.055754899978637695\n",
      "Iteration 4151, Loss: 0.05562965199351311\n",
      "Iteration 4152, Loss: 0.05568766966462135\n",
      "Iteration 4153, Loss: 0.055627427995204926\n",
      "Iteration 4154, Loss: 0.05578077211976051\n",
      "Iteration 4155, Loss: 0.055810652673244476\n",
      "Iteration 4156, Loss: 0.05571004003286362\n",
      "Iteration 4157, Loss: 0.05572943016886711\n",
      "Iteration 4158, Loss: 0.05579587072134018\n",
      "Iteration 4159, Loss: 0.05572756379842758\n",
      "Iteration 4160, Loss: 0.0556870698928833\n",
      "Iteration 4161, Loss: 0.055729787796735764\n",
      "Iteration 4162, Loss: 0.05564681813120842\n",
      "Iteration 4163, Loss: 0.05576809495687485\n",
      "Iteration 4164, Loss: 0.05580047890543938\n",
      "Iteration 4165, Loss: 0.05568981543183327\n",
      "Iteration 4166, Loss: 0.05575597658753395\n",
      "Iteration 4167, Loss: 0.055831871926784515\n",
      "Iteration 4168, Loss: 0.055779021233320236\n",
      "Iteration 4169, Loss: 0.055643800646066666\n",
      "Iteration 4170, Loss: 0.05579706281423569\n",
      "Iteration 4171, Loss: 0.0558273009955883\n",
      "Iteration 4172, Loss: 0.05569601058959961\n",
      "Iteration 4173, Loss: 0.055766504257917404\n",
      "Iteration 4174, Loss: 0.05586334317922592\n",
      "Iteration 4175, Loss: 0.05584704875946045\n",
      "Iteration 4176, Loss: 0.05573391914367676\n",
      "Iteration 4177, Loss: 0.05571635812520981\n",
      "Iteration 4178, Loss: 0.05579034611582756\n",
      "Iteration 4179, Loss: 0.05570324510335922\n",
      "Iteration 4180, Loss: 0.05573197454214096\n",
      "Iteration 4181, Loss: 0.0558011531829834\n",
      "Iteration 4182, Loss: 0.05576698109507561\n",
      "Iteration 4183, Loss: 0.05565599724650383\n",
      "Iteration 4184, Loss: 0.05580425634980202\n",
      "Iteration 4185, Loss: 0.055850982666015625\n",
      "Iteration 4186, Loss: 0.055722832679748535\n",
      "Iteration 4187, Loss: 0.0557430200278759\n",
      "Iteration 4188, Loss: 0.055840056389570236\n",
      "Iteration 4189, Loss: 0.05583369731903076\n",
      "Iteration 4190, Loss: 0.055734556168317795\n",
      "Iteration 4191, Loss: 0.05569478124380112\n",
      "Iteration 4192, Loss: 0.05574778839945793\n",
      "Iteration 4193, Loss: 0.055630963295698166\n",
      "Iteration 4194, Loss: 0.05578557774424553\n",
      "Iteration 4195, Loss: 0.05585809797048569\n",
      "Iteration 4196, Loss: 0.05582603067159653\n",
      "Iteration 4197, Loss: 0.05570201203227043\n",
      "Iteration 4198, Loss: 0.055767498910427094\n",
      "Iteration 4199, Loss: 0.055844902992248535\n",
      "Iteration 4200, Loss: 0.05574182793498039\n",
      "Iteration 4201, Loss: 0.05571186542510986\n",
      "Iteration 4202, Loss: 0.05579396337270737\n",
      "Iteration 4203, Loss: 0.055772148072719574\n",
      "Iteration 4204, Loss: 0.055660367012023926\n",
      "Iteration 4205, Loss: 0.05580858513712883\n",
      "Iteration 4206, Loss: 0.0558701753616333\n",
      "Iteration 4207, Loss: 0.055752042680978775\n",
      "Iteration 4208, Loss: 0.05571440979838371\n",
      "Iteration 4209, Loss: 0.05580572411417961\n",
      "Iteration 4210, Loss: 0.05579356476664543\n",
      "Iteration 4211, Loss: 0.05568941682577133\n",
      "Iteration 4212, Loss: 0.05576102063059807\n",
      "Iteration 4213, Loss: 0.05581669136881828\n",
      "Iteration 4214, Loss: 0.055693745613098145\n",
      "Iteration 4215, Loss: 0.05576002597808838\n",
      "Iteration 4216, Loss: 0.05585412308573723\n",
      "Iteration 4217, Loss: 0.05584431067109108\n",
      "Iteration 4218, Loss: 0.05574135109782219\n",
      "Iteration 4219, Loss: 0.055689215660095215\n",
      "Iteration 4220, Loss: 0.05574456974864006\n",
      "Iteration 4221, Loss: 0.0556289367377758\n",
      "Iteration 4222, Loss: 0.0557832345366478\n",
      "Iteration 4223, Loss: 0.05585130304098129\n",
      "Iteration 4224, Loss: 0.05581530183553696\n",
      "Iteration 4225, Loss: 0.055688660591840744\n",
      "Iteration 4226, Loss: 0.055787328630685806\n",
      "Iteration 4227, Loss: 0.05586802959442139\n",
      "Iteration 4228, Loss: 0.05577254295349121\n",
      "Iteration 4229, Loss: 0.055682383477687836\n",
      "Iteration 4230, Loss: 0.05575963109731674\n",
      "Iteration 4231, Loss: 0.055733125656843185\n",
      "Iteration 4232, Loss: 0.05562293902039528\n",
      "Iteration 4233, Loss: 0.05580262467265129\n",
      "Iteration 4234, Loss: 0.05581120774149895\n",
      "Iteration 4235, Loss: 0.05565023422241211\n",
      "Iteration 4236, Loss: 0.05581454560160637\n",
      "Iteration 4237, Loss: 0.05592545121908188\n",
      "Iteration 4238, Loss: 0.05592477694153786\n",
      "Iteration 4239, Loss: 0.055824361741542816\n",
      "Iteration 4240, Loss: 0.055650513619184494\n",
      "Iteration 4241, Loss: 0.05588607117533684\n",
      "Iteration 4242, Loss: 0.055998604744672775\n",
      "Iteration 4243, Loss: 0.05592549219727516\n",
      "Iteration 4244, Loss: 0.05568695068359375\n",
      "Iteration 4245, Loss: 0.055841684341430664\n",
      "Iteration 4246, Loss: 0.05600452795624733\n",
      "Iteration 4247, Loss: 0.05605538934469223\n",
      "Iteration 4248, Loss: 0.05600377172231674\n",
      "Iteration 4249, Loss: 0.055863380432128906\n",
      "Iteration 4250, Loss: 0.05565027520060539\n",
      "Iteration 4251, Loss: 0.05594206228852272\n",
      "Iteration 4252, Loss: 0.056109391152858734\n",
      "Iteration 4253, Loss: 0.05608610436320305\n",
      "Iteration 4254, Loss: 0.055893223732709885\n",
      "Iteration 4255, Loss: 0.05566596984863281\n",
      "Iteration 4256, Loss: 0.05580989643931389\n",
      "Iteration 4257, Loss: 0.05585678666830063\n",
      "Iteration 4258, Loss: 0.055804572999477386\n",
      "Iteration 4259, Loss: 0.05566231533885002\n",
      "Iteration 4260, Loss: 0.055841170251369476\n",
      "Iteration 4261, Loss: 0.05593971535563469\n",
      "Iteration 4262, Loss: 0.05586211010813713\n",
      "Iteration 4263, Loss: 0.05564848706126213\n",
      "Iteration 4264, Loss: 0.05583099648356438\n",
      "Iteration 4265, Loss: 0.05594595521688461\n",
      "Iteration 4266, Loss: 0.05594329163432121\n",
      "Iteration 4267, Loss: 0.055836718529462814\n",
      "Iteration 4268, Loss: 0.055658578872680664\n",
      "Iteration 4269, Loss: 0.055869024246931076\n",
      "Iteration 4270, Loss: 0.055980049073696136\n",
      "Iteration 4271, Loss: 0.05591476336121559\n",
      "Iteration 4272, Loss: 0.05569068714976311\n",
      "Iteration 4273, Loss: 0.05583183094859123\n",
      "Iteration 4274, Loss: 0.055987559258937836\n",
      "Iteration 4275, Loss: 0.056028883904218674\n",
      "Iteration 4276, Loss: 0.05596780776977539\n",
      "Iteration 4277, Loss: 0.05581720918416977\n",
      "Iteration 4278, Loss: 0.05566660687327385\n",
      "Iteration 4279, Loss: 0.05582758039236069\n",
      "Iteration 4280, Loss: 0.055838070809841156\n",
      "Iteration 4281, Loss: 0.05568695068359375\n",
      "Iteration 4282, Loss: 0.055780887603759766\n",
      "Iteration 4283, Loss: 0.05588654801249504\n",
      "Iteration 4284, Loss: 0.055887579917907715\n",
      "Iteration 4285, Loss: 0.055793605744838715\n",
      "Iteration 4286, Loss: 0.055620670318603516\n",
      "Iteration 4287, Loss: 0.055893223732709885\n",
      "Iteration 4288, Loss: 0.05598581209778786\n",
      "Iteration 4289, Loss: 0.0558985099196434\n",
      "Iteration 4290, Loss: 0.055666446685791016\n",
      "Iteration 4291, Loss: 0.05584847927093506\n",
      "Iteration 4292, Loss: 0.05599598214030266\n",
      "Iteration 4293, Loss: 0.05601934716105461\n",
      "Iteration 4294, Loss: 0.05592998117208481\n",
      "Iteration 4295, Loss: 0.05574842542409897\n",
      "Iteration 4296, Loss: 0.05577266216278076\n",
      "Iteration 4297, Loss: 0.05591082572937012\n",
      "Iteration 4298, Loss: 0.0558730773627758\n",
      "Iteration 4299, Loss: 0.05567391961812973\n",
      "Iteration 4300, Loss: 0.055832505226135254\n",
      "Iteration 4301, Loss: 0.055974166840314865\n",
      "Iteration 4302, Loss: 0.05599578469991684\n",
      "Iteration 4303, Loss: 0.05591090768575668\n",
      "Iteration 4304, Loss: 0.05574179068207741\n",
      "Iteration 4305, Loss: 0.055774930864572525\n",
      "Iteration 4306, Loss: 0.055900853127241135\n",
      "Iteration 4307, Loss: 0.055851977318525314\n",
      "Iteration 4308, Loss: 0.0556485690176487\n",
      "Iteration 4309, Loss: 0.05583592504262924\n",
      "Iteration 4310, Loss: 0.055964432656764984\n",
      "Iteration 4311, Loss: 0.05598358437418938\n",
      "Iteration 4312, Loss: 0.055904828011989594\n",
      "Iteration 4313, Loss: 0.05574234575033188\n",
      "Iteration 4314, Loss: 0.05576356500387192\n",
      "Iteration 4315, Loss: 0.05588388442993164\n",
      "Iteration 4316, Loss: 0.0558222159743309\n",
      "Iteration 4317, Loss: 0.0556415319442749\n",
      "Iteration 4318, Loss: 0.05575430393218994\n",
      "Iteration 4319, Loss: 0.0557883195579052\n",
      "Iteration 4320, Loss: 0.055722080171108246\n",
      "Iteration 4321, Loss: 0.05567276477813721\n",
      "Iteration 4322, Loss: 0.05569203943014145\n",
      "Iteration 4323, Loss: 0.05566195771098137\n",
      "Iteration 4324, Loss: 0.055667560547590256\n",
      "Iteration 4325, Loss: 0.05565901845693588\n",
      "Iteration 4326, Loss: 0.05562242120504379\n",
      "Iteration 4327, Loss: 0.05569326877593994\n",
      "Iteration 4328, Loss: 0.05562440678477287\n",
      "Iteration 4329, Loss: 0.05576789379119873\n",
      "Iteration 4330, Loss: 0.05579793453216553\n",
      "Iteration 4331, Loss: 0.055707454681396484\n",
      "Iteration 4332, Loss: 0.05572060868144035\n",
      "Iteration 4333, Loss: 0.05577492713928223\n",
      "Iteration 4334, Loss: 0.0556817464530468\n",
      "Iteration 4335, Loss: 0.05575307458639145\n",
      "Iteration 4336, Loss: 0.055819712579250336\n",
      "Iteration 4337, Loss: 0.05576189607381821\n",
      "Iteration 4338, Loss: 0.05565333366394043\n",
      "Iteration 4339, Loss: 0.055758558213710785\n",
      "Iteration 4340, Loss: 0.05573960393667221\n",
      "Iteration 4341, Loss: 0.05565810203552246\n",
      "Iteration 4342, Loss: 0.055690012872219086\n",
      "Iteration 4343, Loss: 0.0556495226919651\n",
      "Iteration 4344, Loss: 0.05572346970438957\n",
      "Iteration 4345, Loss: 0.05569271370768547\n",
      "Iteration 4346, Loss: 0.05569779872894287\n",
      "Iteration 4347, Loss: 0.055735234171152115\n",
      "Iteration 4348, Loss: 0.0556773766875267\n",
      "Iteration 4349, Loss: 0.055719971656799316\n",
      "Iteration 4350, Loss: 0.0557255744934082\n",
      "Iteration 4351, Loss: 0.05565110966563225\n",
      "Iteration 4352, Loss: 0.05567272752523422\n",
      "Iteration 4353, Loss: 0.055631041526794434\n",
      "Iteration 4354, Loss: 0.05568572133779526\n",
      "Iteration 4355, Loss: 0.05565977469086647\n",
      "Iteration 4356, Loss: 0.05570177361369133\n",
      "Iteration 4357, Loss: 0.055679403245449066\n",
      "Iteration 4358, Loss: 0.05570098012685776\n",
      "Iteration 4359, Loss: 0.05572617053985596\n",
      "Iteration 4360, Loss: 0.05564983934164047\n",
      "Iteration 4361, Loss: 0.05576865002512932\n",
      "Iteration 4362, Loss: 0.055787764489650726\n",
      "Iteration 4363, Loss: 0.05563950538635254\n",
      "Iteration 4364, Loss: 0.05582030862569809\n",
      "Iteration 4365, Loss: 0.05592580884695053\n",
      "Iteration 4366, Loss: 0.05591408535838127\n",
      "Iteration 4367, Loss: 0.055801115930080414\n",
      "Iteration 4368, Loss: 0.05565985292196274\n",
      "Iteration 4369, Loss: 0.05581521987915039\n",
      "Iteration 4370, Loss: 0.05583624169230461\n",
      "Iteration 4371, Loss: 0.05569247528910637\n",
      "Iteration 4372, Loss: 0.055773377418518066\n",
      "Iteration 4373, Loss: 0.05587514489889145\n",
      "Iteration 4374, Loss: 0.05587351322174072\n",
      "Iteration 4375, Loss: 0.05577906221151352\n",
      "Iteration 4376, Loss: 0.05564026162028313\n",
      "Iteration 4377, Loss: 0.055774569511413574\n",
      "Iteration 4378, Loss: 0.05576261132955551\n",
      "Iteration 4379, Loss: 0.05565444752573967\n",
      "Iteration 4380, Loss: 0.055736344307661057\n",
      "Iteration 4381, Loss: 0.05574532598257065\n",
      "Iteration 4382, Loss: 0.05563465878367424\n",
      "Iteration 4383, Loss: 0.055823247879743576\n",
      "Iteration 4384, Loss: 0.05589652061462402\n",
      "Iteration 4385, Loss: 0.055825673043727875\n",
      "Iteration 4386, Loss: 0.05566481873393059\n",
      "Iteration 4387, Loss: 0.05581057071685791\n",
      "Iteration 4388, Loss: 0.05588909238576889\n",
      "Iteration 4389, Loss: 0.05581927299499512\n",
      "Iteration 4390, Loss: 0.05563187599182129\n",
      "Iteration 4391, Loss: 0.055843155831098557\n",
      "Iteration 4392, Loss: 0.055928152054548264\n",
      "Iteration 4393, Loss: 0.05586354061961174\n",
      "Iteration 4394, Loss: 0.05568039417266846\n",
      "Iteration 4395, Loss: 0.05582328885793686\n",
      "Iteration 4396, Loss: 0.05594523996114731\n",
      "Iteration 4397, Loss: 0.0559234619140625\n",
      "Iteration 4398, Loss: 0.05577632039785385\n",
      "Iteration 4399, Loss: 0.05570908635854721\n",
      "Iteration 4400, Loss: 0.05582098290324211\n",
      "Iteration 4401, Loss: 0.05579523369669914\n",
      "Iteration 4402, Loss: 0.055634140968322754\n",
      "Iteration 4403, Loss: 0.05585766211152077\n",
      "Iteration 4404, Loss: 0.05597337335348129\n",
      "Iteration 4405, Loss: 0.05595298856496811\n",
      "Iteration 4406, Loss: 0.05582042783498764\n",
      "Iteration 4407, Loss: 0.055690448731184006\n",
      "Iteration 4408, Loss: 0.05582380294799805\n",
      "Iteration 4409, Loss: 0.05586302652955055\n",
      "Iteration 4410, Loss: 0.0557536706328392\n",
      "Iteration 4411, Loss: 0.055714212357997894\n",
      "Iteration 4412, Loss: 0.05579487606883049\n",
      "Iteration 4413, Loss: 0.05577035993337631\n",
      "Iteration 4414, Loss: 0.05567622184753418\n",
      "Iteration 4415, Loss: 0.05576566979289055\n",
      "Iteration 4416, Loss: 0.05579805374145508\n",
      "Iteration 4417, Loss: 0.05566903203725815\n",
      "Iteration 4418, Loss: 0.055781763046979904\n",
      "Iteration 4419, Loss: 0.055875182151794434\n",
      "Iteration 4420, Loss: 0.055865250527858734\n",
      "Iteration 4421, Loss: 0.055763326585292816\n",
      "Iteration 4422, Loss: 0.05566549301147461\n",
      "Iteration 4423, Loss: 0.05574449151754379\n",
      "Iteration 4424, Loss: 0.05567169189453125\n",
      "Iteration 4425, Loss: 0.05573531240224838\n",
      "Iteration 4426, Loss: 0.05578883737325668\n",
      "Iteration 4427, Loss: 0.05574147030711174\n",
      "Iteration 4428, Loss: 0.055624209344387054\n",
      "Iteration 4429, Loss: 0.05563875287771225\n",
      "Iteration 4430, Loss: 0.05569358915090561\n",
      "Iteration 4431, Loss: 0.05567101761698723\n",
      "Iteration 4432, Loss: 0.05569462105631828\n",
      "Iteration 4433, Loss: 0.055687546730041504\n",
      "Iteration 4434, Loss: 0.05567586421966553\n",
      "Iteration 4435, Loss: 0.055679403245449066\n",
      "Iteration 4436, Loss: 0.05566195771098137\n",
      "Iteration 4437, Loss: 0.05563628673553467\n",
      "Iteration 4438, Loss: 0.05573968216776848\n",
      "Iteration 4439, Loss: 0.05575438588857651\n",
      "Iteration 4440, Loss: 0.05565929412841797\n",
      "Iteration 4441, Loss: 0.0557711124420166\n",
      "Iteration 4442, Loss: 0.055811408907175064\n",
      "Iteration 4443, Loss: 0.0556952990591526\n",
      "Iteration 4444, Loss: 0.05575629323720932\n",
      "Iteration 4445, Loss: 0.05584045499563217\n",
      "Iteration 4446, Loss: 0.0558013916015625\n",
      "Iteration 4447, Loss: 0.05566469952464104\n",
      "Iteration 4448, Loss: 0.05581462383270264\n",
      "Iteration 4449, Loss: 0.05589429661631584\n",
      "Iteration 4450, Loss: 0.05580441281199455\n",
      "Iteration 4451, Loss: 0.0556512288749218\n",
      "Iteration 4452, Loss: 0.05573447793722153\n",
      "Iteration 4453, Loss: 0.05570320412516594\n",
      "Iteration 4454, Loss: 0.05566807836294174\n",
      "Iteration 4455, Loss: 0.05566668510437012\n",
      "Iteration 4456, Loss: 0.055692195892333984\n",
      "Iteration 4457, Loss: 0.0556952990591526\n",
      "Iteration 4458, Loss: 0.05564340204000473\n",
      "Iteration 4459, Loss: 0.055651310831308365\n",
      "Iteration 4460, Loss: 0.055685244500637054\n",
      "Iteration 4461, Loss: 0.05568186566233635\n",
      "Iteration 4462, Loss: 0.05565027520060539\n",
      "Iteration 4463, Loss: 0.055645667016506195\n",
      "Iteration 4464, Loss: 0.055673640221357346\n",
      "Iteration 4465, Loss: 0.05564868450164795\n",
      "Iteration 4466, Loss: 0.0557221993803978\n",
      "Iteration 4467, Loss: 0.0557069405913353\n",
      "Iteration 4468, Loss: 0.05567137524485588\n",
      "Iteration 4469, Loss: 0.05569370836019516\n",
      "Iteration 4470, Loss: 0.055616579949855804\n",
      "Iteration 4471, Loss: 0.055670104920864105\n",
      "Iteration 4472, Loss: 0.055632274597883224\n",
      "Iteration 4473, Loss: 0.05575660988688469\n",
      "Iteration 4474, Loss: 0.05573944374918938\n",
      "Iteration 4475, Loss: 0.05565611645579338\n",
      "Iteration 4476, Loss: 0.05568739026784897\n",
      "Iteration 4477, Loss: 0.05562317743897438\n",
      "Iteration 4478, Loss: 0.055800240486860275\n",
      "Iteration 4479, Loss: 0.05581267923116684\n",
      "Iteration 4480, Loss: 0.055655717849731445\n",
      "Iteration 4481, Loss: 0.05580819025635719\n",
      "Iteration 4482, Loss: 0.05591853708028793\n",
      "Iteration 4483, Loss: 0.055921874940395355\n",
      "Iteration 4484, Loss: 0.05582869052886963\n",
      "Iteration 4485, Loss: 0.05565488338470459\n",
      "Iteration 4486, Loss: 0.05588706582784653\n",
      "Iteration 4487, Loss: 0.056012991815805435\n",
      "Iteration 4488, Loss: 0.05595477670431137\n",
      "Iteration 4489, Loss: 0.05573078244924545\n",
      "Iteration 4490, Loss: 0.05579948425292969\n",
      "Iteration 4491, Loss: 0.0559542179107666\n",
      "Iteration 4492, Loss: 0.05599864572286606\n",
      "Iteration 4493, Loss: 0.05594345182180405\n",
      "Iteration 4494, Loss: 0.05579885095357895\n",
      "Iteration 4495, Loss: 0.05566251277923584\n",
      "Iteration 4496, Loss: 0.05577012151479721\n",
      "Iteration 4497, Loss: 0.055699191987514496\n",
      "Iteration 4498, Loss: 0.05572144314646721\n",
      "Iteration 4499, Loss: 0.05578390881419182\n",
      "Iteration 4500, Loss: 0.05574699491262436\n",
      "Iteration 4501, Loss: 0.055623769760131836\n",
      "Iteration 4502, Loss: 0.05585881322622299\n",
      "Iteration 4503, Loss: 0.05592195317149162\n",
      "Iteration 4504, Loss: 0.05580580234527588\n",
      "Iteration 4505, Loss: 0.05567368119955063\n",
      "Iteration 4506, Loss: 0.05576622486114502\n",
      "Iteration 4507, Loss: 0.05575565621256828\n",
      "Iteration 4508, Loss: 0.055648211389780045\n",
      "Iteration 4509, Loss: 0.05581887811422348\n",
      "Iteration 4510, Loss: 0.05587911605834961\n",
      "Iteration 4511, Loss: 0.0557633638381958\n",
      "Iteration 4512, Loss: 0.05570312589406967\n",
      "Iteration 4513, Loss: 0.055793166160583496\n",
      "Iteration 4514, Loss: 0.055775683373212814\n",
      "Iteration 4515, Loss: 0.05566088482737541\n",
      "Iteration 4516, Loss: 0.055809300392866135\n",
      "Iteration 4517, Loss: 0.05587681382894516\n",
      "Iteration 4518, Loss: 0.05576737970113754\n",
      "Iteration 4519, Loss: 0.05569569393992424\n",
      "Iteration 4520, Loss: 0.055781684815883636\n",
      "Iteration 4521, Loss: 0.055760424584150314\n",
      "Iteration 4522, Loss: 0.055642686784267426\n",
      "Iteration 4523, Loss: 0.05583556741476059\n",
      "Iteration 4524, Loss: 0.055904190987348557\n",
      "Iteration 4525, Loss: 0.055794477462768555\n",
      "Iteration 4526, Loss: 0.055676382035017014\n",
      "Iteration 4527, Loss: 0.055764198303222656\n",
      "Iteration 4528, Loss: 0.055746160447597504\n",
      "Iteration 4529, Loss: 0.055630963295698166\n",
      "Iteration 4530, Loss: 0.05584847927093506\n",
      "Iteration 4531, Loss: 0.05591448396444321\n",
      "Iteration 4532, Loss: 0.05580294504761696\n",
      "Iteration 4533, Loss: 0.05567201226949692\n",
      "Iteration 4534, Loss: 0.05576257035136223\n",
      "Iteration 4535, Loss: 0.055747948586940765\n",
      "Iteration 4536, Loss: 0.05563453957438469\n",
      "Iteration 4537, Loss: 0.0558423213660717\n",
      "Iteration 4538, Loss: 0.05590884014964104\n",
      "Iteration 4539, Loss: 0.05580047890543938\n",
      "Iteration 4540, Loss: 0.05567216873168945\n",
      "Iteration 4541, Loss: 0.05576304718852043\n",
      "Iteration 4542, Loss: 0.05574870482087135\n",
      "Iteration 4543, Loss: 0.05563155934214592\n",
      "Iteration 4544, Loss: 0.055847130715847015\n",
      "Iteration 4545, Loss: 0.05591928958892822\n",
      "Iteration 4546, Loss: 0.05582308769226074\n",
      "Iteration 4547, Loss: 0.05565687268972397\n",
      "Iteration 4548, Loss: 0.055774252861738205\n",
      "Iteration 4549, Loss: 0.05579730123281479\n",
      "Iteration 4550, Loss: 0.05569811910390854\n",
      "Iteration 4551, Loss: 0.05574492737650871\n",
      "Iteration 4552, Loss: 0.05580739304423332\n",
      "Iteration 4553, Loss: 0.05572497844696045\n",
      "Iteration 4554, Loss: 0.055702172219753265\n",
      "Iteration 4555, Loss: 0.05576173588633537\n",
      "Iteration 4556, Loss: 0.05569612979888916\n",
      "Iteration 4557, Loss: 0.055716078728437424\n",
      "Iteration 4558, Loss: 0.055751919746398926\n",
      "Iteration 4559, Loss: 0.05566056817770004\n",
      "Iteration 4560, Loss: 0.05576002970337868\n",
      "Iteration 4561, Loss: 0.05580369755625725\n",
      "Iteration 4562, Loss: 0.05570829287171364\n",
      "Iteration 4563, Loss: 0.05572724714875221\n",
      "Iteration 4564, Loss: 0.055792175233364105\n",
      "Iteration 4565, Loss: 0.05572446435689926\n",
      "Iteration 4566, Loss: 0.05568516254425049\n",
      "Iteration 4567, Loss: 0.05572915077209473\n",
      "Iteration 4568, Loss: 0.055635493248701096\n",
      "Iteration 4569, Loss: 0.0557987317442894\n",
      "Iteration 4570, Loss: 0.05586322396993637\n",
      "Iteration 4571, Loss: 0.05580202862620354\n",
      "Iteration 4572, Loss: 0.05566772073507309\n",
      "Iteration 4573, Loss: 0.055799804627895355\n",
      "Iteration 4574, Loss: 0.05585813522338867\n",
      "Iteration 4575, Loss: 0.0557582788169384\n",
      "Iteration 4576, Loss: 0.05569394677877426\n",
      "Iteration 4577, Loss: 0.055769406259059906\n",
      "Iteration 4578, Loss: 0.05573296546936035\n",
      "Iteration 4579, Loss: 0.055648207664489746\n",
      "Iteration 4580, Loss: 0.05573161691427231\n",
      "Iteration 4581, Loss: 0.05568420886993408\n",
      "Iteration 4582, Loss: 0.055715881288051605\n",
      "Iteration 4583, Loss: 0.05576149746775627\n",
      "Iteration 4584, Loss: 0.055711351335048676\n",
      "Iteration 4585, Loss: 0.0556672029197216\n",
      "Iteration 4586, Loss: 0.05567348003387451\n",
      "Iteration 4587, Loss: 0.0556848868727684\n",
      "Iteration 4588, Loss: 0.05569728463888168\n",
      "Iteration 4589, Loss: 0.05562440678477287\n",
      "Iteration 4590, Loss: 0.05577477067708969\n",
      "Iteration 4591, Loss: 0.055774688720703125\n",
      "Iteration 4592, Loss: 0.05564646050333977\n",
      "Iteration 4593, Loss: 0.05577421188354492\n",
      "Iteration 4594, Loss: 0.05582022666931152\n",
      "Iteration 4595, Loss: 0.05573594570159912\n",
      "Iteration 4596, Loss: 0.05568818375468254\n",
      "Iteration 4597, Loss: 0.055744171142578125\n",
      "Iteration 4598, Loss: 0.05565758794546127\n",
      "Iteration 4599, Loss: 0.05577055737376213\n",
      "Iteration 4600, Loss: 0.055828772485256195\n",
      "Iteration 4601, Loss: 0.055758558213710785\n",
      "Iteration 4602, Loss: 0.055663906037807465\n",
      "Iteration 4603, Loss: 0.05574393644928932\n",
      "Iteration 4604, Loss: 0.055698953568935394\n",
      "Iteration 4605, Loss: 0.05570729821920395\n",
      "Iteration 4606, Loss: 0.055744968354701996\n",
      "Iteration 4607, Loss: 0.05568361654877663\n",
      "Iteration 4608, Loss: 0.05571647733449936\n",
      "Iteration 4609, Loss: 0.055729709565639496\n",
      "Iteration 4610, Loss: 0.05564141273498535\n",
      "Iteration 4611, Loss: 0.05566795915365219\n",
      "Iteration 4612, Loss: 0.05563807487487793\n",
      "Iteration 4613, Loss: 0.05571882054209709\n",
      "Iteration 4614, Loss: 0.055686354637145996\n",
      "Iteration 4615, Loss: 0.05569803714752197\n",
      "Iteration 4616, Loss: 0.055725257843732834\n",
      "Iteration 4617, Loss: 0.05564276501536369\n",
      "Iteration 4618, Loss: 0.05579209327697754\n",
      "Iteration 4619, Loss: 0.0558321475982666\n",
      "Iteration 4620, Loss: 0.05571750923991203\n",
      "Iteration 4621, Loss: 0.05573515221476555\n",
      "Iteration 4622, Loss: 0.05581879988312721\n",
      "Iteration 4623, Loss: 0.0557861365377903\n",
      "Iteration 4624, Loss: 0.055651985108852386\n",
      "Iteration 4625, Loss: 0.05583091825246811\n",
      "Iteration 4626, Loss: 0.0559137687087059\n",
      "Iteration 4627, Loss: 0.05583059787750244\n",
      "Iteration 4628, Loss: 0.0556412972509861\n",
      "Iteration 4629, Loss: 0.05578760430216789\n",
      "Iteration 4630, Loss: 0.055831555277109146\n",
      "Iteration 4631, Loss: 0.055748581886291504\n",
      "Iteration 4632, Loss: 0.05567491054534912\n",
      "Iteration 4633, Loss: 0.05573483556509018\n",
      "Iteration 4634, Loss: 0.05565262213349342\n",
      "Iteration 4635, Loss: 0.05577198788523674\n",
      "Iteration 4636, Loss: 0.05583183094859123\n",
      "Iteration 4637, Loss: 0.05577508732676506\n",
      "Iteration 4638, Loss: 0.05565941333770752\n",
      "Iteration 4639, Loss: 0.05578593537211418\n",
      "Iteration 4640, Loss: 0.0558089055120945\n",
      "Iteration 4641, Loss: 0.05566927045583725\n",
      "Iteration 4642, Loss: 0.055792491883039474\n",
      "Iteration 4643, Loss: 0.05589481443166733\n",
      "Iteration 4644, Loss: 0.0558905228972435\n",
      "Iteration 4645, Loss: 0.05579213425517082\n",
      "Iteration 4646, Loss: 0.05564972013235092\n",
      "Iteration 4647, Loss: 0.05581840127706528\n",
      "Iteration 4648, Loss: 0.05584537982940674\n",
      "Iteration 4649, Loss: 0.055703166872262955\n",
      "Iteration 4650, Loss: 0.05576598644256592\n",
      "Iteration 4651, Loss: 0.055868785828351974\n",
      "Iteration 4652, Loss: 0.05586616322398186\n",
      "Iteration 4653, Loss: 0.055767301470041275\n",
      "Iteration 4654, Loss: 0.055650316178798676\n",
      "Iteration 4655, Loss: 0.05570129677653313\n",
      "Iteration 4656, Loss: 0.05563954636454582\n",
      "Iteration 4657, Loss: 0.05563000962138176\n",
      "Iteration 4658, Loss: 0.05572677031159401\n",
      "Iteration 4659, Loss: 0.05568023771047592\n",
      "Iteration 4660, Loss: 0.05571933835744858\n",
      "Iteration 4661, Loss: 0.0557662658393383\n",
      "Iteration 4662, Loss: 0.055711906403303146\n",
      "Iteration 4663, Loss: 0.05567161366343498\n",
      "Iteration 4664, Loss: 0.055677734315395355\n",
      "Iteration 4665, Loss: 0.05568647384643555\n",
      "Iteration 4666, Loss: 0.05570137873291969\n",
      "Iteration 4667, Loss: 0.05562913417816162\n",
      "Iteration 4668, Loss: 0.05578228086233139\n",
      "Iteration 4669, Loss: 0.05578216165304184\n",
      "Iteration 4670, Loss: 0.05563144013285637\n",
      "Iteration 4671, Loss: 0.05577544495463371\n",
      "Iteration 4672, Loss: 0.055825553834438324\n",
      "Iteration 4673, Loss: 0.055763959884643555\n",
      "Iteration 4674, Loss: 0.055643998086452484\n",
      "Iteration 4675, Loss: 0.055773258209228516\n",
      "Iteration 4676, Loss: 0.05576201528310776\n",
      "Iteration 4677, Loss: 0.05564042180776596\n",
      "Iteration 4678, Loss: 0.05568655580282211\n",
      "Iteration 4679, Loss: 0.055651985108852386\n",
      "Iteration 4680, Loss: 0.05572843551635742\n",
      "Iteration 4681, Loss: 0.055710118263959885\n",
      "Iteration 4682, Loss: 0.055677417665719986\n",
      "Iteration 4683, Loss: 0.0557074174284935\n",
      "Iteration 4684, Loss: 0.055637720972299576\n",
      "Iteration 4685, Loss: 0.0557885579764843\n",
      "Iteration 4686, Loss: 0.055810652673244476\n",
      "Iteration 4687, Loss: 0.05567093938589096\n",
      "Iteration 4688, Loss: 0.055787764489650726\n",
      "Iteration 4689, Loss: 0.05588440224528313\n",
      "Iteration 4690, Loss: 0.0558626651763916\n",
      "Iteration 4691, Loss: 0.05573872849345207\n",
      "Iteration 4692, Loss: 0.05572156235575676\n",
      "Iteration 4693, Loss: 0.05580425262451172\n",
      "Iteration 4694, Loss: 0.05571981519460678\n",
      "Iteration 4695, Loss: 0.05571766942739487\n",
      "Iteration 4696, Loss: 0.055785417556762695\n",
      "Iteration 4697, Loss: 0.055744968354701996\n",
      "Iteration 4698, Loss: 0.055645547807216644\n",
      "Iteration 4699, Loss: 0.05576753616333008\n",
      "Iteration 4700, Loss: 0.05575474351644516\n",
      "Iteration 4701, Loss: 0.05565150827169418\n",
      "Iteration 4702, Loss: 0.055699944496154785\n",
      "Iteration 4703, Loss: 0.05567225068807602\n",
      "Iteration 4704, Loss: 0.05569859594106674\n",
      "Iteration 4705, Loss: 0.0556795597076416\n",
      "Iteration 4706, Loss: 0.05569549649953842\n",
      "Iteration 4707, Loss: 0.05571850389242172\n",
      "Iteration 4708, Loss: 0.055639903992414474\n",
      "Iteration 4709, Loss: 0.05579475685954094\n",
      "Iteration 4710, Loss: 0.05582809820771217\n",
      "Iteration 4711, Loss: 0.05570002645254135\n",
      "Iteration 4712, Loss: 0.05575820058584213\n",
      "Iteration 4713, Loss: 0.055851105600595474\n",
      "Iteration 4714, Loss: 0.0558323860168457\n",
      "Iteration 4715, Loss: 0.0557149276137352\n",
      "Iteration 4716, Loss: 0.055741988122463226\n",
      "Iteration 4717, Loss: 0.055816177278757095\n",
      "Iteration 4718, Loss: 0.055722832679748535\n",
      "Iteration 4719, Loss: 0.055718183517456055\n",
      "Iteration 4720, Loss: 0.05579134076833725\n",
      "Iteration 4721, Loss: 0.05575649067759514\n",
      "Iteration 4722, Loss: 0.055639784783124924\n",
      "Iteration 4723, Loss: 0.055817048996686935\n",
      "Iteration 4724, Loss: 0.055857423692941666\n",
      "Iteration 4725, Loss: 0.05572263523936272\n",
      "Iteration 4726, Loss: 0.055747389793395996\n",
      "Iteration 4727, Loss: 0.055848442018032074\n",
      "Iteration 4728, Loss: 0.05584506317973137\n",
      "Iteration 4729, Loss: 0.05574802681803703\n",
      "Iteration 4730, Loss: 0.05567439645528793\n",
      "Iteration 4731, Loss: 0.05572732537984848\n",
      "Iteration 4732, Loss: 0.05562683194875717\n",
      "Iteration 4733, Loss: 0.05571584030985832\n",
      "Iteration 4734, Loss: 0.05570896714925766\n",
      "Iteration 4735, Loss: 0.055635493248701096\n",
      "Iteration 4736, Loss: 0.05567626282572746\n",
      "Iteration 4737, Loss: 0.05564967915415764\n",
      "Iteration 4738, Loss: 0.05564669892191887\n",
      "Iteration 4739, Loss: 0.05568206310272217\n",
      "Iteration 4740, Loss: 0.05563279241323471\n",
      "Iteration 4741, Loss: 0.05572076886892319\n",
      "Iteration 4742, Loss: 0.05571341514587402\n",
      "Iteration 4743, Loss: 0.05564320087432861\n",
      "Iteration 4744, Loss: 0.05567586421966553\n",
      "Iteration 4745, Loss: 0.05565440654754639\n",
      "Iteration 4746, Loss: 0.05564673990011215\n",
      "Iteration 4747, Loss: 0.05567920207977295\n",
      "Iteration 4748, Loss: 0.055626992136240005\n",
      "Iteration 4749, Loss: 0.05570554733276367\n",
      "Iteration 4750, Loss: 0.05569084733724594\n",
      "Iteration 4751, Loss: 0.05566156283020973\n",
      "Iteration 4752, Loss: 0.05564558878540993\n",
      "Iteration 4753, Loss: 0.05572355166077614\n",
      "Iteration 4754, Loss: 0.0557425431907177\n",
      "Iteration 4755, Loss: 0.055663030594587326\n",
      "Iteration 4756, Loss: 0.055760860443115234\n",
      "Iteration 4757, Loss: 0.05578581616282463\n",
      "Iteration 4758, Loss: 0.05564121529459953\n",
      "Iteration 4759, Loss: 0.055819034576416016\n",
      "Iteration 4760, Loss: 0.05592461675405502\n",
      "Iteration 4761, Loss: 0.055916231125593185\n",
      "Iteration 4762, Loss: 0.05580791085958481\n",
      "Iteration 4763, Loss: 0.05565575882792473\n",
      "Iteration 4764, Loss: 0.055836718529462814\n",
      "Iteration 4765, Loss: 0.05588635057210922\n",
      "Iteration 4766, Loss: 0.05576145648956299\n",
      "Iteration 4767, Loss: 0.05571623891592026\n",
      "Iteration 4768, Loss: 0.055810414254665375\n",
      "Iteration 4769, Loss: 0.05580512806773186\n",
      "Iteration 4770, Loss: 0.05571047589182854\n",
      "Iteration 4771, Loss: 0.05572656914591789\n",
      "Iteration 4772, Loss: 0.055772941559553146\n",
      "Iteration 4773, Loss: 0.0556563138961792\n",
      "Iteration 4774, Loss: 0.055772267282009125\n",
      "Iteration 4775, Loss: 0.055848561227321625\n",
      "Iteration 4776, Loss: 0.055821143090724945\n",
      "Iteration 4777, Loss: 0.05570078268647194\n",
      "Iteration 4778, Loss: 0.05576459690928459\n",
      "Iteration 4779, Loss: 0.055839501321315765\n",
      "Iteration 4780, Loss: 0.05573884770274162\n",
      "Iteration 4781, Loss: 0.055711548775434494\n",
      "Iteration 4782, Loss: 0.05579157918691635\n",
      "Iteration 4783, Loss: 0.05576777830719948\n",
      "Iteration 4784, Loss: 0.0556514672935009\n",
      "Iteration 4785, Loss: 0.055824678391218185\n",
      "Iteration 4786, Loss: 0.055892907083034515\n",
      "Iteration 4787, Loss: 0.05578450486063957\n",
      "Iteration 4788, Loss: 0.05568321794271469\n",
      "Iteration 4789, Loss: 0.0557689294219017\n",
      "Iteration 4790, Loss: 0.05575168505311012\n",
      "Iteration 4791, Loss: 0.05564066022634506\n",
      "Iteration 4792, Loss: 0.05583401769399643\n",
      "Iteration 4793, Loss: 0.055898189544677734\n",
      "Iteration 4794, Loss: 0.05578720569610596\n",
      "Iteration 4795, Loss: 0.05568361654877663\n",
      "Iteration 4796, Loss: 0.055771354585886\n",
      "Iteration 4797, Loss: 0.05575672909617424\n",
      "Iteration 4798, Loss: 0.05564725399017334\n",
      "Iteration 4799, Loss: 0.05582340806722641\n",
      "Iteration 4800, Loss: 0.05588734522461891\n",
      "Iteration 4801, Loss: 0.05577930063009262\n",
      "Iteration 4802, Loss: 0.05568715184926987\n",
      "Iteration 4803, Loss: 0.0557732991874218\n",
      "Iteration 4804, Loss: 0.05575653165578842\n",
      "Iteration 4805, Loss: 0.055642567574977875\n",
      "Iteration 4806, Loss: 0.05583294481039047\n",
      "Iteration 4807, Loss: 0.055902402848005295\n",
      "Iteration 4808, Loss: 0.05580282211303711\n",
      "Iteration 4809, Loss: 0.05566581338644028\n",
      "Iteration 4810, Loss: 0.055756211280822754\n",
      "Iteration 4811, Loss: 0.05574687570333481\n",
      "Iteration 4812, Loss: 0.055632155388593674\n",
      "Iteration 4813, Loss: 0.05583997815847397\n",
      "Iteration 4814, Loss: 0.05591360852122307\n",
      "Iteration 4815, Loss: 0.05583060160279274\n",
      "Iteration 4816, Loss: 0.055658143013715744\n",
      "Iteration 4817, Loss: 0.05580294132232666\n",
      "Iteration 4818, Loss: 0.05586811155080795\n",
      "Iteration 4819, Loss: 0.05579841509461403\n",
      "Iteration 4820, Loss: 0.055632513016462326\n",
      "Iteration 4821, Loss: 0.05580612272024155\n",
      "Iteration 4822, Loss: 0.055837951600551605\n",
      "Iteration 4823, Loss: 0.0557129792869091\n",
      "Iteration 4824, Loss: 0.055747270584106445\n",
      "Iteration 4825, Loss: 0.05583870783448219\n",
      "Iteration 4826, Loss: 0.05581287667155266\n",
      "Iteration 4827, Loss: 0.05568770691752434\n",
      "Iteration 4828, Loss: 0.055781006813049316\n",
      "Iteration 4829, Loss: 0.055859606713056564\n",
      "Iteration 4830, Loss: 0.055773817002773285\n",
      "Iteration 4831, Loss: 0.055670738220214844\n",
      "Iteration 4832, Loss: 0.05574067682027817\n",
      "Iteration 4833, Loss: 0.05569859594106674\n",
      "Iteration 4834, Loss: 0.05568031594157219\n",
      "Iteration 4835, Loss: 0.055683933198451996\n",
      "Iteration 4836, Loss: 0.05567781254649162\n",
      "Iteration 4837, Loss: 0.05568460747599602\n",
      "Iteration 4838, Loss: 0.055645667016506195\n",
      "Iteration 4839, Loss: 0.055645983666181564\n",
      "Iteration 4840, Loss: 0.0556841716170311\n",
      "Iteration 4841, Loss: 0.05567658320069313\n",
      "Iteration 4842, Loss: 0.0556621178984642\n",
      "Iteration 4843, Loss: 0.0556204728782177\n",
      "Iteration 4844, Loss: 0.05573427677154541\n",
      "Iteration 4845, Loss: 0.05573173612356186\n",
      "Iteration 4846, Loss: 0.05563930794596672\n",
      "Iteration 4847, Loss: 0.05575847998261452\n",
      "Iteration 4848, Loss: 0.05574595928192139\n",
      "Iteration 4849, Loss: 0.055646222084760666\n",
      "Iteration 4850, Loss: 0.05567077919840813\n",
      "Iteration 4851, Loss: 0.055636726319789886\n",
      "Iteration 4852, Loss: 0.0556645393371582\n",
      "Iteration 4853, Loss: 0.05563807487487793\n",
      "Iteration 4854, Loss: 0.05567137524485588\n",
      "Iteration 4855, Loss: 0.05564197152853012\n",
      "Iteration 4856, Loss: 0.05564117431640625\n",
      "Iteration 4857, Loss: 0.05567268654704094\n",
      "Iteration 4858, Loss: 0.055632077157497406\n",
      "Iteration 4859, Loss: 0.055684130638837814\n",
      "Iteration 4860, Loss: 0.055614713579416275\n",
      "Iteration 4861, Loss: 0.055781684815883636\n",
      "Iteration 4862, Loss: 0.055803775787353516\n",
      "Iteration 4863, Loss: 0.05569934844970703\n",
      "Iteration 4864, Loss: 0.05574238300323486\n",
      "Iteration 4865, Loss: 0.05580850690603256\n",
      "Iteration 4866, Loss: 0.05573475360870361\n",
      "Iteration 4867, Loss: 0.05568552017211914\n",
      "Iteration 4868, Loss: 0.05573614686727524\n",
      "Iteration 4869, Loss: 0.05565552040934563\n",
      "Iteration 4870, Loss: 0.05576757714152336\n",
      "Iteration 4871, Loss: 0.05581112951040268\n",
      "Iteration 4872, Loss: 0.055716317147016525\n",
      "Iteration 4873, Loss: 0.05571937561035156\n",
      "Iteration 4874, Loss: 0.05578450486063957\n",
      "Iteration 4875, Loss: 0.05571770668029785\n",
      "Iteration 4876, Loss: 0.055694304406642914\n",
      "Iteration 4877, Loss: 0.055733803659677505\n",
      "Iteration 4878, Loss: 0.05564352124929428\n",
      "Iteration 4879, Loss: 0.055777113884687424\n",
      "Iteration 4880, Loss: 0.055815618485212326\n",
      "Iteration 4881, Loss: 0.055713098496198654\n",
      "Iteration 4882, Loss: 0.055729154497385025\n",
      "Iteration 4883, Loss: 0.055800799280405045\n",
      "Iteration 4884, Loss: 0.05574266240000725\n",
      "Iteration 4885, Loss: 0.05565917491912842\n",
      "Iteration 4886, Loss: 0.055704515427351\n",
      "Iteration 4887, Loss: 0.05562397092580795\n",
      "Iteration 4888, Loss: 0.055709801614284515\n",
      "Iteration 4889, Loss: 0.055709999054670334\n",
      "Iteration 4890, Loss: 0.05561630055308342\n",
      "Iteration 4891, Loss: 0.05584824085235596\n",
      "Iteration 4892, Loss: 0.05589227005839348\n",
      "Iteration 4893, Loss: 0.0557580403983593\n",
      "Iteration 4894, Loss: 0.05572092905640602\n",
      "Iteration 4895, Loss: 0.055822014808654785\n",
      "Iteration 4896, Loss: 0.05581788346171379\n",
      "Iteration 4897, Loss: 0.05571925640106201\n",
      "Iteration 4898, Loss: 0.055714212357997894\n",
      "Iteration 4899, Loss: 0.05576447769999504\n",
      "Iteration 4900, Loss: 0.05563763901591301\n",
      "Iteration 4901, Loss: 0.05580425634980202\n",
      "Iteration 4902, Loss: 0.05590013787150383\n",
      "Iteration 4903, Loss: 0.05589119717478752\n",
      "Iteration 4904, Loss: 0.055788516998291016\n",
      "Iteration 4905, Loss: 0.05563056841492653\n",
      "Iteration 4906, Loss: 0.055757682770490646\n",
      "Iteration 4907, Loss: 0.055724065750837326\n",
      "Iteration 4908, Loss: 0.055677175521850586\n",
      "Iteration 4909, Loss: 0.05571707338094711\n",
      "Iteration 4910, Loss: 0.055660687386989594\n",
      "Iteration 4911, Loss: 0.05574604123830795\n",
      "Iteration 4912, Loss: 0.05576002597808838\n",
      "Iteration 4913, Loss: 0.05564359948039055\n",
      "Iteration 4914, Loss: 0.055764876306056976\n",
      "Iteration 4915, Loss: 0.05580044165253639\n",
      "Iteration 4916, Loss: 0.055709801614284515\n",
      "Iteration 4917, Loss: 0.05572068691253662\n",
      "Iteration 4918, Loss: 0.05577743053436279\n",
      "Iteration 4919, Loss: 0.05569112300872803\n",
      "Iteration 4920, Loss: 0.05573805421590805\n",
      "Iteration 4921, Loss: 0.05579642578959465\n",
      "Iteration 4922, Loss: 0.055727604776620865\n",
      "Iteration 4923, Loss: 0.055684011429548264\n",
      "Iteration 4924, Loss: 0.0557301864027977\n",
      "Iteration 4925, Loss: 0.05564050003886223\n",
      "Iteration 4926, Loss: 0.055784862488508224\n",
      "Iteration 4927, Loss: 0.055848680436611176\n",
      "Iteration 4928, Loss: 0.05579916760325432\n",
      "Iteration 4929, Loss: 0.05567256733775139\n",
      "Iteration 4930, Loss: 0.05580040067434311\n",
      "Iteration 4931, Loss: 0.055865328758955\n",
      "Iteration 4932, Loss: 0.0557636022567749\n",
      "Iteration 4933, Loss: 0.055694304406642914\n",
      "Iteration 4934, Loss: 0.05577322095632553\n",
      "Iteration 4935, Loss: 0.05574604123830795\n",
      "Iteration 4936, Loss: 0.05564224720001221\n",
      "Iteration 4937, Loss: 0.05579781532287598\n",
      "Iteration 4938, Loss: 0.055820465087890625\n",
      "Iteration 4939, Loss: 0.05566970631480217\n",
      "Iteration 4940, Loss: 0.055796027183532715\n",
      "Iteration 4941, Loss: 0.05590597912669182\n",
      "Iteration 4942, Loss: 0.0559106282889843\n",
      "Iteration 4943, Loss: 0.055819831788539886\n",
      "Iteration 4944, Loss: 0.05564491078257561\n",
      "Iteration 4945, Loss: 0.055908799171447754\n",
      "Iteration 4946, Loss: 0.056042514741420746\n",
      "Iteration 4947, Loss: 0.055987875908613205\n",
      "Iteration 4948, Loss: 0.05576443672180176\n",
      "Iteration 4949, Loss: 0.05577492713928223\n",
      "Iteration 4950, Loss: 0.05592934414744377\n",
      "Iteration 4951, Loss: 0.055973611772060394\n",
      "Iteration 4952, Loss: 0.055918220430612564\n",
      "Iteration 4953, Loss: 0.05577385425567627\n",
      "Iteration 4954, Loss: 0.05569589510560036\n",
      "Iteration 4955, Loss: 0.05579610913991928\n",
      "Iteration 4956, Loss: 0.055714964866638184\n",
      "Iteration 4957, Loss: 0.05571746826171875\n",
      "Iteration 4958, Loss: 0.05578645318746567\n",
      "Iteration 4959, Loss: 0.0557551383972168\n",
      "Iteration 4960, Loss: 0.055634379386901855\n",
      "Iteration 4961, Loss: 0.0558549165725708\n",
      "Iteration 4962, Loss: 0.05592803284525871\n",
      "Iteration 4963, Loss: 0.055822454392910004\n",
      "Iteration 4964, Loss: 0.05565647408366203\n",
      "Iteration 4965, Loss: 0.05574623867869377\n",
      "Iteration 4966, Loss: 0.05573785677552223\n",
      "Iteration 4967, Loss: 0.05563442036509514\n",
      "Iteration 4968, Loss: 0.0558314323425293\n",
      "Iteration 4969, Loss: 0.05588909238576889\n",
      "Iteration 4970, Loss: 0.055778663605451584\n",
      "Iteration 4971, Loss: 0.0556892566382885\n",
      "Iteration 4972, Loss: 0.05577731132507324\n",
      "Iteration 4973, Loss: 0.055761776864528656\n",
      "Iteration 4974, Loss: 0.05564602464437485\n",
      "Iteration 4975, Loss: 0.055828772485256195\n",
      "Iteration 4976, Loss: 0.05590105056762695\n",
      "Iteration 4977, Loss: 0.05580854415893555\n",
      "Iteration 4978, Loss: 0.05565973371267319\n",
      "Iteration 4979, Loss: 0.05575935170054436\n",
      "Iteration 4980, Loss: 0.055762093514204025\n",
      "Iteration 4981, Loss: 0.05564912408590317\n",
      "Iteration 4982, Loss: 0.05581434816122055\n",
      "Iteration 4983, Loss: 0.05588841810822487\n",
      "Iteration 4984, Loss: 0.055812954902648926\n",
      "Iteration 4985, Loss: 0.05565845966339111\n",
      "Iteration 4986, Loss: 0.05579368397593498\n",
      "Iteration 4987, Loss: 0.05584073066711426\n",
      "Iteration 4988, Loss: 0.05574556440114975\n",
      "Iteration 4989, Loss: 0.055695097893476486\n",
      "Iteration 4990, Loss: 0.055762890726327896\n",
      "Iteration 4991, Loss: 0.05569970980286598\n",
      "Iteration 4992, Loss: 0.05570578575134277\n",
      "Iteration 4993, Loss: 0.05573968216776848\n",
      "Iteration 4994, Loss: 0.055635690689086914\n",
      "Iteration 4995, Loss: 0.05580270290374756\n",
      "Iteration 4996, Loss: 0.0558626689016819\n",
      "Iteration 4997, Loss: 0.055783748626708984\n",
      "Iteration 4998, Loss: 0.05566021054983139\n",
      "Iteration 4999, Loss: 0.05576467514038086\n",
      "Iteration 5000, Loss: 0.05575494095683098\n",
      "Iteration 5001, Loss: 0.055629849433898926\n",
      "Iteration 5002, Loss: 0.05568520352244377\n",
      "Iteration 5003, Loss: 0.055659811943769455\n",
      "Iteration 5004, Loss: 0.05570396035909653\n",
      "Iteration 5005, Loss: 0.05567149445414543\n",
      "Iteration 5006, Loss: 0.055716436356306076\n",
      "Iteration 5007, Loss: 0.05575565621256828\n",
      "Iteration 5008, Loss: 0.055697642266750336\n",
      "Iteration 5009, Loss: 0.05569418519735336\n",
      "Iteration 5010, Loss: 0.055699627846479416\n",
      "Iteration 5011, Loss: 0.055671416223049164\n",
      "Iteration 5012, Loss: 0.05568913742899895\n",
      "Iteration 5013, Loss: 0.055619362741708755\n",
      "Iteration 5014, Loss: 0.05575605481863022\n",
      "Iteration 5015, Loss: 0.05574309825897217\n",
      "Iteration 5016, Loss: 0.05564781278371811\n",
      "Iteration 5017, Loss: 0.05571413412690163\n",
      "Iteration 5018, Loss: 0.05567455291748047\n",
      "Iteration 5019, Loss: 0.05571905896067619\n",
      "Iteration 5020, Loss: 0.05574365705251694\n",
      "Iteration 5021, Loss: 0.055659256875514984\n",
      "Iteration 5022, Loss: 0.055758994072675705\n",
      "Iteration 5023, Loss: 0.055786728858947754\n",
      "Iteration 5024, Loss: 0.055660367012023926\n",
      "Iteration 5025, Loss: 0.05579594895243645\n",
      "Iteration 5026, Loss: 0.0558854341506958\n",
      "Iteration 5027, Loss: 0.05584883689880371\n",
      "Iteration 5028, Loss: 0.05570995807647705\n",
      "Iteration 5029, Loss: 0.05577031895518303\n",
      "Iteration 5030, Loss: 0.055863022804260254\n",
      "Iteration 5031, Loss: 0.055792730301618576\n",
      "Iteration 5032, Loss: 0.055643003433942795\n",
      "Iteration 5033, Loss: 0.05570308491587639\n",
      "Iteration 5034, Loss: 0.055645547807216644\n",
      "Iteration 5035, Loss: 0.05575871467590332\n",
      "Iteration 5036, Loss: 0.05577278137207031\n",
      "Iteration 5037, Loss: 0.05564407631754875\n",
      "Iteration 5038, Loss: 0.05579491704702377\n",
      "Iteration 5039, Loss: 0.05586191266775131\n",
      "Iteration 5040, Loss: 0.05579758062958717\n",
      "Iteration 5041, Loss: 0.05564828962087631\n",
      "Iteration 5042, Loss: 0.05581542104482651\n",
      "Iteration 5043, Loss: 0.05586802959442139\n",
      "Iteration 5044, Loss: 0.0557560920715332\n",
      "Iteration 5045, Loss: 0.05570423603057861\n",
      "Iteration 5046, Loss: 0.05578943341970444\n",
      "Iteration 5047, Loss: 0.055757008492946625\n",
      "Iteration 5048, Loss: 0.05563577264547348\n",
      "Iteration 5049, Loss: 0.05581514164805412\n",
      "Iteration 5050, Loss: 0.055849991738796234\n",
      "Iteration 5051, Loss: 0.055710237473249435\n",
      "Iteration 5052, Loss: 0.05576082319021225\n",
      "Iteration 5053, Loss: 0.055864814668893814\n",
      "Iteration 5054, Loss: 0.0558595284819603\n",
      "Iteration 5055, Loss: 0.05575752258300781\n",
      "Iteration 5056, Loss: 0.05567292496562004\n",
      "Iteration 5057, Loss: 0.05574365705251694\n",
      "Iteration 5058, Loss: 0.05565985292196274\n",
      "Iteration 5059, Loss: 0.05574874207377434\n",
      "Iteration 5060, Loss: 0.05580659955739975\n",
      "Iteration 5061, Loss: 0.05576535314321518\n",
      "Iteration 5062, Loss: 0.05563557147979736\n",
      "Iteration 5063, Loss: 0.055863700807094574\n",
      "Iteration 5064, Loss: 0.055947065353393555\n",
      "Iteration 5065, Loss: 0.055852413177490234\n",
      "Iteration 5066, Loss: 0.05563827604055405\n",
      "Iteration 5067, Loss: 0.05578414723277092\n",
      "Iteration 5068, Loss: 0.055837392807006836\n",
      "Iteration 5069, Loss: 0.05577973648905754\n",
      "Iteration 5070, Loss: 0.05563533306121826\n",
      "Iteration 5071, Loss: 0.05584442988038063\n",
      "Iteration 5072, Loss: 0.05590824410319328\n",
      "Iteration 5073, Loss: 0.05579817295074463\n",
      "Iteration 5074, Loss: 0.05567328259348869\n",
      "Iteration 5075, Loss: 0.05576034635305405\n",
      "Iteration 5076, Loss: 0.05574270337820053\n",
      "Iteration 5077, Loss: 0.055630169808864594\n",
      "Iteration 5078, Loss: 0.05584586039185524\n",
      "Iteration 5079, Loss: 0.05590800568461418\n",
      "Iteration 5080, Loss: 0.055795155465602875\n",
      "Iteration 5081, Loss: 0.05567896366119385\n",
      "Iteration 5082, Loss: 0.05576809495687485\n",
      "Iteration 5083, Loss: 0.05575494095683098\n",
      "Iteration 5084, Loss: 0.05564634129405022\n",
      "Iteration 5085, Loss: 0.05582305043935776\n",
      "Iteration 5086, Loss: 0.05588607117533684\n",
      "Iteration 5087, Loss: 0.05577751249074936\n",
      "Iteration 5088, Loss: 0.05568838119506836\n",
      "Iteration 5089, Loss: 0.05577477067708969\n",
      "Iteration 5090, Loss: 0.05575760453939438\n",
      "Iteration 5091, Loss: 0.05564304441213608\n",
      "Iteration 5092, Loss: 0.055832862854003906\n",
      "Iteration 5093, Loss: 0.055902957916259766\n",
      "Iteration 5094, Loss: 0.055804137140512466\n",
      "Iteration 5095, Loss: 0.05566469952464104\n",
      "Iteration 5096, Loss: 0.0557553768157959\n",
      "Iteration 5097, Loss: 0.05574687570333481\n",
      "Iteration 5098, Loss: 0.05563203617930412\n",
      "Iteration 5099, Loss: 0.055839817970991135\n",
      "Iteration 5100, Loss: 0.05591360852122307\n",
      "Iteration 5101, Loss: 0.05583079904317856\n",
      "Iteration 5102, Loss: 0.055658381432294846\n",
      "Iteration 5103, Loss: 0.055803339928388596\n",
      "Iteration 5104, Loss: 0.05586930364370346\n",
      "Iteration 5105, Loss: 0.05579984188079834\n",
      "Iteration 5106, Loss: 0.05563247576355934\n",
      "Iteration 5107, Loss: 0.05581331625580788\n",
      "Iteration 5108, Loss: 0.055853210389614105\n",
      "Iteration 5109, Loss: 0.05573602765798569\n",
      "Iteration 5110, Loss: 0.055722396820783615\n",
      "Iteration 5111, Loss: 0.05580946058034897\n",
      "Iteration 5112, Loss: 0.05577973648905754\n",
      "Iteration 5113, Loss: 0.055651191622018814\n",
      "Iteration 5114, Loss: 0.05582448095083237\n",
      "Iteration 5115, Loss: 0.05589834973216057\n",
      "Iteration 5116, Loss: 0.055805206298828125\n",
      "Iteration 5117, Loss: 0.05565524473786354\n",
      "Iteration 5118, Loss: 0.0557427816092968\n",
      "Iteration 5119, Loss: 0.0557229146361351\n",
      "Iteration 5120, Loss: 0.055633626878261566\n",
      "Iteration 5121, Loss: 0.05562524124979973\n",
      "Iteration 5122, Loss: 0.05573241040110588\n",
      "Iteration 5123, Loss: 0.055751364678144455\n",
      "Iteration 5124, Loss: 0.055673401802778244\n",
      "Iteration 5125, Loss: 0.055748820304870605\n",
      "Iteration 5126, Loss: 0.055776678025722504\n",
      "Iteration 5127, Loss: 0.05563469976186752\n",
      "Iteration 5128, Loss: 0.05581923574209213\n",
      "Iteration 5129, Loss: 0.05592366307973862\n",
      "Iteration 5130, Loss: 0.05592024326324463\n",
      "Iteration 5131, Loss: 0.05582118406891823\n",
      "Iteration 5132, Loss: 0.05565170571208\n",
      "Iteration 5133, Loss: 0.05587911605834961\n",
      "Iteration 5134, Loss: 0.055985771119594574\n",
      "Iteration 5135, Loss: 0.05590900033712387\n",
      "Iteration 5136, Loss: 0.05566791817545891\n",
      "Iteration 5137, Loss: 0.055857978761196136\n",
      "Iteration 5138, Loss: 0.05602284520864487\n",
      "Iteration 5139, Loss: 0.05607569217681885\n",
      "Iteration 5140, Loss: 0.05602828785777092\n",
      "Iteration 5141, Loss: 0.05589044466614723\n",
      "Iteration 5142, Loss: 0.055673204362392426\n",
      "Iteration 5143, Loss: 0.05591956898570061\n",
      "Iteration 5144, Loss: 0.05610013008117676\n",
      "Iteration 5145, Loss: 0.05608773231506348\n",
      "Iteration 5146, Loss: 0.05590248107910156\n",
      "Iteration 5147, Loss: 0.05564848706126213\n",
      "Iteration 5148, Loss: 0.0557832345366478\n",
      "Iteration 5149, Loss: 0.05581335350871086\n",
      "Iteration 5150, Loss: 0.05574619770050049\n",
      "Iteration 5151, Loss: 0.055638473480939865\n",
      "Iteration 5152, Loss: 0.05565591901540756\n",
      "Iteration 5153, Loss: 0.05569211766123772\n",
      "Iteration 5154, Loss: 0.0556994304060936\n",
      "Iteration 5155, Loss: 0.055618010461330414\n",
      "Iteration 5156, Loss: 0.055760107934474945\n",
      "Iteration 5157, Loss: 0.05573428049683571\n",
      "Iteration 5158, Loss: 0.05566314980387688\n",
      "Iteration 5159, Loss: 0.05570089817047119\n",
      "Iteration 5160, Loss: 0.055638156831264496\n",
      "Iteration 5161, Loss: 0.055776678025722504\n",
      "Iteration 5162, Loss: 0.05579749867320061\n",
      "Iteration 5163, Loss: 0.055673640221357346\n",
      "Iteration 5164, Loss: 0.0557730607688427\n",
      "Iteration 5165, Loss: 0.055851541459560394\n",
      "Iteration 5166, Loss: 0.05580329895019531\n",
      "Iteration 5167, Loss: 0.0556514672935009\n",
      "Iteration 5168, Loss: 0.055839698761701584\n",
      "Iteration 5169, Loss: 0.05593113228678703\n",
      "Iteration 5170, Loss: 0.05585511773824692\n",
      "Iteration 5171, Loss: 0.055650513619184494\n",
      "Iteration 5172, Loss: 0.05584176629781723\n",
      "Iteration 5173, Loss: 0.055957239121198654\n",
      "Iteration 5174, Loss: 0.05593618005514145\n",
      "Iteration 5175, Loss: 0.055796507745981216\n",
      "Iteration 5176, Loss: 0.05568408966064453\n",
      "Iteration 5177, Loss: 0.05580397695302963\n",
      "Iteration 5178, Loss: 0.0557834729552269\n",
      "Iteration 5179, Loss: 0.0556257963180542\n",
      "Iteration 5180, Loss: 0.05577492713928223\n",
      "Iteration 5181, Loss: 0.055831316858530045\n",
      "Iteration 5182, Loss: 0.05578681081533432\n",
      "Iteration 5183, Loss: 0.05565845966339111\n",
      "Iteration 5184, Loss: 0.05582873150706291\n",
      "Iteration 5185, Loss: 0.05590458959341049\n",
      "Iteration 5186, Loss: 0.055799245834350586\n",
      "Iteration 5187, Loss: 0.055671971291303635\n",
      "Iteration 5188, Loss: 0.05575617402791977\n",
      "Iteration 5189, Loss: 0.05574135109782219\n",
      "Iteration 5190, Loss: 0.055639270693063736\n",
      "Iteration 5191, Loss: 0.055821143090724945\n",
      "Iteration 5192, Loss: 0.05587005987763405\n",
      "Iteration 5193, Loss: 0.05574564263224602\n",
      "Iteration 5194, Loss: 0.05572168156504631\n",
      "Iteration 5195, Loss: 0.055815938860177994\n",
      "Iteration 5196, Loss: 0.05580683797597885\n",
      "Iteration 5197, Loss: 0.05570308491587639\n",
      "Iteration 5198, Loss: 0.05574039742350578\n",
      "Iteration 5199, Loss: 0.05579722300171852\n",
      "Iteration 5200, Loss: 0.05568099021911621\n",
      "Iteration 5201, Loss: 0.05576328560709953\n",
      "Iteration 5202, Loss: 0.05585150048136711\n",
      "Iteration 5203, Loss: 0.055834293365478516\n",
      "Iteration 5204, Loss: 0.055723272264003754\n",
      "Iteration 5205, Loss: 0.05572223663330078\n",
      "Iteration 5206, Loss: 0.0557883195579052\n",
      "Iteration 5207, Loss: 0.05568023771047592\n",
      "Iteration 5208, Loss: 0.05576026439666748\n",
      "Iteration 5209, Loss: 0.055844783782958984\n",
      "Iteration 5210, Loss: 0.05582527443766594\n",
      "Iteration 5211, Loss: 0.05571397393941879\n",
      "Iteration 5212, Loss: 0.05573539063334465\n",
      "Iteration 5213, Loss: 0.055801037698984146\n",
      "Iteration 5214, Loss: 0.055691443383693695\n",
      "Iteration 5215, Loss: 0.05575287342071533\n",
      "Iteration 5216, Loss: 0.05583890527486801\n",
      "Iteration 5217, Loss: 0.05582154169678688\n",
      "Iteration 5218, Loss: 0.055712662637233734\n",
      "Iteration 5219, Loss: 0.05573451519012451\n",
      "Iteration 5220, Loss: 0.055796943604946136\n",
      "Iteration 5221, Loss: 0.055682819336652756\n",
      "Iteration 5222, Loss: 0.055761855095624924\n",
      "Iteration 5223, Loss: 0.055850427597761154\n",
      "Iteration 5224, Loss: 0.055836163461208344\n",
      "Iteration 5225, Loss: 0.05572954937815666\n",
      "Iteration 5226, Loss: 0.055708568543195724\n",
      "Iteration 5227, Loss: 0.055768612772226334\n",
      "Iteration 5228, Loss: 0.05565182492136955\n",
      "Iteration 5229, Loss: 0.05578545853495598\n",
      "Iteration 5230, Loss: 0.05587538331747055\n",
      "Iteration 5231, Loss: 0.05586199089884758\n",
      "Iteration 5232, Loss: 0.055755775421857834\n",
      "Iteration 5233, Loss: 0.0556720532476902\n",
      "Iteration 5234, Loss: 0.05573097988963127\n",
      "Iteration 5235, Loss: 0.055615268647670746\n",
      "Iteration 5236, Loss: 0.055760424584150314\n",
      "Iteration 5237, Loss: 0.05579034611582756\n",
      "Iteration 5238, Loss: 0.05571329593658447\n",
      "Iteration 5239, Loss: 0.055699270218610764\n",
      "Iteration 5240, Loss: 0.055736981332302094\n",
      "Iteration 5241, Loss: 0.055626749992370605\n",
      "Iteration 5242, Loss: 0.055751923471689224\n",
      "Iteration 5243, Loss: 0.05579265207052231\n",
      "Iteration 5244, Loss: 0.055735789239406586\n",
      "Iteration 5245, Loss: 0.055639784783124924\n",
      "Iteration 5246, Loss: 0.055655837059020996\n",
      "Iteration 5247, Loss: 0.05568663403391838\n",
      "Iteration 5248, Loss: 0.05568850040435791\n",
      "Iteration 5249, Loss: 0.05563509836792946\n",
      "Iteration 5250, Loss: 0.05563565343618393\n",
      "Iteration 5251, Loss: 0.05566231533885002\n",
      "Iteration 5252, Loss: 0.05563628673553467\n",
      "Iteration 5253, Loss: 0.05563557147979736\n",
      "Iteration 5254, Loss: 0.05568107217550278\n",
      "Iteration 5255, Loss: 0.05566354840993881\n",
      "Iteration 5256, Loss: 0.05568985268473625\n",
      "Iteration 5257, Loss: 0.05565575882792473\n",
      "Iteration 5258, Loss: 0.055728040635585785\n",
      "Iteration 5259, Loss: 0.05576551333069801\n",
      "Iteration 5260, Loss: 0.055703602731227875\n",
      "Iteration 5261, Loss: 0.055690884590148926\n",
      "Iteration 5262, Loss: 0.05570685863494873\n",
      "Iteration 5263, Loss: 0.05565587803721428\n",
      "Iteration 5264, Loss: 0.0556645393371582\n",
      "Iteration 5265, Loss: 0.055655837059020996\n",
      "Iteration 5266, Loss: 0.05563696473836899\n",
      "Iteration 5267, Loss: 0.05567113682627678\n",
      "Iteration 5268, Loss: 0.05563211813569069\n",
      "Iteration 5269, Loss: 0.055756211280822754\n",
      "Iteration 5270, Loss: 0.05575525760650635\n",
      "Iteration 5271, Loss: 0.05563843622803688\n",
      "Iteration 5272, Loss: 0.05574595928192139\n",
      "Iteration 5273, Loss: 0.05574699491262436\n",
      "Iteration 5274, Loss: 0.05562261864542961\n",
      "Iteration 5275, Loss: 0.05579134076833725\n",
      "Iteration 5276, Loss: 0.05581009387969971\n",
      "Iteration 5277, Loss: 0.055680155754089355\n",
      "Iteration 5278, Loss: 0.05577588081359863\n",
      "Iteration 5279, Loss: 0.05586401745676994\n",
      "Iteration 5280, Loss: 0.05582483857870102\n",
      "Iteration 5281, Loss: 0.05567920207977295\n",
      "Iteration 5282, Loss: 0.0558064803481102\n",
      "Iteration 5283, Loss: 0.05590180680155754\n",
      "Iteration 5284, Loss: 0.05583254620432854\n",
      "Iteration 5285, Loss: 0.05563930794596672\n",
      "Iteration 5286, Loss: 0.05583048239350319\n",
      "Iteration 5287, Loss: 0.055919770151376724\n",
      "Iteration 5288, Loss: 0.05587025731801987\n",
      "Iteration 5289, Loss: 0.05570543184876442\n",
      "Iteration 5290, Loss: 0.05579181760549545\n",
      "Iteration 5291, Loss: 0.055907368659973145\n",
      "Iteration 5292, Loss: 0.055870138108730316\n",
      "Iteration 5293, Loss: 0.05570042133331299\n",
      "Iteration 5294, Loss: 0.05579587072134018\n",
      "Iteration 5295, Loss: 0.055915795266628265\n",
      "Iteration 5296, Loss: 0.05589485540986061\n",
      "Iteration 5297, Loss: 0.055751483887434006\n",
      "Iteration 5298, Loss: 0.05572677031159401\n",
      "Iteration 5299, Loss: 0.05583127588033676\n",
      "Iteration 5300, Loss: 0.05578935146331787\n",
      "Iteration 5301, Loss: 0.055622976273298264\n",
      "Iteration 5302, Loss: 0.055816058069467545\n",
      "Iteration 5303, Loss: 0.055863380432128906\n",
      "Iteration 5304, Loss: 0.05576268956065178\n",
      "Iteration 5305, Loss: 0.05568663403391838\n",
      "Iteration 5306, Loss: 0.05576721951365471\n",
      "Iteration 5307, Loss: 0.05572819709777832\n",
      "Iteration 5308, Loss: 0.05565500259399414\n",
      "Iteration 5309, Loss: 0.05566859245300293\n",
      "Iteration 5310, Loss: 0.055671654641628265\n",
      "Iteration 5311, Loss: 0.05565568059682846\n",
      "Iteration 5312, Loss: 0.05570761486887932\n",
      "Iteration 5313, Loss: 0.05569843575358391\n",
      "Iteration 5314, Loss: 0.05566573515534401\n",
      "Iteration 5315, Loss: 0.055674437433481216\n",
      "Iteration 5316, Loss: 0.055665336549282074\n",
      "Iteration 5317, Loss: 0.05564030259847641\n",
      "Iteration 5318, Loss: 0.0557204894721508\n",
      "Iteration 5319, Loss: 0.05571015924215317\n",
      "Iteration 5320, Loss: 0.05565468594431877\n",
      "Iteration 5321, Loss: 0.055668752640485764\n",
      "Iteration 5322, Loss: 0.05567006394267082\n",
      "Iteration 5323, Loss: 0.05564801022410393\n",
      "Iteration 5324, Loss: 0.05570673942565918\n",
      "Iteration 5325, Loss: 0.05568814277648926\n",
      "Iteration 5326, Loss: 0.05568361282348633\n",
      "Iteration 5327, Loss: 0.055694740265607834\n",
      "Iteration 5328, Loss: 0.05563986301422119\n",
      "Iteration 5329, Loss: 0.05565265938639641\n",
      "Iteration 5330, Loss: 0.05568007752299309\n",
      "Iteration 5331, Loss: 0.05567336454987526\n",
      "Iteration 5332, Loss: 0.05566319078207016\n",
      "Iteration 5333, Loss: 0.05563068389892578\n",
      "Iteration 5334, Loss: 0.05571882054209709\n",
      "Iteration 5335, Loss: 0.055730581283569336\n",
      "Iteration 5336, Loss: 0.05564495176076889\n",
      "Iteration 5337, Loss: 0.05579424276947975\n",
      "Iteration 5338, Loss: 0.05582742020487785\n",
      "Iteration 5339, Loss: 0.05568575859069824\n",
      "Iteration 5340, Loss: 0.05577850341796875\n",
      "Iteration 5341, Loss: 0.05588356778025627\n",
      "Iteration 5342, Loss: 0.05587947368621826\n",
      "Iteration 5343, Loss: 0.055778663605451584\n",
      "Iteration 5344, Loss: 0.055646657943725586\n",
      "Iteration 5345, Loss: 0.05576026812195778\n",
      "Iteration 5346, Loss: 0.05571993440389633\n",
      "Iteration 5347, Loss: 0.05568715184926987\n",
      "Iteration 5348, Loss: 0.055731020867824554\n",
      "Iteration 5349, Loss: 0.05568484589457512\n",
      "Iteration 5350, Loss: 0.05570109933614731\n",
      "Iteration 5351, Loss: 0.055697958916425705\n",
      "Iteration 5352, Loss: 0.055674951523542404\n",
      "Iteration 5353, Loss: 0.055695656687021255\n",
      "Iteration 5354, Loss: 0.05562889575958252\n",
      "Iteration 5355, Loss: 0.055776797235012054\n",
      "Iteration 5356, Loss: 0.05578983202576637\n",
      "Iteration 5357, Loss: 0.055666450411081314\n",
      "Iteration 5358, Loss: 0.0557762011885643\n",
      "Iteration 5359, Loss: 0.0558498315513134\n",
      "Iteration 5360, Loss: 0.055796265602111816\n",
      "Iteration 5361, Loss: 0.055646102875471115\n",
      "Iteration 5362, Loss: 0.05583067983388901\n",
      "Iteration 5363, Loss: 0.05590629577636719\n",
      "Iteration 5364, Loss: 0.055819593369960785\n",
      "Iteration 5365, Loss: 0.05563986301422119\n",
      "Iteration 5366, Loss: 0.05576058477163315\n",
      "Iteration 5367, Loss: 0.05576789379119873\n",
      "Iteration 5368, Loss: 0.055650435388088226\n",
      "Iteration 5369, Loss: 0.05580560490489006\n",
      "Iteration 5370, Loss: 0.05587844178080559\n",
      "Iteration 5371, Loss: 0.05580409616231918\n",
      "Iteration 5372, Loss: 0.055650949478149414\n",
      "Iteration 5373, Loss: 0.055787406861782074\n",
      "Iteration 5374, Loss: 0.0558195523917675\n",
      "Iteration 5375, Loss: 0.055706024169921875\n",
      "Iteration 5376, Loss: 0.055746279656887054\n",
      "Iteration 5377, Loss: 0.05582737922668457\n",
      "Iteration 5378, Loss: 0.05577914044260979\n",
      "Iteration 5379, Loss: 0.05564785376191139\n",
      "Iteration 5380, Loss: 0.055799685418605804\n",
      "Iteration 5381, Loss: 0.05583636090159416\n",
      "Iteration 5382, Loss: 0.0557076558470726\n",
      "Iteration 5383, Loss: 0.055755577981472015\n",
      "Iteration 5384, Loss: 0.05585118383169174\n",
      "Iteration 5385, Loss: 0.05583016201853752\n",
      "Iteration 5386, Loss: 0.055710915476083755\n",
      "Iteration 5387, Loss: 0.05574862286448479\n",
      "Iteration 5388, Loss: 0.05582237243652344\n",
      "Iteration 5389, Loss: 0.05572668835520744\n",
      "Iteration 5390, Loss: 0.05571834370493889\n",
      "Iteration 5391, Loss: 0.055793724954128265\n",
      "Iteration 5392, Loss: 0.05575970932841301\n",
      "Iteration 5393, Loss: 0.05564725399017334\n",
      "Iteration 5394, Loss: 0.055806636810302734\n",
      "Iteration 5395, Loss: 0.05584470555186272\n",
      "Iteration 5396, Loss: 0.05570678040385246\n",
      "Iteration 5397, Loss: 0.05576050281524658\n",
      "Iteration 5398, Loss: 0.055862944573163986\n",
      "Iteration 5399, Loss: 0.05586012452840805\n",
      "Iteration 5400, Loss: 0.055763129144907\n",
      "Iteration 5401, Loss: 0.055656593292951584\n",
      "Iteration 5402, Loss: 0.055730462074279785\n",
      "Iteration 5403, Loss: 0.05565866082906723\n",
      "Iteration 5404, Loss: 0.05573491379618645\n",
      "Iteration 5405, Loss: 0.05577751249074936\n",
      "Iteration 5406, Loss: 0.0557171106338501\n",
      "Iteration 5407, Loss: 0.05567272752523422\n",
      "Iteration 5408, Loss: 0.05568794533610344\n",
      "Iteration 5409, Loss: 0.055668119341135025\n",
      "Iteration 5410, Loss: 0.05567396059632301\n",
      "Iteration 5411, Loss: 0.055650752037763596\n",
      "Iteration 5412, Loss: 0.05562722682952881\n",
      "Iteration 5413, Loss: 0.05568528547883034\n",
      "Iteration 5414, Loss: 0.055645983666181564\n",
      "Iteration 5415, Loss: 0.055735789239406586\n",
      "Iteration 5416, Loss: 0.05573662370443344\n",
      "Iteration 5417, Loss: 0.05563712120056152\n",
      "Iteration 5418, Loss: 0.05570654198527336\n",
      "Iteration 5419, Loss: 0.055654529482126236\n",
      "Iteration 5420, Loss: 0.05574409291148186\n",
      "Iteration 5421, Loss: 0.05578291788697243\n",
      "Iteration 5422, Loss: 0.05571071431040764\n",
      "Iteration 5423, Loss: 0.0556994304060936\n",
      "Iteration 5424, Loss: 0.055738210678100586\n",
      "Iteration 5425, Loss: 0.05563469976186752\n",
      "Iteration 5426, Loss: 0.055780332535505295\n",
      "Iteration 5427, Loss: 0.055847685784101486\n",
      "Iteration 5428, Loss: 0.055811405181884766\n",
      "Iteration 5429, Loss: 0.05568870157003403\n",
      "Iteration 5430, Loss: 0.05578295513987541\n",
      "Iteration 5431, Loss: 0.0558578185737133\n",
      "Iteration 5432, Loss: 0.05575764179229736\n",
      "Iteration 5433, Loss: 0.055697325617074966\n",
      "Iteration 5434, Loss: 0.055777113884687424\n",
      "Iteration 5435, Loss: 0.05575410649180412\n",
      "Iteration 5436, Loss: 0.05564439296722412\n",
      "Iteration 5437, Loss: 0.05582141876220703\n",
      "Iteration 5438, Loss: 0.05587569996714592\n",
      "Iteration 5439, Loss: 0.05575132369995117\n",
      "Iteration 5440, Loss: 0.055717747658491135\n",
      "Iteration 5441, Loss: 0.055812835693359375\n",
      "Iteration 5442, Loss: 0.055804770439863205\n",
      "Iteration 5443, Loss: 0.055703602731227875\n",
      "Iteration 5444, Loss: 0.05573646351695061\n",
      "Iteration 5445, Loss: 0.05578943341970444\n",
      "Iteration 5446, Loss: 0.05566394329071045\n",
      "Iteration 5447, Loss: 0.05578247830271721\n",
      "Iteration 5448, Loss: 0.05587800592184067\n",
      "Iteration 5449, Loss: 0.05586982145905495\n",
      "Iteration 5450, Loss: 0.055768173187971115\n",
      "Iteration 5451, Loss: 0.05564940348267555\n",
      "Iteration 5452, Loss: 0.05570296570658684\n",
      "Iteration 5453, Loss: 0.05563557520508766\n",
      "Iteration 5454, Loss: 0.055624883621931076\n",
      "Iteration 5455, Loss: 0.0557330846786499\n",
      "Iteration 5456, Loss: 0.055685918778181076\n",
      "Iteration 5457, Loss: 0.055714886635541916\n",
      "Iteration 5458, Loss: 0.05576388165354729\n",
      "Iteration 5459, Loss: 0.05571417137980461\n",
      "Iteration 5460, Loss: 0.05565885826945305\n",
      "Iteration 5461, Loss: 0.05565524473786354\n",
      "Iteration 5462, Loss: 0.05570833012461662\n",
      "Iteration 5463, Loss: 0.055731259286403656\n",
      "Iteration 5464, Loss: 0.05565845966339111\n",
      "Iteration 5465, Loss: 0.05576328560709953\n",
      "Iteration 5466, Loss: 0.05578462406992912\n",
      "Iteration 5467, Loss: 0.055631160736083984\n",
      "Iteration 5468, Loss: 0.0558241605758667\n",
      "Iteration 5469, Loss: 0.055935267359018326\n",
      "Iteration 5470, Loss: 0.05594059079885483\n",
      "Iteration 5471, Loss: 0.05585062503814697\n",
      "Iteration 5472, Loss: 0.055676303803920746\n",
      "Iteration 5473, Loss: 0.055862706154584885\n",
      "Iteration 5474, Loss: 0.055995941162109375\n",
      "Iteration 5475, Loss: 0.05594230070710182\n",
      "Iteration 5476, Loss: 0.05572112649679184\n",
      "Iteration 5477, Loss: 0.05580361932516098\n",
      "Iteration 5478, Loss: 0.05595644563436508\n",
      "Iteration 5479, Loss: 0.055999401956796646\n",
      "Iteration 5480, Loss: 0.05594289302825928\n",
      "Iteration 5481, Loss: 0.055797379463911057\n",
      "Iteration 5482, Loss: 0.05566283315420151\n",
      "Iteration 5483, Loss: 0.055764757096767426\n",
      "Iteration 5484, Loss: 0.05568397045135498\n",
      "Iteration 5485, Loss: 0.05573825165629387\n",
      "Iteration 5486, Loss: 0.05580707639455795\n",
      "Iteration 5487, Loss: 0.05577508732676506\n",
      "Iteration 5488, Loss: 0.055652979761362076\n",
      "Iteration 5489, Loss: 0.055830955505371094\n",
      "Iteration 5490, Loss: 0.055906735360622406\n",
      "Iteration 5491, Loss: 0.05580131337046623\n",
      "Iteration 5492, Loss: 0.05566839501261711\n",
      "Iteration 5493, Loss: 0.055752597749233246\n",
      "Iteration 5494, Loss: 0.05573507398366928\n",
      "Iteration 5495, Loss: 0.055625878274440765\n",
      "Iteration 5496, Loss: 0.055851101875305176\n",
      "Iteration 5497, Loss: 0.055912457406520844\n",
      "Iteration 5498, Loss: 0.05579622834920883\n",
      "Iteration 5499, Loss: 0.05567988008260727\n",
      "Iteration 5500, Loss: 0.05577051639556885\n",
      "Iteration 5501, Loss: 0.05575994774699211\n",
      "Iteration 5502, Loss: 0.05565591901540756\n",
      "Iteration 5503, Loss: 0.05580437555909157\n",
      "Iteration 5504, Loss: 0.05586131662130356\n",
      "Iteration 5505, Loss: 0.05574381723999977\n",
      "Iteration 5506, Loss: 0.05571778863668442\n",
      "Iteration 5507, Loss: 0.05580814927816391\n",
      "Iteration 5508, Loss: 0.055794600397348404\n",
      "Iteration 5509, Loss: 0.0556870698928833\n",
      "Iteration 5510, Loss: 0.05576638504862785\n",
      "Iteration 5511, Loss: 0.05582758039236069\n",
      "Iteration 5512, Loss: 0.055714529007673264\n",
      "Iteration 5513, Loss: 0.05573662370443344\n",
      "Iteration 5514, Loss: 0.05582384392619133\n",
      "Iteration 5515, Loss: 0.05580636113882065\n",
      "Iteration 5516, Loss: 0.05569525808095932\n",
      "Iteration 5517, Loss: 0.055759113281965256\n",
      "Iteration 5518, Loss: 0.05582380294799805\n",
      "Iteration 5519, Loss: 0.05571349710226059\n",
      "Iteration 5520, Loss: 0.05573571100831032\n",
      "Iteration 5521, Loss: 0.055821340531110764\n",
      "Iteration 5522, Loss: 0.05580294504761696\n",
      "Iteration 5523, Loss: 0.05569148063659668\n",
      "Iteration 5524, Loss: 0.05576439946889877\n",
      "Iteration 5525, Loss: 0.055829089134931564\n",
      "Iteration 5526, Loss: 0.05571770668029785\n",
      "Iteration 5527, Loss: 0.05573328584432602\n",
      "Iteration 5528, Loss: 0.05581975355744362\n",
      "Iteration 5529, Loss: 0.055802349001169205\n",
      "Iteration 5530, Loss: 0.05569251626729965\n",
      "Iteration 5531, Loss: 0.055761538445949554\n",
      "Iteration 5532, Loss: 0.05582448095083237\n",
      "Iteration 5533, Loss: 0.05571127310395241\n",
      "Iteration 5534, Loss: 0.055739883333444595\n",
      "Iteration 5535, Loss: 0.055827897042036057\n",
      "Iteration 5536, Loss: 0.05581212043762207\n",
      "Iteration 5537, Loss: 0.05570387840270996\n",
      "Iteration 5538, Loss: 0.05574480816721916\n",
      "Iteration 5539, Loss: 0.055806081742048264\n",
      "Iteration 5540, Loss: 0.05569084733724594\n",
      "Iteration 5541, Loss: 0.055756449699401855\n",
      "Iteration 5542, Loss: 0.05584573745727539\n",
      "Iteration 5543, Loss: 0.05583151429891586\n",
      "Iteration 5544, Loss: 0.05572454258799553\n",
      "Iteration 5545, Loss: 0.0557154044508934\n",
      "Iteration 5546, Loss: 0.05577588453888893\n",
      "Iteration 5547, Loss: 0.055659931153059006\n",
      "Iteration 5548, Loss: 0.055779457092285156\n",
      "Iteration 5549, Loss: 0.05586914345622063\n",
      "Iteration 5550, Loss: 0.055855631828308105\n",
      "Iteration 5551, Loss: 0.055749498307704926\n",
      "Iteration 5552, Loss: 0.055681031197309494\n",
      "Iteration 5553, Loss: 0.05574178695678711\n",
      "Iteration 5554, Loss: 0.055628977715969086\n",
      "Iteration 5555, Loss: 0.05578935146331787\n",
      "Iteration 5556, Loss: 0.05586656183004379\n",
      "Iteration 5557, Loss: 0.055839262902736664\n",
      "Iteration 5558, Loss: 0.055719416588544846\n",
      "Iteration 5559, Loss: 0.055737853050231934\n",
      "Iteration 5560, Loss: 0.05581077188253403\n",
      "Iteration 5561, Loss: 0.055704474449157715\n",
      "Iteration 5562, Loss: 0.05574079602956772\n",
      "Iteration 5563, Loss: 0.05582495778799057\n",
      "Iteration 5564, Loss: 0.05580540746450424\n",
      "Iteration 5565, Loss: 0.05569469928741455\n",
      "Iteration 5566, Loss: 0.05576146021485329\n",
      "Iteration 5567, Loss: 0.0558241605758667\n",
      "Iteration 5568, Loss: 0.05570785328745842\n",
      "Iteration 5569, Loss: 0.05574556440114975\n",
      "Iteration 5570, Loss: 0.05583560839295387\n",
      "Iteration 5571, Loss: 0.05582209676504135\n",
      "Iteration 5572, Loss: 0.055716436356306076\n",
      "Iteration 5573, Loss: 0.05572644993662834\n",
      "Iteration 5574, Loss: 0.05578402802348137\n",
      "Iteration 5575, Loss: 0.05566493794322014\n",
      "Iteration 5576, Loss: 0.05577751249074936\n",
      "Iteration 5577, Loss: 0.055867914110422134\n",
      "Iteration 5578, Loss: 0.055854879319667816\n",
      "Iteration 5579, Loss: 0.05574929714202881\n",
      "Iteration 5580, Loss: 0.05568134784698486\n",
      "Iteration 5581, Loss: 0.05573928356170654\n",
      "Iteration 5582, Loss: 0.05562460422515869\n",
      "Iteration 5583, Loss: 0.05578557774424553\n",
      "Iteration 5584, Loss: 0.05585102364420891\n",
      "Iteration 5585, Loss: 0.05581120774149895\n",
      "Iteration 5586, Loss: 0.05568206310272217\n",
      "Iteration 5587, Loss: 0.055797696113586426\n",
      "Iteration 5588, Loss: 0.05587927624583244\n",
      "Iteration 5589, Loss: 0.055786095559597015\n",
      "Iteration 5590, Loss: 0.05567038431763649\n",
      "Iteration 5591, Loss: 0.0557459220290184\n",
      "Iteration 5592, Loss: 0.055717311799526215\n",
      "Iteration 5593, Loss: 0.055636368691921234\n",
      "Iteration 5594, Loss: 0.055668674409389496\n",
      "Iteration 5595, Loss: 0.05565997213125229\n",
      "Iteration 5596, Loss: 0.05564972013235092\n",
      "Iteration 5597, Loss: 0.05569934844970703\n",
      "Iteration 5598, Loss: 0.05565452575683594\n",
      "Iteration 5599, Loss: 0.055732887238264084\n",
      "Iteration 5600, Loss: 0.0557711124420166\n",
      "Iteration 5601, Loss: 0.05570439621806145\n",
      "Iteration 5602, Loss: 0.055697087198495865\n",
      "Iteration 5603, Loss: 0.05571802705526352\n",
      "Iteration 5604, Loss: 0.055643558502197266\n",
      "Iteration 5605, Loss: 0.05564789101481438\n",
      "Iteration 5606, Loss: 0.055682223290205\n",
      "Iteration 5607, Loss: 0.055619798600673676\n",
      "Iteration 5608, Loss: 0.05577500909566879\n",
      "Iteration 5609, Loss: 0.05583095923066139\n",
      "Iteration 5610, Loss: 0.055784426629543304\n",
      "Iteration 5611, Loss: 0.05565226450562477\n",
      "Iteration 5612, Loss: 0.055841248482465744\n",
      "Iteration 5613, Loss: 0.0559212788939476\n",
      "Iteration 5614, Loss: 0.05581895634531975\n",
      "Iteration 5615, Loss: 0.055655401200056076\n",
      "Iteration 5616, Loss: 0.055738091468811035\n",
      "Iteration 5617, Loss: 0.05572044849395752\n",
      "Iteration 5618, Loss: 0.05562373250722885\n",
      "Iteration 5619, Loss: 0.05577067658305168\n",
      "Iteration 5620, Loss: 0.055765312165021896\n",
      "Iteration 5621, Loss: 0.055644236505031586\n",
      "Iteration 5622, Loss: 0.055754583328962326\n",
      "Iteration 5623, Loss: 0.05577417463064194\n",
      "Iteration 5624, Loss: 0.05566525459289551\n",
      "Iteration 5625, Loss: 0.055783193558454514\n",
      "Iteration 5626, Loss: 0.05585245415568352\n",
      "Iteration 5627, Loss: 0.055778346955776215\n",
      "Iteration 5628, Loss: 0.055656831711530685\n",
      "Iteration 5629, Loss: 0.05575120449066162\n",
      "Iteration 5630, Loss: 0.05572744458913803\n",
      "Iteration 5631, Loss: 0.05566009134054184\n",
      "Iteration 5632, Loss: 0.05567868798971176\n",
      "Iteration 5633, Loss: 0.05564916133880615\n",
      "Iteration 5634, Loss: 0.055661920458078384\n",
      "Iteration 5635, Loss: 0.05566763877868652\n",
      "Iteration 5636, Loss: 0.055655479431152344\n",
      "Iteration 5637, Loss: 0.055688146501779556\n",
      "Iteration 5638, Loss: 0.05563974380493164\n",
      "Iteration 5639, Loss: 0.055744849145412445\n",
      "Iteration 5640, Loss: 0.0557885579764843\n",
      "Iteration 5641, Loss: 0.05573431774973869\n",
      "Iteration 5642, Loss: 0.05564042180776596\n",
      "Iteration 5643, Loss: 0.055664461106061935\n",
      "Iteration 5644, Loss: 0.055673204362392426\n",
      "Iteration 5645, Loss: 0.05566839501261711\n",
      "Iteration 5646, Loss: 0.055670659989118576\n",
      "Iteration 5647, Loss: 0.05562850087881088\n",
      "Iteration 5648, Loss: 0.05573872849345207\n",
      "Iteration 5649, Loss: 0.05575474351644516\n",
      "Iteration 5650, Loss: 0.055666010826826096\n",
      "Iteration 5651, Loss: 0.05576392263174057\n",
      "Iteration 5652, Loss: 0.05580361932516098\n",
      "Iteration 5653, Loss: 0.05568377301096916\n",
      "Iteration 5654, Loss: 0.055767856538295746\n",
      "Iteration 5655, Loss: 0.055855635553598404\n",
      "Iteration 5656, Loss: 0.05582861229777336\n",
      "Iteration 5657, Loss: 0.0557049922645092\n",
      "Iteration 5658, Loss: 0.05576034635305405\n",
      "Iteration 5659, Loss: 0.05583890527486801\n",
      "Iteration 5660, Loss: 0.055752914398908615\n",
      "Iteration 5661, Loss: 0.055688660591840744\n",
      "Iteration 5662, Loss: 0.055756811052560806\n",
      "Iteration 5663, Loss: 0.05571659654378891\n",
      "Iteration 5664, Loss: 0.05565730854868889\n",
      "Iteration 5665, Loss: 0.05568532273173332\n",
      "Iteration 5666, Loss: 0.05566032975912094\n",
      "Iteration 5667, Loss: 0.055661797523498535\n",
      "Iteration 5668, Loss: 0.05566152185201645\n",
      "Iteration 5669, Loss: 0.055653613060712814\n",
      "Iteration 5670, Loss: 0.05565973371267319\n",
      "Iteration 5671, Loss: 0.055638909339904785\n",
      "Iteration 5672, Loss: 0.055717986077070236\n",
      "Iteration 5673, Loss: 0.05569633096456528\n",
      "Iteration 5674, Loss: 0.05567833036184311\n",
      "Iteration 5675, Loss: 0.055694740265607834\n",
      "Iteration 5676, Loss: 0.05563751980662346\n",
      "Iteration 5677, Loss: 0.05565190315246582\n",
      "Iteration 5678, Loss: 0.0556357316672802\n",
      "Iteration 5679, Loss: 0.055688343942165375\n",
      "Iteration 5680, Loss: 0.055655043572187424\n",
      "Iteration 5681, Loss: 0.05571484565734863\n",
      "Iteration 5682, Loss: 0.05570920556783676\n",
      "Iteration 5683, Loss: 0.05565603822469711\n",
      "Iteration 5684, Loss: 0.05566171929240227\n",
      "Iteration 5685, Loss: 0.055680952966213226\n",
      "Iteration 5686, Loss: 0.0556541308760643\n",
      "Iteration 5687, Loss: 0.055718742311000824\n",
      "Iteration 5688, Loss: 0.055727243423461914\n",
      "Iteration 5689, Loss: 0.055634062737226486\n",
      "Iteration 5690, Loss: 0.05575653165578842\n",
      "Iteration 5691, Loss: 0.05574456974864006\n",
      "Iteration 5692, Loss: 0.055642686784267426\n",
      "Iteration 5693, Loss: 0.055667560547590256\n",
      "Iteration 5694, Loss: 0.055643562227487564\n",
      "Iteration 5695, Loss: 0.05563938617706299\n",
      "Iteration 5696, Loss: 0.05564657971262932\n",
      "Iteration 5697, Loss: 0.05565985292196274\n",
      "Iteration 5698, Loss: 0.055626433342695236\n",
      "Iteration 5699, Loss: 0.055750928819179535\n",
      "Iteration 5700, Loss: 0.05572522059082985\n",
      "Iteration 5701, Loss: 0.055670224130153656\n",
      "Iteration 5702, Loss: 0.055705033242702484\n",
      "Iteration 5703, Loss: 0.05564026162028313\n",
      "Iteration 5704, Loss: 0.055778346955776215\n",
      "Iteration 5705, Loss: 0.05579479783773422\n",
      "Iteration 5706, Loss: 0.05564848706126213\n",
      "Iteration 5707, Loss: 0.055804572999477386\n",
      "Iteration 5708, Loss: 0.05590212345123291\n",
      "Iteration 5709, Loss: 0.055888138711452484\n",
      "Iteration 5710, Loss: 0.05577651783823967\n",
      "Iteration 5711, Loss: 0.055668991059064865\n",
      "Iteration 5712, Loss: 0.05577167123556137\n",
      "Iteration 5713, Loss: 0.055725496262311935\n",
      "Iteration 5714, Loss: 0.05569112300872803\n",
      "Iteration 5715, Loss: 0.0557379350066185\n",
      "Iteration 5716, Loss: 0.05569648742675781\n",
      "Iteration 5717, Loss: 0.05568194389343262\n",
      "Iteration 5718, Loss: 0.055683016777038574\n",
      "Iteration 5719, Loss: 0.05568309873342514\n",
      "Iteration 5720, Loss: 0.055696964263916016\n",
      "Iteration 5721, Loss: 0.05563966557383537\n",
      "Iteration 5722, Loss: 0.055748384445905685\n",
      "Iteration 5723, Loss: 0.055732451379299164\n",
      "Iteration 5724, Loss: 0.055661242455244064\n",
      "Iteration 5725, Loss: 0.05570022389292717\n",
      "Iteration 5726, Loss: 0.055647771805524826\n",
      "Iteration 5727, Loss: 0.05575315281748772\n",
      "Iteration 5728, Loss: 0.05576865002512932\n",
      "Iteration 5729, Loss: 0.055663108825683594\n",
      "Iteration 5730, Loss: 0.055764321237802505\n",
      "Iteration 5731, Loss: 0.05581287667155266\n",
      "Iteration 5732, Loss: 0.05572402849793434\n",
      "Iteration 5733, Loss: 0.05570705980062485\n",
      "Iteration 5734, Loss: 0.05576726049184799\n",
      "Iteration 5735, Loss: 0.05569382756948471\n",
      "Iteration 5736, Loss: 0.055720649659633636\n",
      "Iteration 5737, Loss: 0.055763840675354004\n",
      "Iteration 5738, Loss: 0.05567093938589096\n",
      "Iteration 5739, Loss: 0.055761776864528656\n",
      "Iteration 5740, Loss: 0.055820465087890625\n",
      "Iteration 5741, Loss: 0.05574445053935051\n",
      "Iteration 5742, Loss: 0.05567868798971176\n",
      "Iteration 5743, Loss: 0.05573972314596176\n",
      "Iteration 5744, Loss: 0.05567236989736557\n",
      "Iteration 5745, Loss: 0.05574449151754379\n",
      "Iteration 5746, Loss: 0.055790625512599945\n",
      "Iteration 5747, Loss: 0.05572120472788811\n",
      "Iteration 5748, Loss: 0.05569195747375488\n",
      "Iteration 5749, Loss: 0.055737338960170746\n",
      "Iteration 5750, Loss: 0.055653929710388184\n",
      "Iteration 5751, Loss: 0.05576161667704582\n",
      "Iteration 5752, Loss: 0.05581855773925781\n",
      "Iteration 5753, Loss: 0.055771712213754654\n",
      "Iteration 5754, Loss: 0.05565842241048813\n",
      "Iteration 5755, Loss: 0.05579952523112297\n",
      "Iteration 5756, Loss: 0.05583878606557846\n",
      "Iteration 5757, Loss: 0.055705904960632324\n",
      "Iteration 5758, Loss: 0.0557587556540966\n",
      "Iteration 5759, Loss: 0.05585825815796852\n",
      "Iteration 5760, Loss: 0.05585388466715813\n",
      "Iteration 5761, Loss: 0.05575684830546379\n",
      "Iteration 5762, Loss: 0.05566485971212387\n",
      "Iteration 5763, Loss: 0.05572712421417236\n",
      "Iteration 5764, Loss: 0.05564181134104729\n",
      "Iteration 5765, Loss: 0.05574015900492668\n",
      "Iteration 5766, Loss: 0.055771831423044205\n",
      "Iteration 5767, Loss: 0.055696964263916016\n",
      "Iteration 5768, Loss: 0.05571595951914787\n",
      "Iteration 5769, Loss: 0.055745285004377365\n",
      "Iteration 5770, Loss: 0.05561951920390129\n",
      "Iteration 5771, Loss: 0.05574381351470947\n",
      "Iteration 5772, Loss: 0.05573427677154541\n",
      "Iteration 5773, Loss: 0.05564054101705551\n",
      "Iteration 5774, Loss: 0.05571810528635979\n",
      "Iteration 5775, Loss: 0.05566779896616936\n",
      "Iteration 5776, Loss: 0.0557330884039402\n",
      "Iteration 5777, Loss: 0.055776357650756836\n",
      "Iteration 5778, Loss: 0.055713098496198654\n",
      "Iteration 5779, Loss: 0.0556870736181736\n",
      "Iteration 5780, Loss: 0.0557154044508934\n",
      "Iteration 5781, Loss: 0.05564526841044426\n",
      "Iteration 5782, Loss: 0.0556744746863842\n",
      "Iteration 5783, Loss: 0.055648528039455414\n",
      "Iteration 5784, Loss: 0.05571814626455307\n",
      "Iteration 5785, Loss: 0.055690132081508636\n",
      "Iteration 5786, Loss: 0.055694662034511566\n",
      "Iteration 5787, Loss: 0.055725179612636566\n",
      "Iteration 5788, Loss: 0.05565293878316879\n",
      "Iteration 5789, Loss: 0.055769842118024826\n",
      "Iteration 5790, Loss: 0.055796824395656586\n",
      "Iteration 5791, Loss: 0.055665455758571625\n",
      "Iteration 5792, Loss: 0.055785100907087326\n",
      "Iteration 5793, Loss: 0.055875301361083984\n",
      "Iteration 5794, Loss: 0.05585114285349846\n",
      "Iteration 5795, Loss: 0.055728040635585785\n",
      "Iteration 5796, Loss: 0.055732689797878265\n",
      "Iteration 5797, Loss: 0.05581442639231682\n",
      "Iteration 5798, Loss: 0.05573241040110588\n",
      "Iteration 5799, Loss: 0.05570332333445549\n",
      "Iteration 5800, Loss: 0.055768728256225586\n",
      "Iteration 5801, Loss: 0.0557272844016552\n",
      "Iteration 5802, Loss: 0.05565134808421135\n",
      "Iteration 5803, Loss: 0.055712662637233734\n",
      "Iteration 5804, Loss: 0.05565246194601059\n",
      "Iteration 5805, Loss: 0.055728040635585785\n",
      "Iteration 5806, Loss: 0.05576014891266823\n",
      "Iteration 5807, Loss: 0.05569108575582504\n",
      "Iteration 5808, Loss: 0.055716753005981445\n",
      "Iteration 5809, Loss: 0.05573670193552971\n",
      "Iteration 5810, Loss: 0.05563310906291008\n",
      "Iteration 5811, Loss: 0.05566374585032463\n",
      "Iteration 5812, Loss: 0.055640578269958496\n",
      "Iteration 5813, Loss: 0.05565134808421135\n",
      "Iteration 5814, Loss: 0.05561892315745354\n",
      "Iteration 5815, Loss: 0.055694304406642914\n",
      "Iteration 5816, Loss: 0.055678050965070724\n",
      "Iteration 5817, Loss: 0.05567014589905739\n",
      "Iteration 5818, Loss: 0.055633388459682465\n",
      "Iteration 5819, Loss: 0.055745840072631836\n",
      "Iteration 5820, Loss: 0.05578645318746567\n",
      "Iteration 5821, Loss: 0.055728714913129807\n",
      "Iteration 5822, Loss: 0.05565289780497551\n",
      "Iteration 5823, Loss: 0.055670421570539474\n",
      "Iteration 5824, Loss: 0.055679600685834885\n",
      "Iteration 5825, Loss: 0.05568520352244377\n",
      "Iteration 5826, Loss: 0.05563557520508766\n",
      "Iteration 5827, Loss: 0.055667679756879807\n",
      "Iteration 5828, Loss: 0.05563787743449211\n",
      "Iteration 5829, Loss: 0.05570133775472641\n",
      "Iteration 5830, Loss: 0.05563922971487045\n",
      "Iteration 5831, Loss: 0.05575823783874512\n",
      "Iteration 5832, Loss: 0.055813394486904144\n",
      "Iteration 5833, Loss: 0.055768370628356934\n",
      "Iteration 5834, Loss: 0.05563763901591301\n",
      "Iteration 5835, Loss: 0.055858414620161057\n",
      "Iteration 5836, Loss: 0.05593649670481682\n",
      "Iteration 5837, Loss: 0.05583349987864494\n",
      "Iteration 5838, Loss: 0.05564650148153305\n",
      "Iteration 5839, Loss: 0.05573571100831032\n",
      "Iteration 5840, Loss: 0.055727485567331314\n",
      "Iteration 5841, Loss: 0.05562691017985344\n",
      "Iteration 5842, Loss: 0.055835090577602386\n",
      "Iteration 5843, Loss: 0.05588698387145996\n",
      "Iteration 5844, Loss: 0.05577107518911362\n",
      "Iteration 5845, Loss: 0.05569760128855705\n",
      "Iteration 5846, Loss: 0.055787842720746994\n",
      "Iteration 5847, Loss: 0.05577345937490463\n",
      "Iteration 5848, Loss: 0.055659692734479904\n",
      "Iteration 5849, Loss: 0.055808667093515396\n",
      "Iteration 5850, Loss: 0.05587832257151604\n",
      "Iteration 5851, Loss: 0.0557810477912426\n",
      "Iteration 5852, Loss: 0.055677175521850586\n",
      "Iteration 5853, Loss: 0.05575939267873764\n",
      "Iteration 5854, Loss: 0.055737774819135666\n",
      "Iteration 5855, Loss: 0.055614154785871506\n",
      "Iteration 5856, Loss: 0.0557636022567749\n",
      "Iteration 5857, Loss: 0.055781446397304535\n",
      "Iteration 5858, Loss: 0.055686675012111664\n",
      "Iteration 5859, Loss: 0.0557459220290184\n",
      "Iteration 5860, Loss: 0.05579603090882301\n",
      "Iteration 5861, Loss: 0.05569259449839592\n",
      "Iteration 5862, Loss: 0.055750053375959396\n",
      "Iteration 5863, Loss: 0.055825356394052505\n",
      "Iteration 5864, Loss: 0.05577759072184563\n",
      "Iteration 5865, Loss: 0.055649399757385254\n",
      "Iteration 5866, Loss: 0.055804453790187836\n",
      "Iteration 5867, Loss: 0.05584593862295151\n",
      "Iteration 5868, Loss: 0.055717431008815765\n",
      "Iteration 5869, Loss: 0.05574699491262436\n",
      "Iteration 5870, Loss: 0.05584343522787094\n",
      "Iteration 5871, Loss: 0.0558270625770092\n",
      "Iteration 5872, Loss: 0.05571437254548073\n",
      "Iteration 5873, Loss: 0.05573726072907448\n",
      "Iteration 5874, Loss: 0.05580437183380127\n",
      "Iteration 5875, Loss: 0.055700384080410004\n",
      "Iteration 5876, Loss: 0.05574413388967514\n",
      "Iteration 5877, Loss: 0.05582507699728012\n",
      "Iteration 5878, Loss: 0.05579904839396477\n",
      "Iteration 5879, Loss: 0.055684130638837814\n",
      "Iteration 5880, Loss: 0.0557788610458374\n",
      "Iteration 5881, Loss: 0.05584331601858139\n",
      "Iteration 5882, Loss: 0.05573078244924545\n",
      "Iteration 5883, Loss: 0.05572613328695297\n",
      "Iteration 5884, Loss: 0.05581347271800041\n",
      "Iteration 5885, Loss: 0.05579618737101555\n",
      "Iteration 5886, Loss: 0.05568806454539299\n",
      "Iteration 5887, Loss: 0.05576582998037338\n",
      "Iteration 5888, Loss: 0.0558246374130249\n",
      "Iteration 5889, Loss: 0.05570566654205322\n",
      "Iteration 5890, Loss: 0.0557481050491333\n",
      "Iteration 5891, Loss: 0.05583934113383293\n",
      "Iteration 5892, Loss: 0.055826544761657715\n",
      "Iteration 5893, Loss: 0.05572160333395004\n",
      "Iteration 5894, Loss: 0.05571794509887695\n",
      "Iteration 5895, Loss: 0.055775564163923264\n",
      "Iteration 5896, Loss: 0.0556592158973217\n",
      "Iteration 5897, Loss: 0.05577695369720459\n",
      "Iteration 5898, Loss: 0.05586385726928711\n",
      "Iteration 5899, Loss: 0.05584780499339104\n",
      "Iteration 5900, Loss: 0.05573968216776848\n",
      "Iteration 5901, Loss: 0.05569612979888916\n",
      "Iteration 5902, Loss: 0.055757008492946625\n",
      "Iteration 5903, Loss: 0.05564216896891594\n",
      "Iteration 5904, Loss: 0.05578756332397461\n",
      "Iteration 5905, Loss: 0.05587196350097656\n",
      "Iteration 5906, Loss: 0.055853210389614105\n",
      "Iteration 5907, Loss: 0.05574170872569084\n",
      "Iteration 5908, Loss: 0.055697523057460785\n",
      "Iteration 5909, Loss: 0.055763762444257736\n",
      "Iteration 5910, Loss: 0.055653732270002365\n",
      "Iteration 5911, Loss: 0.05577981844544411\n",
      "Iteration 5912, Loss: 0.05586576461791992\n",
      "Iteration 5913, Loss: 0.05584895610809326\n",
      "Iteration 5914, Loss: 0.05573992058634758\n",
      "Iteration 5915, Loss: 0.055696725845336914\n",
      "Iteration 5916, Loss: 0.055759552866220474\n",
      "Iteration 5917, Loss: 0.05564546585083008\n",
      "Iteration 5918, Loss: 0.05578712746500969\n",
      "Iteration 5919, Loss: 0.05587482824921608\n",
      "Iteration 5920, Loss: 0.055859170854091644\n",
      "Iteration 5921, Loss: 0.05575108528137207\n",
      "Iteration 5922, Loss: 0.05568035691976547\n",
      "Iteration 5923, Loss: 0.05574095621705055\n",
      "Iteration 5924, Loss: 0.05562270060181618\n",
      "Iteration 5925, Loss: 0.055807750672101974\n",
      "Iteration 5926, Loss: 0.05589882656931877\n",
      "Iteration 5927, Loss: 0.05588630959391594\n",
      "Iteration 5928, Loss: 0.05578073114156723\n",
      "Iteration 5929, Loss: 0.05563664808869362\n",
      "Iteration 5930, Loss: 0.05569469928741455\n",
      "Iteration 5931, Loss: 0.055638354271650314\n",
      "Iteration 5932, Loss: 0.055626511573791504\n",
      "Iteration 5933, Loss: 0.05572923272848129\n",
      "Iteration 5934, Loss: 0.05568540096282959\n",
      "Iteration 5935, Loss: 0.05571083351969719\n",
      "Iteration 5936, Loss: 0.05575549602508545\n",
      "Iteration 5937, Loss: 0.05569974705576897\n",
      "Iteration 5938, Loss: 0.055686794221401215\n",
      "Iteration 5939, Loss: 0.05569303035736084\n",
      "Iteration 5940, Loss: 0.05567213147878647\n",
      "Iteration 5941, Loss: 0.05568655580282211\n",
      "Iteration 5942, Loss: 0.05562480539083481\n",
      "Iteration 5943, Loss: 0.05569656938314438\n",
      "Iteration 5944, Loss: 0.05564495176076889\n",
      "Iteration 5945, Loss: 0.05573416128754616\n",
      "Iteration 5946, Loss: 0.055746518075466156\n",
      "Iteration 5947, Loss: 0.05563787743449211\n",
      "Iteration 5948, Loss: 0.0557987317442894\n",
      "Iteration 5949, Loss: 0.05584597587585449\n",
      "Iteration 5950, Loss: 0.05574135109782219\n",
      "Iteration 5951, Loss: 0.055706463754177094\n",
      "Iteration 5952, Loss: 0.05578450486063957\n",
      "Iteration 5953, Loss: 0.05573960393667221\n",
      "Iteration 5954, Loss: 0.055644236505031586\n",
      "Iteration 5955, Loss: 0.05566573143005371\n",
      "Iteration 5956, Loss: 0.05567542836070061\n",
      "Iteration 5957, Loss: 0.05566505715250969\n",
      "Iteration 5958, Loss: 0.05568373203277588\n",
      "Iteration 5959, Loss: 0.0556565560400486\n",
      "Iteration 5960, Loss: 0.05572235956788063\n",
      "Iteration 5961, Loss: 0.05574552342295647\n",
      "Iteration 5962, Loss: 0.05566183850169182\n",
      "Iteration 5963, Loss: 0.055761538445949554\n",
      "Iteration 5964, Loss: 0.05579336732625961\n",
      "Iteration 5965, Loss: 0.05566366761922836\n",
      "Iteration 5966, Loss: 0.05579113960266113\n",
      "Iteration 5967, Loss: 0.055884718894958496\n",
      "Iteration 5968, Loss: 0.055857621133327484\n",
      "Iteration 5969, Loss: 0.05572926998138428\n",
      "Iteration 5970, Loss: 0.05573713779449463\n",
      "Iteration 5971, Loss: 0.055823247879743576\n",
      "Iteration 5972, Loss: 0.055745046585798264\n",
      "Iteration 5973, Loss: 0.055691562592983246\n",
      "Iteration 5974, Loss: 0.05575438588857651\n",
      "Iteration 5975, Loss: 0.05570987984538078\n",
      "Iteration 5976, Loss: 0.055670104920864105\n",
      "Iteration 5977, Loss: 0.05569148063659668\n",
      "Iteration 5978, Loss: 0.0556635856628418\n",
      "Iteration 5979, Loss: 0.05566819757223129\n",
      "Iteration 5980, Loss: 0.05565742775797844\n",
      "Iteration 5981, Loss: 0.055667996406555176\n",
      "Iteration 5982, Loss: 0.05565301701426506\n",
      "Iteration 5983, Loss: 0.05565941333770752\n",
      "Iteration 5984, Loss: 0.05565301701426506\n",
      "Iteration 5985, Loss: 0.055661641061306\n",
      "Iteration 5986, Loss: 0.05564415827393532\n",
      "Iteration 5987, Loss: 0.055706702172756195\n",
      "Iteration 5988, Loss: 0.05566096678376198\n",
      "Iteration 5989, Loss: 0.055728159844875336\n",
      "Iteration 5990, Loss: 0.05577123165130615\n",
      "Iteration 5991, Loss: 0.05571448802947998\n",
      "Iteration 5992, Loss: 0.05566906929016113\n",
      "Iteration 5993, Loss: 0.05567800998687744\n",
      "Iteration 5994, Loss: 0.05568162724375725\n",
      "Iteration 5995, Loss: 0.05569478124380112\n",
      "Iteration 5996, Loss: 0.05562404915690422\n",
      "Iteration 5997, Loss: 0.05575720593333244\n",
      "Iteration 5998, Loss: 0.0557330846786499\n",
      "Iteration 5999, Loss: 0.05566366761922836\n",
      "Iteration 6000, Loss: 0.055700939148664474\n",
      "Iteration 6001, Loss: 0.05564026162028313\n",
      "Iteration 6002, Loss: 0.05577151104807854\n",
      "Iteration 6003, Loss: 0.05578824132680893\n",
      "Iteration 6004, Loss: 0.05566048622131348\n",
      "Iteration 6005, Loss: 0.05578216165304184\n",
      "Iteration 6006, Loss: 0.055857859551906586\n",
      "Iteration 6007, Loss: 0.05580723285675049\n",
      "Iteration 6008, Loss: 0.05565492436289787\n",
      "Iteration 6009, Loss: 0.055835526436567307\n",
      "Iteration 6010, Loss: 0.05592679977416992\n",
      "Iteration 6011, Loss: 0.055849991738796234\n",
      "Iteration 6012, Loss: 0.05564459413290024\n",
      "Iteration 6013, Loss: 0.0558418445289135\n",
      "Iteration 6014, Loss: 0.055951714515686035\n",
      "Iteration 6015, Loss: 0.05592489242553711\n",
      "Iteration 6016, Loss: 0.05578041076660156\n",
      "Iteration 6017, Loss: 0.05570157617330551\n",
      "Iteration 6018, Loss: 0.055815063416957855\n",
      "Iteration 6019, Loss: 0.05578351020812988\n",
      "Iteration 6020, Loss: 0.055620912462472916\n",
      "Iteration 6021, Loss: 0.05571802705526352\n",
      "Iteration 6022, Loss: 0.055720966309309006\n",
      "Iteration 6023, Loss: 0.05563469976186752\n",
      "Iteration 6024, Loss: 0.05580385774374008\n",
      "Iteration 6025, Loss: 0.05583127588033676\n",
      "Iteration 6026, Loss: 0.05568484589457512\n",
      "Iteration 6027, Loss: 0.05577993392944336\n",
      "Iteration 6028, Loss: 0.05588706582784653\n",
      "Iteration 6029, Loss: 0.055888812988996506\n",
      "Iteration 6030, Loss: 0.055796068161726\n",
      "Iteration 6031, Loss: 0.05561963841319084\n",
      "Iteration 6032, Loss: 0.055936817079782486\n",
      "Iteration 6033, Loss: 0.05606667324900627\n",
      "Iteration 6034, Loss: 0.05600837990641594\n",
      "Iteration 6035, Loss: 0.05578240007162094\n",
      "Iteration 6036, Loss: 0.05576173588633537\n",
      "Iteration 6037, Loss: 0.055917542427778244\n",
      "Iteration 6038, Loss: 0.05596359819173813\n",
      "Iteration 6039, Loss: 0.05590995401144028\n",
      "Iteration 6040, Loss: 0.05576757714152336\n",
      "Iteration 6041, Loss: 0.05570022389292717\n",
      "Iteration 6042, Loss: 0.05579853057861328\n",
      "Iteration 6043, Loss: 0.0557146891951561\n",
      "Iteration 6044, Loss: 0.05571810528635979\n",
      "Iteration 6045, Loss: 0.055788397789001465\n",
      "Iteration 6046, Loss: 0.05575823783874512\n",
      "Iteration 6047, Loss: 0.05563771724700928\n",
      "Iteration 6048, Loss: 0.055850230157375336\n",
      "Iteration 6049, Loss: 0.05592421814799309\n",
      "Iteration 6050, Loss: 0.05581867694854736\n",
      "Iteration 6051, Loss: 0.05565754696726799\n",
      "Iteration 6052, Loss: 0.05574480816721916\n",
      "Iteration 6053, Loss: 0.05573241040110588\n",
      "Iteration 6054, Loss: 0.05562702938914299\n",
      "Iteration 6055, Loss: 0.055842481553554535\n",
      "Iteration 6056, Loss: 0.05590089410543442\n",
      "Iteration 6057, Loss: 0.055789314210414886\n",
      "Iteration 6058, Loss: 0.05568178743124008\n",
      "Iteration 6059, Loss: 0.055771712213754654\n",
      "Iteration 6060, Loss: 0.05575966835021973\n",
      "Iteration 6061, Loss: 0.05564805120229721\n",
      "Iteration 6062, Loss: 0.05582086369395256\n",
      "Iteration 6063, Loss: 0.05588897317647934\n",
      "Iteration 6064, Loss: 0.05579241365194321\n",
      "Iteration 6065, Loss: 0.05567006394267082\n",
      "Iteration 6066, Loss: 0.05575792118906975\n",
      "Iteration 6067, Loss: 0.05574476718902588\n",
      "Iteration 6068, Loss: 0.05562353506684303\n",
      "Iteration 6069, Loss: 0.05584685131907463\n",
      "Iteration 6070, Loss: 0.05592600628733635\n",
      "Iteration 6071, Loss: 0.055856626480817795\n",
      "Iteration 6072, Loss: 0.055681269615888596\n",
      "Iteration 6073, Loss: 0.05581510439515114\n",
      "Iteration 6074, Loss: 0.0559237040579319\n",
      "Iteration 6075, Loss: 0.05588984489440918\n",
      "Iteration 6076, Loss: 0.055729351937770844\n",
      "Iteration 6077, Loss: 0.0557633675634861\n",
      "Iteration 6078, Loss: 0.05587887763977051\n",
      "Iteration 6079, Loss: 0.05584939569234848\n",
      "Iteration 6080, Loss: 0.05569382756948471\n",
      "Iteration 6081, Loss: 0.05579352378845215\n",
      "Iteration 6082, Loss: 0.05590168759226799\n",
      "Iteration 6083, Loss: 0.055864810943603516\n",
      "Iteration 6084, Loss: 0.05570678040385246\n",
      "Iteration 6085, Loss: 0.05578235909342766\n",
      "Iteration 6086, Loss: 0.055891793221235275\n",
      "Iteration 6087, Loss: 0.05585623160004616\n",
      "Iteration 6088, Loss: 0.05569247528910637\n",
      "Iteration 6089, Loss: 0.055802229791879654\n",
      "Iteration 6090, Loss: 0.055917900055646896\n",
      "Iteration 6091, Loss: 0.05588905140757561\n",
      "Iteration 6092, Loss: 0.05573948472738266\n",
      "Iteration 6093, Loss: 0.05574600026011467\n",
      "Iteration 6094, Loss: 0.05585325136780739\n",
      "Iteration 6095, Loss: 0.05581899732351303\n",
      "Iteration 6096, Loss: 0.05565293878316879\n",
      "Iteration 6097, Loss: 0.055845461785793304\n",
      "Iteration 6098, Loss: 0.055963240563869476\n",
      "Iteration 6099, Loss: 0.0559389591217041\n",
      "Iteration 6100, Loss: 0.05579777806997299\n",
      "Iteration 6101, Loss: 0.05570169538259506\n",
      "Iteration 6102, Loss: 0.05582106113433838\n",
      "Iteration 6103, Loss: 0.05582535266876221\n",
      "Iteration 6104, Loss: 0.05569211766123772\n",
      "Iteration 6105, Loss: 0.05578172579407692\n",
      "Iteration 6106, Loss: 0.05587681382894516\n",
      "Iteration 6107, Loss: 0.055854082107543945\n",
      "Iteration 6108, Loss: 0.055739205330610275\n",
      "Iteration 6109, Loss: 0.05572565644979477\n",
      "Iteration 6110, Loss: 0.05580560490489006\n",
      "Iteration 6111, Loss: 0.05574393272399902\n",
      "Iteration 6112, Loss: 0.055690567940473557\n",
      "Iteration 6113, Loss: 0.05574214458465576\n",
      "Iteration 6114, Loss: 0.055707138031721115\n",
      "Iteration 6115, Loss: 0.05567598715424538\n",
      "Iteration 6116, Loss: 0.05570252984762192\n",
      "Iteration 6117, Loss: 0.05566481873393059\n",
      "Iteration 6118, Loss: 0.055687904357910156\n",
      "Iteration 6119, Loss: 0.05568154901266098\n",
      "Iteration 6120, Loss: 0.05566565319895744\n",
      "Iteration 6121, Loss: 0.055658381432294846\n",
      "Iteration 6122, Loss: 0.05566966533660889\n",
      "Iteration 6123, Loss: 0.05565059557557106\n",
      "Iteration 6124, Loss: 0.05571385473012924\n",
      "Iteration 6125, Loss: 0.05569704622030258\n",
      "Iteration 6126, Loss: 0.05567586421966553\n",
      "Iteration 6127, Loss: 0.05569251626729965\n",
      "Iteration 6128, Loss: 0.05563724413514137\n",
      "Iteration 6129, Loss: 0.055657945573329926\n",
      "Iteration 6130, Loss: 0.055631719529628754\n",
      "Iteration 6131, Loss: 0.05571973696351051\n",
      "Iteration 6132, Loss: 0.05567952245473862\n",
      "Iteration 6133, Loss: 0.055709920823574066\n",
      "Iteration 6134, Loss: 0.05574290081858635\n",
      "Iteration 6135, Loss: 0.05566553398966789\n",
      "Iteration 6136, Loss: 0.05575716868042946\n",
      "Iteration 6137, Loss: 0.055789750069379807\n",
      "Iteration 6138, Loss: 0.05566366761922836\n",
      "Iteration 6139, Loss: 0.05578593537211418\n",
      "Iteration 6140, Loss: 0.05587232485413551\n",
      "Iteration 6141, Loss: 0.05583655834197998\n",
      "Iteration 6142, Loss: 0.055699270218610764\n",
      "Iteration 6143, Loss: 0.05577981844544411\n",
      "Iteration 6144, Loss: 0.05586985871195793\n",
      "Iteration 6145, Loss: 0.05579209327697754\n",
      "Iteration 6146, Loss: 0.05565047264099121\n",
      "Iteration 6147, Loss: 0.05571846291422844\n",
      "Iteration 6148, Loss: 0.05567050352692604\n",
      "Iteration 6149, Loss: 0.05572112649679184\n",
      "Iteration 6150, Loss: 0.05573062226176262\n",
      "Iteration 6151, Loss: 0.05563732236623764\n",
      "Iteration 6152, Loss: 0.055690567940473557\n",
      "Iteration 6153, Loss: 0.055625997483730316\n",
      "Iteration 6154, Loss: 0.055773138999938965\n",
      "Iteration 6155, Loss: 0.055819831788539886\n",
      "Iteration 6156, Loss: 0.05575772374868393\n",
      "Iteration 6157, Loss: 0.055656395852565765\n",
      "Iteration 6158, Loss: 0.05576225370168686\n",
      "Iteration 6159, Loss: 0.05574270337820053\n",
      "Iteration 6160, Loss: 0.05566199868917465\n",
      "Iteration 6161, Loss: 0.055700384080410004\n",
      "Iteration 6162, Loss: 0.05566596984863281\n",
      "Iteration 6163, Loss: 0.05570916458964348\n",
      "Iteration 6164, Loss: 0.05568611994385719\n",
      "Iteration 6165, Loss: 0.0556974820792675\n",
      "Iteration 6166, Loss: 0.05572899430990219\n",
      "Iteration 6167, Loss: 0.05566450208425522\n",
      "Iteration 6168, Loss: 0.0557476282119751\n",
      "Iteration 6169, Loss: 0.0557609423995018\n",
      "Iteration 6170, Loss: 0.05562710762023926\n",
      "Iteration 6171, Loss: 0.055724941194057465\n",
      "Iteration 6172, Loss: 0.05571695417165756\n",
      "Iteration 6173, Loss: 0.05563529580831528\n",
      "Iteration 6174, Loss: 0.05568572133779526\n",
      "Iteration 6175, Loss: 0.05563831701874733\n",
      "Iteration 6176, Loss: 0.05566084757447243\n",
      "Iteration 6177, Loss: 0.05562623590230942\n",
      "Iteration 6178, Loss: 0.05569370836019516\n",
      "Iteration 6179, Loss: 0.05568289756774902\n",
      "Iteration 6180, Loss: 0.05565556138753891\n",
      "Iteration 6181, Loss: 0.05562460422515869\n",
      "Iteration 6182, Loss: 0.05571429058909416\n",
      "Iteration 6183, Loss: 0.05570375919342041\n",
      "Iteration 6184, Loss: 0.0556485690176487\n",
      "Iteration 6185, Loss: 0.0556691512465477\n",
      "Iteration 6186, Loss: 0.055668316781520844\n",
      "Iteration 6187, Loss: 0.055660050362348557\n",
      "Iteration 6188, Loss: 0.0556788444519043\n",
      "Iteration 6189, Loss: 0.05563557147979736\n",
      "Iteration 6190, Loss: 0.055738769471645355\n",
      "Iteration 6191, Loss: 0.05577429383993149\n",
      "Iteration 6192, Loss: 0.055713098496198654\n",
      "Iteration 6193, Loss: 0.05567634105682373\n",
      "Iteration 6194, Loss: 0.05568739026784897\n",
      "Iteration 6195, Loss: 0.05567566677927971\n",
      "Iteration 6196, Loss: 0.05568937584757805\n",
      "Iteration 6197, Loss: 0.05562285706400871\n",
      "Iteration 6198, Loss: 0.05572938919067383\n",
      "Iteration 6199, Loss: 0.055698078125715256\n",
      "Iteration 6200, Loss: 0.05568552017211914\n",
      "Iteration 6201, Loss: 0.05570944398641586\n",
      "Iteration 6202, Loss: 0.05561733618378639\n",
      "Iteration 6203, Loss: 0.05581244081258774\n",
      "Iteration 6204, Loss: 0.05586652085185051\n",
      "Iteration 6205, Loss: 0.05578959360718727\n",
      "Iteration 6206, Loss: 0.05566716566681862\n",
      "Iteration 6207, Loss: 0.05578112602233887\n",
      "Iteration 6208, Loss: 0.05580031871795654\n",
      "Iteration 6209, Loss: 0.05567169189453125\n",
      "Iteration 6210, Loss: 0.05578923225402832\n",
      "Iteration 6211, Loss: 0.05588281527161598\n",
      "Iteration 6212, Loss: 0.05586191266775131\n",
      "Iteration 6213, Loss: 0.05574723333120346\n",
      "Iteration 6214, Loss: 0.055711906403303146\n",
      "Iteration 6215, Loss: 0.055794600397348404\n",
      "Iteration 6216, Loss: 0.05573121830821037\n",
      "Iteration 6217, Loss: 0.0557001456618309\n",
      "Iteration 6218, Loss: 0.055753789842128754\n",
      "Iteration 6219, Loss: 0.05571635812520981\n",
      "Iteration 6220, Loss: 0.05566748231649399\n",
      "Iteration 6221, Loss: 0.05571194738149643\n",
      "Iteration 6222, Loss: 0.05566509813070297\n",
      "Iteration 6223, Loss: 0.05570626258850098\n",
      "Iteration 6224, Loss: 0.055722396820783615\n",
      "Iteration 6225, Loss: 0.055642884224653244\n",
      "Iteration 6226, Loss: 0.055791180580854416\n",
      "Iteration 6227, Loss: 0.05582404509186745\n",
      "Iteration 6228, Loss: 0.05570086091756821\n",
      "Iteration 6229, Loss: 0.05575370788574219\n",
      "Iteration 6230, Loss: 0.05584069341421127\n",
      "Iteration 6231, Loss: 0.05580862611532211\n",
      "Iteration 6232, Loss: 0.055671773850917816\n",
      "Iteration 6233, Loss: 0.055812202394008636\n",
      "Iteration 6234, Loss: 0.05590113252401352\n",
      "Iteration 6235, Loss: 0.055820148438215256\n",
      "Iteration 6236, Loss: 0.05564292520284653\n",
      "Iteration 6237, Loss: 0.05576833337545395\n",
      "Iteration 6238, Loss: 0.05578712746500969\n",
      "Iteration 6239, Loss: 0.0556795597076416\n",
      "Iteration 6240, Loss: 0.055769167840480804\n",
      "Iteration 6241, Loss: 0.05583822727203369\n",
      "Iteration 6242, Loss: 0.05576205253601074\n",
      "Iteration 6243, Loss: 0.055669426918029785\n",
      "Iteration 6244, Loss: 0.05574306100606918\n",
      "Iteration 6245, Loss: 0.055698834359645844\n",
      "Iteration 6246, Loss: 0.05570090189576149\n",
      "Iteration 6247, Loss: 0.05572720617055893\n",
      "Iteration 6248, Loss: 0.05565456673502922\n",
      "Iteration 6249, Loss: 0.05574246495962143\n",
      "Iteration 6250, Loss: 0.05574735254049301\n",
      "Iteration 6251, Loss: 0.05562270060181618\n",
      "Iteration 6252, Loss: 0.055627625435590744\n",
      "Iteration 6253, Loss: 0.05571484938263893\n",
      "Iteration 6254, Loss: 0.05567312240600586\n",
      "Iteration 6255, Loss: 0.05571572110056877\n",
      "Iteration 6256, Loss: 0.05574754998087883\n",
      "Iteration 6257, Loss: 0.05566879361867905\n",
      "Iteration 6258, Loss: 0.05575219914317131\n",
      "Iteration 6259, Loss: 0.05578720569610596\n",
      "Iteration 6260, Loss: 0.05566708371043205\n",
      "Iteration 6261, Loss: 0.05578017607331276\n",
      "Iteration 6262, Loss: 0.05586469545960426\n",
      "Iteration 6263, Loss: 0.0558297261595726\n",
      "Iteration 6264, Loss: 0.0556974820792675\n",
      "Iteration 6265, Loss: 0.055776480585336685\n",
      "Iteration 6266, Loss: 0.05586191266775131\n",
      "Iteration 6267, Loss: 0.05578530207276344\n",
      "Iteration 6268, Loss: 0.055653177201747894\n",
      "Iteration 6269, Loss: 0.055716078728437424\n",
      "Iteration 6270, Loss: 0.05566585436463356\n",
      "Iteration 6271, Loss: 0.055726051330566406\n",
      "Iteration 6272, Loss: 0.055734436959028244\n",
      "Iteration 6273, Loss: 0.05563414469361305\n",
      "Iteration 6274, Loss: 0.055679481476545334\n",
      "Iteration 6275, Loss: 0.05562230199575424\n",
      "Iteration 6276, Loss: 0.05567328259348869\n",
      "Iteration 6277, Loss: 0.05563374608755112\n",
      "Iteration 6278, Loss: 0.055751800537109375\n",
      "Iteration 6279, Loss: 0.05573912709951401\n",
      "Iteration 6280, Loss: 0.05564991757273674\n",
      "Iteration 6281, Loss: 0.055681031197309494\n",
      "Iteration 6282, Loss: 0.05561947822570801\n",
      "Iteration 6283, Loss: 0.05572156235575676\n",
      "Iteration 6284, Loss: 0.05571794509887695\n",
      "Iteration 6285, Loss: 0.055638473480939865\n",
      "Iteration 6286, Loss: 0.05574743077158928\n",
      "Iteration 6287, Loss: 0.055718425661325455\n",
      "Iteration 6288, Loss: 0.055681031197309494\n",
      "Iteration 6289, Loss: 0.05571905896067619\n",
      "Iteration 6290, Loss: 0.05566438287496567\n",
      "Iteration 6291, Loss: 0.05573384091258049\n",
      "Iteration 6292, Loss: 0.05573415756225586\n",
      "Iteration 6293, Loss: 0.05565035343170166\n",
      "Iteration 6294, Loss: 0.055676184594631195\n",
      "Iteration 6295, Loss: 0.055628180503845215\n",
      "Iteration 6296, Loss: 0.05572279542684555\n",
      "Iteration 6297, Loss: 0.055703602731227875\n",
      "Iteration 6298, Loss: 0.05566704645752907\n",
      "Iteration 6299, Loss: 0.05568500608205795\n",
      "Iteration 6300, Loss: 0.05565476417541504\n",
      "Iteration 6301, Loss: 0.05565059185028076\n",
      "Iteration 6302, Loss: 0.05566410347819328\n",
      "Iteration 6303, Loss: 0.055637478828430176\n",
      "Iteration 6304, Loss: 0.05567797273397446\n",
      "Iteration 6305, Loss: 0.05565742775797844\n",
      "Iteration 6306, Loss: 0.05570352450013161\n",
      "Iteration 6307, Loss: 0.05567344278097153\n",
      "Iteration 6308, Loss: 0.05571071431040764\n",
      "Iteration 6309, Loss: 0.055745285004377365\n",
      "Iteration 6310, Loss: 0.055679403245449066\n",
      "Iteration 6311, Loss: 0.05572656914591789\n",
      "Iteration 6312, Loss: 0.05574381351470947\n",
      "Iteration 6313, Loss: 0.055626749992370605\n",
      "Iteration 6314, Loss: 0.05565603822469711\n",
      "Iteration 6315, Loss: 0.05565587803721428\n",
      "Iteration 6316, Loss: 0.05562814325094223\n",
      "Iteration 6317, Loss: 0.05567721650004387\n",
      "Iteration 6318, Loss: 0.055635176599025726\n",
      "Iteration 6319, Loss: 0.05563434213399887\n",
      "Iteration 6320, Loss: 0.055691760033369064\n",
      "Iteration 6321, Loss: 0.055660247802734375\n",
      "Iteration 6322, Loss: 0.05571150779724121\n",
      "Iteration 6323, Loss: 0.055709682404994965\n",
      "Iteration 6324, Loss: 0.05565110966563225\n",
      "Iteration 6325, Loss: 0.055656276643276215\n",
      "Iteration 6326, Loss: 0.055689454078674316\n",
      "Iteration 6327, Loss: 0.055667560547590256\n",
      "Iteration 6328, Loss: 0.05569915100932121\n",
      "Iteration 6329, Loss: 0.05570113658905029\n",
      "Iteration 6330, Loss: 0.055647969245910645\n",
      "Iteration 6331, Loss: 0.05563784018158913\n",
      "Iteration 6332, Loss: 0.05572402849793434\n",
      "Iteration 6333, Loss: 0.055725615471601486\n",
      "Iteration 6334, Loss: 0.0556408166885376\n",
      "Iteration 6335, Loss: 0.05574433133006096\n",
      "Iteration 6336, Loss: 0.055720411241054535\n",
      "Iteration 6337, Loss: 0.05567324161529541\n",
      "Iteration 6338, Loss: 0.05570423603057861\n",
      "Iteration 6339, Loss: 0.055641017854213715\n",
      "Iteration 6340, Loss: 0.05576547235250473\n",
      "Iteration 6341, Loss: 0.05576511472463608\n",
      "Iteration 6342, Loss: 0.05562679097056389\n",
      "Iteration 6343, Loss: 0.05565492436289787\n",
      "Iteration 6344, Loss: 0.055642448365688324\n",
      "Iteration 6345, Loss: 0.05565830320119858\n",
      "Iteration 6346, Loss: 0.055628061294555664\n",
      "Iteration 6347, Loss: 0.05575152486562729\n",
      "Iteration 6348, Loss: 0.055724941194057465\n",
      "Iteration 6349, Loss: 0.05567292496562004\n",
      "Iteration 6350, Loss: 0.05570976063609123\n",
      "Iteration 6351, Loss: 0.05564979836344719\n",
      "Iteration 6352, Loss: 0.05576014891266823\n",
      "Iteration 6353, Loss: 0.05576833337545395\n",
      "Iteration 6354, Loss: 0.055622976273298264\n",
      "Iteration 6355, Loss: 0.05572684854269028\n",
      "Iteration 6356, Loss: 0.055721841752529144\n",
      "Iteration 6357, Loss: 0.05563267320394516\n",
      "Iteration 6358, Loss: 0.055739760398864746\n",
      "Iteration 6359, Loss: 0.05569899082183838\n",
      "Iteration 6360, Loss: 0.055701617151498795\n",
      "Iteration 6361, Loss: 0.055746160447597504\n",
      "Iteration 6362, Loss: 0.05569323152303696\n",
      "Iteration 6363, Loss: 0.055692873895168304\n",
      "Iteration 6364, Loss: 0.055694423615932465\n",
      "Iteration 6365, Loss: 0.05567670240998268\n",
      "Iteration 6366, Loss: 0.05569605156779289\n",
      "Iteration 6367, Loss: 0.0556260347366333\n",
      "Iteration 6368, Loss: 0.05578847974538803\n",
      "Iteration 6369, Loss: 0.05579177662730217\n",
      "Iteration 6370, Loss: 0.05563298985362053\n",
      "Iteration 6371, Loss: 0.055809102952480316\n",
      "Iteration 6372, Loss: 0.05589668080210686\n",
      "Iteration 6373, Loss: 0.05586818978190422\n",
      "Iteration 6374, Loss: 0.05574023723602295\n",
      "Iteration 6375, Loss: 0.055724065750837326\n",
      "Iteration 6376, Loss: 0.05581089109182358\n",
      "Iteration 6377, Loss: 0.05573054403066635\n",
      "Iteration 6378, Loss: 0.055706821382045746\n",
      "Iteration 6379, Loss: 0.05577234551310539\n",
      "Iteration 6380, Loss: 0.05573320388793945\n",
      "Iteration 6381, Loss: 0.05564967915415764\n",
      "Iteration 6382, Loss: 0.05573968216776848\n",
      "Iteration 6383, Loss: 0.05570157617330551\n",
      "Iteration 6384, Loss: 0.055697761476039886\n",
      "Iteration 6385, Loss: 0.0557376965880394\n",
      "Iteration 6386, Loss: 0.05568560212850571\n",
      "Iteration 6387, Loss: 0.05570578947663307\n",
      "Iteration 6388, Loss: 0.05570697784423828\n",
      "Iteration 6389, Loss: 0.055667441338300705\n",
      "Iteration 6390, Loss: 0.055688463151454926\n",
      "Iteration 6391, Loss: 0.05562949180603027\n",
      "Iteration 6392, Loss: 0.05575184151530266\n",
      "Iteration 6393, Loss: 0.055749259889125824\n",
      "Iteration 6394, Loss: 0.05564745515584946\n",
      "Iteration 6395, Loss: 0.05574818700551987\n",
      "Iteration 6396, Loss: 0.05575140565633774\n",
      "Iteration 6397, Loss: 0.055617135018110275\n",
      "Iteration 6398, Loss: 0.0557916983962059\n",
      "Iteration 6399, Loss: 0.05581068992614746\n",
      "Iteration 6400, Loss: 0.05568774789571762\n",
      "Iteration 6401, Loss: 0.055764518678188324\n",
      "Iteration 6402, Loss: 0.055847764015197754\n",
      "Iteration 6403, Loss: 0.055803656578063965\n",
      "Iteration 6404, Loss: 0.0556567907333374\n",
      "Iteration 6405, Loss: 0.055826544761657715\n",
      "Iteration 6406, Loss: 0.05591472238302231\n",
      "Iteration 6407, Loss: 0.05584220215678215\n",
      "Iteration 6408, Loss: 0.055643998086452484\n",
      "Iteration 6409, Loss: 0.05584200471639633\n",
      "Iteration 6410, Loss: 0.05594992637634277\n",
      "Iteration 6411, Loss: 0.05592135712504387\n",
      "Iteration 6412, Loss: 0.05577782914042473\n",
      "Iteration 6413, Loss: 0.055708132684230804\n",
      "Iteration 6414, Loss: 0.05582038685679436\n",
      "Iteration 6415, Loss: 0.055796585977077484\n",
      "Iteration 6416, Loss: 0.05563624948263168\n",
      "Iteration 6417, Loss: 0.05582984536886215\n",
      "Iteration 6418, Loss: 0.05593463033437729\n",
      "Iteration 6419, Loss: 0.05592525377869606\n",
      "Iteration 6420, Loss: 0.05581843852996826\n",
      "Iteration 6421, Loss: 0.0556720532476902\n",
      "Iteration 6422, Loss: 0.055833857506513596\n",
      "Iteration 6423, Loss: 0.055899541825056076\n",
      "Iteration 6424, Loss: 0.05579463765025139\n",
      "Iteration 6425, Loss: 0.05567967891693115\n",
      "Iteration 6426, Loss: 0.055763762444257736\n",
      "Iteration 6427, Loss: 0.0557536706328392\n",
      "Iteration 6428, Loss: 0.05566271394491196\n",
      "Iteration 6429, Loss: 0.055780891329050064\n",
      "Iteration 6430, Loss: 0.05581565946340561\n",
      "Iteration 6431, Loss: 0.05567813292145729\n",
      "Iteration 6432, Loss: 0.05577898025512695\n",
      "Iteration 6433, Loss: 0.05587903782725334\n",
      "Iteration 6434, Loss: 0.055872999131679535\n",
      "Iteration 6435, Loss: 0.055771354585886\n",
      "Iteration 6436, Loss: 0.05564725399017334\n",
      "Iteration 6437, Loss: 0.055709004402160645\n",
      "Iteration 6438, Loss: 0.05563021078705788\n",
      "Iteration 6439, Loss: 0.055683378130197525\n",
      "Iteration 6440, Loss: 0.05564085766673088\n",
      "Iteration 6441, Loss: 0.055749934166669846\n",
      "Iteration 6442, Loss: 0.05575871467590332\n",
      "Iteration 6443, Loss: 0.055642448365688324\n",
      "Iteration 6444, Loss: 0.05577925965189934\n",
      "Iteration 6445, Loss: 0.055820267647504807\n",
      "Iteration 6446, Loss: 0.05572319030761719\n",
      "Iteration 6447, Loss: 0.055714093148708344\n",
      "Iteration 6448, Loss: 0.05578120797872543\n",
      "Iteration 6449, Loss: 0.05571440979838371\n",
      "Iteration 6450, Loss: 0.05569645017385483\n",
      "Iteration 6451, Loss: 0.05573694035410881\n",
      "Iteration 6452, Loss: 0.05564602464437485\n",
      "Iteration 6453, Loss: 0.0557788610458374\n",
      "Iteration 6454, Loss: 0.05582476034760475\n",
      "Iteration 6455, Loss: 0.05572982877492905\n",
      "Iteration 6456, Loss: 0.055707018822431564\n",
      "Iteration 6457, Loss: 0.055775050073862076\n",
      "Iteration 6458, Loss: 0.05571325868368149\n",
      "Iteration 6459, Loss: 0.055692318826913834\n",
      "Iteration 6460, Loss: 0.055727243423461914\n",
      "Iteration 6461, Loss: 0.055633269250392914\n",
      "Iteration 6462, Loss: 0.05578406900167465\n",
      "Iteration 6463, Loss: 0.055815063416957855\n",
      "Iteration 6464, Loss: 0.05570225045084953\n",
      "Iteration 6465, Loss: 0.055745720863342285\n",
      "Iteration 6466, Loss: 0.05582348629832268\n",
      "Iteration 6467, Loss: 0.05577143281698227\n",
      "Iteration 6468, Loss: 0.05563310906291008\n",
      "Iteration 6469, Loss: 0.05577604100108147\n",
      "Iteration 6470, Loss: 0.055769603699445724\n",
      "Iteration 6471, Loss: 0.055622220039367676\n",
      "Iteration 6472, Loss: 0.055661641061306\n",
      "Iteration 6473, Loss: 0.05564427375793457\n",
      "Iteration 6474, Loss: 0.05563489720225334\n",
      "Iteration 6475, Loss: 0.05566442012786865\n",
      "Iteration 6476, Loss: 0.05562770739197731\n",
      "Iteration 6477, Loss: 0.05568035691976547\n",
      "Iteration 6478, Loss: 0.05563235282897949\n",
      "Iteration 6479, Loss: 0.05564260855317116\n",
      "Iteration 6480, Loss: 0.05567582696676254\n",
      "Iteration 6481, Loss: 0.055641453713178635\n",
      "Iteration 6482, Loss: 0.055728714913129807\n",
      "Iteration 6483, Loss: 0.055716317147016525\n",
      "Iteration 6484, Loss: 0.05565870180726051\n",
      "Iteration 6485, Loss: 0.05567733570933342\n",
      "Iteration 6486, Loss: 0.055646300315856934\n",
      "Iteration 6487, Loss: 0.0556289367377758\n",
      "Iteration 6488, Loss: 0.055677615106105804\n",
      "Iteration 6489, Loss: 0.05563267320394516\n",
      "Iteration 6490, Loss: 0.055633943527936935\n",
      "Iteration 6491, Loss: 0.055691562592983246\n",
      "Iteration 6492, Loss: 0.05566692724823952\n",
      "Iteration 6493, Loss: 0.05569760128855705\n",
      "Iteration 6494, Loss: 0.05568544194102287\n",
      "Iteration 6495, Loss: 0.055685363709926605\n",
      "Iteration 6496, Loss: 0.05569680780172348\n",
      "Iteration 6497, Loss: 0.055636290460824966\n",
      "Iteration 6498, Loss: 0.055678967386484146\n",
      "Iteration 6499, Loss: 0.055643241852521896\n",
      "Iteration 6500, Loss: 0.05565321445465088\n",
      "Iteration 6501, Loss: 0.05565126985311508\n",
      "Iteration 6502, Loss: 0.05565500259399414\n",
      "Iteration 6503, Loss: 0.055631719529628754\n",
      "Iteration 6504, Loss: 0.055730026215314865\n",
      "Iteration 6505, Loss: 0.055698078125715256\n",
      "Iteration 6506, Loss: 0.055691007524728775\n",
      "Iteration 6507, Loss: 0.05572398751974106\n",
      "Iteration 6508, Loss: 0.05565226450562477\n",
      "Iteration 6509, Loss: 0.05576876923441887\n",
      "Iteration 6510, Loss: 0.055796507745981216\n",
      "Iteration 6511, Loss: 0.055666446685791016\n",
      "Iteration 6512, Loss: 0.055783871561288834\n",
      "Iteration 6513, Loss: 0.05587327480316162\n",
      "Iteration 6514, Loss: 0.05584708973765373\n",
      "Iteration 6515, Loss: 0.05572168156504631\n",
      "Iteration 6516, Loss: 0.05574190616607666\n",
      "Iteration 6517, Loss: 0.05582507699728012\n",
      "Iteration 6518, Loss: 0.055745165795087814\n",
      "Iteration 6519, Loss: 0.05569028854370117\n",
      "Iteration 6520, Loss: 0.05575418472290039\n",
      "Iteration 6521, Loss: 0.055709801614284515\n",
      "Iteration 6522, Loss: 0.05566815659403801\n",
      "Iteration 6523, Loss: 0.05568560212850571\n",
      "Iteration 6524, Loss: 0.05566895008087158\n",
      "Iteration 6525, Loss: 0.05567058175802231\n",
      "Iteration 6526, Loss: 0.05566009134054184\n",
      "Iteration 6527, Loss: 0.05565214157104492\n",
      "Iteration 6528, Loss: 0.05567852780222893\n",
      "Iteration 6529, Loss: 0.05566994473338127\n",
      "Iteration 6530, Loss: 0.055671654641628265\n",
      "Iteration 6531, Loss: 0.05563231557607651\n",
      "Iteration 6532, Loss: 0.05573026463389397\n",
      "Iteration 6533, Loss: 0.05574719235301018\n",
      "Iteration 6534, Loss: 0.055661559104919434\n",
      "Iteration 6535, Loss: 0.055769920349121094\n",
      "Iteration 6536, Loss: 0.05580493062734604\n",
      "Iteration 6537, Loss: 0.05567137524485588\n",
      "Iteration 6538, Loss: 0.055786095559597015\n",
      "Iteration 6539, Loss: 0.05588400736451149\n",
      "Iteration 6540, Loss: 0.05586664006114006\n",
      "Iteration 6541, Loss: 0.05574977770447731\n",
      "Iteration 6542, Loss: 0.05570157617330551\n",
      "Iteration 6543, Loss: 0.05578192323446274\n",
      "Iteration 6544, Loss: 0.0556999072432518\n",
      "Iteration 6545, Loss: 0.055731575936079025\n",
      "Iteration 6546, Loss: 0.055797819048166275\n",
      "Iteration 6547, Loss: 0.05576189607381821\n",
      "Iteration 6548, Loss: 0.05565293878316879\n",
      "Iteration 6549, Loss: 0.055804137140512466\n",
      "Iteration 6550, Loss: 0.05584371089935303\n",
      "Iteration 6551, Loss: 0.055709682404994965\n",
      "Iteration 6552, Loss: 0.055755577981472015\n",
      "Iteration 6553, Loss: 0.05585479736328125\n",
      "Iteration 6554, Loss: 0.05585090443491936\n",
      "Iteration 6555, Loss: 0.055753909051418304\n",
      "Iteration 6556, Loss: 0.05566605180501938\n",
      "Iteration 6557, Loss: 0.05571973696351051\n",
      "Iteration 6558, Loss: 0.055636487901210785\n",
      "Iteration 6559, Loss: 0.05571083351969719\n",
      "Iteration 6560, Loss: 0.05570042133331299\n",
      "Iteration 6561, Loss: 0.055650435388088226\n",
      "Iteration 6562, Loss: 0.055636048316955566\n",
      "Iteration 6563, Loss: 0.05572915077209473\n",
      "Iteration 6564, Loss: 0.05574440956115723\n",
      "Iteration 6565, Loss: 0.05566025152802467\n",
      "Iteration 6566, Loss: 0.05576412007212639\n",
      "Iteration 6567, Loss: 0.05579423904418945\n",
      "Iteration 6568, Loss: 0.05566120147705078\n",
      "Iteration 6569, Loss: 0.055794280022382736\n",
      "Iteration 6570, Loss: 0.055891357362270355\n",
      "Iteration 6571, Loss: 0.05587617680430412\n",
      "Iteration 6572, Loss: 0.05576380342245102\n",
      "Iteration 6573, Loss: 0.0556846484541893\n",
      "Iteration 6574, Loss: 0.05577472969889641\n",
      "Iteration 6575, Loss: 0.05571659654378891\n",
      "Iteration 6576, Loss: 0.05570518970489502\n",
      "Iteration 6577, Loss: 0.05575704574584961\n",
      "Iteration 6578, Loss: 0.05571679398417473\n",
      "Iteration 6579, Loss: 0.0556621178984642\n",
      "Iteration 6580, Loss: 0.055704474449157715\n",
      "Iteration 6581, Loss: 0.055658940225839615\n",
      "Iteration 6582, Loss: 0.05569871515035629\n",
      "Iteration 6583, Loss: 0.05569736286997795\n",
      "Iteration 6584, Loss: 0.055643241852521896\n",
      "Iteration 6585, Loss: 0.05565254017710686\n",
      "Iteration 6586, Loss: 0.055650077760219574\n",
      "Iteration 6587, Loss: 0.05565289780497551\n",
      "Iteration 6588, Loss: 0.05565663427114487\n",
      "Iteration 6589, Loss: 0.05563346669077873\n",
      "Iteration 6590, Loss: 0.05573165416717529\n",
      "Iteration 6591, Loss: 0.055702369660139084\n",
      "Iteration 6592, Loss: 0.05568655580282211\n",
      "Iteration 6593, Loss: 0.05571790784597397\n",
      "Iteration 6594, Loss: 0.05564264580607414\n",
      "Iteration 6595, Loss: 0.05578581616282463\n",
      "Iteration 6596, Loss: 0.05581633374094963\n",
      "Iteration 6597, Loss: 0.055687785148620605\n",
      "Iteration 6598, Loss: 0.05576753616333008\n",
      "Iteration 6599, Loss: 0.05585845559835434\n",
      "Iteration 6600, Loss: 0.05582936853170395\n",
      "Iteration 6601, Loss: 0.05569680780172348\n",
      "Iteration 6602, Loss: 0.055777471512556076\n",
      "Iteration 6603, Loss: 0.05586354061961174\n",
      "Iteration 6604, Loss: 0.055780015885829926\n",
      "Iteration 6605, Loss: 0.05566450208425522\n",
      "Iteration 6606, Loss: 0.055735033005476\n",
      "Iteration 6607, Loss: 0.055692076683044434\n",
      "Iteration 6608, Loss: 0.05569029226899147\n",
      "Iteration 6609, Loss: 0.05569462105631828\n",
      "Iteration 6610, Loss: 0.055666446685791016\n",
      "Iteration 6611, Loss: 0.055672649294137955\n",
      "Iteration 6612, Loss: 0.05566072463989258\n",
      "Iteration 6613, Loss: 0.055620431900024414\n",
      "Iteration 6614, Loss: 0.055757880210876465\n",
      "Iteration 6615, Loss: 0.05576884746551514\n",
      "Iteration 6616, Loss: 0.05566354840993881\n",
      "Iteration 6617, Loss: 0.055774133652448654\n",
      "Iteration 6618, Loss: 0.05582718178629875\n",
      "Iteration 6619, Loss: 0.05573062226176262\n",
      "Iteration 6620, Loss: 0.05570908635854721\n",
      "Iteration 6621, Loss: 0.05577925965189934\n",
      "Iteration 6622, Loss: 0.05572275444865227\n",
      "Iteration 6623, Loss: 0.055674079805612564\n",
      "Iteration 6624, Loss: 0.055704157799482346\n",
      "Iteration 6625, Loss: 0.05563255399465561\n",
      "Iteration 6626, Loss: 0.055616021156311035\n",
      "Iteration 6627, Loss: 0.05572788044810295\n",
      "Iteration 6628, Loss: 0.05566922947764397\n",
      "Iteration 6629, Loss: 0.05573352426290512\n",
      "Iteration 6630, Loss: 0.05578649416565895\n",
      "Iteration 6631, Loss: 0.055738210678100586\n",
      "Iteration 6632, Loss: 0.05563318729400635\n",
      "Iteration 6633, Loss: 0.05570495128631592\n",
      "Iteration 6634, Loss: 0.055640220642089844\n",
      "Iteration 6635, Loss: 0.05573272705078125\n",
      "Iteration 6636, Loss: 0.05575001239776611\n",
      "Iteration 6637, Loss: 0.05565544217824936\n",
      "Iteration 6638, Loss: 0.05578359216451645\n",
      "Iteration 6639, Loss: 0.055829763412475586\n",
      "Iteration 6640, Loss: 0.05571548268198967\n",
      "Iteration 6641, Loss: 0.05573670193552971\n",
      "Iteration 6642, Loss: 0.05582074448466301\n",
      "Iteration 6643, Loss: 0.05578526109457016\n",
      "Iteration 6644, Loss: 0.05564995855093002\n",
      "Iteration 6645, Loss: 0.05583254620432854\n",
      "Iteration 6646, Loss: 0.05591118708252907\n",
      "Iteration 6647, Loss: 0.05581700801849365\n",
      "Iteration 6648, Loss: 0.0556492805480957\n",
      "Iteration 6649, Loss: 0.05574663728475571\n",
      "Iteration 6650, Loss: 0.05573594570159912\n",
      "Iteration 6651, Loss: 0.055615466088056564\n",
      "Iteration 6652, Loss: 0.05579197406768799\n",
      "Iteration 6653, Loss: 0.05578577518463135\n",
      "Iteration 6654, Loss: 0.055621229112148285\n",
      "Iteration 6655, Loss: 0.0557703971862793\n",
      "Iteration 6656, Loss: 0.05580469220876694\n",
      "Iteration 6657, Loss: 0.05572390556335449\n",
      "Iteration 6658, Loss: 0.05569422245025635\n",
      "Iteration 6659, Loss: 0.055741988122463226\n",
      "Iteration 6660, Loss: 0.05563938617706299\n",
      "Iteration 6661, Loss: 0.05579010769724846\n",
      "Iteration 6662, Loss: 0.05586497113108635\n",
      "Iteration 6663, Loss: 0.055827897042036057\n",
      "Iteration 6664, Loss: 0.055701177567243576\n",
      "Iteration 6665, Loss: 0.05577234551310539\n",
      "Iteration 6666, Loss: 0.05585090443491936\n",
      "Iteration 6667, Loss: 0.05575629323720932\n",
      "Iteration 6668, Loss: 0.05569612979888916\n",
      "Iteration 6669, Loss: 0.05577163025736809\n",
      "Iteration 6670, Loss: 0.05574194714426994\n",
      "Iteration 6671, Loss: 0.05564264580607414\n",
      "Iteration 6672, Loss: 0.05578657239675522\n",
      "Iteration 6673, Loss: 0.0557965449988842\n",
      "Iteration 6674, Loss: 0.055645667016506195\n",
      "Iteration 6675, Loss: 0.0557965449988842\n",
      "Iteration 6676, Loss: 0.055887702852487564\n",
      "Iteration 6677, Loss: 0.05587335675954819\n",
      "Iteration 6678, Loss: 0.05576412007212639\n",
      "Iteration 6679, Loss: 0.05566839501261711\n",
      "Iteration 6680, Loss: 0.055740319192409515\n",
      "Iteration 6681, Loss: 0.05564447492361069\n",
      "Iteration 6682, Loss: 0.05577230453491211\n",
      "Iteration 6683, Loss: 0.055844347923994064\n",
      "Iteration 6684, Loss: 0.05581514164805412\n",
      "Iteration 6685, Loss: 0.055695176124572754\n",
      "Iteration 6686, Loss: 0.055771589279174805\n",
      "Iteration 6687, Loss: 0.055845145136117935\n",
      "Iteration 6688, Loss: 0.05573773384094238\n",
      "Iteration 6689, Loss: 0.05571707338094711\n",
      "Iteration 6690, Loss: 0.055802106857299805\n",
      "Iteration 6691, Loss: 0.05578470602631569\n",
      "Iteration 6692, Loss: 0.05567523092031479\n",
      "Iteration 6693, Loss: 0.05578533932566643\n",
      "Iteration 6694, Loss: 0.05584677308797836\n",
      "Iteration 6695, Loss: 0.05572843551635742\n",
      "Iteration 6696, Loss: 0.05573093891143799\n",
      "Iteration 6697, Loss: 0.05582233518362045\n",
      "Iteration 6698, Loss: 0.05581068992614746\n",
      "Iteration 6699, Loss: 0.05570630356669426\n",
      "Iteration 6700, Loss: 0.055737338960170746\n",
      "Iteration 6701, Loss: 0.05579356476664543\n",
      "Iteration 6702, Loss: 0.05567125603556633\n",
      "Iteration 6703, Loss: 0.05577544495463371\n",
      "Iteration 6704, Loss: 0.05586898326873779\n",
      "Iteration 6705, Loss: 0.05585877224802971\n",
      "Iteration 6706, Loss: 0.055755577981472015\n",
      "Iteration 6707, Loss: 0.05566906929016113\n",
      "Iteration 6708, Loss: 0.055724263191223145\n",
      "Iteration 6709, Loss: 0.055620670318603516\n",
      "Iteration 6710, Loss: 0.05567268654704094\n",
      "Iteration 6711, Loss: 0.05562233924865723\n",
      "Iteration 6712, Loss: 0.05570133775472641\n",
      "Iteration 6713, Loss: 0.055622540414333344\n",
      "Iteration 6714, Loss: 0.05577477067708969\n",
      "Iteration 6715, Loss: 0.05583135411143303\n",
      "Iteration 6716, Loss: 0.0557834729552269\n",
      "Iteration 6717, Loss: 0.05565035715699196\n",
      "Iteration 6718, Loss: 0.05584069341421127\n",
      "Iteration 6719, Loss: 0.055917464196681976\n",
      "Iteration 6720, Loss: 0.05581291764974594\n",
      "Iteration 6721, Loss: 0.05566052719950676\n",
      "Iteration 6722, Loss: 0.05574393272399902\n",
      "Iteration 6723, Loss: 0.055725932121276855\n",
      "Iteration 6724, Loss: 0.05562500283122063\n",
      "Iteration 6725, Loss: 0.05580572411417961\n",
      "Iteration 6726, Loss: 0.05582539364695549\n",
      "Iteration 6727, Loss: 0.05568277835845947\n",
      "Iteration 6728, Loss: 0.055778466165065765\n",
      "Iteration 6729, Loss: 0.05587887763977051\n",
      "Iteration 6730, Loss: 0.05586954206228256\n",
      "Iteration 6731, Loss: 0.05576201528310776\n",
      "Iteration 6732, Loss: 0.05567216873168945\n",
      "Iteration 6733, Loss: 0.05574846267700195\n",
      "Iteration 6734, Loss: 0.055663030594587326\n",
      "Iteration 6735, Loss: 0.05575621500611305\n",
      "Iteration 6736, Loss: 0.055824439972639084\n",
      "Iteration 6737, Loss: 0.05579269304871559\n",
      "Iteration 6738, Loss: 0.055672407150268555\n",
      "Iteration 6739, Loss: 0.05580393597483635\n",
      "Iteration 6740, Loss: 0.05587617680430412\n",
      "Iteration 6741, Loss: 0.05576876923441887\n",
      "Iteration 6742, Loss: 0.055695295333862305\n",
      "Iteration 6743, Loss: 0.05578049272298813\n",
      "Iteration 6744, Loss: 0.0557633638381958\n",
      "Iteration 6745, Loss: 0.05565520375967026\n",
      "Iteration 6746, Loss: 0.055812519043684006\n",
      "Iteration 6747, Loss: 0.055872559547424316\n",
      "Iteration 6748, Loss: 0.05575283616781235\n",
      "Iteration 6749, Loss: 0.05571460723876953\n",
      "Iteration 6750, Loss: 0.05580691620707512\n",
      "Iteration 6751, Loss: 0.05579622834920883\n",
      "Iteration 6752, Loss: 0.05569247528910637\n",
      "Iteration 6753, Loss: 0.055756133049726486\n",
      "Iteration 6754, Loss: 0.055811285972595215\n",
      "Iteration 6755, Loss: 0.05568802356719971\n",
      "Iteration 6756, Loss: 0.05576479807496071\n",
      "Iteration 6757, Loss: 0.055859290063381195\n",
      "Iteration 6758, Loss: 0.05584995076060295\n",
      "Iteration 6759, Loss: 0.05574743077158928\n",
      "Iteration 6760, Loss: 0.05568007752299309\n",
      "Iteration 6761, Loss: 0.05573439598083496\n",
      "Iteration 6762, Loss: 0.05561650171875954\n",
      "Iteration 6763, Loss: 0.05574489012360573\n",
      "Iteration 6764, Loss: 0.05575525760650635\n",
      "Iteration 6765, Loss: 0.055663228034973145\n",
      "Iteration 6766, Loss: 0.05576944351196289\n",
      "Iteration 6767, Loss: 0.0558091439306736\n",
      "Iteration 6768, Loss: 0.05568838119506836\n",
      "Iteration 6769, Loss: 0.055765312165021896\n",
      "Iteration 6770, Loss: 0.05585428327322006\n",
      "Iteration 6771, Loss: 0.05582916736602783\n",
      "Iteration 6772, Loss: 0.055707935243844986\n",
      "Iteration 6773, Loss: 0.05575522035360336\n",
      "Iteration 6774, Loss: 0.055831991136074066\n",
      "Iteration 6775, Loss: 0.055743854492902756\n",
      "Iteration 6776, Loss: 0.05569891259074211\n",
      "Iteration 6777, Loss: 0.0557684525847435\n",
      "Iteration 6778, Loss: 0.055730704218149185\n",
      "Iteration 6779, Loss: 0.05564522743225098\n",
      "Iteration 6780, Loss: 0.05572553724050522\n",
      "Iteration 6781, Loss: 0.05567086115479469\n",
      "Iteration 6782, Loss: 0.05572688579559326\n",
      "Iteration 6783, Loss: 0.0557732991874218\n",
      "Iteration 6784, Loss: 0.055720530450344086\n",
      "Iteration 6785, Loss: 0.05565854161977768\n",
      "Iteration 6786, Loss: 0.05566072836518288\n",
      "Iteration 6787, Loss: 0.05570066347718239\n",
      "Iteration 6788, Loss: 0.05571750923991203\n",
      "Iteration 6789, Loss: 0.055637918412685394\n",
      "Iteration 6790, Loss: 0.05579960346221924\n",
      "Iteration 6791, Loss: 0.05582670494914055\n",
      "Iteration 6792, Loss: 0.05567844957113266\n",
      "Iteration 6793, Loss: 0.055789314210414886\n",
      "Iteration 6794, Loss: 0.05589854717254639\n",
      "Iteration 6795, Loss: 0.05590013787150383\n",
      "Iteration 6796, Loss: 0.055805206298828125\n",
      "Iteration 6797, Loss: 0.05563545599579811\n",
      "Iteration 6798, Loss: 0.05589298531413078\n",
      "Iteration 6799, Loss: 0.05599498748779297\n",
      "Iteration 6800, Loss: 0.055913686752319336\n",
      "Iteration 6801, Loss: 0.05567336082458496\n",
      "Iteration 6802, Loss: 0.05585026741027832\n",
      "Iteration 6803, Loss: 0.0560101680457592\n",
      "Iteration 6804, Loss: 0.05605916306376457\n",
      "Iteration 6805, Loss: 0.056007545441389084\n",
      "Iteration 6806, Loss: 0.05586596578359604\n",
      "Iteration 6807, Loss: 0.05564546585083008\n",
      "Iteration 6808, Loss: 0.055960338562726974\n",
      "Iteration 6809, Loss: 0.05614165589213371\n",
      "Iteration 6810, Loss: 0.05613156408071518\n",
      "Iteration 6811, Loss: 0.05594988912343979\n",
      "Iteration 6812, Loss: 0.05563187971711159\n",
      "Iteration 6813, Loss: 0.05588197708129883\n",
      "Iteration 6814, Loss: 0.05603436753153801\n",
      "Iteration 6815, Loss: 0.05607231706380844\n",
      "Iteration 6816, Loss: 0.0560077466070652\n",
      "Iteration 6817, Loss: 0.055852536112070084\n",
      "Iteration 6818, Loss: 0.05565448850393295\n",
      "Iteration 6819, Loss: 0.05589401721954346\n",
      "Iteration 6820, Loss: 0.05599745362997055\n",
      "Iteration 6821, Loss: 0.05591881275177002\n",
      "Iteration 6822, Loss: 0.055677853524684906\n",
      "Iteration 6823, Loss: 0.05584951490163803\n",
      "Iteration 6824, Loss: 0.05601302906870842\n",
      "Iteration 6825, Loss: 0.05606496334075928\n",
      "Iteration 6826, Loss: 0.056016091257333755\n",
      "Iteration 6827, Loss: 0.05587681382894516\n",
      "Iteration 6828, Loss: 0.05565830320119858\n",
      "Iteration 6829, Loss: 0.055943846702575684\n",
      "Iteration 6830, Loss: 0.05612564459443092\n",
      "Iteration 6831, Loss: 0.056113921105861664\n",
      "Iteration 6832, Loss: 0.05592811107635498\n",
      "Iteration 6833, Loss: 0.0556316003203392\n",
      "Iteration 6834, Loss: 0.05577206611633301\n",
      "Iteration 6835, Loss: 0.05580771341919899\n",
      "Iteration 6836, Loss: 0.0557452067732811\n",
      "Iteration 6837, Loss: 0.05563541501760483\n",
      "Iteration 6838, Loss: 0.05565059185028076\n",
      "Iteration 6839, Loss: 0.0556996688246727\n",
      "Iteration 6840, Loss: 0.05571039766073227\n",
      "Iteration 6841, Loss: 0.055627308785915375\n",
      "Iteration 6842, Loss: 0.05582062527537346\n",
      "Iteration 6843, Loss: 0.05585269257426262\n",
      "Iteration 6844, Loss: 0.05570821091532707\n",
      "Iteration 6845, Loss: 0.055764202028512955\n",
      "Iteration 6846, Loss: 0.055871449410915375\n",
      "Iteration 6847, Loss: 0.05587327852845192\n",
      "Iteration 6848, Loss: 0.055780332535505295\n",
      "Iteration 6849, Loss: 0.055626314133405685\n",
      "Iteration 6850, Loss: 0.055721085518598557\n",
      "Iteration 6851, Loss: 0.05565885826945305\n",
      "Iteration 6852, Loss: 0.055738210678100586\n",
      "Iteration 6853, Loss: 0.05578120797872543\n",
      "Iteration 6854, Loss: 0.055715642869472504\n",
      "Iteration 6855, Loss: 0.055684689432382584\n",
      "Iteration 6856, Loss: 0.05571270361542702\n",
      "Iteration 6857, Loss: 0.05563871189951897\n",
      "Iteration 6858, Loss: 0.05564161390066147\n",
      "Iteration 6859, Loss: 0.055681150406599045\n",
      "Iteration 6860, Loss: 0.055619239807128906\n",
      "Iteration 6861, Loss: 0.05569545552134514\n",
      "Iteration 6862, Loss: 0.055651068687438965\n",
      "Iteration 6863, Loss: 0.0557352714240551\n",
      "Iteration 6864, Loss: 0.05574480816721916\n",
      "Iteration 6865, Loss: 0.05563489720225334\n",
      "Iteration 6866, Loss: 0.0557708740234375\n",
      "Iteration 6867, Loss: 0.05578657239675522\n",
      "Iteration 6868, Loss: 0.05565905570983887\n",
      "Iteration 6869, Loss: 0.05579618737101555\n",
      "Iteration 6870, Loss: 0.05588039010763168\n",
      "Iteration 6871, Loss: 0.055835090577602386\n",
      "Iteration 6872, Loss: 0.05568905919790268\n",
      "Iteration 6873, Loss: 0.05579535290598869\n",
      "Iteration 6874, Loss: 0.05588972568511963\n",
      "Iteration 6875, Loss: 0.05582781881093979\n",
      "Iteration 6876, Loss: 0.055631123483181\n",
      "Iteration 6877, Loss: 0.05586477369070053\n",
      "Iteration 6878, Loss: 0.055980801582336426\n",
      "Iteration 6879, Loss: 0.0559571199119091\n",
      "Iteration 6880, Loss: 0.05581720918416977\n",
      "Iteration 6881, Loss: 0.05569060891866684\n",
      "Iteration 6882, Loss: 0.05582321062684059\n",
      "Iteration 6883, Loss: 0.05585046857595444\n",
      "Iteration 6884, Loss: 0.05573233217000961\n",
      "Iteration 6885, Loss: 0.05573717877268791\n",
      "Iteration 6886, Loss: 0.055823009461164474\n",
      "Iteration 6887, Loss: 0.05579638481140137\n",
      "Iteration 6888, Loss: 0.05569005012512207\n",
      "Iteration 6889, Loss: 0.05576483532786369\n",
      "Iteration 6890, Loss: 0.05581863969564438\n",
      "Iteration 6891, Loss: 0.055712223052978516\n",
      "Iteration 6892, Loss: 0.055739760398864746\n",
      "Iteration 6893, Loss: 0.055821143090724945\n",
      "Iteration 6894, Loss: 0.05579666420817375\n",
      "Iteration 6895, Loss: 0.0556868314743042\n",
      "Iteration 6896, Loss: 0.05577151104807854\n",
      "Iteration 6897, Loss: 0.05583151429891586\n",
      "Iteration 6898, Loss: 0.0557202510535717\n",
      "Iteration 6899, Loss: 0.055734794586896896\n",
      "Iteration 6900, Loss: 0.055820904672145844\n",
      "Iteration 6901, Loss: 0.055803537368774414\n",
      "Iteration 6902, Loss: 0.055695854127407074\n",
      "Iteration 6903, Loss: 0.055757325142621994\n",
      "Iteration 6904, Loss: 0.05581764504313469\n",
      "Iteration 6905, Loss: 0.05570296570658684\n",
      "Iteration 6906, Loss: 0.05574905872344971\n",
      "Iteration 6907, Loss: 0.055837951600551605\n",
      "Iteration 6908, Loss: 0.055823564529418945\n",
      "Iteration 6909, Loss: 0.05571722984313965\n",
      "Iteration 6910, Loss: 0.05572656914591789\n",
      "Iteration 6911, Loss: 0.0557863712310791\n",
      "Iteration 6912, Loss: 0.05567014217376709\n",
      "Iteration 6913, Loss: 0.055773500353097916\n",
      "Iteration 6914, Loss: 0.05586330220103264\n",
      "Iteration 6915, Loss: 0.0558498315513134\n",
      "Iteration 6916, Loss: 0.05574405565857887\n",
      "Iteration 6917, Loss: 0.05568961426615715\n",
      "Iteration 6918, Loss: 0.055749259889125824\n",
      "Iteration 6919, Loss: 0.055633705109357834\n",
      "Iteration 6920, Loss: 0.05579455941915512\n",
      "Iteration 6921, Loss: 0.055878959596157074\n",
      "Iteration 6922, Loss: 0.055859170854091644\n",
      "Iteration 6923, Loss: 0.05574667453765869\n",
      "Iteration 6924, Loss: 0.055694106966257095\n",
      "Iteration 6925, Loss: 0.05576014518737793\n",
      "Iteration 6926, Loss: 0.05564924329519272\n",
      "Iteration 6927, Loss: 0.05578303709626198\n",
      "Iteration 6928, Loss: 0.055867791175842285\n",
      "Iteration 6929, Loss: 0.05584995076060295\n",
      "Iteration 6930, Loss: 0.0557398796081543\n",
      "Iteration 6931, Loss: 0.055700384080410004\n",
      "Iteration 6932, Loss: 0.05576324835419655\n",
      "Iteration 6933, Loss: 0.05564824864268303\n",
      "Iteration 6934, Loss: 0.05578704923391342\n",
      "Iteration 6935, Loss: 0.055874548852443695\n",
      "Iteration 6936, Loss: 0.05585889145731926\n",
      "Iteration 6937, Loss: 0.055750612169504166\n",
      "Iteration 6938, Loss: 0.05568345636129379\n",
      "Iteration 6939, Loss: 0.05574488639831543\n",
      "Iteration 6940, Loss: 0.05562901496887207\n",
      "Iteration 6941, Loss: 0.055802904069423676\n",
      "Iteration 6942, Loss: 0.05589219182729721\n",
      "Iteration 6943, Loss: 0.05587812513113022\n",
      "Iteration 6944, Loss: 0.055771354585886\n",
      "Iteration 6945, Loss: 0.05565337464213371\n",
      "Iteration 6946, Loss: 0.05571286007761955\n",
      "Iteration 6947, Loss: 0.05562615394592285\n",
      "Iteration 6948, Loss: 0.05561741441488266\n",
      "Iteration 6949, Loss: 0.055730342864990234\n",
      "Iteration 6950, Loss: 0.055711787194013596\n",
      "Iteration 6951, Loss: 0.05566016957163811\n",
      "Iteration 6952, Loss: 0.0556848868727684\n",
      "Iteration 6953, Loss: 0.05565154552459717\n",
      "Iteration 6954, Loss: 0.05565718933939934\n",
      "Iteration 6955, Loss: 0.0556468591094017\n",
      "Iteration 6956, Loss: 0.055683378130197525\n",
      "Iteration 6957, Loss: 0.05564491078257561\n",
      "Iteration 6958, Loss: 0.055715564638376236\n",
      "Iteration 6959, Loss: 0.05570487305521965\n",
      "Iteration 6960, Loss: 0.05565953254699707\n",
      "Iteration 6961, Loss: 0.05566215515136719\n",
      "Iteration 6962, Loss: 0.05568786710500717\n",
      "Iteration 6963, Loss: 0.0556713342666626\n",
      "Iteration 6964, Loss: 0.05569307133555412\n",
      "Iteration 6965, Loss: 0.0556919202208519\n",
      "Iteration 6966, Loss: 0.0556611642241478\n",
      "Iteration 6967, Loss: 0.055651307106018066\n",
      "Iteration 6968, Loss: 0.05571107193827629\n",
      "Iteration 6969, Loss: 0.055706143379211426\n",
      "Iteration 6970, Loss: 0.05565333738923073\n",
      "Iteration 6971, Loss: 0.05567248910665512\n",
      "Iteration 6972, Loss: 0.05566474050283432\n",
      "Iteration 6973, Loss: 0.055649757385253906\n",
      "Iteration 6974, Loss: 0.05568786710500717\n",
      "Iteration 6975, Loss: 0.05564276501536369\n",
      "Iteration 6976, Loss: 0.055748384445905685\n",
      "Iteration 6977, Loss: 0.05578716844320297\n",
      "Iteration 6978, Loss: 0.05572104454040527\n",
      "Iteration 6979, Loss: 0.05568210408091545\n",
      "Iteration 6980, Loss: 0.055718980729579926\n",
      "Iteration 6981, Loss: 0.0556417740881443\n",
      "Iteration 6982, Loss: 0.05570324510335922\n",
      "Iteration 6983, Loss: 0.05570244789123535\n",
      "Iteration 6984, Loss: 0.05562540143728256\n",
      "Iteration 6985, Loss: 0.05570332333445549\n",
      "Iteration 6986, Loss: 0.05567371845245361\n",
      "Iteration 6987, Loss: 0.05569573491811752\n",
      "Iteration 6988, Loss: 0.05569323152303696\n",
      "Iteration 6989, Loss: 0.055669110268354416\n",
      "Iteration 6990, Loss: 0.05566839501261711\n",
      "Iteration 6991, Loss: 0.055675946176052094\n",
      "Iteration 6992, Loss: 0.05565158650279045\n",
      "Iteration 6993, Loss: 0.055725179612636566\n",
      "Iteration 6994, Loss: 0.05573936551809311\n",
      "Iteration 6995, Loss: 0.05565023794770241\n",
      "Iteration 6996, Loss: 0.05576912686228752\n",
      "Iteration 6997, Loss: 0.055793605744838715\n",
      "Iteration 6998, Loss: 0.05566108226776123\n",
      "Iteration 6999, Loss: 0.05579646676778793\n",
      "Iteration 7000, Loss: 0.05589139834046364\n",
      "Iteration 7001, Loss: 0.055867794901132584\n",
      "Iteration 7002, Loss: 0.05574532598257065\n",
      "Iteration 7003, Loss: 0.0557173527777195\n",
      "Iteration 7004, Loss: 0.0558040551841259\n",
      "Iteration 7005, Loss: 0.055737417191267014\n",
      "Iteration 7006, Loss: 0.05569314956665039\n",
      "Iteration 7007, Loss: 0.05574775114655495\n",
      "Iteration 7008, Loss: 0.05570399761199951\n",
      "Iteration 7009, Loss: 0.05567900463938713\n",
      "Iteration 7010, Loss: 0.05569613352417946\n",
      "Iteration 7011, Loss: 0.05566652864217758\n",
      "Iteration 7012, Loss: 0.05567511171102524\n",
      "Iteration 7013, Loss: 0.055657271295785904\n",
      "Iteration 7014, Loss: 0.05568496510386467\n",
      "Iteration 7015, Loss: 0.05565083399415016\n",
      "Iteration 7016, Loss: 0.05569470301270485\n",
      "Iteration 7017, Loss: 0.05567868798971176\n",
      "Iteration 7018, Loss: 0.05568584054708481\n",
      "Iteration 7019, Loss: 0.05567578598856926\n",
      "Iteration 7020, Loss: 0.055686358362436295\n",
      "Iteration 7021, Loss: 0.055688899010419846\n",
      "Iteration 7022, Loss: 0.05565476417541504\n",
      "Iteration 7023, Loss: 0.05563020706176758\n",
      "Iteration 7024, Loss: 0.055744171142578125\n",
      "Iteration 7025, Loss: 0.055752795189619064\n",
      "Iteration 7026, Loss: 0.055651627480983734\n",
      "Iteration 7027, Loss: 0.055778902024030685\n",
      "Iteration 7028, Loss: 0.055816650390625\n",
      "Iteration 7029, Loss: 0.055699825286865234\n",
      "Iteration 7030, Loss: 0.05575362965464592\n",
      "Iteration 7031, Loss: 0.05583767220377922\n",
      "Iteration 7032, Loss: 0.05579742044210434\n",
      "Iteration 7033, Loss: 0.0556589774787426\n",
      "Iteration 7034, Loss: 0.055821578949689865\n",
      "Iteration 7035, Loss: 0.05590009689331055\n",
      "Iteration 7036, Loss: 0.055809300392866135\n",
      "Iteration 7037, Loss: 0.05564979836344719\n",
      "Iteration 7038, Loss: 0.05573956295847893\n",
      "Iteration 7039, Loss: 0.05571572110056877\n",
      "Iteration 7040, Loss: 0.055650196969509125\n",
      "Iteration 7041, Loss: 0.055646102875471115\n",
      "Iteration 7042, Loss: 0.05571365728974342\n",
      "Iteration 7043, Loss: 0.05571580305695534\n",
      "Iteration 7044, Loss: 0.05563740059733391\n",
      "Iteration 7045, Loss: 0.0557253360748291\n",
      "Iteration 7046, Loss: 0.05567411705851555\n",
      "Iteration 7047, Loss: 0.05572764202952385\n",
      "Iteration 7048, Loss: 0.05577715486288071\n",
      "Iteration 7049, Loss: 0.05572708696126938\n",
      "Iteration 7050, Loss: 0.055649202316999435\n",
      "Iteration 7051, Loss: 0.05568528175354004\n",
      "Iteration 7052, Loss: 0.05565492436289787\n",
      "Iteration 7053, Loss: 0.05566362664103508\n",
      "Iteration 7054, Loss: 0.05565293878316879\n",
      "Iteration 7055, Loss: 0.055667757987976074\n",
      "Iteration 7056, Loss: 0.055649999529123306\n",
      "Iteration 7057, Loss: 0.055698197335004807\n",
      "Iteration 7058, Loss: 0.05564860627055168\n",
      "Iteration 7059, Loss: 0.055742859840393066\n",
      "Iteration 7060, Loss: 0.05579030513763428\n",
      "Iteration 7061, Loss: 0.055739086121320724\n",
      "Iteration 7062, Loss: 0.05563358590006828\n",
      "Iteration 7063, Loss: 0.055695414543151855\n",
      "Iteration 7064, Loss: 0.05564141273498535\n",
      "Iteration 7065, Loss: 0.055712662637233734\n",
      "Iteration 7066, Loss: 0.05569601058959961\n",
      "Iteration 7067, Loss: 0.05567173287272453\n",
      "Iteration 7068, Loss: 0.055673759430646896\n",
      "Iteration 7069, Loss: 0.05567598342895508\n",
      "Iteration 7070, Loss: 0.055661242455244064\n",
      "Iteration 7071, Loss: 0.05570244789123535\n",
      "Iteration 7072, Loss: 0.055698197335004807\n",
      "Iteration 7073, Loss: 0.055658940225839615\n",
      "Iteration 7074, Loss: 0.055659178644418716\n",
      "Iteration 7075, Loss: 0.055694859474897385\n",
      "Iteration 7076, Loss: 0.05568262189626694\n",
      "Iteration 7077, Loss: 0.0556766614317894\n",
      "Iteration 7078, Loss: 0.05567324534058571\n",
      "Iteration 7079, Loss: 0.05568563938140869\n",
      "Iteration 7080, Loss: 0.05567765235900879\n",
      "Iteration 7081, Loss: 0.05567765235900879\n",
      "Iteration 7082, Loss: 0.05566927045583725\n",
      "Iteration 7083, Loss: 0.055693190544843674\n",
      "Iteration 7084, Loss: 0.05568873882293701\n",
      "Iteration 7085, Loss: 0.05566597357392311\n",
      "Iteration 7086, Loss: 0.055662158876657486\n",
      "Iteration 7087, Loss: 0.05569656938314438\n",
      "Iteration 7088, Loss: 0.055688899010419846\n",
      "Iteration 7089, Loss: 0.05566760152578354\n",
      "Iteration 7090, Loss: 0.0556662492454052\n",
      "Iteration 7091, Loss: 0.05569116398692131\n",
      "Iteration 7092, Loss: 0.05568277835845947\n",
      "Iteration 7093, Loss: 0.05567193031311035\n",
      "Iteration 7094, Loss: 0.055665694177150726\n",
      "Iteration 7095, Loss: 0.05569684877991676\n",
      "Iteration 7096, Loss: 0.055693428963422775\n",
      "Iteration 7097, Loss: 0.055659931153059006\n",
      "Iteration 7098, Loss: 0.05566664785146713\n",
      "Iteration 7099, Loss: 0.05568230524659157\n",
      "Iteration 7100, Loss: 0.05567082017660141\n",
      "Iteration 7101, Loss: 0.0556795597076416\n",
      "Iteration 7102, Loss: 0.055659614503383636\n",
      "Iteration 7103, Loss: 0.05571572110056877\n",
      "Iteration 7104, Loss: 0.055729709565639496\n",
      "Iteration 7105, Loss: 0.055649083107709885\n",
      "Iteration 7106, Loss: 0.05576233193278313\n",
      "Iteration 7107, Loss: 0.0557703971862793\n",
      "Iteration 7108, Loss: 0.05561443418264389\n",
      "Iteration 7109, Loss: 0.055824797600507736\n",
      "Iteration 7110, Loss: 0.05589946359395981\n",
      "Iteration 7111, Loss: 0.05584859848022461\n",
      "Iteration 7112, Loss: 0.055701736360788345\n",
      "Iteration 7113, Loss: 0.05578887462615967\n",
      "Iteration 7114, Loss: 0.05588575452566147\n",
      "Iteration 7115, Loss: 0.055819474160671234\n",
      "Iteration 7116, Loss: 0.055619996041059494\n",
      "Iteration 7117, Loss: 0.055746834725141525\n",
      "Iteration 7118, Loss: 0.055733244866132736\n",
      "Iteration 7119, Loss: 0.05564316362142563\n",
      "Iteration 7120, Loss: 0.055702172219753265\n",
      "Iteration 7121, Loss: 0.055636487901210785\n",
      "Iteration 7122, Loss: 0.05576241388916969\n",
      "Iteration 7123, Loss: 0.05581160634756088\n",
      "Iteration 7124, Loss: 0.055755615234375\n",
      "Iteration 7125, Loss: 0.05564769357442856\n",
      "Iteration 7126, Loss: 0.05577341839671135\n",
      "Iteration 7127, Loss: 0.05576304718852043\n",
      "Iteration 7128, Loss: 0.055648963898420334\n",
      "Iteration 7129, Loss: 0.05571186542510986\n",
      "Iteration 7130, Loss: 0.055702053010463715\n",
      "Iteration 7131, Loss: 0.05564328283071518\n",
      "Iteration 7132, Loss: 0.055657029151916504\n",
      "Iteration 7133, Loss: 0.05565126985311508\n",
      "Iteration 7134, Loss: 0.055649518966674805\n",
      "Iteration 7135, Loss: 0.05567105859518051\n",
      "Iteration 7136, Loss: 0.055654726922512054\n",
      "Iteration 7137, Loss: 0.055699072778224945\n",
      "Iteration 7138, Loss: 0.05565619468688965\n",
      "Iteration 7139, Loss: 0.05573431774973869\n",
      "Iteration 7140, Loss: 0.05577965825796127\n",
      "Iteration 7141, Loss: 0.05572668835520744\n",
      "Iteration 7142, Loss: 0.0556485652923584\n",
      "Iteration 7143, Loss: 0.05565250292420387\n",
      "Iteration 7144, Loss: 0.05570344254374504\n",
      "Iteration 7145, Loss: 0.055717986077070236\n",
      "Iteration 7146, Loss: 0.05563756078481674\n",
      "Iteration 7147, Loss: 0.055802784860134125\n",
      "Iteration 7148, Loss: 0.055833104997873306\n",
      "Iteration 7149, Loss: 0.055689334869384766\n",
      "Iteration 7150, Loss: 0.05577707290649414\n",
      "Iteration 7151, Loss: 0.055882811546325684\n",
      "Iteration 7152, Loss: 0.055883608758449554\n",
      "Iteration 7153, Loss: 0.05578943341970444\n",
      "Iteration 7154, Loss: 0.05562230199575424\n",
      "Iteration 7155, Loss: 0.05582694336771965\n",
      "Iteration 7156, Loss: 0.055853646248579025\n",
      "Iteration 7157, Loss: 0.055709123611450195\n",
      "Iteration 7158, Loss: 0.05576348304748535\n",
      "Iteration 7159, Loss: 0.05586894601583481\n",
      "Iteration 7160, Loss: 0.055864494293928146\n",
      "Iteration 7161, Loss: 0.05576137825846672\n",
      "Iteration 7162, Loss: 0.05566740036010742\n",
      "Iteration 7163, Loss: 0.05573471635580063\n",
      "Iteration 7164, Loss: 0.055639706552028656\n",
      "Iteration 7165, Loss: 0.055766068398952484\n",
      "Iteration 7166, Loss: 0.05582742020487785\n",
      "Iteration 7167, Loss: 0.05578859895467758\n",
      "Iteration 7168, Loss: 0.05566000938415527\n",
      "Iteration 7169, Loss: 0.05583115667104721\n",
      "Iteration 7170, Loss: 0.05591432377696037\n",
      "Iteration 7171, Loss: 0.05581708997488022\n",
      "Iteration 7172, Loss: 0.055653851479291916\n",
      "Iteration 7173, Loss: 0.05573606863617897\n",
      "Iteration 7174, Loss: 0.05571905896067619\n",
      "Iteration 7175, Loss: 0.05561904236674309\n",
      "Iteration 7176, Loss: 0.05568814277648926\n",
      "Iteration 7177, Loss: 0.05564729496836662\n",
      "Iteration 7178, Loss: 0.05573054403066635\n",
      "Iteration 7179, Loss: 0.0557202510535717\n",
      "Iteration 7180, Loss: 0.05565758794546127\n",
      "Iteration 7181, Loss: 0.055676184594631195\n",
      "Iteration 7182, Loss: 0.055646538734436035\n",
      "Iteration 7183, Loss: 0.055628299713134766\n",
      "Iteration 7184, Loss: 0.05567312240600586\n",
      "Iteration 7185, Loss: 0.05563875287771225\n",
      "Iteration 7186, Loss: 0.055627744644880295\n",
      "Iteration 7187, Loss: 0.05570821091532707\n",
      "Iteration 7188, Loss: 0.05569180101156235\n",
      "Iteration 7189, Loss: 0.05566652864217758\n",
      "Iteration 7190, Loss: 0.05566505715250969\n",
      "Iteration 7191, Loss: 0.05569470301270485\n",
      "Iteration 7192, Loss: 0.05569605156779289\n",
      "Iteration 7193, Loss: 0.05565015599131584\n",
      "Iteration 7194, Loss: 0.055685482919216156\n",
      "Iteration 7195, Loss: 0.05564701929688454\n",
      "Iteration 7196, Loss: 0.05567499250173569\n",
      "Iteration 7197, Loss: 0.055646538734436035\n",
      "Iteration 7198, Loss: 0.0557306632399559\n",
      "Iteration 7199, Loss: 0.05572076886892319\n",
      "Iteration 7200, Loss: 0.05565870180726051\n",
      "Iteration 7201, Loss: 0.05568981170654297\n",
      "Iteration 7202, Loss: 0.055633269250392914\n",
      "Iteration 7203, Loss: 0.05572044849395752\n",
      "Iteration 7204, Loss: 0.05572919175028801\n",
      "Iteration 7205, Loss: 0.05565023422241211\n",
      "Iteration 7206, Loss: 0.055773261934518814\n",
      "Iteration 7207, Loss: 0.05578943341970444\n",
      "Iteration 7208, Loss: 0.0556340217590332\n",
      "Iteration 7209, Loss: 0.055825673043727875\n",
      "Iteration 7210, Loss: 0.05593820661306381\n",
      "Iteration 7211, Loss: 0.05594512075185776\n",
      "Iteration 7212, Loss: 0.055856626480817795\n",
      "Iteration 7213, Loss: 0.055683575570583344\n",
      "Iteration 7214, Loss: 0.05585324764251709\n",
      "Iteration 7215, Loss: 0.055984579026699066\n",
      "Iteration 7216, Loss: 0.05592934414744377\n",
      "Iteration 7217, Loss: 0.05570697784423828\n",
      "Iteration 7218, Loss: 0.05581657215952873\n",
      "Iteration 7219, Loss: 0.05597043037414551\n",
      "Iteration 7220, Loss: 0.05601410195231438\n",
      "Iteration 7221, Loss: 0.05595819279551506\n",
      "Iteration 7222, Loss: 0.055813513696193695\n",
      "Iteration 7223, Loss: 0.05564260482788086\n",
      "Iteration 7224, Loss: 0.0557454451918602\n",
      "Iteration 7225, Loss: 0.05566585436463356\n",
      "Iteration 7226, Loss: 0.05575215816497803\n",
      "Iteration 7227, Loss: 0.05582030862569809\n",
      "Iteration 7228, Loss: 0.05578756332397461\n",
      "Iteration 7229, Loss: 0.055664461106061935\n",
      "Iteration 7230, Loss: 0.05581867694854736\n",
      "Iteration 7231, Loss: 0.05589525029063225\n",
      "Iteration 7232, Loss: 0.055790387094020844\n",
      "Iteration 7233, Loss: 0.055677853524684906\n",
      "Iteration 7234, Loss: 0.05576149746775627\n",
      "Iteration 7235, Loss: 0.055742621421813965\n",
      "Iteration 7236, Loss: 0.055633388459682465\n",
      "Iteration 7237, Loss: 0.0558420829474926\n",
      "Iteration 7238, Loss: 0.05590327829122543\n",
      "Iteration 7239, Loss: 0.05578522011637688\n",
      "Iteration 7240, Loss: 0.05568913742899895\n",
      "Iteration 7241, Loss: 0.055780332535505295\n",
      "Iteration 7242, Loss: 0.05576964467763901\n",
      "Iteration 7243, Loss: 0.0556664876639843\n",
      "Iteration 7244, Loss: 0.05578995123505592\n",
      "Iteration 7245, Loss: 0.0558452233672142\n",
      "Iteration 7246, Loss: 0.05572331324219704\n",
      "Iteration 7247, Loss: 0.055736660957336426\n",
      "Iteration 7248, Loss: 0.055829647928476334\n",
      "Iteration 7249, Loss: 0.055819395929574966\n",
      "Iteration 7250, Loss: 0.05571623891592026\n",
      "Iteration 7251, Loss: 0.05572259798645973\n",
      "Iteration 7252, Loss: 0.055778346955776215\n",
      "Iteration 7253, Loss: 0.05565754696726799\n",
      "Iteration 7254, Loss: 0.055783990770578384\n",
      "Iteration 7255, Loss: 0.05587577819824219\n",
      "Iteration 7256, Loss: 0.05586377903819084\n",
      "Iteration 7257, Loss: 0.05575835704803467\n",
      "Iteration 7258, Loss: 0.05566903203725815\n",
      "Iteration 7259, Loss: 0.05573197454214096\n",
      "Iteration 7260, Loss: 0.05562778562307358\n",
      "Iteration 7261, Loss: 0.05577043816447258\n",
      "Iteration 7262, Loss: 0.05582674592733383\n",
      "Iteration 7263, Loss: 0.055778782814741135\n",
      "Iteration 7264, Loss: 0.05564439296722412\n",
      "Iteration 7265, Loss: 0.05585026741027832\n",
      "Iteration 7266, Loss: 0.055930059403181076\n",
      "Iteration 7267, Loss: 0.05582773685455322\n",
      "Iteration 7268, Loss: 0.05564789101481438\n",
      "Iteration 7269, Loss: 0.05572986975312233\n",
      "Iteration 7270, Loss: 0.05571051687002182\n",
      "Iteration 7271, Loss: 0.055630527436733246\n",
      "Iteration 7272, Loss: 0.055668238550424576\n",
      "Iteration 7273, Loss: 0.05564495176076889\n",
      "Iteration 7274, Loss: 0.05565015599131584\n",
      "Iteration 7275, Loss: 0.05567578598856926\n",
      "Iteration 7276, Loss: 0.055651985108852386\n",
      "Iteration 7277, Loss: 0.05570578575134277\n",
      "Iteration 7278, Loss: 0.05567967891693115\n",
      "Iteration 7279, Loss: 0.055702488869428635\n",
      "Iteration 7280, Loss: 0.05572744458913803\n",
      "Iteration 7281, Loss: 0.05564793199300766\n",
      "Iteration 7282, Loss: 0.055772941559553146\n",
      "Iteration 7283, Loss: 0.05579523369669914\n",
      "Iteration 7284, Loss: 0.05565254017710686\n",
      "Iteration 7285, Loss: 0.05580703541636467\n",
      "Iteration 7286, Loss: 0.055908203125\n",
      "Iteration 7287, Loss: 0.0558907613158226\n",
      "Iteration 7288, Loss: 0.055771391838788986\n",
      "Iteration 7289, Loss: 0.05568472668528557\n",
      "Iteration 7290, Loss: 0.05578335374593735\n",
      "Iteration 7291, Loss: 0.05573185533285141\n",
      "Iteration 7292, Loss: 0.0556916818022728\n",
      "Iteration 7293, Loss: 0.0557401180267334\n",
      "Iteration 7294, Loss: 0.055700741708278656\n",
      "Iteration 7295, Loss: 0.05567812919616699\n",
      "Iteration 7296, Loss: 0.0556897334754467\n",
      "Iteration 7297, Loss: 0.055672526359558105\n",
      "Iteration 7298, Loss: 0.05568075552582741\n",
      "Iteration 7299, Loss: 0.05565611645579338\n",
      "Iteration 7300, Loss: 0.055694978684186935\n",
      "Iteration 7301, Loss: 0.055661361664533615\n",
      "Iteration 7302, Loss: 0.0557025708258152\n",
      "Iteration 7303, Loss: 0.055708371102809906\n",
      "Iteration 7304, Loss: 0.05562591552734375\n",
      "Iteration 7305, Loss: 0.055627863854169846\n",
      "Iteration 7306, Loss: 0.05569060891866684\n",
      "Iteration 7307, Loss: 0.05563640967011452\n",
      "Iteration 7308, Loss: 0.05575064942240715\n",
      "Iteration 7309, Loss: 0.05577119439840317\n",
      "Iteration 7310, Loss: 0.05567117780447006\n",
      "Iteration 7311, Loss: 0.05576348677277565\n",
      "Iteration 7312, Loss: 0.055818479508161545\n",
      "Iteration 7313, Loss: 0.05572740361094475\n",
      "Iteration 7314, Loss: 0.05570634454488754\n",
      "Iteration 7315, Loss: 0.05577194690704346\n",
      "Iteration 7316, Loss: 0.05571115016937256\n",
      "Iteration 7317, Loss: 0.05568981543183327\n",
      "Iteration 7318, Loss: 0.05572191998362541\n",
      "Iteration 7319, Loss: 0.055616460740566254\n",
      "Iteration 7320, Loss: 0.055775485932826996\n",
      "Iteration 7321, Loss: 0.05577345937490463\n",
      "Iteration 7322, Loss: 0.05564161390066147\n",
      "Iteration 7323, Loss: 0.055782996118068695\n",
      "Iteration 7324, Loss: 0.055831193923950195\n",
      "Iteration 7325, Loss: 0.05574588105082512\n",
      "Iteration 7326, Loss: 0.05568119138479233\n",
      "Iteration 7327, Loss: 0.05574357509613037\n",
      "Iteration 7328, Loss: 0.05566795915365219\n",
      "Iteration 7329, Loss: 0.05575299263000488\n",
      "Iteration 7330, Loss: 0.055804334580898285\n",
      "Iteration 7331, Loss: 0.05573074147105217\n",
      "Iteration 7332, Loss: 0.05568603798747063\n",
      "Iteration 7333, Loss: 0.05573884770274162\n",
      "Iteration 7334, Loss: 0.05565707013010979\n",
      "Iteration 7335, Loss: 0.05576582998037338\n",
      "Iteration 7336, Loss: 0.05582471936941147\n",
      "Iteration 7337, Loss: 0.05576920509338379\n",
      "Iteration 7338, Loss: 0.05565810576081276\n",
      "Iteration 7339, Loss: 0.05578259751200676\n",
      "Iteration 7340, Loss: 0.05579603090882301\n",
      "Iteration 7341, Loss: 0.0556485652923584\n",
      "Iteration 7342, Loss: 0.05580028146505356\n",
      "Iteration 7343, Loss: 0.05589791387319565\n",
      "Iteration 7344, Loss: 0.05589072033762932\n",
      "Iteration 7345, Loss: 0.055789947509765625\n",
      "Iteration 7346, Loss: 0.055637799203395844\n",
      "Iteration 7347, Loss: 0.055809736251831055\n",
      "Iteration 7348, Loss: 0.05582845211029053\n",
      "Iteration 7349, Loss: 0.055692158639431\n",
      "Iteration 7350, Loss: 0.055766187608242035\n",
      "Iteration 7351, Loss: 0.0558597669005394\n",
      "Iteration 7352, Loss: 0.055844347923994064\n",
      "Iteration 7353, Loss: 0.05572966858744621\n",
      "Iteration 7354, Loss: 0.05571862310171127\n",
      "Iteration 7355, Loss: 0.055790387094020844\n",
      "Iteration 7356, Loss: 0.05569378659129143\n",
      "Iteration 7357, Loss: 0.055741552263498306\n",
      "Iteration 7358, Loss: 0.0558168888092041\n",
      "Iteration 7359, Loss: 0.055783990770578384\n",
      "Iteration 7360, Loss: 0.05566128343343735\n",
      "Iteration 7361, Loss: 0.05581192299723625\n",
      "Iteration 7362, Loss: 0.055882375687360764\n",
      "Iteration 7363, Loss: 0.05577930063009262\n",
      "Iteration 7364, Loss: 0.05568110942840576\n",
      "Iteration 7365, Loss: 0.05576304718852043\n",
      "Iteration 7366, Loss: 0.05573968216776848\n",
      "Iteration 7367, Loss: 0.05562623590230942\n",
      "Iteration 7368, Loss: 0.05583830922842026\n",
      "Iteration 7369, Loss: 0.05588674545288086\n",
      "Iteration 7370, Loss: 0.05575740337371826\n",
      "Iteration 7371, Loss: 0.05571639537811279\n",
      "Iteration 7372, Loss: 0.05581434816122055\n",
      "Iteration 7373, Loss: 0.05580858513712883\n",
      "Iteration 7374, Loss: 0.05570964142680168\n",
      "Iteration 7375, Loss: 0.055725932121276855\n",
      "Iteration 7376, Loss: 0.05577671527862549\n",
      "Iteration 7377, Loss: 0.05565039440989494\n",
      "Iteration 7378, Loss: 0.05579312890768051\n",
      "Iteration 7379, Loss: 0.05588877573609352\n",
      "Iteration 7380, Loss: 0.05588042736053467\n",
      "Iteration 7381, Loss: 0.055778466165065765\n",
      "Iteration 7382, Loss: 0.055637162178754807\n",
      "Iteration 7383, Loss: 0.0557098388671875\n",
      "Iteration 7384, Loss: 0.055630091577768326\n",
      "Iteration 7385, Loss: 0.05572887510061264\n",
      "Iteration 7386, Loss: 0.05573391914367676\n",
      "Iteration 7387, Loss: 0.05563473701477051\n",
      "Iteration 7388, Loss: 0.05579877272248268\n",
      "Iteration 7389, Loss: 0.05582785606384277\n",
      "Iteration 7390, Loss: 0.05568969249725342\n",
      "Iteration 7391, Loss: 0.0557740144431591\n",
      "Iteration 7392, Loss: 0.055874668061733246\n",
      "Iteration 7393, Loss: 0.05585924908518791\n",
      "Iteration 7394, Loss: 0.05574282258749008\n",
      "Iteration 7395, Loss: 0.0557071790099144\n",
      "Iteration 7396, Loss: 0.055784109979867935\n",
      "Iteration 7397, Loss: 0.05569537729024887\n",
      "Iteration 7398, Loss: 0.05573880672454834\n",
      "Iteration 7399, Loss: 0.05580965802073479\n",
      "Iteration 7400, Loss: 0.05577465146780014\n",
      "Iteration 7401, Loss: 0.05565957352519035\n",
      "Iteration 7402, Loss: 0.055806878954172134\n",
      "Iteration 7403, Loss: 0.05586131662130356\n",
      "Iteration 7404, Loss: 0.05573829263448715\n",
      "Iteration 7405, Loss: 0.05572772026062012\n",
      "Iteration 7406, Loss: 0.055821459740400314\n",
      "Iteration 7407, Loss: 0.05581124871969223\n",
      "Iteration 7408, Loss: 0.055709563195705414\n",
      "Iteration 7409, Loss: 0.055731140077114105\n",
      "Iteration 7410, Loss: 0.055784665048122406\n",
      "Iteration 7411, Loss: 0.05566426366567612\n",
      "Iteration 7412, Loss: 0.05577707663178444\n",
      "Iteration 7413, Loss: 0.05586695671081543\n",
      "Iteration 7414, Loss: 0.055853407829999924\n",
      "Iteration 7415, Loss: 0.05574743077158928\n",
      "Iteration 7416, Loss: 0.05568333715200424\n",
      "Iteration 7417, Loss: 0.05574214830994606\n",
      "Iteration 7418, Loss: 0.055631279945373535\n",
      "Iteration 7419, Loss: 0.0557786226272583\n",
      "Iteration 7420, Loss: 0.055843714624643326\n",
      "Iteration 7421, Loss: 0.05580437183380127\n",
      "Iteration 7422, Loss: 0.05567483231425285\n",
      "Iteration 7423, Loss: 0.055806003510951996\n",
      "Iteration 7424, Loss: 0.05588801950216293\n",
      "Iteration 7425, Loss: 0.05579424276947975\n",
      "Iteration 7426, Loss: 0.05566398426890373\n",
      "Iteration 7427, Loss: 0.055740837007761\n",
      "Iteration 7428, Loss: 0.05571373552083969\n",
      "Iteration 7429, Loss: 0.055637162178754807\n",
      "Iteration 7430, Loss: 0.055641692131757736\n",
      "Iteration 7431, Loss: 0.05569680780172348\n",
      "Iteration 7432, Loss: 0.055697642266750336\n",
      "Iteration 7433, Loss: 0.05562528222799301\n",
      "Iteration 7434, Loss: 0.05566541478037834\n",
      "Iteration 7435, Loss: 0.055640898644924164\n",
      "Iteration 7436, Loss: 0.05564948171377182\n",
      "Iteration 7437, Loss: 0.055672645568847656\n",
      "Iteration 7438, Loss: 0.05565552040934563\n",
      "Iteration 7439, Loss: 0.0556970052421093\n",
      "Iteration 7440, Loss: 0.055658262223005295\n",
      "Iteration 7441, Loss: 0.0557301864027977\n",
      "Iteration 7442, Loss: 0.05577123165130615\n",
      "Iteration 7443, Loss: 0.055711470544338226\n",
      "Iteration 7444, Loss: 0.055678848177194595\n",
      "Iteration 7445, Loss: 0.055695097893476486\n",
      "Iteration 7446, Loss: 0.055666130036115646\n",
      "Iteration 7447, Loss: 0.05567511171102524\n",
      "Iteration 7448, Loss: 0.05564717575907707\n",
      "Iteration 7449, Loss: 0.055669307708740234\n",
      "Iteration 7450, Loss: 0.0556463822722435\n",
      "Iteration 7451, Loss: 0.05566640943288803\n",
      "Iteration 7452, Loss: 0.05564054101705551\n",
      "Iteration 7453, Loss: 0.055665891617536545\n",
      "Iteration 7454, Loss: 0.055635809898376465\n",
      "Iteration 7455, Loss: 0.05574214458465576\n",
      "Iteration 7456, Loss: 0.05572644993662834\n",
      "Iteration 7457, Loss: 0.05565953254699707\n",
      "Iteration 7458, Loss: 0.05569037050008774\n",
      "Iteration 7459, Loss: 0.05562357231974602\n",
      "Iteration 7460, Loss: 0.05577417463064194\n",
      "Iteration 7461, Loss: 0.05579773709177971\n",
      "Iteration 7462, Loss: 0.05569732189178467\n",
      "Iteration 7463, Loss: 0.05573960393667221\n",
      "Iteration 7464, Loss: 0.0558016300201416\n",
      "Iteration 7465, Loss: 0.05572358891367912\n",
      "Iteration 7466, Loss: 0.05569923296570778\n",
      "Iteration 7467, Loss: 0.05575212091207504\n",
      "Iteration 7468, Loss: 0.055675946176052094\n",
      "Iteration 7469, Loss: 0.05574015900492668\n",
      "Iteration 7470, Loss: 0.05578017234802246\n",
      "Iteration 7471, Loss: 0.05568079277873039\n",
      "Iteration 7472, Loss: 0.055754344910383224\n",
      "Iteration 7473, Loss: 0.05581935495138168\n",
      "Iteration 7474, Loss: 0.05575088784098625\n",
      "Iteration 7475, Loss: 0.05566231533885002\n",
      "Iteration 7476, Loss: 0.055723391473293304\n",
      "Iteration 7477, Loss: 0.05564955994486809\n",
      "Iteration 7478, Loss: 0.05576459690928459\n",
      "Iteration 7479, Loss: 0.05581875890493393\n",
      "Iteration 7480, Loss: 0.05575927346944809\n",
      "Iteration 7481, Loss: 0.05565667152404785\n",
      "Iteration 7482, Loss: 0.05576491728425026\n",
      "Iteration 7483, Loss: 0.055750250816345215\n",
      "Iteration 7484, Loss: 0.05565448850393295\n",
      "Iteration 7485, Loss: 0.05569788068532944\n",
      "Iteration 7486, Loss: 0.055673323571681976\n",
      "Iteration 7487, Loss: 0.055688660591840744\n",
      "Iteration 7488, Loss: 0.055663466453552246\n",
      "Iteration 7489, Loss: 0.055705904960632324\n",
      "Iteration 7490, Loss: 0.05572966858744621\n",
      "Iteration 7491, Loss: 0.0556563138961792\n",
      "Iteration 7492, Loss: 0.05576654523611069\n",
      "Iteration 7493, Loss: 0.05579165741801262\n",
      "Iteration 7494, Loss: 0.055652301758527756\n",
      "Iteration 7495, Loss: 0.05579638481140137\n",
      "Iteration 7496, Loss: 0.05589139461517334\n",
      "Iteration 7497, Loss: 0.055876415222883224\n",
      "Iteration 7498, Loss: 0.05576467886567116\n",
      "Iteration 7499, Loss: 0.055676501244306564\n",
      "Iteration 7500, Loss: 0.05576273053884506\n",
      "Iteration 7501, Loss: 0.05569255352020264\n",
      "Iteration 7502, Loss: 0.05572644993662834\n",
      "Iteration 7503, Loss: 0.055785976350307465\n",
      "Iteration 7504, Loss: 0.05574774742126465\n",
      "Iteration 7505, Loss: 0.05563922971487045\n",
      "Iteration 7506, Loss: 0.05580493062734604\n",
      "Iteration 7507, Loss: 0.05583059787750244\n",
      "Iteration 7508, Loss: 0.055687032639980316\n",
      "Iteration 7509, Loss: 0.055775683373212814\n",
      "Iteration 7510, Loss: 0.055878523737192154\n",
      "Iteration 7511, Loss: 0.055871885269880295\n",
      "Iteration 7512, Loss: 0.05576690286397934\n",
      "Iteration 7513, Loss: 0.05565953627228737\n",
      "Iteration 7514, Loss: 0.05573117733001709\n",
      "Iteration 7515, Loss: 0.05564061924815178\n",
      "Iteration 7516, Loss: 0.05576102063059807\n",
      "Iteration 7517, Loss: 0.05582018941640854\n",
      "Iteration 7518, Loss: 0.05577906221151352\n",
      "Iteration 7519, Loss: 0.05564801022410393\n",
      "Iteration 7520, Loss: 0.05584720894694328\n",
      "Iteration 7521, Loss: 0.05593296140432358\n",
      "Iteration 7522, Loss: 0.05583799257874489\n",
      "Iteration 7523, Loss: 0.05563652515411377\n",
      "Iteration 7524, Loss: 0.05573248863220215\n",
      "Iteration 7525, Loss: 0.05572926998138428\n",
      "Iteration 7526, Loss: 0.05562691017985344\n",
      "Iteration 7527, Loss: 0.05583576485514641\n",
      "Iteration 7528, Loss: 0.05589060112833977\n",
      "Iteration 7529, Loss: 0.055776361376047134\n",
      "Iteration 7530, Loss: 0.05569096654653549\n",
      "Iteration 7531, Loss: 0.055780768394470215\n",
      "Iteration 7532, Loss: 0.05576559156179428\n",
      "Iteration 7533, Loss: 0.05565178394317627\n",
      "Iteration 7534, Loss: 0.05581756681203842\n",
      "Iteration 7535, Loss: 0.05588678643107414\n",
      "Iteration 7536, Loss: 0.05578792095184326\n",
      "Iteration 7537, Loss: 0.0556720495223999\n",
      "Iteration 7538, Loss: 0.05575677007436752\n",
      "Iteration 7539, Loss: 0.055738452821969986\n",
      "Iteration 7540, Loss: 0.055616579949855804\n",
      "Iteration 7541, Loss: 0.055857859551906586\n",
      "Iteration 7542, Loss: 0.055936139076948166\n",
      "Iteration 7543, Loss: 0.05585980415344238\n",
      "Iteration 7544, Loss: 0.05567372217774391\n",
      "Iteration 7545, Loss: 0.055821895599365234\n",
      "Iteration 7546, Loss: 0.05593371391296387\n",
      "Iteration 7547, Loss: 0.055908363312482834\n",
      "Iteration 7548, Loss: 0.05576146021485329\n",
      "Iteration 7549, Loss: 0.055719416588544846\n",
      "Iteration 7550, Loss: 0.055827777832746506\n",
      "Iteration 7551, Loss: 0.05579058453440666\n",
      "Iteration 7552, Loss: 0.055622342973947525\n",
      "Iteration 7553, Loss: 0.05585277080535889\n",
      "Iteration 7554, Loss: 0.055940669029951096\n",
      "Iteration 7555, Loss: 0.055880509316921234\n",
      "Iteration 7556, Loss: 0.05570860952138901\n",
      "Iteration 7557, Loss: 0.05579277127981186\n",
      "Iteration 7558, Loss: 0.05590963363647461\n",
      "Iteration 7559, Loss: 0.05588440224528313\n",
      "Iteration 7560, Loss: 0.05572875589132309\n",
      "Iteration 7561, Loss: 0.05576018616557121\n",
      "Iteration 7562, Loss: 0.0558723621070385\n",
      "Iteration 7563, Loss: 0.055841684341430664\n",
      "Iteration 7564, Loss: 0.05569561570882797\n",
      "Iteration 7565, Loss: 0.055781882256269455\n",
      "Iteration 7566, Loss: 0.05587887763977051\n",
      "Iteration 7567, Loss: 0.05582913011312485\n",
      "Iteration 7568, Loss: 0.05565138906240463\n",
      "Iteration 7569, Loss: 0.05584939569234848\n",
      "Iteration 7570, Loss: 0.05596888065338135\n",
      "Iteration 7571, Loss: 0.055942654609680176\n",
      "Iteration 7572, Loss: 0.05579444020986557\n",
      "Iteration 7573, Loss: 0.05570352450013161\n",
      "Iteration 7574, Loss: 0.05582384392619133\n",
      "Iteration 7575, Loss: 0.055822134017944336\n",
      "Iteration 7576, Loss: 0.055684447288513184\n",
      "Iteration 7577, Loss: 0.0557912215590477\n",
      "Iteration 7578, Loss: 0.055889569222927094\n",
      "Iteration 7579, Loss: 0.05586346238851547\n",
      "Iteration 7580, Loss: 0.055740438401699066\n",
      "Iteration 7581, Loss: 0.0557301864027977\n",
      "Iteration 7582, Loss: 0.05581700801849365\n",
      "Iteration 7583, Loss: 0.055763326585292816\n",
      "Iteration 7584, Loss: 0.055665694177150726\n",
      "Iteration 7585, Loss: 0.055716436356306076\n",
      "Iteration 7586, Loss: 0.0556899718940258\n",
      "Iteration 7587, Loss: 0.05567733570933342\n",
      "Iteration 7588, Loss: 0.055680952966213226\n",
      "Iteration 7589, Loss: 0.055678367614746094\n",
      "Iteration 7590, Loss: 0.0556824617087841\n",
      "Iteration 7591, Loss: 0.05565575882792473\n",
      "Iteration 7592, Loss: 0.05568861961364746\n",
      "Iteration 7593, Loss: 0.05565238371491432\n",
      "Iteration 7594, Loss: 0.05569525808095932\n",
      "Iteration 7595, Loss: 0.055684808641672134\n",
      "Iteration 7596, Loss: 0.05567070096731186\n",
      "Iteration 7597, Loss: 0.05565377324819565\n",
      "Iteration 7598, Loss: 0.05570773407816887\n",
      "Iteration 7599, Loss: 0.05571047589182854\n",
      "Iteration 7600, Loss: 0.055634818971157074\n",
      "Iteration 7601, Loss: 0.05568091198801994\n",
      "Iteration 7602, Loss: 0.05564069747924805\n",
      "Iteration 7603, Loss: 0.055650316178798676\n",
      "Iteration 7604, Loss: 0.0556490421295166\n",
      "Iteration 7605, Loss: 0.05565750598907471\n",
      "Iteration 7606, Loss: 0.05563287064433098\n",
      "Iteration 7607, Loss: 0.05573451891541481\n",
      "Iteration 7608, Loss: 0.055702805519104004\n",
      "Iteration 7609, Loss: 0.0556894950568676\n",
      "Iteration 7610, Loss: 0.05572696775197983\n",
      "Iteration 7611, Loss: 0.055664580315351486\n",
      "Iteration 7612, Loss: 0.05574178695678711\n",
      "Iteration 7613, Loss: 0.05575498193502426\n",
      "Iteration 7614, Loss: 0.05562659353017807\n",
      "Iteration 7615, Loss: 0.055719535797834396\n",
      "Iteration 7616, Loss: 0.05570133775472641\n",
      "Iteration 7617, Loss: 0.055660687386989594\n",
      "Iteration 7618, Loss: 0.0556621178984642\n",
      "Iteration 7619, Loss: 0.0556919202208519\n",
      "Iteration 7620, Loss: 0.05568715184926987\n",
      "Iteration 7621, Loss: 0.05566064640879631\n",
      "Iteration 7622, Loss: 0.05565603822469711\n",
      "Iteration 7623, Loss: 0.05570030212402344\n",
      "Iteration 7624, Loss: 0.05570070073008537\n",
      "Iteration 7625, Loss: 0.05564717575907707\n",
      "Iteration 7626, Loss: 0.055695295333862305\n",
      "Iteration 7627, Loss: 0.055640459060668945\n",
      "Iteration 7628, Loss: 0.055715203285217285\n",
      "Iteration 7629, Loss: 0.05572744458913803\n",
      "Iteration 7630, Loss: 0.0556391105055809\n",
      "Iteration 7631, Loss: 0.05580512806773186\n",
      "Iteration 7632, Loss: 0.055844228714704514\n",
      "Iteration 7633, Loss: 0.055711351335048676\n",
      "Iteration 7634, Loss: 0.055752795189619064\n",
      "Iteration 7635, Loss: 0.05585157871246338\n",
      "Iteration 7636, Loss: 0.05583910271525383\n",
      "Iteration 7637, Loss: 0.055728040635585785\n",
      "Iteration 7638, Loss: 0.055716317147016525\n",
      "Iteration 7639, Loss: 0.05578279495239258\n",
      "Iteration 7640, Loss: 0.055676065385341644\n",
      "Iteration 7641, Loss: 0.05576344579458237\n",
      "Iteration 7642, Loss: 0.05584665387868881\n",
      "Iteration 7643, Loss: 0.055822454392910004\n",
      "Iteration 7644, Loss: 0.055706582963466644\n",
      "Iteration 7645, Loss: 0.055751483887434006\n",
      "Iteration 7646, Loss: 0.055820345878601074\n",
      "Iteration 7647, Loss: 0.05571305751800537\n",
      "Iteration 7648, Loss: 0.05573586747050285\n",
      "Iteration 7649, Loss: 0.05581974983215332\n",
      "Iteration 7650, Loss: 0.0557992085814476\n",
      "Iteration 7651, Loss: 0.05568898096680641\n",
      "Iteration 7652, Loss: 0.05576813593506813\n",
      "Iteration 7653, Loss: 0.055829405784606934\n",
      "Iteration 7654, Loss: 0.055712901055812836\n",
      "Iteration 7655, Loss: 0.05574151128530502\n",
      "Iteration 7656, Loss: 0.05583127588033676\n",
      "Iteration 7657, Loss: 0.05581752583384514\n",
      "Iteration 7658, Loss: 0.05571218580007553\n",
      "Iteration 7659, Loss: 0.055731337517499924\n",
      "Iteration 7660, Loss: 0.055788956582546234\n",
      "Iteration 7661, Loss: 0.05567070096731186\n",
      "Iteration 7662, Loss: 0.05577123537659645\n",
      "Iteration 7663, Loss: 0.055860720574855804\n",
      "Iteration 7664, Loss: 0.05584716796875\n",
      "Iteration 7665, Loss: 0.055741190910339355\n",
      "Iteration 7666, Loss: 0.055691640824079514\n",
      "Iteration 7667, Loss: 0.055750373750925064\n",
      "Iteration 7668, Loss: 0.05563676729798317\n",
      "Iteration 7669, Loss: 0.05578482151031494\n",
      "Iteration 7670, Loss: 0.0558621883392334\n",
      "Iteration 7671, Loss: 0.05583596229553223\n",
      "Iteration 7672, Loss: 0.05571770668029785\n",
      "Iteration 7673, Loss: 0.05573781579732895\n",
      "Iteration 7674, Loss: 0.055810850113630295\n",
      "Iteration 7675, Loss: 0.055707577615976334\n",
      "Iteration 7676, Loss: 0.055735550820827484\n",
      "Iteration 7677, Loss: 0.055817168205976486\n",
      "Iteration 7678, Loss: 0.055795710533857346\n",
      "Iteration 7679, Loss: 0.05568317696452141\n",
      "Iteration 7680, Loss: 0.05577743053436279\n",
      "Iteration 7681, Loss: 0.05584288015961647\n",
      "Iteration 7682, Loss: 0.05573093891143799\n",
      "Iteration 7683, Loss: 0.055723946541547775\n",
      "Iteration 7684, Loss: 0.055811166763305664\n",
      "Iteration 7685, Loss: 0.05579511448740959\n",
      "Iteration 7686, Loss: 0.055687110871076584\n",
      "Iteration 7687, Loss: 0.055766426026821136\n",
      "Iteration 7688, Loss: 0.05582726001739502\n",
      "Iteration 7689, Loss: 0.055710677057504654\n",
      "Iteration 7690, Loss: 0.055741988122463226\n",
      "Iteration 7691, Loss: 0.05583222955465317\n",
      "Iteration 7692, Loss: 0.055819034576416016\n",
      "Iteration 7693, Loss: 0.05571313947439194\n",
      "Iteration 7694, Loss: 0.055729031562805176\n",
      "Iteration 7695, Loss: 0.055787764489650726\n",
      "Iteration 7696, Loss: 0.05566922947764397\n",
      "Iteration 7697, Loss: 0.05577389523386955\n",
      "Iteration 7698, Loss: 0.055865250527858734\n",
      "Iteration 7699, Loss: 0.05585305020213127\n",
      "Iteration 7700, Loss: 0.055747829377651215\n",
      "Iteration 7701, Loss: 0.055681269615888596\n",
      "Iteration 7702, Loss: 0.0557405948638916\n",
      "Iteration 7703, Loss: 0.055626433342695236\n",
      "Iteration 7704, Loss: 0.05579157918691635\n",
      "Iteration 7705, Loss: 0.055869102478027344\n",
      "Iteration 7706, Loss: 0.05584287643432617\n",
      "Iteration 7707, Loss: 0.055724143981933594\n",
      "Iteration 7708, Loss: 0.05572986602783203\n",
      "Iteration 7709, Loss: 0.05580202862620354\n",
      "Iteration 7710, Loss: 0.05569521710276604\n",
      "Iteration 7711, Loss: 0.055747829377651215\n",
      "Iteration 7712, Loss: 0.055832069367170334\n",
      "Iteration 7713, Loss: 0.05581303685903549\n",
      "Iteration 7714, Loss: 0.05570264905691147\n",
      "Iteration 7715, Loss: 0.05574941635131836\n",
      "Iteration 7716, Loss: 0.05581200122833252\n",
      "Iteration 7717, Loss: 0.055695854127407074\n",
      "Iteration 7718, Loss: 0.05575255677103996\n",
      "Iteration 7719, Loss: 0.0558423213660717\n",
      "Iteration 7720, Loss: 0.05582873150706291\n",
      "Iteration 7721, Loss: 0.0557229146361351\n",
      "Iteration 7722, Loss: 0.055716197937726974\n",
      "Iteration 7723, Loss: 0.055774372071027756\n",
      "Iteration 7724, Loss: 0.05565643310546875\n",
      "Iteration 7725, Loss: 0.05578120797872543\n",
      "Iteration 7726, Loss: 0.05587009713053703\n",
      "Iteration 7727, Loss: 0.05585587024688721\n",
      "Iteration 7728, Loss: 0.05574909970164299\n",
      "Iteration 7729, Loss: 0.05568119138479233\n",
      "Iteration 7730, Loss: 0.055740438401699066\n",
      "Iteration 7731, Loss: 0.05562285706400871\n",
      "Iteration 7732, Loss: 0.05580254644155502\n",
      "Iteration 7733, Loss: 0.05588650703430176\n",
      "Iteration 7734, Loss: 0.055866364389657974\n",
      "Iteration 7735, Loss: 0.05575374886393547\n",
      "Iteration 7736, Loss: 0.055685125291347504\n",
      "Iteration 7737, Loss: 0.05575820058584213\n",
      "Iteration 7738, Loss: 0.05566183850169182\n",
      "Iteration 7739, Loss: 0.05576205626130104\n",
      "Iteration 7740, Loss: 0.05583691969513893\n",
      "Iteration 7741, Loss: 0.055810652673244476\n",
      "Iteration 7742, Loss: 0.05569382756948471\n",
      "Iteration 7743, Loss: 0.05576900765299797\n",
      "Iteration 7744, Loss: 0.05583898350596428\n",
      "Iteration 7745, Loss: 0.055728793144226074\n",
      "Iteration 7746, Loss: 0.05572470277547836\n",
      "Iteration 7747, Loss: 0.0558113269507885\n",
      "Iteration 7748, Loss: 0.05579535290598869\n",
      "Iteration 7749, Loss: 0.055687349289655685\n",
      "Iteration 7750, Loss: 0.05576634407043457\n",
      "Iteration 7751, Loss: 0.055826663970947266\n",
      "Iteration 7752, Loss: 0.0557074174284935\n",
      "Iteration 7753, Loss: 0.05574631690979004\n",
      "Iteration 7754, Loss: 0.05583819001913071\n",
      "Iteration 7755, Loss: 0.05582702159881592\n",
      "Iteration 7756, Loss: 0.05572279542684555\n",
      "Iteration 7757, Loss: 0.05571361631155014\n",
      "Iteration 7758, Loss: 0.05576964467763901\n",
      "Iteration 7759, Loss: 0.05564761161804199\n",
      "Iteration 7760, Loss: 0.0557916983962059\n",
      "Iteration 7761, Loss: 0.055884961038827896\n",
      "Iteration 7762, Loss: 0.055874310433864594\n",
      "Iteration 7763, Loss: 0.055770717561244965\n",
      "Iteration 7764, Loss: 0.055647969245910645\n",
      "Iteration 7765, Loss: 0.05570380017161369\n",
      "Iteration 7766, Loss: 0.0556330680847168\n",
      "Iteration 7767, Loss: 0.05562174320220947\n",
      "Iteration 7768, Loss: 0.055736344307661057\n",
      "Iteration 7769, Loss: 0.05569235607981682\n",
      "Iteration 7770, Loss: 0.05570618435740471\n",
      "Iteration 7771, Loss: 0.055751364678144455\n",
      "Iteration 7772, Loss: 0.05569680780172348\n",
      "Iteration 7773, Loss: 0.05568929761648178\n",
      "Iteration 7774, Loss: 0.05569382756948471\n",
      "Iteration 7775, Loss: 0.05567300319671631\n",
      "Iteration 7776, Loss: 0.055689215660095215\n",
      "Iteration 7777, Loss: 0.055619798600673676\n",
      "Iteration 7778, Loss: 0.055729031562805176\n",
      "Iteration 7779, Loss: 0.055686596781015396\n",
      "Iteration 7780, Loss: 0.055704712867736816\n",
      "Iteration 7781, Loss: 0.055738966912031174\n",
      "Iteration 7782, Loss: 0.05566132068634033\n",
      "Iteration 7783, Loss: 0.05576173588633537\n",
      "Iteration 7784, Loss: 0.05579555407166481\n",
      "Iteration 7785, Loss: 0.055673640221357346\n",
      "Iteration 7786, Loss: 0.055774372071027756\n",
      "Iteration 7787, Loss: 0.05585845559835434\n",
      "Iteration 7788, Loss: 0.05581967160105705\n",
      "Iteration 7789, Loss: 0.05567924305796623\n",
      "Iteration 7790, Loss: 0.05580354109406471\n",
      "Iteration 7791, Loss: 0.055893540382385254\n",
      "Iteration 7792, Loss: 0.05581486597657204\n",
      "Iteration 7793, Loss: 0.05563696473836899\n",
      "Iteration 7794, Loss: 0.05574632063508034\n",
      "Iteration 7795, Loss: 0.05573856830596924\n",
      "Iteration 7796, Loss: 0.055622540414333344\n",
      "Iteration 7797, Loss: 0.05575001239776611\n",
      "Iteration 7798, Loss: 0.05571528524160385\n",
      "Iteration 7799, Loss: 0.05568286031484604\n",
      "Iteration 7800, Loss: 0.05572148412466049\n",
      "Iteration 7801, Loss: 0.05565798282623291\n",
      "Iteration 7802, Loss: 0.055750492960214615\n",
      "Iteration 7803, Loss: 0.05576300621032715\n",
      "Iteration 7804, Loss: 0.05562051385641098\n",
      "Iteration 7805, Loss: 0.05573797598481178\n",
      "Iteration 7806, Loss: 0.05573360249400139\n",
      "Iteration 7807, Loss: 0.05563501641154289\n",
      "Iteration 7808, Loss: 0.05575529858469963\n",
      "Iteration 7809, Loss: 0.05573829263448715\n",
      "Iteration 7810, Loss: 0.055652182549238205\n",
      "Iteration 7811, Loss: 0.055678606033325195\n",
      "Iteration 7812, Loss: 0.055626511573791504\n",
      "Iteration 7813, Loss: 0.05568873882293701\n",
      "Iteration 7814, Loss: 0.055634818971157074\n",
      "Iteration 7815, Loss: 0.055730342864990234\n",
      "Iteration 7816, Loss: 0.05572402849793434\n",
      "Iteration 7817, Loss: 0.05564066022634506\n",
      "Iteration 7818, Loss: 0.05568317696452141\n",
      "Iteration 7819, Loss: 0.05563418194651604\n",
      "Iteration 7820, Loss: 0.055667757987976074\n",
      "Iteration 7821, Loss: 0.0556316003203392\n",
      "Iteration 7822, Loss: 0.05575494095683098\n",
      "Iteration 7823, Loss: 0.05574623867869377\n",
      "Iteration 7824, Loss: 0.05564375966787338\n",
      "Iteration 7825, Loss: 0.05569835752248764\n",
      "Iteration 7826, Loss: 0.05565071105957031\n",
      "Iteration 7827, Loss: 0.0557456836104393\n",
      "Iteration 7828, Loss: 0.05576809495687485\n",
      "Iteration 7829, Loss: 0.055671654641628265\n",
      "Iteration 7830, Loss: 0.05575549602508545\n",
      "Iteration 7831, Loss: 0.055804453790187836\n",
      "Iteration 7832, Loss: 0.05571091175079346\n",
      "Iteration 7833, Loss: 0.055723827332258224\n",
      "Iteration 7834, Loss: 0.055788878351449966\n",
      "Iteration 7835, Loss: 0.055725932121276855\n",
      "Iteration 7836, Loss: 0.05567821115255356\n",
      "Iteration 7837, Loss: 0.05572076886892319\n",
      "Iteration 7838, Loss: 0.05562981218099594\n",
      "Iteration 7839, Loss: 0.055779777467250824\n",
      "Iteration 7840, Loss: 0.05583890527486801\n",
      "Iteration 7841, Loss: 0.05579157918691635\n",
      "Iteration 7842, Loss: 0.05566585063934326\n",
      "Iteration 7843, Loss: 0.055808186531066895\n",
      "Iteration 7844, Loss: 0.05587450787425041\n",
      "Iteration 7845, Loss: 0.05576789379119873\n",
      "Iteration 7846, Loss: 0.05569295212626457\n",
      "Iteration 7847, Loss: 0.05577616021037102\n",
      "Iteration 7848, Loss: 0.0557558573782444\n",
      "Iteration 7849, Loss: 0.055648330599069595\n",
      "Iteration 7850, Loss: 0.055812954902648926\n",
      "Iteration 7851, Loss: 0.05586520954966545\n",
      "Iteration 7852, Loss: 0.0557401180267334\n",
      "Iteration 7853, Loss: 0.05572601407766342\n",
      "Iteration 7854, Loss: 0.0558215007185936\n",
      "Iteration 7855, Loss: 0.05581331253051758\n",
      "Iteration 7856, Loss: 0.0557122640311718\n",
      "Iteration 7857, Loss: 0.05572414770722389\n",
      "Iteration 7858, Loss: 0.05577743053436279\n",
      "Iteration 7859, Loss: 0.055653057992458344\n",
      "Iteration 7860, Loss: 0.05578947067260742\n",
      "Iteration 7861, Loss: 0.05588444322347641\n",
      "Iteration 7862, Loss: 0.05587565898895264\n",
      "Iteration 7863, Loss: 0.05577349662780762\n",
      "Iteration 7864, Loss: 0.055642884224653244\n",
      "Iteration 7865, Loss: 0.05570201203227043\n",
      "Iteration 7866, Loss: 0.05563322827219963\n",
      "Iteration 7867, Loss: 0.05564979836344719\n",
      "Iteration 7868, Loss: 0.0556594543159008\n",
      "Iteration 7869, Loss: 0.05563465878367424\n",
      "Iteration 7870, Loss: 0.05565929785370827\n",
      "Iteration 7871, Loss: 0.05565333738923073\n",
      "Iteration 7872, Loss: 0.055630724877119064\n",
      "Iteration 7873, Loss: 0.055726371705532074\n",
      "Iteration 7874, Loss: 0.05568023771047592\n",
      "Iteration 7875, Loss: 0.055717628449201584\n",
      "Iteration 7876, Loss: 0.05576590821146965\n",
      "Iteration 7877, Loss: 0.05571591854095459\n",
      "Iteration 7878, Loss: 0.055657267570495605\n",
      "Iteration 7879, Loss: 0.05565476790070534\n",
      "Iteration 7880, Loss: 0.05570749565958977\n",
      "Iteration 7881, Loss: 0.05572974681854248\n",
      "Iteration 7882, Loss: 0.0556563138961792\n",
      "Iteration 7883, Loss: 0.05576634407043457\n",
      "Iteration 7884, Loss: 0.055788278579711914\n",
      "Iteration 7885, Loss: 0.05563477799296379\n",
      "Iteration 7886, Loss: 0.05582229420542717\n",
      "Iteration 7887, Loss: 0.055934708565473557\n",
      "Iteration 7888, Loss: 0.0559409074485302\n",
      "Iteration 7889, Loss: 0.05585150048136711\n",
      "Iteration 7890, Loss: 0.055677615106105804\n",
      "Iteration 7891, Loss: 0.05586020275950432\n",
      "Iteration 7892, Loss: 0.05599244683980942\n",
      "Iteration 7893, Loss: 0.05593721196055412\n",
      "Iteration 7894, Loss: 0.05571448802947998\n",
      "Iteration 7895, Loss: 0.0558093786239624\n",
      "Iteration 7896, Loss: 0.05596328154206276\n",
      "Iteration 7897, Loss: 0.05600690841674805\n",
      "Iteration 7898, Loss: 0.055950962007045746\n",
      "Iteration 7899, Loss: 0.05580615997314453\n",
      "Iteration 7900, Loss: 0.055650435388088226\n",
      "Iteration 7901, Loss: 0.05575351044535637\n",
      "Iteration 7902, Loss: 0.055677734315395355\n",
      "Iteration 7903, Loss: 0.055737655609846115\n",
      "Iteration 7904, Loss: 0.055801473557949066\n",
      "Iteration 7905, Loss: 0.05576435849070549\n",
      "Iteration 7906, Loss: 0.05563756078481674\n",
      "Iteration 7907, Loss: 0.0558575801551342\n",
      "Iteration 7908, Loss: 0.05593840405344963\n",
      "Iteration 7909, Loss: 0.05583934113383293\n",
      "Iteration 7910, Loss: 0.05563871189951897\n",
      "Iteration 7911, Loss: 0.0557330846786499\n",
      "Iteration 7912, Loss: 0.05572807788848877\n",
      "Iteration 7913, Loss: 0.055624883621931076\n",
      "Iteration 7914, Loss: 0.055841803550720215\n",
      "Iteration 7915, Loss: 0.05589791387319565\n",
      "Iteration 7916, Loss: 0.055783748626708984\n",
      "Iteration 7917, Loss: 0.055686675012111664\n",
      "Iteration 7918, Loss: 0.055776678025722504\n",
      "Iteration 7919, Loss: 0.0557631254196167\n",
      "Iteration 7920, Loss: 0.05565095320343971\n",
      "Iteration 7921, Loss: 0.05581812188029289\n",
      "Iteration 7922, Loss: 0.055886153131723404\n",
      "Iteration 7923, Loss: 0.0557866096496582\n",
      "Iteration 7924, Loss: 0.055674873292446136\n",
      "Iteration 7925, Loss: 0.05575963109731674\n",
      "Iteration 7926, Loss: 0.05574147030711174\n",
      "Iteration 7927, Loss: 0.05561963841319084\n",
      "Iteration 7928, Loss: 0.05585821717977524\n",
      "Iteration 7929, Loss: 0.05593705177307129\n",
      "Iteration 7930, Loss: 0.05585988610982895\n",
      "Iteration 7931, Loss: 0.05567129701375961\n",
      "Iteration 7932, Loss: 0.055823683738708496\n",
      "Iteration 7933, Loss: 0.05593506619334221\n",
      "Iteration 7934, Loss: 0.05591055005788803\n",
      "Iteration 7935, Loss: 0.055766187608242035\n",
      "Iteration 7936, Loss: 0.05571337789297104\n",
      "Iteration 7937, Loss: 0.05582066625356674\n",
      "Iteration 7938, Loss: 0.05578216165304184\n",
      "Iteration 7939, Loss: 0.05561820790171623\n",
      "Iteration 7940, Loss: 0.055720847100019455\n",
      "Iteration 7941, Loss: 0.055667560547590256\n",
      "Iteration 7942, Loss: 0.0557306632399559\n",
      "Iteration 7943, Loss: 0.055769920349121094\n",
      "Iteration 7944, Loss: 0.05569422245025635\n",
      "Iteration 7945, Loss: 0.05571993440389633\n",
      "Iteration 7946, Loss: 0.055754899978637695\n",
      "Iteration 7947, Loss: 0.05563453957438469\n",
      "Iteration 7948, Loss: 0.05581279844045639\n",
      "Iteration 7949, Loss: 0.055896203964948654\n",
      "Iteration 7950, Loss: 0.05585392564535141\n",
      "Iteration 7951, Loss: 0.05571170896291733\n",
      "Iteration 7952, Loss: 0.055772148072719574\n",
      "Iteration 7953, Loss: 0.0558677539229393\n",
      "Iteration 7954, Loss: 0.055801115930080414\n",
      "Iteration 7955, Loss: 0.05563303083181381\n",
      "Iteration 7956, Loss: 0.05568981170654297\n",
      "Iteration 7957, Loss: 0.055631719529628754\n",
      "Iteration 7958, Loss: 0.055768370628356934\n",
      "Iteration 7959, Loss: 0.05577031895518303\n",
      "Iteration 7960, Loss: 0.055630963295698166\n",
      "Iteration 7961, Loss: 0.05577496811747551\n",
      "Iteration 7962, Loss: 0.05580934137105942\n",
      "Iteration 7963, Loss: 0.05571639537811279\n",
      "Iteration 7964, Loss: 0.05571512505412102\n",
      "Iteration 7965, Loss: 0.055774807929992676\n",
      "Iteration 7966, Loss: 0.055690765380859375\n",
      "Iteration 7967, Loss: 0.05573658272624016\n",
      "Iteration 7968, Loss: 0.055793922394514084\n",
      "Iteration 7969, Loss: 0.055723510682582855\n",
      "Iteration 7970, Loss: 0.05568818375468254\n",
      "Iteration 7971, Loss: 0.05573364347219467\n",
      "Iteration 7972, Loss: 0.05563831329345703\n",
      "Iteration 7973, Loss: 0.05579424276947975\n",
      "Iteration 7974, Loss: 0.055862508714199066\n",
      "Iteration 7975, Loss: 0.05580946058034897\n",
      "Iteration 7976, Loss: 0.05567260831594467\n",
      "Iteration 7977, Loss: 0.055806759744882584\n",
      "Iteration 7978, Loss: 0.055880311876535416\n",
      "Iteration 7979, Loss: 0.055785179138183594\n",
      "Iteration 7980, Loss: 0.05567002296447754\n",
      "Iteration 7981, Loss: 0.055744610726833344\n",
      "Iteration 7982, Loss: 0.05570749565958977\n",
      "Iteration 7983, Loss: 0.055663906037807465\n",
      "Iteration 7984, Loss: 0.055673521012067795\n",
      "Iteration 7985, Loss: 0.05568265914916992\n",
      "Iteration 7986, Loss: 0.0556865930557251\n",
      "Iteration 7987, Loss: 0.0556468591094017\n",
      "Iteration 7988, Loss: 0.05567964166402817\n",
      "Iteration 7989, Loss: 0.055647414177656174\n",
      "Iteration 7990, Loss: 0.0556718111038208\n",
      "Iteration 7991, Loss: 0.05563163757324219\n",
      "Iteration 7992, Loss: 0.055739521980285645\n",
      "Iteration 7993, Loss: 0.05575541779398918\n",
      "Iteration 7994, Loss: 0.05566903203725815\n",
      "Iteration 7995, Loss: 0.05575641244649887\n",
      "Iteration 7996, Loss: 0.05578943341970444\n",
      "Iteration 7997, Loss: 0.055659692734479904\n",
      "Iteration 7998, Loss: 0.055796705186367035\n",
      "Iteration 7999, Loss: 0.05589195340871811\n",
      "Iteration 8000, Loss: 0.05586874857544899\n",
      "Iteration 8001, Loss: 0.05574556440114975\n",
      "Iteration 8002, Loss: 0.05571671575307846\n",
      "Iteration 8003, Loss: 0.05580246448516846\n",
      "Iteration 8004, Loss: 0.055728357285261154\n",
      "Iteration 8005, Loss: 0.05570650100708008\n",
      "Iteration 8006, Loss: 0.05576710030436516\n",
      "Iteration 8007, Loss: 0.05572625249624252\n",
      "Iteration 8008, Loss: 0.05565933510661125\n",
      "Iteration 8009, Loss: 0.05572311207652092\n",
      "Iteration 8010, Loss: 0.055674079805612564\n",
      "Iteration 8011, Loss: 0.055715203285217285\n",
      "Iteration 8012, Loss: 0.05575088784098625\n",
      "Iteration 8013, Loss: 0.05569124594330788\n",
      "Iteration 8014, Loss: 0.055707138031721115\n",
      "Iteration 8015, Loss: 0.05571746826171875\n",
      "Iteration 8016, Loss: 0.055654287338256836\n",
      "Iteration 8017, Loss: 0.055676184594631195\n",
      "Iteration 8018, Loss: 0.05563374608755112\n",
      "Iteration 8019, Loss: 0.055698078125715256\n",
      "Iteration 8020, Loss: 0.055680833756923676\n",
      "Iteration 8021, Loss: 0.05567356199026108\n",
      "Iteration 8022, Loss: 0.05565909668803215\n",
      "Iteration 8023, Loss: 0.05571039766073227\n",
      "Iteration 8024, Loss: 0.05572577565908432\n",
      "Iteration 8025, Loss: 0.05564960092306137\n",
      "Iteration 8026, Loss: 0.055764757096767426\n",
      "Iteration 8027, Loss: 0.05577421188354492\n",
      "Iteration 8028, Loss: 0.05561519041657448\n",
      "Iteration 8029, Loss: 0.05581017583608627\n",
      "Iteration 8030, Loss: 0.055897437036037445\n",
      "Iteration 8031, Loss: 0.055881619453430176\n",
      "Iteration 8032, Loss: 0.055773019790649414\n",
      "Iteration 8033, Loss: 0.05565166473388672\n",
      "Iteration 8034, Loss: 0.05571281909942627\n",
      "Iteration 8035, Loss: 0.05562397092580795\n",
      "Iteration 8036, Loss: 0.055626314133405685\n",
      "Iteration 8037, Loss: 0.05571126937866211\n",
      "Iteration 8038, Loss: 0.05569390580058098\n",
      "Iteration 8039, Loss: 0.0556672029197216\n",
      "Iteration 8040, Loss: 0.055667877197265625\n",
      "Iteration 8041, Loss: 0.055689774453639984\n",
      "Iteration 8042, Loss: 0.05568655580282211\n",
      "Iteration 8043, Loss: 0.05566040799021721\n",
      "Iteration 8044, Loss: 0.05565834417939186\n",
      "Iteration 8045, Loss: 0.05569521710276604\n",
      "Iteration 8046, Loss: 0.055693428963422775\n",
      "Iteration 8047, Loss: 0.05565190315246582\n",
      "Iteration 8048, Loss: 0.05567833036184311\n",
      "Iteration 8049, Loss: 0.05565563961863518\n",
      "Iteration 8050, Loss: 0.05565913766622543\n",
      "Iteration 8051, Loss: 0.05565313622355461\n",
      "Iteration 8052, Loss: 0.05567248910665512\n",
      "Iteration 8053, Loss: 0.055650196969509125\n",
      "Iteration 8054, Loss: 0.055698275566101074\n",
      "Iteration 8055, Loss: 0.05565842241048813\n",
      "Iteration 8056, Loss: 0.05573197454214096\n",
      "Iteration 8057, Loss: 0.055768612772226334\n",
      "Iteration 8058, Loss: 0.055700063705444336\n",
      "Iteration 8059, Loss: 0.055705469101667404\n",
      "Iteration 8060, Loss: 0.055730823427438736\n",
      "Iteration 8061, Loss: 0.055631160736083984\n",
      "Iteration 8062, Loss: 0.05567034333944321\n",
      "Iteration 8063, Loss: 0.05563267320394516\n",
      "Iteration 8064, Loss: 0.055753111839294434\n",
      "Iteration 8065, Loss: 0.05574210733175278\n",
      "Iteration 8066, Loss: 0.05564769357442856\n",
      "Iteration 8067, Loss: 0.055693309754133224\n",
      "Iteration 8068, Loss: 0.05564117431640625\n",
      "Iteration 8069, Loss: 0.05575939267873764\n",
      "Iteration 8070, Loss: 0.05578251928091049\n",
      "Iteration 8071, Loss: 0.05568099021911621\n",
      "Iteration 8072, Loss: 0.05575358867645264\n",
      "Iteration 8073, Loss: 0.05581232160329819\n",
      "Iteration 8074, Loss: 0.05573165416717529\n",
      "Iteration 8075, Loss: 0.055692754685878754\n",
      "Iteration 8076, Loss: 0.05574830621480942\n",
      "Iteration 8077, Loss: 0.05567125603556633\n",
      "Iteration 8078, Loss: 0.05574818700551987\n",
      "Iteration 8079, Loss: 0.05579289048910141\n",
      "Iteration 8080, Loss: 0.05570197105407715\n",
      "Iteration 8081, Loss: 0.05572744458913803\n",
      "Iteration 8082, Loss: 0.055785976350307465\n",
      "Iteration 8083, Loss: 0.055709920823574066\n",
      "Iteration 8084, Loss: 0.055709127336740494\n",
      "Iteration 8085, Loss: 0.05575728416442871\n",
      "Iteration 8086, Loss: 0.055672209709882736\n",
      "Iteration 8087, Loss: 0.05575088784098625\n",
      "Iteration 8088, Loss: 0.055800121277570724\n",
      "Iteration 8089, Loss: 0.055711548775434494\n",
      "Iteration 8090, Loss: 0.05571715161204338\n",
      "Iteration 8091, Loss: 0.05577639862895012\n",
      "Iteration 8092, Loss: 0.05570165440440178\n",
      "Iteration 8093, Loss: 0.05571473017334938\n",
      "Iteration 8094, Loss: 0.055760424584150314\n",
      "Iteration 8095, Loss: 0.05567077919840813\n",
      "Iteration 8096, Loss: 0.05575796216726303\n",
      "Iteration 8097, Loss: 0.05581307411193848\n",
      "Iteration 8098, Loss: 0.0557323694229126\n",
      "Iteration 8099, Loss: 0.05569136142730713\n",
      "Iteration 8100, Loss: 0.05574862286448479\n",
      "Iteration 8101, Loss: 0.05567356199026108\n",
      "Iteration 8102, Loss: 0.05574874207377434\n",
      "Iteration 8103, Loss: 0.05579809471964836\n",
      "Iteration 8104, Loss: 0.05572311207652092\n",
      "Iteration 8105, Loss: 0.05569525808095932\n",
      "Iteration 8106, Loss: 0.055745720863342285\n",
      "Iteration 8107, Loss: 0.05566450208425522\n",
      "Iteration 8108, Loss: 0.0557611808180809\n",
      "Iteration 8109, Loss: 0.05581863969564438\n",
      "Iteration 8110, Loss: 0.05576106160879135\n",
      "Iteration 8111, Loss: 0.055663347244262695\n",
      "Iteration 8112, Loss: 0.0557636022567749\n",
      "Iteration 8113, Loss: 0.055756889283657074\n",
      "Iteration 8114, Loss: 0.055645983666181564\n",
      "Iteration 8115, Loss: 0.055705271661281586\n",
      "Iteration 8116, Loss: 0.05569791793823242\n",
      "Iteration 8117, Loss: 0.055642370134592056\n",
      "Iteration 8118, Loss: 0.055687032639980316\n",
      "Iteration 8119, Loss: 0.05565397068858147\n",
      "Iteration 8120, Loss: 0.0557023286819458\n",
      "Iteration 8121, Loss: 0.05568079277873039\n",
      "Iteration 8122, Loss: 0.05569680780172348\n",
      "Iteration 8123, Loss: 0.0557120256125927\n",
      "Iteration 8124, Loss: 0.05563871189951897\n",
      "Iteration 8125, Loss: 0.05574091523885727\n",
      "Iteration 8126, Loss: 0.05571393296122551\n",
      "Iteration 8127, Loss: 0.05568011850118637\n",
      "Iteration 8128, Loss: 0.05571440979838371\n",
      "Iteration 8129, Loss: 0.055652420967817307\n",
      "Iteration 8130, Loss: 0.055755339562892914\n",
      "Iteration 8131, Loss: 0.05576273053884506\n",
      "Iteration 8132, Loss: 0.05562162399291992\n",
      "Iteration 8133, Loss: 0.05564066022634506\n",
      "Iteration 8134, Loss: 0.05567892640829086\n",
      "Iteration 8135, Loss: 0.05562087148427963\n",
      "Iteration 8136, Loss: 0.05573507398366928\n",
      "Iteration 8137, Loss: 0.05571671575307846\n",
      "Iteration 8138, Loss: 0.05565834045410156\n",
      "Iteration 8139, Loss: 0.05568389222025871\n",
      "Iteration 8140, Loss: 0.05564502999186516\n",
      "Iteration 8141, Loss: 0.05565476790070534\n",
      "Iteration 8142, Loss: 0.05563775822520256\n",
      "Iteration 8143, Loss: 0.055697839707136154\n",
      "Iteration 8144, Loss: 0.055661801248788834\n",
      "Iteration 8145, Loss: 0.055713098496198654\n",
      "Iteration 8146, Loss: 0.05572275444865227\n",
      "Iteration 8147, Loss: 0.055624090135097504\n",
      "Iteration 8148, Loss: 0.05573129653930664\n",
      "Iteration 8149, Loss: 0.05567880719900131\n",
      "Iteration 8150, Loss: 0.05572422593832016\n",
      "Iteration 8151, Loss: 0.055774055421352386\n",
      "Iteration 8152, Loss: 0.05572124570608139\n",
      "Iteration 8153, Loss: 0.055659811943769455\n",
      "Iteration 8154, Loss: 0.055680714547634125\n",
      "Iteration 8155, Loss: 0.05567074194550514\n",
      "Iteration 8156, Loss: 0.055674076080322266\n",
      "Iteration 8157, Loss: 0.05565313622355461\n",
      "Iteration 8158, Loss: 0.05565822124481201\n",
      "Iteration 8159, Loss: 0.05565643310546875\n",
      "Iteration 8160, Loss: 0.05564209073781967\n",
      "Iteration 8161, Loss: 0.05570554733276367\n",
      "Iteration 8162, Loss: 0.0556795597076416\n",
      "Iteration 8163, Loss: 0.055692076683044434\n",
      "Iteration 8164, Loss: 0.0557025671005249\n",
      "Iteration 8165, Loss: 0.05563485622406006\n",
      "Iteration 8166, Loss: 0.0556158646941185\n",
      "Iteration 8167, Loss: 0.05571126937866211\n",
      "Iteration 8168, Loss: 0.055664222687482834\n",
      "Iteration 8169, Loss: 0.0557246208190918\n",
      "Iteration 8170, Loss: 0.05575045198202133\n",
      "Iteration 8171, Loss: 0.05565734952688217\n",
      "Iteration 8172, Loss: 0.05577516928315163\n",
      "Iteration 8173, Loss: 0.05582293123006821\n",
      "Iteration 8174, Loss: 0.05572148412466049\n",
      "Iteration 8175, Loss: 0.05572068691253662\n",
      "Iteration 8176, Loss: 0.05579451844096184\n",
      "Iteration 8177, Loss: 0.05574500560760498\n",
      "Iteration 8178, Loss: 0.05564491078257561\n",
      "Iteration 8179, Loss: 0.055704277008771896\n",
      "Iteration 8180, Loss: 0.05563024803996086\n",
      "Iteration 8181, Loss: 0.05573849007487297\n",
      "Iteration 8182, Loss: 0.05576900765299797\n",
      "Iteration 8183, Loss: 0.05570177361369133\n",
      "Iteration 8184, Loss: 0.05569855496287346\n",
      "Iteration 8185, Loss: 0.055714450776576996\n",
      "Iteration 8186, Loss: 0.05565258115530014\n",
      "Iteration 8187, Loss: 0.05566398426890373\n",
      "Iteration 8188, Loss: 0.05565381050109863\n",
      "Iteration 8189, Loss: 0.05563267320394516\n",
      "Iteration 8190, Loss: 0.055647414177656174\n",
      "Iteration 8191, Loss: 0.05566974729299545\n",
      "Iteration 8192, Loss: 0.05565154552459717\n",
      "Iteration 8193, Loss: 0.05570705980062485\n",
      "Iteration 8194, Loss: 0.05566906929016113\n",
      "Iteration 8195, Loss: 0.05572152137756348\n",
      "Iteration 8196, Loss: 0.05576392263174057\n",
      "Iteration 8197, Loss: 0.05570729821920395\n",
      "Iteration 8198, Loss: 0.055679045617580414\n",
      "Iteration 8199, Loss: 0.055685561150312424\n",
      "Iteration 8200, Loss: 0.055680274963378906\n",
      "Iteration 8201, Loss: 0.05569617077708244\n",
      "Iteration 8202, Loss: 0.05562552064657211\n",
      "Iteration 8203, Loss: 0.055777233093976974\n",
      "Iteration 8204, Loss: 0.055772703140974045\n",
      "Iteration 8205, Loss: 0.05563497543334961\n",
      "Iteration 8206, Loss: 0.0557481050491333\n",
      "Iteration 8207, Loss: 0.05576547235250473\n",
      "Iteration 8208, Loss: 0.05566736310720444\n",
      "Iteration 8209, Loss: 0.05577266588807106\n",
      "Iteration 8210, Loss: 0.055827897042036057\n",
      "Iteration 8211, Loss: 0.05573078244924545\n",
      "Iteration 8212, Loss: 0.055709999054670334\n",
      "Iteration 8213, Loss: 0.05578235909342766\n",
      "Iteration 8214, Loss: 0.05573447793722153\n",
      "Iteration 8215, Loss: 0.05565091222524643\n",
      "Iteration 8216, Loss: 0.05568397417664528\n",
      "Iteration 8217, Loss: 0.055653613060712814\n",
      "Iteration 8218, Loss: 0.05564459413290024\n",
      "Iteration 8219, Loss: 0.05568981170654297\n",
      "Iteration 8220, Loss: 0.05563203617930412\n",
      "Iteration 8221, Loss: 0.055761657655239105\n",
      "Iteration 8222, Loss: 0.055814146995544434\n",
      "Iteration 8223, Loss: 0.05576638504862785\n",
      "Iteration 8224, Loss: 0.055637240409851074\n",
      "Iteration 8225, Loss: 0.05584530159831047\n",
      "Iteration 8226, Loss: 0.05590963363647461\n",
      "Iteration 8227, Loss: 0.05579344555735588\n",
      "Iteration 8228, Loss: 0.05568234249949455\n",
      "Iteration 8229, Loss: 0.05577266216278076\n",
      "Iteration 8230, Loss: 0.05576082319021225\n",
      "Iteration 8231, Loss: 0.05565690994262695\n",
      "Iteration 8232, Loss: 0.055804651230573654\n",
      "Iteration 8233, Loss: 0.05586036294698715\n",
      "Iteration 8234, Loss: 0.05573630705475807\n",
      "Iteration 8235, Loss: 0.05572911351919174\n",
      "Iteration 8236, Loss: 0.055823806673288345\n",
      "Iteration 8237, Loss: 0.055814824998378754\n",
      "Iteration 8238, Loss: 0.055712781846523285\n",
      "Iteration 8239, Loss: 0.05572621151804924\n",
      "Iteration 8240, Loss: 0.055779777467250824\n",
      "Iteration 8241, Loss: 0.05565524101257324\n",
      "Iteration 8242, Loss: 0.0557890348136425\n",
      "Iteration 8243, Loss: 0.05588400363922119\n",
      "Iteration 8244, Loss: 0.05587494373321533\n",
      "Iteration 8245, Loss: 0.05577246472239494\n",
      "Iteration 8246, Loss: 0.05564618110656738\n",
      "Iteration 8247, Loss: 0.05570554733276367\n",
      "Iteration 8248, Loss: 0.05563310906291008\n",
      "Iteration 8249, Loss: 0.055670540779829025\n",
      "Iteration 8250, Loss: 0.05561582371592522\n",
      "Iteration 8251, Loss: 0.05570821091532707\n",
      "Iteration 8252, Loss: 0.05570435896515846\n",
      "Iteration 8253, Loss: 0.05561789125204086\n",
      "Iteration 8254, Loss: 0.05565933510661125\n",
      "Iteration 8255, Loss: 0.055632393807172775\n",
      "Iteration 8256, Loss: 0.05567769333720207\n",
      "Iteration 8257, Loss: 0.05563247203826904\n",
      "Iteration 8258, Loss: 0.05569195747375488\n",
      "Iteration 8259, Loss: 0.05563148111104965\n",
      "Iteration 8260, Loss: 0.05577075481414795\n",
      "Iteration 8261, Loss: 0.05581367388367653\n",
      "Iteration 8262, Loss: 0.05573948472738266\n",
      "Iteration 8263, Loss: 0.05567670240998268\n",
      "Iteration 8264, Loss: 0.05573507398366928\n",
      "Iteration 8265, Loss: 0.0556592158973217\n",
      "Iteration 8266, Loss: 0.05575231835246086\n",
      "Iteration 8267, Loss: 0.055807553231716156\n",
      "Iteration 8268, Loss: 0.055759429931640625\n",
      "Iteration 8269, Loss: 0.055650435388088226\n",
      "Iteration 8270, Loss: 0.05579380318522453\n",
      "Iteration 8271, Loss: 0.05581307411193848\n",
      "Iteration 8272, Loss: 0.05566783994436264\n",
      "Iteration 8273, Loss: 0.05578736588358879\n",
      "Iteration 8274, Loss: 0.05588662624359131\n",
      "Iteration 8275, Loss: 0.05588154122233391\n",
      "Iteration 8276, Loss: 0.05578247830271721\n",
      "Iteration 8277, Loss: 0.05562877655029297\n",
      "Iteration 8278, Loss: 0.05568023771047592\n",
      "Iteration 8279, Loss: 0.055653057992458344\n",
      "Iteration 8280, Loss: 0.05564252659678459\n",
      "Iteration 8281, Loss: 0.0557098388671875\n",
      "Iteration 8282, Loss: 0.055664777755737305\n",
      "Iteration 8283, Loss: 0.05572855845093727\n",
      "Iteration 8284, Loss: 0.05577441304922104\n",
      "Iteration 8285, Loss: 0.055721282958984375\n",
      "Iteration 8286, Loss: 0.05565643683075905\n",
      "Iteration 8287, Loss: 0.055663906037807465\n",
      "Iteration 8288, Loss: 0.05569314956665039\n",
      "Iteration 8289, Loss: 0.05570697784423828\n",
      "Iteration 8290, Loss: 0.05562937632203102\n",
      "Iteration 8291, Loss: 0.055803459137678146\n",
      "Iteration 8292, Loss: 0.05582340806722641\n",
      "Iteration 8293, Loss: 0.055668871849775314\n",
      "Iteration 8294, Loss: 0.05579861253499985\n",
      "Iteration 8295, Loss: 0.05591082572937012\n",
      "Iteration 8296, Loss: 0.05591503903269768\n",
      "Iteration 8297, Loss: 0.05582225322723389\n",
      "Iteration 8298, Loss: 0.055648207664489746\n",
      "Iteration 8299, Loss: 0.05589727684855461\n",
      "Iteration 8300, Loss: 0.05602320283651352\n",
      "Iteration 8301, Loss: 0.055961254984140396\n",
      "Iteration 8302, Loss: 0.05573205277323723\n",
      "Iteration 8303, Loss: 0.05580195039510727\n",
      "Iteration 8304, Loss: 0.0559595450758934\n",
      "Iteration 8305, Loss: 0.056006669998168945\n",
      "Iteration 8306, Loss: 0.05595306679606438\n",
      "Iteration 8307, Loss: 0.05581080913543701\n",
      "Iteration 8308, Loss: 0.05564642325043678\n",
      "Iteration 8309, Loss: 0.05576896667480469\n",
      "Iteration 8310, Loss: 0.05572402477264404\n",
      "Iteration 8311, Loss: 0.055686235427856445\n",
      "Iteration 8312, Loss: 0.0557330846786499\n",
      "Iteration 8313, Loss: 0.05568552017211914\n",
      "Iteration 8314, Loss: 0.055702369660139084\n",
      "Iteration 8315, Loss: 0.05570225045084953\n",
      "Iteration 8316, Loss: 0.05566931143403053\n",
      "Iteration 8317, Loss: 0.055688582360744476\n",
      "Iteration 8318, Loss: 0.05562524124979973\n",
      "Iteration 8319, Loss: 0.0557304248213768\n",
      "Iteration 8320, Loss: 0.055723827332258224\n",
      "Iteration 8321, Loss: 0.05564693734049797\n",
      "Iteration 8322, Loss: 0.055724382400512695\n",
      "Iteration 8323, Loss: 0.05568178743124008\n",
      "Iteration 8324, Loss: 0.055716753005981445\n",
      "Iteration 8325, Loss: 0.05575815960764885\n",
      "Iteration 8326, Loss: 0.055700063705444336\n",
      "Iteration 8327, Loss: 0.05569351091980934\n",
      "Iteration 8328, Loss: 0.05570685863494873\n",
      "Iteration 8329, Loss: 0.055661641061306\n",
      "Iteration 8330, Loss: 0.05567411705851555\n",
      "Iteration 8331, Loss: 0.055647969245910645\n",
      "Iteration 8332, Loss: 0.05568186566233635\n",
      "Iteration 8333, Loss: 0.05564574524760246\n",
      "Iteration 8334, Loss: 0.05569704622030258\n",
      "Iteration 8335, Loss: 0.05566922947764397\n",
      "Iteration 8336, Loss: 0.055709920823574066\n",
      "Iteration 8337, Loss: 0.05572180077433586\n",
      "Iteration 8338, Loss: 0.05563799664378166\n",
      "Iteration 8339, Loss: 0.05573912709951401\n",
      "Iteration 8340, Loss: 0.05571194738149643\n",
      "Iteration 8341, Loss: 0.05568055436015129\n",
      "Iteration 8342, Loss: 0.05571158975362778\n",
      "Iteration 8343, Loss: 0.055643241852521896\n",
      "Iteration 8344, Loss: 0.05577008053660393\n",
      "Iteration 8345, Loss: 0.05577918142080307\n",
      "Iteration 8346, Loss: 0.055619679391384125\n",
      "Iteration 8347, Loss: 0.05583079904317856\n",
      "Iteration 8348, Loss: 0.0559283122420311\n",
      "Iteration 8349, Loss: 0.05590812489390373\n",
      "Iteration 8350, Loss: 0.05578720569610596\n",
      "Iteration 8351, Loss: 0.05567646026611328\n",
      "Iteration 8352, Loss: 0.05579543486237526\n",
      "Iteration 8353, Loss: 0.0557740144431591\n",
      "Iteration 8354, Loss: 0.05565687268972397\n",
      "Iteration 8355, Loss: 0.055726051330566406\n",
      "Iteration 8356, Loss: 0.05573670193552971\n",
      "Iteration 8357, Loss: 0.055661916732788086\n",
      "Iteration 8358, Loss: 0.05576467886567116\n",
      "Iteration 8359, Loss: 0.05578943341970444\n",
      "Iteration 8360, Loss: 0.05566323176026344\n",
      "Iteration 8361, Loss: 0.05577544495463371\n",
      "Iteration 8362, Loss: 0.055853765457868576\n",
      "Iteration 8363, Loss: 0.05581899732351303\n",
      "Iteration 8364, Loss: 0.055685799568891525\n",
      "Iteration 8365, Loss: 0.05579233169555664\n",
      "Iteration 8366, Loss: 0.05587935820221901\n",
      "Iteration 8367, Loss: 0.05579885095357895\n",
      "Iteration 8368, Loss: 0.055648885667324066\n",
      "Iteration 8369, Loss: 0.055727243423461914\n",
      "Iteration 8370, Loss: 0.05569533631205559\n",
      "Iteration 8371, Loss: 0.05567725747823715\n",
      "Iteration 8372, Loss: 0.055675946176052094\n",
      "Iteration 8373, Loss: 0.05568321794271469\n",
      "Iteration 8374, Loss: 0.0556870736181736\n",
      "Iteration 8375, Loss: 0.0556488074362278\n",
      "Iteration 8376, Loss: 0.055628541857004166\n",
      "Iteration 8377, Loss: 0.055724382400512695\n",
      "Iteration 8378, Loss: 0.055742066353559494\n",
      "Iteration 8379, Loss: 0.05566497892141342\n",
      "Iteration 8380, Loss: 0.055759549140930176\n",
      "Iteration 8381, Loss: 0.05578557774424553\n",
      "Iteration 8382, Loss: 0.05563740059733391\n",
      "Iteration 8383, Loss: 0.05581808462738991\n",
      "Iteration 8384, Loss: 0.055927079170942307\n",
      "Iteration 8385, Loss: 0.05593041703104973\n",
      "Iteration 8386, Loss: 0.05583846941590309\n",
      "Iteration 8387, Loss: 0.055663466453552246\n",
      "Iteration 8388, Loss: 0.055880945175886154\n",
      "Iteration 8389, Loss: 0.056013546884059906\n",
      "Iteration 8390, Loss: 0.05595993995666504\n",
      "Iteration 8391, Loss: 0.05573849007487297\n",
      "Iteration 8392, Loss: 0.05579173564910889\n",
      "Iteration 8393, Loss: 0.055944763123989105\n",
      "Iteration 8394, Loss: 0.055987995117902756\n",
      "Iteration 8395, Loss: 0.055931925773620605\n",
      "Iteration 8396, Loss: 0.0557868517935276\n",
      "Iteration 8397, Loss: 0.055677495896816254\n",
      "Iteration 8398, Loss: 0.05577906221151352\n",
      "Iteration 8399, Loss: 0.055698197335004807\n",
      "Iteration 8400, Loss: 0.05572831630706787\n",
      "Iteration 8401, Loss: 0.05579730123281479\n",
      "Iteration 8402, Loss: 0.0557657890021801\n",
      "Iteration 8403, Loss: 0.05564407631754875\n",
      "Iteration 8404, Loss: 0.05584339424967766\n",
      "Iteration 8405, Loss: 0.05591829866170883\n",
      "Iteration 8406, Loss: 0.05581188201904297\n",
      "Iteration 8407, Loss: 0.05566199868917465\n",
      "Iteration 8408, Loss: 0.05574643611907959\n",
      "Iteration 8409, Loss: 0.05572943016886711\n",
      "Iteration 8410, Loss: 0.055620912462472916\n",
      "Iteration 8411, Loss: 0.05585690587759018\n",
      "Iteration 8412, Loss: 0.05591690540313721\n",
      "Iteration 8413, Loss: 0.05579948425292969\n",
      "Iteration 8414, Loss: 0.05567844957113266\n",
      "Iteration 8415, Loss: 0.055770158767700195\n",
      "Iteration 8416, Loss: 0.055760305374860764\n",
      "Iteration 8417, Loss: 0.05565687268972397\n",
      "Iteration 8418, Loss: 0.05580282211303711\n",
      "Iteration 8419, Loss: 0.05585936829447746\n",
      "Iteration 8420, Loss: 0.055742304772138596\n",
      "Iteration 8421, Loss: 0.055719178169965744\n",
      "Iteration 8422, Loss: 0.055808864533901215\n",
      "Iteration 8423, Loss: 0.055794715881347656\n",
      "Iteration 8424, Loss: 0.05568615719676018\n",
      "Iteration 8425, Loss: 0.055769287049770355\n",
      "Iteration 8426, Loss: 0.055831752717494965\n",
      "Iteration 8427, Loss: 0.05572032928466797\n",
      "Iteration 8428, Loss: 0.05573137849569321\n",
      "Iteration 8429, Loss: 0.055817484855651855\n",
      "Iteration 8430, Loss: 0.05579876899719238\n",
      "Iteration 8431, Loss: 0.055686235427856445\n",
      "Iteration 8432, Loss: 0.05577302351593971\n",
      "Iteration 8433, Loss: 0.055838942527770996\n",
      "Iteration 8434, Loss: 0.05573026463389397\n",
      "Iteration 8435, Loss: 0.055722396820783615\n",
      "Iteration 8436, Loss: 0.05580715462565422\n",
      "Iteration 8437, Loss: 0.05578760430216789\n",
      "Iteration 8438, Loss: 0.055674873292446136\n",
      "Iteration 8439, Loss: 0.05578812211751938\n",
      "Iteration 8440, Loss: 0.05585380643606186\n",
      "Iteration 8441, Loss: 0.05574389547109604\n",
      "Iteration 8442, Loss: 0.05571325868368149\n",
      "Iteration 8443, Loss: 0.0557989701628685\n",
      "Iteration 8444, Loss: 0.05578092858195305\n",
      "Iteration 8445, Loss: 0.05566974729299545\n",
      "Iteration 8446, Loss: 0.055793646723032\n",
      "Iteration 8447, Loss: 0.055857621133327484\n",
      "Iteration 8448, Loss: 0.055745482444763184\n",
      "Iteration 8449, Loss: 0.05571349710226059\n",
      "Iteration 8450, Loss: 0.055800676345825195\n",
      "Iteration 8451, Loss: 0.0557841882109642\n",
      "Iteration 8452, Loss: 0.05567455291748047\n",
      "Iteration 8453, Loss: 0.05578557774424553\n",
      "Iteration 8454, Loss: 0.055848244577646255\n",
      "Iteration 8455, Loss: 0.055734794586896896\n",
      "Iteration 8456, Loss: 0.05572235584259033\n",
      "Iteration 8457, Loss: 0.05581025406718254\n",
      "Iteration 8458, Loss: 0.055794596672058105\n",
      "Iteration 8459, Loss: 0.05568544194102287\n",
      "Iteration 8460, Loss: 0.055770277976989746\n",
      "Iteration 8461, Loss: 0.05583266541361809\n",
      "Iteration 8462, Loss: 0.05571866035461426\n",
      "Iteration 8463, Loss: 0.055734794586896896\n",
      "Iteration 8464, Loss: 0.05582293123006821\n",
      "Iteration 8465, Loss: 0.055807314813137054\n",
      "Iteration 8466, Loss: 0.05569867417216301\n",
      "Iteration 8467, Loss: 0.05575263500213623\n",
      "Iteration 8468, Loss: 0.05581466481089592\n",
      "Iteration 8469, Loss: 0.05570010468363762\n",
      "Iteration 8470, Loss: 0.05574909970164299\n",
      "Iteration 8471, Loss: 0.05583755299448967\n",
      "Iteration 8472, Loss: 0.05582225322723389\n",
      "Iteration 8473, Loss: 0.05571425333619118\n",
      "Iteration 8474, Loss: 0.055731259286403656\n",
      "Iteration 8475, Loss: 0.05579284951090813\n",
      "Iteration 8476, Loss: 0.055678170174360275\n",
      "Iteration 8477, Loss: 0.05576590821146965\n",
      "Iteration 8478, Loss: 0.055854957550764084\n",
      "Iteration 8479, Loss: 0.05584041401743889\n",
      "Iteration 8480, Loss: 0.0557330846786499\n",
      "Iteration 8481, Loss: 0.055704716593027115\n",
      "Iteration 8482, Loss: 0.05576598644256592\n",
      "Iteration 8483, Loss: 0.05565182492136955\n",
      "Iteration 8484, Loss: 0.055784109979867935\n",
      "Iteration 8485, Loss: 0.05587192624807358\n",
      "Iteration 8486, Loss: 0.055856943130493164\n",
      "Iteration 8487, Loss: 0.05574945732951164\n",
      "Iteration 8488, Loss: 0.055682700127363205\n",
      "Iteration 8489, Loss: 0.05574365705251694\n",
      "Iteration 8490, Loss: 0.05562782660126686\n",
      "Iteration 8491, Loss: 0.055798016488552094\n",
      "Iteration 8492, Loss: 0.055882059037685394\n",
      "Iteration 8493, Loss: 0.055861711502075195\n",
      "Iteration 8494, Loss: 0.055748503655195236\n",
      "Iteration 8495, Loss: 0.05569223687052727\n",
      "Iteration 8496, Loss: 0.05576014891266823\n",
      "Iteration 8497, Loss: 0.05565337464213371\n",
      "Iteration 8498, Loss: 0.055775485932826996\n",
      "Iteration 8499, Loss: 0.055856626480817795\n",
      "Iteration 8500, Loss: 0.05583548918366432\n",
      "Iteration 8501, Loss: 0.055722836405038834\n",
      "Iteration 8502, Loss: 0.055725257843732834\n",
      "Iteration 8503, Loss: 0.055790822952985764\n",
      "Iteration 8504, Loss: 0.05567777529358864\n",
      "Iteration 8505, Loss: 0.055764079093933105\n",
      "Iteration 8506, Loss: 0.05585161969065666\n",
      "Iteration 8507, Loss: 0.055836282670497894\n",
      "Iteration 8508, Loss: 0.055728595703840256\n",
      "Iteration 8509, Loss: 0.05571126937866211\n",
      "Iteration 8510, Loss: 0.05577167123556137\n",
      "Iteration 8511, Loss: 0.05565381050109863\n",
      "Iteration 8512, Loss: 0.05578505992889404\n",
      "Iteration 8513, Loss: 0.055875420570373535\n",
      "Iteration 8514, Loss: 0.05586230754852295\n",
      "Iteration 8515, Loss: 0.05575653165578842\n",
      "Iteration 8516, Loss: 0.05567125603556633\n",
      "Iteration 8517, Loss: 0.05573137849569321\n",
      "Iteration 8518, Loss: 0.05561920255422592\n",
      "Iteration 8519, Loss: 0.055772505700588226\n",
      "Iteration 8520, Loss: 0.05582181736826897\n",
      "Iteration 8521, Loss: 0.055765509605407715\n",
      "Iteration 8522, Loss: 0.055638670921325684\n",
      "Iteration 8523, Loss: 0.055814746767282486\n",
      "Iteration 8524, Loss: 0.05584506317973137\n",
      "Iteration 8525, Loss: 0.05570153519511223\n",
      "Iteration 8526, Loss: 0.05576714128255844\n",
      "Iteration 8527, Loss: 0.05587240308523178\n",
      "Iteration 8528, Loss: 0.0558730773627758\n",
      "Iteration 8529, Loss: 0.05578005313873291\n",
      "Iteration 8530, Loss: 0.05562770739197731\n",
      "Iteration 8531, Loss: 0.05573705956339836\n",
      "Iteration 8532, Loss: 0.05569338798522949\n",
      "Iteration 8533, Loss: 0.05570058152079582\n",
      "Iteration 8534, Loss: 0.055737100541591644\n",
      "Iteration 8535, Loss: 0.05566596984863281\n",
      "Iteration 8536, Loss: 0.05575164407491684\n",
      "Iteration 8537, Loss: 0.055781327188014984\n",
      "Iteration 8538, Loss: 0.05566052719950676\n",
      "Iteration 8539, Loss: 0.05578120797872543\n",
      "Iteration 8540, Loss: 0.055859606713056564\n",
      "Iteration 8541, Loss: 0.05581701174378395\n",
      "Iteration 8542, Loss: 0.05567685887217522\n",
      "Iteration 8543, Loss: 0.055804572999477386\n",
      "Iteration 8544, Loss: 0.05589274689555168\n",
      "Iteration 8545, Loss: 0.055818043649196625\n",
      "Iteration 8546, Loss: 0.05563076585531235\n",
      "Iteration 8547, Loss: 0.055758122354745865\n",
      "Iteration 8548, Loss: 0.05576245114207268\n",
      "Iteration 8549, Loss: 0.05564495176076889\n",
      "Iteration 8550, Loss: 0.05580369755625725\n",
      "Iteration 8551, Loss: 0.05586620420217514\n",
      "Iteration 8552, Loss: 0.05578017607331276\n",
      "Iteration 8553, Loss: 0.05566295236349106\n",
      "Iteration 8554, Loss: 0.055748581886291504\n",
      "Iteration 8555, Loss: 0.055716197937726974\n",
      "Iteration 8556, Loss: 0.055669426918029785\n",
      "Iteration 8557, Loss: 0.055684447288513184\n",
      "Iteration 8558, Loss: 0.055652979761362076\n",
      "Iteration 8559, Loss: 0.05565297603607178\n",
      "Iteration 8560, Loss: 0.05569271370768547\n",
      "Iteration 8561, Loss: 0.05568266287446022\n",
      "Iteration 8562, Loss: 0.055667560547590256\n",
      "Iteration 8563, Loss: 0.055657509714365005\n",
      "Iteration 8564, Loss: 0.05570562928915024\n",
      "Iteration 8565, Loss: 0.05571329966187477\n",
      "Iteration 8566, Loss: 0.05564308166503906\n",
      "Iteration 8567, Loss: 0.055743101984262466\n",
      "Iteration 8568, Loss: 0.05571746826171875\n",
      "Iteration 8569, Loss: 0.05567920207977295\n",
      "Iteration 8570, Loss: 0.05571472644805908\n",
      "Iteration 8571, Loss: 0.055658143013715744\n",
      "Iteration 8572, Loss: 0.05574294179677963\n",
      "Iteration 8573, Loss: 0.055743101984262466\n",
      "Iteration 8574, Loss: 0.05564375966787338\n",
      "Iteration 8575, Loss: 0.055672090500593185\n",
      "Iteration 8576, Loss: 0.055627863854169846\n",
      "Iteration 8577, Loss: 0.0557173527777195\n",
      "Iteration 8578, Loss: 0.0556974820792675\n",
      "Iteration 8579, Loss: 0.05567002296447754\n",
      "Iteration 8580, Loss: 0.055681031197309494\n",
      "Iteration 8581, Loss: 0.055666010826826096\n",
      "Iteration 8582, Loss: 0.055652737617492676\n",
      "Iteration 8583, Loss: 0.05569096654653549\n",
      "Iteration 8584, Loss: 0.05566100403666496\n",
      "Iteration 8585, Loss: 0.05572069063782692\n",
      "Iteration 8586, Loss: 0.05574047565460205\n",
      "Iteration 8587, Loss: 0.05565166473388672\n",
      "Iteration 8588, Loss: 0.05577131360769272\n",
      "Iteration 8589, Loss: 0.05580095574259758\n",
      "Iteration 8590, Loss: 0.05567268654704094\n",
      "Iteration 8591, Loss: 0.055783867835998535\n",
      "Iteration 8592, Loss: 0.055875424295663834\n",
      "Iteration 8593, Loss: 0.05584407225251198\n",
      "Iteration 8594, Loss: 0.05571039766073227\n",
      "Iteration 8595, Loss: 0.0557633638381958\n",
      "Iteration 8596, Loss: 0.05585161969065666\n",
      "Iteration 8597, Loss: 0.05577421188354492\n",
      "Iteration 8598, Loss: 0.05566398426890373\n",
      "Iteration 8599, Loss: 0.05572652816772461\n",
      "Iteration 8600, Loss: 0.055675748735666275\n",
      "Iteration 8601, Loss: 0.05571480840444565\n",
      "Iteration 8602, Loss: 0.05572255700826645\n",
      "Iteration 8603, Loss: 0.055643558502197266\n",
      "Iteration 8604, Loss: 0.05565381050109863\n",
      "Iteration 8605, Loss: 0.05567976087331772\n",
      "Iteration 8606, Loss: 0.05563855171203613\n",
      "Iteration 8607, Loss: 0.05573960393667221\n",
      "Iteration 8608, Loss: 0.05575462430715561\n",
      "Iteration 8609, Loss: 0.05565425008535385\n",
      "Iteration 8610, Loss: 0.0557812862098217\n",
      "Iteration 8611, Loss: 0.05582861229777336\n",
      "Iteration 8612, Loss: 0.05572191998362541\n",
      "Iteration 8613, Loss: 0.0557248629629612\n",
      "Iteration 8614, Loss: 0.05580246448516846\n",
      "Iteration 8615, Loss: 0.05575593560934067\n",
      "Iteration 8616, Loss: 0.05563485994935036\n",
      "Iteration 8617, Loss: 0.05574580281972885\n",
      "Iteration 8618, Loss: 0.055704955011606216\n",
      "Iteration 8619, Loss: 0.055697761476039886\n",
      "Iteration 8620, Loss: 0.055740874260663986\n",
      "Iteration 8621, Loss: 0.055685680359601974\n",
      "Iteration 8622, Loss: 0.055706463754177094\n",
      "Iteration 8623, Loss: 0.05571039766073227\n",
      "Iteration 8624, Loss: 0.055663906037807465\n",
      "Iteration 8625, Loss: 0.05568099021911621\n",
      "Iteration 8626, Loss: 0.055633626878261566\n",
      "Iteration 8627, Loss: 0.055702053010463715\n",
      "Iteration 8628, Loss: 0.05565734952688217\n",
      "Iteration 8629, Loss: 0.05572203919291496\n",
      "Iteration 8630, Loss: 0.055740319192409515\n",
      "Iteration 8631, Loss: 0.055641770362854004\n",
      "Iteration 8632, Loss: 0.05579690262675285\n",
      "Iteration 8633, Loss: 0.055845342576503754\n",
      "Iteration 8634, Loss: 0.055740755051374435\n",
      "Iteration 8635, Loss: 0.055707577615976334\n",
      "Iteration 8636, Loss: 0.055786214768886566\n",
      "Iteration 8637, Loss: 0.055746279656887054\n",
      "Iteration 8638, Loss: 0.05563116446137428\n",
      "Iteration 8639, Loss: 0.055702608078718185\n",
      "Iteration 8640, Loss: 0.05562428757548332\n",
      "Iteration 8641, Loss: 0.05574822425842285\n",
      "Iteration 8642, Loss: 0.0557810477912426\n",
      "Iteration 8643, Loss: 0.05571214482188225\n",
      "Iteration 8644, Loss: 0.055688146501779556\n",
      "Iteration 8645, Loss: 0.05571075528860092\n",
      "Iteration 8646, Loss: 0.055650316178798676\n",
      "Iteration 8647, Loss: 0.05565818399190903\n",
      "Iteration 8648, Loss: 0.055662039667367935\n",
      "Iteration 8649, Loss: 0.055635176599025726\n",
      "Iteration 8650, Loss: 0.055669985711574554\n",
      "Iteration 8651, Loss: 0.05562404915690422\n",
      "Iteration 8652, Loss: 0.05577119439840317\n",
      "Iteration 8653, Loss: 0.055777352303266525\n",
      "Iteration 8654, Loss: 0.055649284273386\n",
      "Iteration 8655, Loss: 0.05578899383544922\n",
      "Iteration 8656, Loss: 0.055855076760053635\n",
      "Iteration 8657, Loss: 0.055789750069379807\n",
      "Iteration 8658, Loss: 0.05564693734049797\n",
      "Iteration 8659, Loss: 0.05579730123281479\n",
      "Iteration 8660, Loss: 0.05583035945892334\n",
      "Iteration 8661, Loss: 0.0557047538459301\n",
      "Iteration 8662, Loss: 0.055755458772182465\n",
      "Iteration 8663, Loss: 0.05584848299622536\n",
      "Iteration 8664, Loss: 0.055826347321271896\n",
      "Iteration 8665, Loss: 0.05570876970887184\n",
      "Iteration 8666, Loss: 0.05574985593557358\n",
      "Iteration 8667, Loss: 0.05582352727651596\n",
      "Iteration 8668, Loss: 0.05573352426290512\n",
      "Iteration 8669, Loss: 0.05570816993713379\n",
      "Iteration 8670, Loss: 0.055779021233320236\n",
      "Iteration 8671, Loss: 0.055742740631103516\n",
      "Iteration 8672, Loss: 0.055642884224653244\n",
      "Iteration 8673, Loss: 0.055770836770534515\n",
      "Iteration 8674, Loss: 0.05576268956065178\n",
      "Iteration 8675, Loss: 0.055640339851379395\n",
      "Iteration 8676, Loss: 0.05569791793823242\n",
      "Iteration 8677, Loss: 0.05567260831594467\n",
      "Iteration 8678, Loss: 0.055693548172712326\n",
      "Iteration 8679, Loss: 0.055673521012067795\n",
      "Iteration 8680, Loss: 0.055701255798339844\n",
      "Iteration 8681, Loss: 0.05572478100657463\n",
      "Iteration 8682, Loss: 0.05564375966787338\n",
      "Iteration 8683, Loss: 0.05578935518860817\n",
      "Iteration 8684, Loss: 0.055820707231760025\n",
      "Iteration 8685, Loss: 0.05568476766347885\n",
      "Iteration 8686, Loss: 0.05577560514211655\n",
      "Iteration 8687, Loss: 0.055874112993478775\n",
      "Iteration 8688, Loss: 0.05585714429616928\n",
      "Iteration 8689, Loss: 0.05573948472738266\n",
      "Iteration 8690, Loss: 0.055711984634399414\n",
      "Iteration 8691, Loss: 0.05578859895467758\n",
      "Iteration 8692, Loss: 0.05569712445139885\n",
      "Iteration 8693, Loss: 0.05573924630880356\n",
      "Iteration 8694, Loss: 0.055812083184719086\n",
      "Iteration 8695, Loss: 0.05577826499938965\n",
      "Iteration 8696, Loss: 0.05566171929240227\n",
      "Iteration 8697, Loss: 0.05580699443817139\n",
      "Iteration 8698, Loss: 0.05586520954966545\n",
      "Iteration 8699, Loss: 0.05574536323547363\n",
      "Iteration 8700, Loss: 0.055720727890729904\n",
      "Iteration 8701, Loss: 0.05581260100007057\n",
      "Iteration 8702, Loss: 0.05580019950866699\n",
      "Iteration 8703, Loss: 0.05569656938314438\n",
      "Iteration 8704, Loss: 0.0557510070502758\n",
      "Iteration 8705, Loss: 0.055805765092372894\n",
      "Iteration 8706, Loss: 0.0556841716170311\n",
      "Iteration 8707, Loss: 0.0557657890021801\n",
      "Iteration 8708, Loss: 0.05585837364196777\n",
      "Iteration 8709, Loss: 0.05584697052836418\n",
      "Iteration 8710, Loss: 0.055743299424648285\n",
      "Iteration 8711, Loss: 0.055688224732875824\n",
      "Iteration 8712, Loss: 0.05574643984436989\n",
      "Iteration 8713, Loss: 0.05563831701874733\n",
      "Iteration 8714, Loss: 0.05577198788523674\n",
      "Iteration 8715, Loss: 0.05583687871694565\n",
      "Iteration 8716, Loss: 0.05579877272248268\n",
      "Iteration 8717, Loss: 0.0556696280837059\n",
      "Iteration 8718, Loss: 0.055814266204833984\n",
      "Iteration 8719, Loss: 0.055896759033203125\n",
      "Iteration 8720, Loss: 0.05580195039510727\n",
      "Iteration 8721, Loss: 0.055660128593444824\n",
      "Iteration 8722, Loss: 0.05573900789022446\n",
      "Iteration 8723, Loss: 0.055715322494506836\n",
      "Iteration 8724, Loss: 0.0556308850646019\n",
      "Iteration 8725, Loss: 0.055624328553676605\n",
      "Iteration 8726, Loss: 0.055710356682538986\n",
      "Iteration 8727, Loss: 0.055692993104457855\n",
      "Iteration 8728, Loss: 0.05566585063934326\n",
      "Iteration 8729, Loss: 0.05566307157278061\n",
      "Iteration 8730, Loss: 0.05569744110107422\n",
      "Iteration 8731, Loss: 0.05569958686828613\n",
      "Iteration 8732, Loss: 0.0556466206908226\n",
      "Iteration 8733, Loss: 0.05568786710500717\n",
      "Iteration 8734, Loss: 0.05564228817820549\n",
      "Iteration 8735, Loss: 0.05568377301096916\n",
      "Iteration 8736, Loss: 0.0556621178984642\n",
      "Iteration 8737, Loss: 0.055703483521938324\n",
      "Iteration 8738, Loss: 0.05568544194102287\n",
      "Iteration 8739, Loss: 0.055688899010419846\n",
      "Iteration 8740, Loss: 0.055709004402160645\n",
      "Iteration 8741, Loss: 0.055623531341552734\n",
      "Iteration 8742, Loss: 0.05581120774149895\n",
      "Iteration 8743, Loss: 0.05584017559885979\n",
      "Iteration 8744, Loss: 0.055704474449157715\n",
      "Iteration 8745, Loss: 0.05575963109731674\n",
      "Iteration 8746, Loss: 0.055858731269836426\n",
      "Iteration 8747, Loss: 0.055848006159067154\n",
      "Iteration 8748, Loss: 0.05573916435241699\n",
      "Iteration 8749, Loss: 0.055701058357954025\n",
      "Iteration 8750, Loss: 0.055769048631191254\n",
      "Iteration 8751, Loss: 0.0556693896651268\n",
      "Iteration 8752, Loss: 0.05576400086283684\n",
      "Iteration 8753, Loss: 0.055842362344264984\n",
      "Iteration 8754, Loss: 0.055816296488046646\n",
      "Iteration 8755, Loss: 0.055699948221445084\n",
      "Iteration 8756, Loss: 0.05576058477163315\n",
      "Iteration 8757, Loss: 0.05583056062459946\n",
      "Iteration 8758, Loss: 0.05572625249624252\n",
      "Iteration 8759, Loss: 0.05572378635406494\n",
      "Iteration 8760, Loss: 0.05580592155456543\n",
      "Iteration 8761, Loss: 0.05578470602631569\n",
      "Iteration 8762, Loss: 0.055673837661743164\n",
      "Iteration 8763, Loss: 0.05578843876719475\n",
      "Iteration 8764, Loss: 0.05585070699453354\n",
      "Iteration 8765, Loss: 0.05573571100831032\n",
      "Iteration 8766, Loss: 0.055723074823617935\n",
      "Iteration 8767, Loss: 0.05581216141581535\n",
      "Iteration 8768, Loss: 0.055798135697841644\n",
      "Iteration 8769, Loss: 0.0556916818022728\n",
      "Iteration 8770, Loss: 0.05575951188802719\n",
      "Iteration 8771, Loss: 0.055818360298871994\n",
      "Iteration 8772, Loss: 0.05569970980286598\n",
      "Iteration 8773, Loss: 0.055752359330654144\n",
      "Iteration 8774, Loss: 0.05584375187754631\n",
      "Iteration 8775, Loss: 0.055831752717494965\n",
      "Iteration 8776, Loss: 0.055727045983076096\n",
      "Iteration 8777, Loss: 0.05570987984538078\n",
      "Iteration 8778, Loss: 0.05576769635081291\n",
      "Iteration 8779, Loss: 0.05564828962087631\n",
      "Iteration 8780, Loss: 0.05579054728150368\n",
      "Iteration 8781, Loss: 0.05588245764374733\n",
      "Iteration 8782, Loss: 0.05587073415517807\n",
      "Iteration 8783, Loss: 0.055766068398952484\n",
      "Iteration 8784, Loss: 0.055657029151916504\n",
      "Iteration 8785, Loss: 0.055716875940561295\n",
      "Iteration 8786, Loss: 0.055624883621931076\n",
      "Iteration 8787, Loss: 0.055684804916381836\n",
      "Iteration 8788, Loss: 0.05563942715525627\n",
      "Iteration 8789, Loss: 0.05575239658355713\n",
      "Iteration 8790, Loss: 0.05576002970337868\n",
      "Iteration 8791, Loss: 0.05564109608530998\n",
      "Iteration 8792, Loss: 0.05577798932790756\n",
      "Iteration 8793, Loss: 0.05581708997488022\n",
      "Iteration 8794, Loss: 0.05571850389242172\n",
      "Iteration 8795, Loss: 0.055719614028930664\n",
      "Iteration 8796, Loss: 0.05578728765249252\n",
      "Iteration 8797, Loss: 0.05572080612182617\n",
      "Iteration 8798, Loss: 0.055688343942165375\n",
      "Iteration 8799, Loss: 0.055729907006025314\n",
      "Iteration 8800, Loss: 0.05563577264547348\n",
      "Iteration 8801, Loss: 0.0557987317442894\n",
      "Iteration 8802, Loss: 0.055855993181467056\n",
      "Iteration 8803, Loss: 0.05577743053436279\n",
      "Iteration 8804, Loss: 0.05566596984863281\n",
      "Iteration 8805, Loss: 0.055763326585292816\n",
      "Iteration 8806, Loss: 0.05575307458639145\n",
      "Iteration 8807, Loss: 0.05563628673553467\n",
      "Iteration 8808, Loss: 0.055705033242702484\n",
      "Iteration 8809, Loss: 0.055699944496154785\n",
      "Iteration 8810, Loss: 0.05563819780945778\n",
      "Iteration 8811, Loss: 0.055710792541503906\n",
      "Iteration 8812, Loss: 0.05566922947764397\n",
      "Iteration 8813, Loss: 0.05571150779724121\n",
      "Iteration 8814, Loss: 0.0557328462600708\n",
      "Iteration 8815, Loss: 0.05563732236623764\n",
      "Iteration 8816, Loss: 0.05580794811248779\n",
      "Iteration 8817, Loss: 0.055859290063381195\n",
      "Iteration 8818, Loss: 0.0557558573782444\n",
      "Iteration 8819, Loss: 0.05569728463888168\n",
      "Iteration 8820, Loss: 0.05577830597758293\n",
      "Iteration 8821, Loss: 0.055742304772138596\n",
      "Iteration 8822, Loss: 0.055633705109357834\n",
      "Iteration 8823, Loss: 0.05564228817820549\n",
      "Iteration 8824, Loss: 0.05569637194275856\n",
      "Iteration 8825, Loss: 0.05567328259348869\n",
      "Iteration 8826, Loss: 0.05569573491811752\n",
      "Iteration 8827, Loss: 0.05569728463888168\n",
      "Iteration 8828, Loss: 0.05565520375967026\n",
      "Iteration 8829, Loss: 0.055649757385253906\n",
      "Iteration 8830, Loss: 0.055707138031721115\n",
      "Iteration 8831, Loss: 0.05569791793823242\n",
      "Iteration 8832, Loss: 0.0556618794798851\n",
      "Iteration 8833, Loss: 0.05567034333944321\n",
      "Iteration 8834, Loss: 0.05567626282572746\n",
      "Iteration 8835, Loss: 0.05566021054983139\n",
      "Iteration 8836, Loss: 0.055691562592983246\n",
      "Iteration 8837, Loss: 0.05567359924316406\n",
      "Iteration 8838, Loss: 0.05569815635681152\n",
      "Iteration 8839, Loss: 0.05570630356669426\n",
      "Iteration 8840, Loss: 0.0556415356695652\n",
      "Iteration 8841, Loss: 0.0556974820792675\n",
      "Iteration 8842, Loss: 0.055633705109357834\n",
      "Iteration 8843, Loss: 0.055740080773830414\n",
      "Iteration 8844, Loss: 0.05577469244599342\n",
      "Iteration 8845, Loss: 0.05571103096008301\n",
      "Iteration 8846, Loss: 0.05568186566233635\n",
      "Iteration 8847, Loss: 0.055694304406642914\n",
      "Iteration 8848, Loss: 0.0556693896651268\n",
      "Iteration 8849, Loss: 0.05568230152130127\n",
      "Iteration 8850, Loss: 0.05562834069132805\n",
      "Iteration 8851, Loss: 0.05566227808594704\n",
      "Iteration 8852, Loss: 0.05563092231750488\n",
      "Iteration 8853, Loss: 0.05569954961538315\n",
      "Iteration 8854, Loss: 0.05563211441040039\n",
      "Iteration 8855, Loss: 0.05575653165578842\n",
      "Iteration 8856, Loss: 0.05580151081085205\n",
      "Iteration 8857, Loss: 0.055743418633937836\n",
      "Iteration 8858, Loss: 0.055643919855356216\n",
      "Iteration 8859, Loss: 0.055722158402204514\n",
      "Iteration 8860, Loss: 0.055659059435129166\n",
      "Iteration 8861, Loss: 0.0557357482612133\n",
      "Iteration 8862, Loss: 0.0557815246284008\n",
      "Iteration 8863, Loss: 0.05572597309947014\n",
      "Iteration 8864, Loss: 0.05565401166677475\n",
      "Iteration 8865, Loss: 0.05566143989562988\n",
      "Iteration 8866, Loss: 0.055696725845336914\n",
      "Iteration 8867, Loss: 0.05571115016937256\n",
      "Iteration 8868, Loss: 0.055632393807172775\n",
      "Iteration 8869, Loss: 0.055799685418605804\n",
      "Iteration 8870, Loss: 0.05582007020711899\n",
      "Iteration 8871, Loss: 0.05566537752747536\n",
      "Iteration 8872, Loss: 0.05580099672079086\n",
      "Iteration 8873, Loss: 0.05591364949941635\n",
      "Iteration 8874, Loss: 0.055920444428920746\n",
      "Iteration 8875, Loss: 0.055831871926784515\n",
      "Iteration 8876, Loss: 0.05565905570983887\n",
      "Iteration 8877, Loss: 0.05588531494140625\n",
      "Iteration 8878, Loss: 0.05601660534739494\n",
      "Iteration 8879, Loss: 0.05596085637807846\n",
      "Iteration 8880, Loss: 0.055737536400556564\n",
      "Iteration 8881, Loss: 0.05579352378845215\n",
      "Iteration 8882, Loss: 0.05594750493764877\n",
      "Iteration 8883, Loss: 0.05599157139658928\n",
      "Iteration 8884, Loss: 0.055936019867658615\n",
      "Iteration 8885, Loss: 0.05579201504588127\n",
      "Iteration 8886, Loss: 0.05566958710551262\n",
      "Iteration 8887, Loss: 0.05576976388692856\n",
      "Iteration 8888, Loss: 0.055689096450805664\n",
      "Iteration 8889, Loss: 0.05573428049683571\n",
      "Iteration 8890, Loss: 0.05580242723226547\n",
      "Iteration 8891, Loss: 0.05576956644654274\n",
      "Iteration 8892, Loss: 0.055646222084760666\n",
      "Iteration 8893, Loss: 0.055841367691755295\n",
      "Iteration 8894, Loss: 0.055918335914611816\n",
      "Iteration 8895, Loss: 0.055815380066633224\n",
      "Iteration 8896, Loss: 0.055657826364040375\n",
      "Iteration 8897, Loss: 0.05574234575033188\n",
      "Iteration 8898, Loss: 0.05572617053985596\n",
      "Iteration 8899, Loss: 0.05561792850494385\n",
      "Iteration 8900, Loss: 0.055851858109235764\n",
      "Iteration 8901, Loss: 0.05590975657105446\n",
      "Iteration 8902, Loss: 0.05579972267150879\n",
      "Iteration 8903, Loss: 0.055674515664577484\n",
      "Iteration 8904, Loss: 0.05576702207326889\n",
      "Iteration 8905, Loss: 0.055758874863386154\n",
      "Iteration 8906, Loss: 0.05564705654978752\n",
      "Iteration 8907, Loss: 0.05582122132182121\n",
      "Iteration 8908, Loss: 0.05589107796549797\n",
      "Iteration 8909, Loss: 0.0558016337454319\n",
      "Iteration 8910, Loss: 0.05566263198852539\n",
      "Iteration 8911, Loss: 0.05576241388916969\n",
      "Iteration 8912, Loss: 0.05576483532786369\n",
      "Iteration 8913, Loss: 0.05564558506011963\n",
      "Iteration 8914, Loss: 0.05582074448466301\n",
      "Iteration 8915, Loss: 0.055901847779750824\n",
      "Iteration 8916, Loss: 0.05583930015563965\n",
      "Iteration 8917, Loss: 0.05567654222249985\n",
      "Iteration 8918, Loss: 0.055811647325754166\n",
      "Iteration 8919, Loss: 0.05590752884745598\n",
      "Iteration 8920, Loss: 0.05585503578186035\n",
      "Iteration 8921, Loss: 0.05567371845245361\n",
      "Iteration 8922, Loss: 0.055833540856838226\n",
      "Iteration 8923, Loss: 0.055959902703762054\n",
      "Iteration 8924, Loss: 0.05594118684530258\n",
      "Iteration 8925, Loss: 0.05579666420817375\n",
      "Iteration 8926, Loss: 0.055693112313747406\n",
      "Iteration 8927, Loss: 0.05581232160329819\n",
      "Iteration 8928, Loss: 0.05579940602183342\n",
      "Iteration 8929, Loss: 0.055645983666181564\n",
      "Iteration 8930, Loss: 0.05583703890442848\n",
      "Iteration 8931, Loss: 0.055947981774806976\n",
      "Iteration 8932, Loss: 0.05592819303274155\n",
      "Iteration 8933, Loss: 0.055799007415771484\n",
      "Iteration 8934, Loss: 0.05568937584757805\n",
      "Iteration 8935, Loss: 0.05581287667155266\n",
      "Iteration 8936, Loss: 0.0558144673705101\n",
      "Iteration 8937, Loss: 0.05566966533660889\n",
      "Iteration 8938, Loss: 0.05579432100057602\n",
      "Iteration 8939, Loss: 0.05589143559336662\n",
      "Iteration 8940, Loss: 0.0558798722922802\n",
      "Iteration 8941, Loss: 0.05577544495463371\n",
      "Iteration 8942, Loss: 0.055675506591796875\n",
      "Iteration 8943, Loss: 0.05578367039561272\n",
      "Iteration 8944, Loss: 0.055758874863386154\n",
      "Iteration 8945, Loss: 0.055671852082014084\n",
      "Iteration 8946, Loss: 0.055725179612636566\n",
      "Iteration 8947, Loss: 0.05572231858968735\n",
      "Iteration 8948, Loss: 0.05565027520060539\n",
      "Iteration 8949, Loss: 0.055758677423000336\n",
      "Iteration 8950, Loss: 0.05577210709452629\n",
      "Iteration 8951, Loss: 0.05567101761698723\n",
      "Iteration 8952, Loss: 0.055755339562892914\n",
      "Iteration 8953, Loss: 0.055801670998334885\n",
      "Iteration 8954, Loss: 0.05571349710226059\n",
      "Iteration 8955, Loss: 0.05571858212351799\n",
      "Iteration 8956, Loss: 0.05577715486288071\n",
      "Iteration 8957, Loss: 0.05570133775472641\n",
      "Iteration 8958, Loss: 0.05571568012237549\n",
      "Iteration 8959, Loss: 0.055761974304914474\n",
      "Iteration 8960, Loss: 0.05567216873168945\n",
      "Iteration 8961, Loss: 0.055759232491254807\n",
      "Iteration 8962, Loss: 0.055816493928432465\n",
      "Iteration 8963, Loss: 0.055738650262355804\n",
      "Iteration 8964, Loss: 0.0556846484541893\n",
      "Iteration 8965, Loss: 0.055742423981428146\n",
      "Iteration 8966, Loss: 0.055667243897914886\n",
      "Iteration 8967, Loss: 0.05575593560934067\n",
      "Iteration 8968, Loss: 0.05580707639455795\n",
      "Iteration 8969, Loss: 0.05573391914367676\n",
      "Iteration 8970, Loss: 0.055685244500637054\n",
      "Iteration 8971, Loss: 0.05574015900492668\n",
      "Iteration 8972, Loss: 0.05566386505961418\n",
      "Iteration 8973, Loss: 0.055757127702236176\n",
      "Iteration 8974, Loss: 0.05581212043762207\n",
      "Iteration 8975, Loss: 0.05575549975037575\n",
      "Iteration 8976, Loss: 0.055661123245954514\n",
      "Iteration 8977, Loss: 0.05575982853770256\n",
      "Iteration 8978, Loss: 0.05573992058634758\n",
      "Iteration 8979, Loss: 0.055667560547590256\n",
      "Iteration 8980, Loss: 0.0557049922645092\n",
      "Iteration 8981, Loss: 0.05567304417490959\n",
      "Iteration 8982, Loss: 0.05569935217499733\n",
      "Iteration 8983, Loss: 0.05567657947540283\n",
      "Iteration 8984, Loss: 0.05570137873291969\n",
      "Iteration 8985, Loss: 0.05572855845093727\n",
      "Iteration 8986, Loss: 0.05566283315420151\n",
      "Iteration 8987, Loss: 0.055751483887434006\n",
      "Iteration 8988, Loss: 0.055766306817531586\n",
      "Iteration 8989, Loss: 0.05563124269247055\n",
      "Iteration 8990, Loss: 0.05575970932841301\n",
      "Iteration 8991, Loss: 0.0557936429977417\n",
      "Iteration 8992, Loss: 0.05571584030985832\n",
      "Iteration 8993, Loss: 0.055698394775390625\n",
      "Iteration 8994, Loss: 0.05573960393667221\n",
      "Iteration 8995, Loss: 0.0556265152990818\n",
      "Iteration 8996, Loss: 0.05580969899892807\n",
      "Iteration 8997, Loss: 0.055893462151288986\n",
      "Iteration 8998, Loss: 0.05586695671081543\n",
      "Iteration 8999, Loss: 0.05574699491262436\n",
      "Iteration 9000, Loss: 0.05571305751800537\n",
      "Iteration 9001, Loss: 0.05579753965139389\n",
      "Iteration 9002, Loss: 0.0557253360748291\n",
      "Iteration 9003, Loss: 0.05570849031209946\n",
      "Iteration 9004, Loss: 0.05576857179403305\n",
      "Iteration 9005, Loss: 0.05573197454214096\n",
      "Iteration 9006, Loss: 0.05565313622355461\n",
      "Iteration 9007, Loss: 0.05574476718902588\n",
      "Iteration 9008, Loss: 0.05571437254548073\n",
      "Iteration 9009, Loss: 0.055686235427856445\n",
      "Iteration 9010, Loss: 0.055724505335092545\n",
      "Iteration 9011, Loss: 0.05567348003387451\n",
      "Iteration 9012, Loss: 0.05572390928864479\n",
      "Iteration 9013, Loss: 0.05572466179728508\n",
      "Iteration 9014, Loss: 0.05565762519836426\n",
      "Iteration 9015, Loss: 0.055686794221401215\n",
      "Iteration 9016, Loss: 0.05563465878367424\n",
      "Iteration 9017, Loss: 0.05575041100382805\n",
      "Iteration 9018, Loss: 0.055755615234375\n",
      "Iteration 9019, Loss: 0.055656157433986664\n",
      "Iteration 9020, Loss: 0.055761538445949554\n",
      "Iteration 9021, Loss: 0.055789947509765625\n",
      "Iteration 9022, Loss: 0.05567328259348869\n",
      "Iteration 9023, Loss: 0.05578017607331276\n",
      "Iteration 9024, Loss: 0.055862586945295334\n",
      "Iteration 9025, Loss: 0.055818598717451096\n",
      "Iteration 9026, Loss: 0.05568131059408188\n",
      "Iteration 9027, Loss: 0.055795829743146896\n",
      "Iteration 9028, Loss: 0.05587772652506828\n",
      "Iteration 9029, Loss: 0.05580159276723862\n",
      "Iteration 9030, Loss: 0.055638156831264496\n",
      "Iteration 9031, Loss: 0.055707335472106934\n",
      "Iteration 9032, Loss: 0.05565480515360832\n",
      "Iteration 9033, Loss: 0.05574337765574455\n",
      "Iteration 9034, Loss: 0.0557611808180809\n",
      "Iteration 9035, Loss: 0.05564558506011963\n",
      "Iteration 9036, Loss: 0.05578891560435295\n",
      "Iteration 9037, Loss: 0.05584617704153061\n",
      "Iteration 9038, Loss: 0.05576678365468979\n",
      "Iteration 9039, Loss: 0.055665016174316406\n",
      "Iteration 9040, Loss: 0.05574552342295647\n",
      "Iteration 9041, Loss: 0.05570201203227043\n",
      "Iteration 9042, Loss: 0.05570058152079582\n",
      "Iteration 9043, Loss: 0.05573181435465813\n",
      "Iteration 9044, Loss: 0.05566442012786865\n",
      "Iteration 9045, Loss: 0.05573829263448715\n",
      "Iteration 9046, Loss: 0.055749695748090744\n",
      "Iteration 9047, Loss: 0.05561908334493637\n",
      "Iteration 9048, Loss: 0.055649641901254654\n",
      "Iteration 9049, Loss: 0.055644791573286057\n",
      "Iteration 9050, Loss: 0.05565909668803215\n",
      "Iteration 9051, Loss: 0.05562889948487282\n",
      "Iteration 9052, Loss: 0.05575255677103996\n",
      "Iteration 9053, Loss: 0.055726926773786545\n",
      "Iteration 9054, Loss: 0.055671654641628265\n",
      "Iteration 9055, Loss: 0.055707816034555435\n",
      "Iteration 9056, Loss: 0.055646222084760666\n",
      "Iteration 9057, Loss: 0.05576789751648903\n",
      "Iteration 9058, Loss: 0.055776916444301605\n",
      "Iteration 9059, Loss: 0.055617254227399826\n",
      "Iteration 9060, Loss: 0.05578768253326416\n",
      "Iteration 9061, Loss: 0.05584033578634262\n",
      "Iteration 9062, Loss: 0.05577973648905754\n",
      "Iteration 9063, Loss: 0.05565039440989494\n",
      "Iteration 9064, Loss: 0.05581049248576164\n",
      "Iteration 9065, Loss: 0.055847883224487305\n",
      "Iteration 9066, Loss: 0.055711548775434494\n",
      "Iteration 9067, Loss: 0.05575820058584213\n",
      "Iteration 9068, Loss: 0.055859606713056564\n",
      "Iteration 9069, Loss: 0.05585380643606186\n",
      "Iteration 9070, Loss: 0.055754028260707855\n",
      "Iteration 9071, Loss: 0.05567701905965805\n",
      "Iteration 9072, Loss: 0.05574623867869377\n",
      "Iteration 9073, Loss: 0.05566636845469475\n",
      "Iteration 9074, Loss: 0.05574071407318115\n",
      "Iteration 9075, Loss: 0.055794477462768555\n",
      "Iteration 9076, Loss: 0.05575009435415268\n",
      "Iteration 9077, Loss: 0.05562051385641098\n",
      "Iteration 9078, Loss: 0.05583151429891586\n",
      "Iteration 9079, Loss: 0.055878400802612305\n",
      "Iteration 9080, Loss: 0.055769406259059906\n",
      "Iteration 9081, Loss: 0.0556948184967041\n",
      "Iteration 9082, Loss: 0.05578172206878662\n",
      "Iteration 9083, Loss: 0.05575939267873764\n",
      "Iteration 9084, Loss: 0.055627863854169846\n",
      "Iteration 9085, Loss: 0.05585801601409912\n",
      "Iteration 9086, Loss: 0.05594814196228981\n",
      "Iteration 9087, Loss: 0.05588396638631821\n",
      "Iteration 9088, Loss: 0.055699706077575684\n",
      "Iteration 9089, Loss: 0.055806320160627365\n",
      "Iteration 9090, Loss: 0.05593065544962883\n",
      "Iteration 9091, Loss: 0.05591996759176254\n",
      "Iteration 9092, Loss: 0.055786967277526855\n",
      "Iteration 9093, Loss: 0.05568353459239006\n",
      "Iteration 9094, Loss: 0.05578943341970444\n",
      "Iteration 9095, Loss: 0.055752597749233246\n",
      "Iteration 9096, Loss: 0.05564729496836662\n",
      "Iteration 9097, Loss: 0.05567598715424538\n",
      "Iteration 9098, Loss: 0.05564066022634506\n",
      "Iteration 9099, Loss: 0.05567225068807602\n",
      "Iteration 9100, Loss: 0.05564677715301514\n",
      "Iteration 9101, Loss: 0.05564793199300766\n",
      "Iteration 9102, Loss: 0.05567145720124245\n",
      "Iteration 9103, Loss: 0.0556357316672802\n",
      "Iteration 9104, Loss: 0.055681150406599045\n",
      "Iteration 9105, Loss: 0.05561960116028786\n",
      "Iteration 9106, Loss: 0.0557839497923851\n",
      "Iteration 9107, Loss: 0.055827897042036057\n",
      "Iteration 9108, Loss: 0.05575629323720932\n",
      "Iteration 9109, Loss: 0.05566708371043205\n",
      "Iteration 9110, Loss: 0.05575108528137207\n",
      "Iteration 9111, Loss: 0.05571405217051506\n",
      "Iteration 9112, Loss: 0.0556926354765892\n",
      "Iteration 9113, Loss: 0.055729273706674576\n",
      "Iteration 9114, Loss: 0.05568007752299309\n",
      "Iteration 9115, Loss: 0.055708568543195724\n",
      "Iteration 9116, Loss: 0.05570594593882561\n",
      "Iteration 9117, Loss: 0.05567316338419914\n",
      "Iteration 9118, Loss: 0.05569251626729965\n",
      "Iteration 9119, Loss: 0.05564447492361069\n",
      "Iteration 9120, Loss: 0.055731773376464844\n",
      "Iteration 9121, Loss: 0.05570022389292717\n",
      "Iteration 9122, Loss: 0.05569374933838844\n",
      "Iteration 9123, Loss: 0.05573097988963127\n",
      "Iteration 9124, Loss: 0.05566990748047829\n",
      "Iteration 9125, Loss: 0.05573682114481926\n",
      "Iteration 9126, Loss: 0.055747829377651215\n",
      "Iteration 9127, Loss: 0.055634260177612305\n",
      "Iteration 9128, Loss: 0.05568913742899895\n",
      "Iteration 9129, Loss: 0.05564197152853012\n",
      "Iteration 9130, Loss: 0.055755577981472015\n",
      "Iteration 9131, Loss: 0.05577131360769272\n",
      "Iteration 9132, Loss: 0.05566021054983139\n",
      "Iteration 9133, Loss: 0.055774372071027756\n",
      "Iteration 9134, Loss: 0.05583147332072258\n",
      "Iteration 9135, Loss: 0.05575057119131088\n",
      "Iteration 9136, Loss: 0.05567514896392822\n",
      "Iteration 9137, Loss: 0.05573562905192375\n",
      "Iteration 9138, Loss: 0.05566072836518288\n",
      "Iteration 9139, Loss: 0.05575982853770256\n",
      "Iteration 9140, Loss: 0.05580834671854973\n",
      "Iteration 9141, Loss: 0.055728357285261154\n",
      "Iteration 9142, Loss: 0.05569469928741455\n",
      "Iteration 9143, Loss: 0.05575017258524895\n",
      "Iteration 9144, Loss: 0.055669549852609634\n",
      "Iteration 9145, Loss: 0.05575792118906975\n",
      "Iteration 9146, Loss: 0.05581462383270264\n",
      "Iteration 9147, Loss: 0.055749379098415375\n",
      "Iteration 9148, Loss: 0.055668752640485764\n",
      "Iteration 9149, Loss: 0.05574127286672592\n",
      "Iteration 9150, Loss: 0.055692195892333984\n",
      "Iteration 9151, Loss: 0.05571647733449936\n",
      "Iteration 9152, Loss: 0.055757325142621994\n",
      "Iteration 9153, Loss: 0.055701255798339844\n",
      "Iteration 9154, Loss: 0.055695176124572754\n",
      "Iteration 9155, Loss: 0.05571238324046135\n",
      "Iteration 9156, Loss: 0.055658698081970215\n",
      "Iteration 9157, Loss: 0.055677615106105804\n",
      "Iteration 9158, Loss: 0.055655162781476974\n",
      "Iteration 9159, Loss: 0.05569958686828613\n",
      "Iteration 9160, Loss: 0.05566128343343735\n",
      "Iteration 9161, Loss: 0.05571472644805908\n",
      "Iteration 9162, Loss: 0.05573924630880356\n",
      "Iteration 9163, Loss: 0.0556592158973217\n",
      "Iteration 9164, Loss: 0.05576952546834946\n",
      "Iteration 9165, Loss: 0.05580481141805649\n",
      "Iteration 9166, Loss: 0.05568031594157219\n",
      "Iteration 9167, Loss: 0.05577067658305168\n",
      "Iteration 9168, Loss: 0.055859606713056564\n",
      "Iteration 9169, Loss: 0.05583449453115463\n",
      "Iteration 9170, Loss: 0.05571099370718002\n",
      "Iteration 9171, Loss: 0.05575275421142578\n",
      "Iteration 9172, Loss: 0.055832426995038986\n",
      "Iteration 9173, Loss: 0.05574667453765869\n",
      "Iteration 9174, Loss: 0.055693309754133224\n",
      "Iteration 9175, Loss: 0.055761538445949554\n",
      "Iteration 9176, Loss: 0.05572128668427467\n",
      "Iteration 9177, Loss: 0.05565222352743149\n",
      "Iteration 9178, Loss: 0.055687468498945236\n",
      "Iteration 9179, Loss: 0.05565408989787102\n",
      "Iteration 9180, Loss: 0.055660687386989594\n",
      "Iteration 9181, Loss: 0.055652499198913574\n",
      "Iteration 9182, Loss: 0.05567129701375961\n",
      "Iteration 9183, Loss: 0.0556485652923584\n",
      "Iteration 9184, Loss: 0.05569728463888168\n",
      "Iteration 9185, Loss: 0.05565425008535385\n",
      "Iteration 9186, Loss: 0.05573714151978493\n",
      "Iteration 9187, Loss: 0.05577727407217026\n",
      "Iteration 9188, Loss: 0.05571504682302475\n",
      "Iteration 9189, Loss: 0.055681705474853516\n",
      "Iteration 9190, Loss: 0.05570892617106438\n",
      "Iteration 9191, Loss: 0.05564967915415764\n",
      "Iteration 9192, Loss: 0.05566779896616936\n",
      "Iteration 9193, Loss: 0.05564383789896965\n",
      "Iteration 9194, Loss: 0.05570534989237785\n",
      "Iteration 9195, Loss: 0.055672090500593185\n",
      "Iteration 9196, Loss: 0.05570447817444801\n",
      "Iteration 9197, Loss: 0.055720847100019455\n",
      "Iteration 9198, Loss: 0.05562106892466545\n",
      "Iteration 9199, Loss: 0.05582205578684807\n",
      "Iteration 9200, Loss: 0.0558604821562767\n",
      "Iteration 9201, Loss: 0.05573757737874985\n",
      "Iteration 9202, Loss: 0.05572621151804924\n",
      "Iteration 9203, Loss: 0.05581756681203842\n",
      "Iteration 9204, Loss: 0.05579209327697754\n",
      "Iteration 9205, Loss: 0.055661045014858246\n",
      "Iteration 9206, Loss: 0.055821143090724945\n",
      "Iteration 9207, Loss: 0.05590478703379631\n",
      "Iteration 9208, Loss: 0.05581867694854736\n",
      "Iteration 9209, Loss: 0.05564892664551735\n",
      "Iteration 9210, Loss: 0.05576543137431145\n",
      "Iteration 9211, Loss: 0.05577993392944336\n",
      "Iteration 9212, Loss: 0.055670302361249924\n",
      "Iteration 9213, Loss: 0.05578327551484108\n",
      "Iteration 9214, Loss: 0.05585400387644768\n",
      "Iteration 9215, Loss: 0.05577806755900383\n",
      "Iteration 9216, Loss: 0.05566442385315895\n",
      "Iteration 9217, Loss: 0.05575549602508545\n",
      "Iteration 9218, Loss: 0.055739324539899826\n",
      "Iteration 9219, Loss: 0.055642567574977875\n",
      "Iteration 9220, Loss: 0.05567380040884018\n",
      "Iteration 9221, Loss: 0.05564582347869873\n",
      "Iteration 9222, Loss: 0.05570598691701889\n",
      "Iteration 9223, Loss: 0.05565563961863518\n",
      "Iteration 9224, Loss: 0.055734992027282715\n",
      "Iteration 9225, Loss: 0.05577965825796127\n",
      "Iteration 9226, Loss: 0.055723629891872406\n",
      "Iteration 9227, Loss: 0.05565690994262695\n",
      "Iteration 9228, Loss: 0.05566426366567612\n",
      "Iteration 9229, Loss: 0.055694978684186935\n",
      "Iteration 9230, Loss: 0.055709920823574066\n",
      "Iteration 9231, Loss: 0.055631957948207855\n",
      "Iteration 9232, Loss: 0.055801670998334885\n",
      "Iteration 9233, Loss: 0.055823009461164474\n",
      "Iteration 9234, Loss: 0.05567026510834694\n",
      "Iteration 9235, Loss: 0.05579634755849838\n",
      "Iteration 9236, Loss: 0.05590740963816643\n",
      "Iteration 9237, Loss: 0.05591293424367905\n",
      "Iteration 9238, Loss: 0.055823564529418945\n",
      "Iteration 9239, Loss: 0.05564955994486809\n",
      "Iteration 9240, Loss: 0.0559004582464695\n",
      "Iteration 9241, Loss: 0.05603325739502907\n",
      "Iteration 9242, Loss: 0.055979013442993164\n",
      "Iteration 9243, Loss: 0.055757682770490646\n",
      "Iteration 9244, Loss: 0.05577806755900383\n",
      "Iteration 9245, Loss: 0.05593077465891838\n",
      "Iteration 9246, Loss: 0.055974047631025314\n",
      "Iteration 9247, Loss: 0.05591806024312973\n",
      "Iteration 9248, Loss: 0.0557732991874218\n",
      "Iteration 9249, Loss: 0.055696409195661545\n",
      "Iteration 9250, Loss: 0.05579721927642822\n",
      "Iteration 9251, Loss: 0.05571591854095459\n",
      "Iteration 9252, Loss: 0.05571603775024414\n",
      "Iteration 9253, Loss: 0.055785100907087326\n",
      "Iteration 9254, Loss: 0.055753350257873535\n",
      "Iteration 9255, Loss: 0.05563108250498772\n",
      "Iteration 9256, Loss: 0.055861830711364746\n",
      "Iteration 9257, Loss: 0.055937252938747406\n",
      "Iteration 9258, Loss: 0.05583222955465317\n",
      "Iteration 9259, Loss: 0.055647652596235275\n",
      "Iteration 9260, Loss: 0.055734992027282715\n",
      "Iteration 9261, Loss: 0.05572223663330078\n",
      "Iteration 9262, Loss: 0.055616579949855804\n",
      "Iteration 9263, Loss: 0.05584848299622536\n",
      "Iteration 9264, Loss: 0.055902957916259766\n",
      "Iteration 9265, Loss: 0.05579090490937233\n",
      "Iteration 9266, Loss: 0.055681150406599045\n",
      "Iteration 9267, Loss: 0.055772703140974045\n",
      "Iteration 9268, Loss: 0.05576125904917717\n",
      "Iteration 9269, Loss: 0.05564669892191887\n",
      "Iteration 9270, Loss: 0.0558243989944458\n",
      "Iteration 9271, Loss: 0.05589691922068596\n",
      "Iteration 9272, Loss: 0.0558089055120945\n",
      "Iteration 9273, Loss: 0.05565842241048813\n",
      "Iteration 9274, Loss: 0.05576678365468979\n",
      "Iteration 9275, Loss: 0.05577985569834709\n",
      "Iteration 9276, Loss: 0.055669430643320084\n",
      "Iteration 9277, Loss: 0.05578633397817612\n",
      "Iteration 9278, Loss: 0.05585968494415283\n",
      "Iteration 9279, Loss: 0.05578991025686264\n",
      "Iteration 9280, Loss: 0.05565778538584709\n",
      "Iteration 9281, Loss: 0.055773500353097916\n",
      "Iteration 9282, Loss: 0.05578649044036865\n",
      "Iteration 9283, Loss: 0.055653613060712814\n",
      "Iteration 9284, Loss: 0.05581256002187729\n",
      "Iteration 9285, Loss: 0.055908720940351486\n",
      "Iteration 9286, Loss: 0.055877089500427246\n",
      "Iteration 9287, Loss: 0.05574170872569084\n",
      "Iteration 9288, Loss: 0.05573682114481926\n",
      "Iteration 9289, Loss: 0.05583302304148674\n",
      "Iteration 9290, Loss: 0.0557764396071434\n",
      "Iteration 9291, Loss: 0.055651310831308365\n",
      "Iteration 9292, Loss: 0.05570241063833237\n",
      "Iteration 9293, Loss: 0.055663228034973145\n",
      "Iteration 9294, Loss: 0.05571162700653076\n",
      "Iteration 9295, Loss: 0.05569171905517578\n",
      "Iteration 9296, Loss: 0.05569350719451904\n",
      "Iteration 9297, Loss: 0.055721960961818695\n",
      "Iteration 9298, Loss: 0.05565647408366203\n",
      "Iteration 9299, Loss: 0.055753909051418304\n",
      "Iteration 9300, Loss: 0.05576169490814209\n",
      "Iteration 9301, Loss: 0.05562480539083481\n",
      "Iteration 9302, Loss: 0.055665574967861176\n",
      "Iteration 9303, Loss: 0.05561407655477524\n",
      "Iteration 9304, Loss: 0.05575382709503174\n",
      "Iteration 9305, Loss: 0.05575740709900856\n",
      "Iteration 9306, Loss: 0.0556541308760643\n",
      "Iteration 9307, Loss: 0.055777549743652344\n",
      "Iteration 9308, Loss: 0.05581609532237053\n",
      "Iteration 9309, Loss: 0.05569931119680405\n",
      "Iteration 9310, Loss: 0.05575383082032204\n",
      "Iteration 9311, Loss: 0.05583858862519264\n",
      "Iteration 9312, Loss: 0.0557992085814476\n",
      "Iteration 9313, Loss: 0.05566251277923584\n",
      "Iteration 9314, Loss: 0.055816374719142914\n",
      "Iteration 9315, Loss: 0.05589409917593002\n",
      "Iteration 9316, Loss: 0.055802904069423676\n",
      "Iteration 9317, Loss: 0.05565369129180908\n",
      "Iteration 9318, Loss: 0.055736225098371506\n",
      "Iteration 9319, Loss: 0.05570463463664055\n",
      "Iteration 9320, Loss: 0.055667243897914886\n",
      "Iteration 9321, Loss: 0.055666010826826096\n",
      "Iteration 9322, Loss: 0.05569346994161606\n",
      "Iteration 9323, Loss: 0.0556970052421093\n",
      "Iteration 9324, Loss: 0.055641889572143555\n",
      "Iteration 9325, Loss: 0.055658381432294846\n",
      "Iteration 9326, Loss: 0.05567435547709465\n",
      "Iteration 9327, Loss: 0.05566605180501938\n",
      "Iteration 9328, Loss: 0.05567499250173569\n",
      "Iteration 9329, Loss: 0.055631719529628754\n",
      "Iteration 9330, Loss: 0.055738210678100586\n",
      "Iteration 9331, Loss: 0.05577024072408676\n",
      "Iteration 9332, Loss: 0.05570483207702637\n",
      "Iteration 9333, Loss: 0.05569227784872055\n",
      "Iteration 9334, Loss: 0.05570753663778305\n",
      "Iteration 9335, Loss: 0.055656593292951584\n",
      "Iteration 9336, Loss: 0.05566728115081787\n",
      "Iteration 9337, Loss: 0.055649202316999435\n",
      "Iteration 9338, Loss: 0.05563477799296379\n",
      "Iteration 9339, Loss: 0.055649641901254654\n",
      "Iteration 9340, Loss: 0.05565822124481201\n",
      "Iteration 9341, Loss: 0.055626749992370605\n",
      "Iteration 9342, Loss: 0.055743973702192307\n",
      "Iteration 9343, Loss: 0.055713336914777756\n",
      "Iteration 9344, Loss: 0.0556815080344677\n",
      "Iteration 9345, Loss: 0.055718742311000824\n",
      "Iteration 9346, Loss: 0.0556567944586277\n",
      "Iteration 9347, Loss: 0.05575144290924072\n",
      "Iteration 9348, Loss: 0.05576356500387192\n",
      "Iteration 9349, Loss: 0.05562293529510498\n",
      "Iteration 9350, Loss: 0.05574647709727287\n",
      "Iteration 9351, Loss: 0.05575665086507797\n",
      "Iteration 9352, Loss: 0.05565663427114487\n",
      "Iteration 9353, Loss: 0.055780135095119476\n",
      "Iteration 9354, Loss: 0.05582813546061516\n",
      "Iteration 9355, Loss: 0.05572088807821274\n",
      "Iteration 9356, Loss: 0.055726055055856705\n",
      "Iteration 9357, Loss: 0.05580540746450424\n",
      "Iteration 9358, Loss: 0.05576698109507561\n",
      "Iteration 9359, Loss: 0.05563867464661598\n",
      "Iteration 9360, Loss: 0.05581764504313469\n",
      "Iteration 9361, Loss: 0.055865369737148285\n",
      "Iteration 9362, Loss: 0.055743973702192307\n",
      "Iteration 9363, Loss: 0.055719416588544846\n",
      "Iteration 9364, Loss: 0.05581160634756088\n",
      "Iteration 9365, Loss: 0.055795710533857346\n",
      "Iteration 9366, Loss: 0.05568448826670647\n",
      "Iteration 9367, Loss: 0.05577131360769272\n",
      "Iteration 9368, Loss: 0.05583596229553223\n",
      "Iteration 9369, Loss: 0.05572867393493652\n",
      "Iteration 9370, Loss: 0.05572168156504631\n",
      "Iteration 9371, Loss: 0.05580528825521469\n",
      "Iteration 9372, Loss: 0.05578240007162094\n",
      "Iteration 9373, Loss: 0.05566704645752907\n",
      "Iteration 9374, Loss: 0.05579932779073715\n",
      "Iteration 9375, Loss: 0.05586664006114006\n",
      "Iteration 9376, Loss: 0.055759429931640625\n",
      "Iteration 9377, Loss: 0.05569859594106674\n",
      "Iteration 9378, Loss: 0.05578283593058586\n",
      "Iteration 9379, Loss: 0.05576241388916969\n",
      "Iteration 9380, Loss: 0.0556488037109375\n",
      "Iteration 9381, Loss: 0.055822018533945084\n",
      "Iteration 9382, Loss: 0.05588718503713608\n",
      "Iteration 9383, Loss: 0.0557764396071434\n",
      "Iteration 9384, Loss: 0.055689018219709396\n",
      "Iteration 9385, Loss: 0.05577576160430908\n",
      "Iteration 9386, Loss: 0.055759232491254807\n",
      "Iteration 9387, Loss: 0.055648766458034515\n",
      "Iteration 9388, Loss: 0.05582007020711899\n",
      "Iteration 9389, Loss: 0.05588388815522194\n",
      "Iteration 9390, Loss: 0.05577262490987778\n",
      "Iteration 9391, Loss: 0.055692158639431\n",
      "Iteration 9392, Loss: 0.05577965825796127\n",
      "Iteration 9393, Loss: 0.05576356500387192\n",
      "Iteration 9394, Loss: 0.05565277859568596\n",
      "Iteration 9395, Loss: 0.05581466481089592\n",
      "Iteration 9396, Loss: 0.055879514664411545\n",
      "Iteration 9397, Loss: 0.05577067658305168\n",
      "Iteration 9398, Loss: 0.0556916818022728\n",
      "Iteration 9399, Loss: 0.055777471512556076\n",
      "Iteration 9400, Loss: 0.05575975030660629\n",
      "Iteration 9401, Loss: 0.05564634129405022\n",
      "Iteration 9402, Loss: 0.05582607164978981\n",
      "Iteration 9403, Loss: 0.05589413642883301\n",
      "Iteration 9404, Loss: 0.05579066276550293\n",
      "Iteration 9405, Loss: 0.0556742362678051\n",
      "Iteration 9406, Loss: 0.05575994774699211\n",
      "Iteration 9407, Loss: 0.055743537843227386\n",
      "Iteration 9408, Loss: 0.055627983063459396\n",
      "Iteration 9409, Loss: 0.05584907531738281\n",
      "Iteration 9410, Loss: 0.05592123791575432\n",
      "Iteration 9411, Loss: 0.05582845211029053\n",
      "Iteration 9412, Loss: 0.05565237998962402\n",
      "Iteration 9413, Loss: 0.05578049272298813\n",
      "Iteration 9414, Loss: 0.055819038301706314\n",
      "Iteration 9415, Loss: 0.05573487654328346\n",
      "Iteration 9416, Loss: 0.05568687245249748\n",
      "Iteration 9417, Loss: 0.05573884770274162\n",
      "Iteration 9418, Loss: 0.0556459054350853\n",
      "Iteration 9419, Loss: 0.05578116700053215\n",
      "Iteration 9420, Loss: 0.05583922192454338\n",
      "Iteration 9421, Loss: 0.055768970400094986\n",
      "Iteration 9422, Loss: 0.05565989017486572\n",
      "Iteration 9423, Loss: 0.05575760453939438\n",
      "Iteration 9424, Loss: 0.055739205330610275\n",
      "Iteration 9425, Loss: 0.05565464496612549\n",
      "Iteration 9426, Loss: 0.055684685707092285\n",
      "Iteration 9427, Loss: 0.05565055459737778\n",
      "Iteration 9428, Loss: 0.055711112916469574\n",
      "Iteration 9429, Loss: 0.055670659989118576\n",
      "Iteration 9430, Loss: 0.05571838468313217\n",
      "Iteration 9431, Loss: 0.05575982853770256\n",
      "Iteration 9432, Loss: 0.05570332333445549\n",
      "Iteration 9433, Loss: 0.05568313971161842\n",
      "Iteration 9434, Loss: 0.05568782612681389\n",
      "Iteration 9435, Loss: 0.05567880719900131\n",
      "Iteration 9436, Loss: 0.05569656938314438\n",
      "Iteration 9437, Loss: 0.05561916157603264\n",
      "Iteration 9438, Loss: 0.05582154169678688\n",
      "Iteration 9439, Loss: 0.05584772676229477\n",
      "Iteration 9440, Loss: 0.05569899082183838\n",
      "Iteration 9441, Loss: 0.05577230826020241\n",
      "Iteration 9442, Loss: 0.055881381034851074\n",
      "Iteration 9443, Loss: 0.05588257312774658\n",
      "Iteration 9444, Loss: 0.05578657239675522\n",
      "Iteration 9445, Loss: 0.055631719529628754\n",
      "Iteration 9446, Loss: 0.05580254644155502\n",
      "Iteration 9447, Loss: 0.05581089109182358\n",
      "Iteration 9448, Loss: 0.05566207692027092\n",
      "Iteration 9449, Loss: 0.05579078197479248\n",
      "Iteration 9450, Loss: 0.05588678643107414\n",
      "Iteration 9451, Loss: 0.05587363243103027\n",
      "Iteration 9452, Loss: 0.05576284974813461\n",
      "Iteration 9453, Loss: 0.05567380040884018\n",
      "Iteration 9454, Loss: 0.05575164407491684\n",
      "Iteration 9455, Loss: 0.05566553398966789\n",
      "Iteration 9456, Loss: 0.05575474351644516\n",
      "Iteration 9457, Loss: 0.05582420155405998\n",
      "Iteration 9458, Loss: 0.055792611092329025\n",
      "Iteration 9459, Loss: 0.055673085153102875\n",
      "Iteration 9460, Loss: 0.05579964444041252\n",
      "Iteration 9461, Loss: 0.05587109178304672\n",
      "Iteration 9462, Loss: 0.05576292797923088\n",
      "Iteration 9463, Loss: 0.055698197335004807\n",
      "Iteration 9464, Loss: 0.055783431977033615\n",
      "Iteration 9465, Loss: 0.05576666444540024\n",
      "Iteration 9466, Loss: 0.055658143013715744\n",
      "Iteration 9467, Loss: 0.05580612272024155\n",
      "Iteration 9468, Loss: 0.05586600303649902\n",
      "Iteration 9469, Loss: 0.055746715515851974\n",
      "Iteration 9470, Loss: 0.05571739003062248\n",
      "Iteration 9471, Loss: 0.05580946058034897\n",
      "Iteration 9472, Loss: 0.05579833313822746\n",
      "Iteration 9473, Loss: 0.055694978684186935\n",
      "Iteration 9474, Loss: 0.05575069040060043\n",
      "Iteration 9475, Loss: 0.055806003510951996\n",
      "Iteration 9476, Loss: 0.05568297952413559\n",
      "Iteration 9477, Loss: 0.05576638504862785\n",
      "Iteration 9478, Loss: 0.0558604821562767\n",
      "Iteration 9479, Loss: 0.05585102364420891\n",
      "Iteration 9480, Loss: 0.0557483434677124\n",
      "Iteration 9481, Loss: 0.05567697808146477\n",
      "Iteration 9482, Loss: 0.055731695145368576\n",
      "Iteration 9483, Loss: 0.05561407655477524\n",
      "Iteration 9484, Loss: 0.055685244500637054\n",
      "Iteration 9485, Loss: 0.055631957948207855\n",
      "Iteration 9486, Loss: 0.055748943239450455\n",
      "Iteration 9487, Loss: 0.055743973702192307\n",
      "Iteration 9488, Loss: 0.0556359700858593\n",
      "Iteration 9489, Loss: 0.05570439621806145\n",
      "Iteration 9490, Loss: 0.055652420967817307\n",
      "Iteration 9491, Loss: 0.055746398866176605\n",
      "Iteration 9492, Loss: 0.0557812862098217\n",
      "Iteration 9493, Loss: 0.055698078125715256\n",
      "Iteration 9494, Loss: 0.055722713470458984\n",
      "Iteration 9495, Loss: 0.05576813220977783\n",
      "Iteration 9496, Loss: 0.05566450208425522\n",
      "Iteration 9497, Loss: 0.05577671900391579\n",
      "Iteration 9498, Loss: 0.05585126206278801\n",
      "Iteration 9499, Loss: 0.05580051988363266\n",
      "Iteration 9500, Loss: 0.05566394329071045\n",
      "Iteration 9501, Loss: 0.05580922216176987\n",
      "Iteration 9502, Loss: 0.05587736889719963\n",
      "Iteration 9503, Loss: 0.055778466165065765\n",
      "Iteration 9504, Loss: 0.05567622184753418\n",
      "Iteration 9505, Loss: 0.05575358867645264\n",
      "Iteration 9506, Loss: 0.05571679398417473\n",
      "Iteration 9507, Loss: 0.055653415620326996\n",
      "Iteration 9508, Loss: 0.055673837661743164\n",
      "Iteration 9509, Loss: 0.055671773850917816\n",
      "Iteration 9510, Loss: 0.055669188499450684\n",
      "Iteration 9511, Loss: 0.05566466227173805\n",
      "Iteration 9512, Loss: 0.05563966557383537\n",
      "Iteration 9513, Loss: 0.0557025671005249\n",
      "Iteration 9514, Loss: 0.05570852756500244\n",
      "Iteration 9515, Loss: 0.05562114715576172\n",
      "Iteration 9516, Loss: 0.05582638829946518\n",
      "Iteration 9517, Loss: 0.055863261222839355\n",
      "Iteration 9518, Loss: 0.055730462074279785\n",
      "Iteration 9519, Loss: 0.05573717877268791\n",
      "Iteration 9520, Loss: 0.055836163461208344\n",
      "Iteration 9521, Loss: 0.05582841485738754\n",
      "Iteration 9522, Loss: 0.0557246208190918\n",
      "Iteration 9523, Loss: 0.05571115016937256\n",
      "Iteration 9524, Loss: 0.05576964467763901\n",
      "Iteration 9525, Loss: 0.055655959993600845\n",
      "Iteration 9526, Loss: 0.055781763046979904\n",
      "Iteration 9527, Loss: 0.05586854740977287\n",
      "Iteration 9528, Loss: 0.05584927648305893\n",
      "Iteration 9529, Loss: 0.05573674291372299\n",
      "Iteration 9530, Loss: 0.05570809170603752\n",
      "Iteration 9531, Loss: 0.0557788610458374\n",
      "Iteration 9532, Loss: 0.055680397897958755\n",
      "Iteration 9533, Loss: 0.05575355142354965\n",
      "Iteration 9534, Loss: 0.05583139508962631\n",
      "Iteration 9535, Loss: 0.055807434022426605\n",
      "Iteration 9536, Loss: 0.05569378659129143\n",
      "Iteration 9537, Loss: 0.05576523393392563\n",
      "Iteration 9538, Loss: 0.0558319129049778\n",
      "Iteration 9539, Loss: 0.05572100728750229\n",
      "Iteration 9540, Loss: 0.05573093891143799\n",
      "Iteration 9541, Loss: 0.05581772327423096\n",
      "Iteration 9542, Loss: 0.05580174922943115\n",
      "Iteration 9543, Loss: 0.0556943416595459\n",
      "Iteration 9544, Loss: 0.0557563342154026\n",
      "Iteration 9545, Loss: 0.05581609532237053\n",
      "Iteration 9546, Loss: 0.05569815635681152\n",
      "Iteration 9547, Loss: 0.05575228109955788\n",
      "Iteration 9548, Loss: 0.05584343522787094\n",
      "Iteration 9549, Loss: 0.05583151429891586\n",
      "Iteration 9550, Loss: 0.05572684854269028\n",
      "Iteration 9551, Loss: 0.055708568543195724\n",
      "Iteration 9552, Loss: 0.0557657890021801\n",
      "Iteration 9553, Loss: 0.05564507097005844\n",
      "Iteration 9554, Loss: 0.05579277127981186\n",
      "Iteration 9555, Loss: 0.05588535591959953\n",
      "Iteration 9556, Loss: 0.055874310433864594\n",
      "Iteration 9557, Loss: 0.055770281702280045\n",
      "Iteration 9558, Loss: 0.05564900487661362\n",
      "Iteration 9559, Loss: 0.055706024169921875\n",
      "Iteration 9560, Loss: 0.05563076585531235\n",
      "Iteration 9561, Loss: 0.05562857910990715\n",
      "Iteration 9562, Loss: 0.055709682404994965\n",
      "Iteration 9563, Loss: 0.05566418170928955\n",
      "Iteration 9564, Loss: 0.05572422593832016\n",
      "Iteration 9565, Loss: 0.055756013840436935\n",
      "Iteration 9566, Loss: 0.055674076080322266\n",
      "Iteration 9567, Loss: 0.0557483471930027\n",
      "Iteration 9568, Loss: 0.0557866096496582\n",
      "Iteration 9569, Loss: 0.05566839501261711\n",
      "Iteration 9570, Loss: 0.05577818676829338\n",
      "Iteration 9571, Loss: 0.05586123466491699\n",
      "Iteration 9572, Loss: 0.05582007020711899\n",
      "Iteration 9573, Loss: 0.0556797981262207\n",
      "Iteration 9574, Loss: 0.055801909416913986\n",
      "Iteration 9575, Loss: 0.05589008703827858\n",
      "Iteration 9576, Loss: 0.05581049248576164\n",
      "Iteration 9577, Loss: 0.05563811585307121\n",
      "Iteration 9578, Loss: 0.05572943016886711\n",
      "Iteration 9579, Loss: 0.055702172219753265\n",
      "Iteration 9580, Loss: 0.05566906929016113\n",
      "Iteration 9581, Loss: 0.05567280575633049\n",
      "Iteration 9582, Loss: 0.05567777156829834\n",
      "Iteration 9583, Loss: 0.05567018315196037\n",
      "Iteration 9584, Loss: 0.055682223290205\n",
      "Iteration 9585, Loss: 0.05566621199250221\n",
      "Iteration 9586, Loss: 0.05570022389292717\n",
      "Iteration 9587, Loss: 0.055705588310956955\n",
      "Iteration 9588, Loss: 0.055638592690229416\n",
      "Iteration 9589, Loss: 0.05567439645528793\n",
      "Iteration 9590, Loss: 0.05565003678202629\n",
      "Iteration 9591, Loss: 0.05564407631754875\n",
      "Iteration 9592, Loss: 0.05567697808146477\n",
      "Iteration 9593, Loss: 0.05562961474061012\n",
      "Iteration 9594, Loss: 0.055684804916381836\n",
      "Iteration 9595, Loss: 0.055643480271101\n",
      "Iteration 9596, Loss: 0.05574258416891098\n",
      "Iteration 9597, Loss: 0.05575132742524147\n",
      "Iteration 9598, Loss: 0.055638354271650314\n",
      "Iteration 9599, Loss: 0.0557759627699852\n",
      "Iteration 9600, Loss: 0.05580636113882065\n",
      "Iteration 9601, Loss: 0.05569493770599365\n",
      "Iteration 9602, Loss: 0.05575088784098625\n",
      "Iteration 9603, Loss: 0.05582690238952637\n",
      "Iteration 9604, Loss: 0.055770277976989746\n",
      "Iteration 9605, Loss: 0.05563998594880104\n",
      "Iteration 9606, Loss: 0.055759310722351074\n",
      "Iteration 9607, Loss: 0.05573956295847893\n",
      "Iteration 9608, Loss: 0.05565246194601059\n",
      "Iteration 9609, Loss: 0.05567868798971176\n",
      "Iteration 9610, Loss: 0.05563187971711159\n",
      "Iteration 9611, Loss: 0.055698078125715256\n",
      "Iteration 9612, Loss: 0.05564097687602043\n",
      "Iteration 9613, Loss: 0.05573670193552971\n",
      "Iteration 9614, Loss: 0.05575982853770256\n",
      "Iteration 9615, Loss: 0.05567161366343498\n",
      "Iteration 9616, Loss: 0.05575617402791977\n",
      "Iteration 9617, Loss: 0.05580016225576401\n",
      "Iteration 9618, Loss: 0.05568941682577133\n",
      "Iteration 9619, Loss: 0.055754780769348145\n",
      "Iteration 9620, Loss: 0.055834971368312836\n",
      "Iteration 9621, Loss: 0.055796388536691666\n",
      "Iteration 9622, Loss: 0.05566410347819328\n",
      "Iteration 9623, Loss: 0.055809300392866135\n",
      "Iteration 9624, Loss: 0.05588531494140625\n",
      "Iteration 9625, Loss: 0.055795710533857346\n",
      "Iteration 9626, Loss: 0.05565500631928444\n",
      "Iteration 9627, Loss: 0.05573141947388649\n",
      "Iteration 9628, Loss: 0.055695854127407074\n",
      "Iteration 9629, Loss: 0.055675946176052094\n",
      "Iteration 9630, Loss: 0.05567467212677002\n",
      "Iteration 9631, Loss: 0.05568560212850571\n",
      "Iteration 9632, Loss: 0.055693309754133224\n",
      "Iteration 9633, Loss: 0.0556364469230175\n",
      "Iteration 9634, Loss: 0.055669866502285004\n",
      "Iteration 9635, Loss: 0.055651865899562836\n",
      "Iteration 9636, Loss: 0.05564336106181145\n",
      "Iteration 9637, Loss: 0.05569462105631828\n",
      "Iteration 9638, Loss: 0.05564209073781967\n",
      "Iteration 9639, Loss: 0.055741190910339355\n",
      "Iteration 9640, Loss: 0.0557759627699852\n",
      "Iteration 9641, Loss: 0.055702488869428635\n",
      "Iteration 9642, Loss: 0.05570586770772934\n",
      "Iteration 9643, Loss: 0.055736105889081955\n",
      "Iteration 9644, Loss: 0.0556185282766819\n",
      "Iteration 9645, Loss: 0.05564383789896965\n",
      "Iteration 9646, Loss: 0.05565917491912842\n",
      "Iteration 9647, Loss: 0.05563652887940407\n",
      "Iteration 9648, Loss: 0.05563414469361305\n",
      "Iteration 9649, Loss: 0.05568397417664528\n",
      "Iteration 9650, Loss: 0.05567145347595215\n",
      "Iteration 9651, Loss: 0.055670980364084244\n",
      "Iteration 9652, Loss: 0.05562695115804672\n",
      "Iteration 9653, Loss: 0.0557553768157959\n",
      "Iteration 9654, Loss: 0.05580214783549309\n",
      "Iteration 9655, Loss: 0.05575013533234596\n",
      "Iteration 9656, Loss: 0.05561848729848862\n",
      "Iteration 9657, Loss: 0.055787645280361176\n",
      "Iteration 9658, Loss: 0.05578470230102539\n",
      "Iteration 9659, Loss: 0.05563950911164284\n",
      "Iteration 9660, Loss: 0.05578450486063957\n",
      "Iteration 9661, Loss: 0.055843234062194824\n",
      "Iteration 9662, Loss: 0.05577830597758293\n",
      "Iteration 9663, Loss: 0.05564097687602043\n",
      "Iteration 9664, Loss: 0.05579221621155739\n",
      "Iteration 9665, Loss: 0.05580449476838112\n",
      "Iteration 9666, Loss: 0.0556485690176487\n",
      "Iteration 9667, Loss: 0.05581891909241676\n",
      "Iteration 9668, Loss: 0.05593252182006836\n",
      "Iteration 9669, Loss: 0.05593244358897209\n",
      "Iteration 9670, Loss: 0.05583282560110092\n",
      "Iteration 9671, Loss: 0.05566466227173805\n",
      "Iteration 9672, Loss: 0.055865369737148285\n",
      "Iteration 9673, Loss: 0.055973730981349945\n",
      "Iteration 9674, Loss: 0.0558980330824852\n",
      "Iteration 9675, Loss: 0.05566084384918213\n",
      "Iteration 9676, Loss: 0.05585603043437004\n",
      "Iteration 9677, Loss: 0.05601450055837631\n",
      "Iteration 9678, Loss: 0.05606158822774887\n",
      "Iteration 9679, Loss: 0.056007783859968185\n",
      "Iteration 9680, Loss: 0.055864494293928146\n",
      "Iteration 9681, Loss: 0.05564948171377182\n",
      "Iteration 9682, Loss: 0.055944882333278656\n",
      "Iteration 9683, Loss: 0.05611376091837883\n",
      "Iteration 9684, Loss: 0.056092582643032074\n",
      "Iteration 9685, Loss: 0.05590176582336426\n",
      "Iteration 9686, Loss: 0.05566056817770004\n",
      "Iteration 9687, Loss: 0.055809419602155685\n",
      "Iteration 9688, Loss: 0.05586421489715576\n",
      "Iteration 9689, Loss: 0.055818162858486176\n",
      "Iteration 9690, Loss: 0.05567968264222145\n",
      "Iteration 9691, Loss: 0.05581247806549072\n",
      "Iteration 9692, Loss: 0.055907249450683594\n",
      "Iteration 9693, Loss: 0.055827341973781586\n",
      "Iteration 9694, Loss: 0.05564320087432861\n",
      "Iteration 9695, Loss: 0.05576888844370842\n",
      "Iteration 9696, Loss: 0.05580274388194084\n",
      "Iteration 9697, Loss: 0.05572152137756348\n",
      "Iteration 9698, Loss: 0.05569478124380112\n",
      "Iteration 9699, Loss: 0.05573960393667221\n",
      "Iteration 9700, Loss: 0.05563322827219963\n",
      "Iteration 9701, Loss: 0.05579996481537819\n",
      "Iteration 9702, Loss: 0.055868152529001236\n",
      "Iteration 9703, Loss: 0.055811524391174316\n",
      "Iteration 9704, Loss: 0.05567089840769768\n",
      "Iteration 9705, Loss: 0.05580584332346916\n",
      "Iteration 9706, Loss: 0.055882059037685394\n",
      "Iteration 9707, Loss: 0.05579817667603493\n",
      "Iteration 9708, Loss: 0.0556459054350853\n",
      "Iteration 9709, Loss: 0.05571385473012924\n",
      "Iteration 9710, Loss: 0.055666130036115646\n",
      "Iteration 9711, Loss: 0.055721960961818695\n",
      "Iteration 9712, Loss: 0.055729590356349945\n",
      "Iteration 9713, Loss: 0.055635254830121994\n",
      "Iteration 9714, Loss: 0.05567232891917229\n",
      "Iteration 9715, Loss: 0.055637359619140625\n",
      "Iteration 9716, Loss: 0.0556466206908226\n",
      "Iteration 9717, Loss: 0.055636804550886154\n",
      "Iteration 9718, Loss: 0.05567082017660141\n",
      "Iteration 9719, Loss: 0.055646657943725586\n",
      "Iteration 9720, Loss: 0.05571914091706276\n",
      "Iteration 9721, Loss: 0.05568965524435043\n",
      "Iteration 9722, Loss: 0.05569863319396973\n",
      "Iteration 9723, Loss: 0.055735353380441666\n",
      "Iteration 9724, Loss: 0.055673837661743164\n",
      "Iteration 9725, Loss: 0.05572768300771713\n",
      "Iteration 9726, Loss: 0.055738650262355804\n",
      "Iteration 9727, Loss: 0.055635493248701096\n",
      "Iteration 9728, Loss: 0.05565309897065163\n",
      "Iteration 9729, Loss: 0.05566227808594704\n",
      "Iteration 9730, Loss: 0.05562850087881088\n",
      "Iteration 9731, Loss: 0.05566176027059555\n",
      "Iteration 9732, Loss: 0.05565115064382553\n",
      "Iteration 9733, Loss: 0.05562615767121315\n",
      "Iteration 9734, Loss: 0.05574079602956772\n",
      "Iteration 9735, Loss: 0.05570276826620102\n",
      "Iteration 9736, Loss: 0.05569577217102051\n",
      "Iteration 9737, Loss: 0.055739205330610275\n",
      "Iteration 9738, Loss: 0.05568433180451393\n",
      "Iteration 9739, Loss: 0.055705469101667404\n",
      "Iteration 9740, Loss: 0.05570745840668678\n",
      "Iteration 9741, Loss: 0.055665891617536545\n",
      "Iteration 9742, Loss: 0.05568563938140869\n",
      "Iteration 9743, Loss: 0.0556178092956543\n",
      "Iteration 9744, Loss: 0.0557401217520237\n",
      "Iteration 9745, Loss: 0.055713534355163574\n",
      "Iteration 9746, Loss: 0.05567062273621559\n",
      "Iteration 9747, Loss: 0.05569911003112793\n",
      "Iteration 9748, Loss: 0.05562090873718262\n",
      "Iteration 9749, Loss: 0.055767182260751724\n",
      "Iteration 9750, Loss: 0.05579765886068344\n",
      "Iteration 9751, Loss: 0.0557151660323143\n",
      "Iteration 9752, Loss: 0.05570578575134277\n",
      "Iteration 9753, Loss: 0.05575358867645264\n",
      "Iteration 9754, Loss: 0.05565333366394043\n",
      "Iteration 9755, Loss: 0.05578037351369858\n",
      "Iteration 9756, Loss: 0.05585432052612305\n",
      "Iteration 9757, Loss: 0.055813949555158615\n",
      "Iteration 9758, Loss: 0.0556865930557251\n",
      "Iteration 9759, Loss: 0.055787645280361176\n",
      "Iteration 9760, Loss: 0.0558621883392334\n",
      "Iteration 9761, Loss: 0.05576467514038086\n",
      "Iteration 9762, Loss: 0.0556899718940258\n",
      "Iteration 9763, Loss: 0.055766623467206955\n",
      "Iteration 9764, Loss: 0.055736344307661057\n",
      "Iteration 9765, Loss: 0.05564069747924805\n",
      "Iteration 9766, Loss: 0.05576976388692856\n",
      "Iteration 9767, Loss: 0.0557607039809227\n",
      "Iteration 9768, Loss: 0.05564352124929428\n",
      "Iteration 9769, Loss: 0.05570479482412338\n",
      "Iteration 9770, Loss: 0.055685997009277344\n",
      "Iteration 9771, Loss: 0.055670224130153656\n",
      "Iteration 9772, Loss: 0.0556509904563427\n",
      "Iteration 9773, Loss: 0.055709801614284515\n",
      "Iteration 9774, Loss: 0.055719852447509766\n",
      "Iteration 9775, Loss: 0.055628661066293716\n",
      "Iteration 9776, Loss: 0.055791378021240234\n",
      "Iteration 9777, Loss: 0.05580552667379379\n",
      "Iteration 9778, Loss: 0.055653613060712814\n",
      "Iteration 9779, Loss: 0.05580807104706764\n",
      "Iteration 9780, Loss: 0.055915795266628265\n",
      "Iteration 9781, Loss: 0.05591320991516113\n",
      "Iteration 9782, Loss: 0.05581335350871086\n",
      "Iteration 9783, Loss: 0.05564848706126213\n",
      "Iteration 9784, Loss: 0.05586417764425278\n",
      "Iteration 9785, Loss: 0.055951714515686035\n",
      "Iteration 9786, Loss: 0.055858612060546875\n",
      "Iteration 9787, Loss: 0.055621545761823654\n",
      "Iteration 9788, Loss: 0.05575395002961159\n",
      "Iteration 9789, Loss: 0.055784981697797775\n",
      "Iteration 9790, Loss: 0.055714964866638184\n",
      "Iteration 9791, Loss: 0.055683813989162445\n",
      "Iteration 9792, Loss: 0.055706024169921875\n",
      "Iteration 9793, Loss: 0.05565281957387924\n",
      "Iteration 9794, Loss: 0.05565913766622543\n",
      "Iteration 9795, Loss: 0.05566474050283432\n",
      "Iteration 9796, Loss: 0.055624011904001236\n",
      "Iteration 9797, Loss: 0.05568810552358627\n",
      "Iteration 9798, Loss: 0.0556483268737793\n",
      "Iteration 9799, Loss: 0.05573264881968498\n",
      "Iteration 9800, Loss: 0.055735986679792404\n",
      "Iteration 9801, Loss: 0.05563374608755112\n",
      "Iteration 9802, Loss: 0.05571560189127922\n",
      "Iteration 9803, Loss: 0.05566958710551262\n",
      "Iteration 9804, Loss: 0.05572577565908432\n",
      "Iteration 9805, Loss: 0.055761419236660004\n",
      "Iteration 9806, Loss: 0.05568520352244377\n",
      "Iteration 9807, Loss: 0.05572883412241936\n",
      "Iteration 9808, Loss: 0.05576201528310776\n",
      "Iteration 9809, Loss: 0.055638235062360764\n",
      "Iteration 9810, Loss: 0.055811844766139984\n",
      "Iteration 9811, Loss: 0.05590057373046875\n",
      "Iteration 9812, Loss: 0.055865805596113205\n",
      "Iteration 9813, Loss: 0.05572974681854248\n",
      "Iteration 9814, Loss: 0.05574588105082512\n",
      "Iteration 9815, Loss: 0.055839382112026215\n",
      "Iteration 9816, Loss: 0.055772386491298676\n",
      "Iteration 9817, Loss: 0.05566060543060303\n",
      "Iteration 9818, Loss: 0.055715564638376236\n",
      "Iteration 9819, Loss: 0.055668991059064865\n",
      "Iteration 9820, Loss: 0.05571385473012924\n",
      "Iteration 9821, Loss: 0.055708687752485275\n",
      "Iteration 9822, Loss: 0.05566760152578354\n",
      "Iteration 9823, Loss: 0.055685244500637054\n",
      "Iteration 9824, Loss: 0.055634498596191406\n",
      "Iteration 9825, Loss: 0.05570169538259506\n",
      "Iteration 9826, Loss: 0.05564602464437485\n",
      "Iteration 9827, Loss: 0.05573427677154541\n",
      "Iteration 9828, Loss: 0.05576372146606445\n",
      "Iteration 9829, Loss: 0.05568429082632065\n",
      "Iteration 9830, Loss: 0.05573376268148422\n",
      "Iteration 9831, Loss: 0.055770277976989746\n",
      "Iteration 9832, Loss: 0.055649518966674805\n",
      "Iteration 9833, Loss: 0.05579523369669914\n",
      "Iteration 9834, Loss: 0.05588022992014885\n",
      "Iteration 9835, Loss: 0.055847130715847015\n",
      "Iteration 9836, Loss: 0.05571727082133293\n",
      "Iteration 9837, Loss: 0.05575207993388176\n",
      "Iteration 9838, Loss: 0.055838584899902344\n",
      "Iteration 9839, Loss: 0.05576543137431145\n",
      "Iteration 9840, Loss: 0.05566863343119621\n",
      "Iteration 9841, Loss: 0.055727165192365646\n",
      "Iteration 9842, Loss: 0.05568091198801994\n",
      "Iteration 9843, Loss: 0.05570054426789284\n",
      "Iteration 9844, Loss: 0.05570189282298088\n",
      "Iteration 9845, Loss: 0.05566664785146713\n",
      "Iteration 9846, Loss: 0.055678289383649826\n",
      "Iteration 9847, Loss: 0.055644355714321136\n",
      "Iteration 9848, Loss: 0.05566442385315895\n",
      "Iteration 9849, Loss: 0.05565468594431877\n",
      "Iteration 9850, Loss: 0.05564459413290024\n",
      "Iteration 9851, Loss: 0.055696964263916016\n",
      "Iteration 9852, Loss: 0.055650632828474045\n",
      "Iteration 9853, Loss: 0.055731020867824554\n",
      "Iteration 9854, Loss: 0.05576050654053688\n",
      "Iteration 9855, Loss: 0.05567976087331772\n",
      "Iteration 9856, Loss: 0.055741190910339355\n",
      "Iteration 9857, Loss: 0.05577731132507324\n",
      "Iteration 9858, Loss: 0.05565289780497551\n",
      "Iteration 9859, Loss: 0.05579511448740959\n",
      "Iteration 9860, Loss: 0.055882178246974945\n",
      "Iteration 9861, Loss: 0.05584748834371567\n",
      "Iteration 9862, Loss: 0.05571230500936508\n",
      "Iteration 9863, Loss: 0.0557628870010376\n",
      "Iteration 9864, Loss: 0.05585257336497307\n",
      "Iteration 9865, Loss: 0.05577616021037102\n",
      "Iteration 9866, Loss: 0.05566183850169182\n",
      "Iteration 9867, Loss: 0.05572358891367912\n",
      "Iteration 9868, Loss: 0.055674195289611816\n",
      "Iteration 9869, Loss: 0.055713098496198654\n",
      "Iteration 9870, Loss: 0.055717192590236664\n",
      "Iteration 9871, Loss: 0.055650871247053146\n",
      "Iteration 9872, Loss: 0.05566072463989258\n",
      "Iteration 9873, Loss: 0.05566704273223877\n",
      "Iteration 9874, Loss: 0.055618129670619965\n",
      "Iteration 9875, Loss: 0.055763762444257736\n",
      "Iteration 9876, Loss: 0.05578116700053215\n",
      "Iteration 9877, Loss: 0.05568023771047592\n",
      "Iteration 9878, Loss: 0.05575553700327873\n",
      "Iteration 9879, Loss: 0.055811844766139984\n",
      "Iteration 9880, Loss: 0.055719733238220215\n",
      "Iteration 9881, Loss: 0.055714886635541916\n",
      "Iteration 9882, Loss: 0.05578096956014633\n",
      "Iteration 9883, Loss: 0.05572033300995827\n",
      "Iteration 9884, Loss: 0.055679284036159515\n",
      "Iteration 9885, Loss: 0.05571389198303223\n",
      "Iteration 9886, Loss: 0.05561971664428711\n",
      "Iteration 9887, Loss: 0.0556538924574852\n",
      "Iteration 9888, Loss: 0.055630527436733246\n",
      "Iteration 9889, Loss: 0.055670659989118576\n",
      "Iteration 9890, Loss: 0.05563513562083244\n",
      "Iteration 9891, Loss: 0.055738531053066254\n",
      "Iteration 9892, Loss: 0.055716753005981445\n",
      "Iteration 9893, Loss: 0.055670976638793945\n",
      "Iteration 9894, Loss: 0.05570098012685776\n",
      "Iteration 9895, Loss: 0.05562949180603027\n",
      "Iteration 9896, Loss: 0.05579650402069092\n",
      "Iteration 9897, Loss: 0.055818598717451096\n",
      "Iteration 9898, Loss: 0.05567558854818344\n",
      "Iteration 9899, Loss: 0.05578470602631569\n",
      "Iteration 9900, Loss: 0.055886946618556976\n",
      "Iteration 9901, Loss: 0.05587911978363991\n",
      "Iteration 9902, Loss: 0.05577377602458\n",
      "Iteration 9903, Loss: 0.05565854161977768\n",
      "Iteration 9904, Loss: 0.05575621500611305\n",
      "Iteration 9905, Loss: 0.055699948221445084\n",
      "Iteration 9906, Loss: 0.05571151152253151\n",
      "Iteration 9907, Loss: 0.055763643234968185\n",
      "Iteration 9908, Loss: 0.05572033300995827\n",
      "Iteration 9909, Loss: 0.05565027520060539\n",
      "Iteration 9910, Loss: 0.055685799568891525\n",
      "Iteration 9911, Loss: 0.05565337464213371\n",
      "Iteration 9912, Loss: 0.05567038059234619\n",
      "Iteration 9913, Loss: 0.055641770362854004\n",
      "Iteration 9914, Loss: 0.05570252984762192\n",
      "Iteration 9915, Loss: 0.05569958686828613\n",
      "Iteration 9916, Loss: 0.05564538761973381\n",
      "Iteration 9917, Loss: 0.05570165440440178\n",
      "Iteration 9918, Loss: 0.05564634129405022\n",
      "Iteration 9919, Loss: 0.05572521686553955\n",
      "Iteration 9920, Loss: 0.05574953928589821\n",
      "Iteration 9921, Loss: 0.05566927045583725\n",
      "Iteration 9922, Loss: 0.05575541779398918\n",
      "Iteration 9923, Loss: 0.055788200348615646\n",
      "Iteration 9924, Loss: 0.05565563961863518\n",
      "Iteration 9925, Loss: 0.055795155465602875\n",
      "Iteration 9926, Loss: 0.05588845536112785\n",
      "Iteration 9927, Loss: 0.05586441606283188\n",
      "Iteration 9928, Loss: 0.05573984235525131\n",
      "Iteration 9929, Loss: 0.05572013184428215\n",
      "Iteration 9930, Loss: 0.055804293602705\n",
      "Iteration 9931, Loss: 0.05572303384542465\n",
      "Iteration 9932, Loss: 0.05571238324046135\n",
      "Iteration 9933, Loss: 0.05577802658081055\n",
      "Iteration 9934, Loss: 0.05573832988739014\n",
      "Iteration 9935, Loss: 0.0556485652923584\n",
      "Iteration 9936, Loss: 0.05575323477387428\n",
      "Iteration 9937, Loss: 0.055728714913129807\n",
      "Iteration 9938, Loss: 0.05567292496562004\n",
      "Iteration 9939, Loss: 0.055711034685373306\n",
      "Iteration 9940, Loss: 0.05566803738474846\n",
      "Iteration 9941, Loss: 0.055717431008815765\n",
      "Iteration 9942, Loss: 0.05570793151855469\n",
      "Iteration 9943, Loss: 0.05567244812846184\n",
      "Iteration 9944, Loss: 0.055698636919260025\n",
      "Iteration 9945, Loss: 0.0556366853415966\n",
      "Iteration 9946, Loss: 0.055771153420209885\n",
      "Iteration 9947, Loss: 0.05578426644206047\n",
      "Iteration 9948, Loss: 0.05565802380442619\n",
      "Iteration 9949, Loss: 0.055779896676540375\n",
      "Iteration 9950, Loss: 0.055851422250270844\n",
      "Iteration 9951, Loss: 0.055799007415771484\n",
      "Iteration 9952, Loss: 0.055651307106018066\n",
      "Iteration 9953, Loss: 0.05582980439066887\n",
      "Iteration 9954, Loss: 0.055911146104335785\n",
      "Iteration 9955, Loss: 0.05582873150706291\n",
      "Iteration 9956, Loss: 0.055634062737226486\n",
      "Iteration 9957, Loss: 0.05578732490539551\n",
      "Iteration 9958, Loss: 0.05582650750875473\n",
      "Iteration 9959, Loss: 0.055736105889081955\n",
      "Iteration 9960, Loss: 0.055693190544843674\n",
      "Iteration 9961, Loss: 0.055755697190761566\n",
      "Iteration 9962, Loss: 0.05567864701151848\n",
      "Iteration 9963, Loss: 0.05574294179677963\n",
      "Iteration 9964, Loss: 0.055795393884181976\n",
      "Iteration 9965, Loss: 0.05572478100657463\n",
      "Iteration 9966, Loss: 0.05568742752075195\n",
      "Iteration 9967, Loss: 0.05573562905192375\n",
      "Iteration 9968, Loss: 0.05565190315246582\n",
      "Iteration 9969, Loss: 0.05576781556010246\n",
      "Iteration 9970, Loss: 0.05582726001739502\n",
      "Iteration 9971, Loss: 0.055776916444301605\n",
      "Iteration 9972, Loss: 0.055661678314208984\n",
      "Iteration 9973, Loss: 0.05579352378845215\n",
      "Iteration 9974, Loss: 0.05583270639181137\n",
      "Iteration 9975, Loss: 0.055703602731227875\n",
      "Iteration 9976, Loss: 0.05575764179229736\n",
      "Iteration 9977, Loss: 0.05585436150431633\n",
      "Iteration 9978, Loss: 0.05584605783224106\n",
      "Iteration 9979, Loss: 0.0557452067732811\n",
      "Iteration 9980, Loss: 0.05568563938140869\n",
      "Iteration 9981, Loss: 0.05575069040060043\n",
      "Iteration 9982, Loss: 0.055659692734479904\n",
      "Iteration 9983, Loss: 0.05575370788574219\n",
      "Iteration 9984, Loss: 0.05581780523061752\n",
      "Iteration 9985, Loss: 0.055781565606594086\n",
      "Iteration 9986, Loss: 0.05565532296895981\n",
      "Iteration 9987, Loss: 0.05583294481039047\n",
      "Iteration 9988, Loss: 0.055912815034389496\n",
      "Iteration 9989, Loss: 0.05581049248576164\n",
      "Iteration 9990, Loss: 0.05565985292196274\n",
      "Iteration 9991, Loss: 0.05574334040284157\n",
      "Iteration 9992, Loss: 0.055724821984767914\n",
      "Iteration 9993, Loss: 0.0556134395301342\n",
      "Iteration 9994, Loss: 0.05587005615234375\n",
      "Iteration 9995, Loss: 0.0559338741004467\n",
      "Iteration 9996, Loss: 0.055818162858486176\n",
      "Iteration 9997, Loss: 0.05566326901316643\n",
      "Iteration 9998, Loss: 0.05575704574584961\n",
      "Iteration 9999, Loss: 0.05574703589081764\n",
      "Iteration 10000, Loss: 0.055639468133449554\n",
      "Iteration 10001, Loss: 0.05582869052886963\n",
      "Iteration 10002, Loss: 0.05588897317647934\n",
      "Iteration 10003, Loss: 0.055773697793483734\n",
      "Iteration 10004, Loss: 0.05569414421916008\n",
      "Iteration 10005, Loss: 0.05578434839844704\n",
      "Iteration 10006, Loss: 0.05576726049184799\n",
      "Iteration 10007, Loss: 0.05565158650279045\n",
      "Iteration 10008, Loss: 0.05582046881318092\n",
      "Iteration 10009, Loss: 0.05588917061686516\n",
      "Iteration 10010, Loss: 0.05578180402517319\n",
      "Iteration 10011, Loss: 0.05568266287446022\n",
      "Iteration 10012, Loss: 0.055768970400094986\n",
      "Iteration 10013, Loss: 0.05574818700551987\n",
      "Iteration 10014, Loss: 0.055628180503845215\n",
      "Iteration 10015, Loss: 0.05585480108857155\n",
      "Iteration 10016, Loss: 0.0559266023337841\n",
      "Iteration 10017, Loss: 0.0558222159743309\n",
      "Iteration 10018, Loss: 0.05565480515360832\n",
      "Iteration 10019, Loss: 0.05575494095683098\n",
      "Iteration 10020, Loss: 0.055753469467163086\n",
      "Iteration 10021, Loss: 0.055643320083618164\n",
      "Iteration 10022, Loss: 0.05582074448466301\n",
      "Iteration 10023, Loss: 0.055887382477521896\n",
      "Iteration 10024, Loss: 0.05579165741801262\n",
      "Iteration 10025, Loss: 0.05567014217376709\n",
      "Iteration 10026, Loss: 0.055762212723493576\n",
      "Iteration 10027, Loss: 0.05575088784098625\n",
      "Iteration 10028, Loss: 0.05562460795044899\n",
      "Iteration 10029, Loss: 0.055828772485256195\n",
      "Iteration 10030, Loss: 0.05590590089559555\n",
      "Iteration 10031, Loss: 0.05585118383169174\n",
      "Iteration 10032, Loss: 0.055703483521938324\n",
      "Iteration 10033, Loss: 0.05578434467315674\n",
      "Iteration 10034, Loss: 0.05587991327047348\n",
      "Iteration 10035, Loss: 0.05582603067159653\n",
      "Iteration 10036, Loss: 0.05563275143504143\n",
      "Iteration 10037, Loss: 0.055876974016427994\n",
      "Iteration 10038, Loss: 0.056016288697719574\n",
      "Iteration 10039, Loss: 0.05602455139160156\n",
      "Iteration 10040, Loss: 0.05592072010040283\n",
      "Iteration 10041, Loss: 0.05574027821421623\n",
      "Iteration 10042, Loss: 0.0557967834174633\n",
      "Iteration 10043, Loss: 0.05593069642782211\n",
      "Iteration 10044, Loss: 0.05591022968292236\n",
      "Iteration 10045, Loss: 0.055740080773830414\n",
      "Iteration 10046, Loss: 0.055764876306056976\n",
      "Iteration 10047, Loss: 0.055887024849653244\n",
      "Iteration 10048, Loss: 0.055892109870910645\n",
      "Iteration 10049, Loss: 0.055798254907131195\n",
      "Iteration 10050, Loss: 0.05567272752523422\n",
      "Iteration 10051, Loss: 0.05581001564860344\n",
      "Iteration 10052, Loss: 0.05584875866770744\n",
      "Iteration 10053, Loss: 0.055722832679748535\n",
      "Iteration 10054, Loss: 0.055743735283613205\n",
      "Iteration 10055, Loss: 0.05583751201629639\n",
      "Iteration 10056, Loss: 0.05582992359995842\n",
      "Iteration 10057, Loss: 0.05573292821645737\n",
      "Iteration 10058, Loss: 0.055698953568935394\n",
      "Iteration 10059, Loss: 0.0557551383972168\n",
      "Iteration 10060, Loss: 0.05565691366791725\n",
      "Iteration 10061, Loss: 0.0557558536529541\n",
      "Iteration 10062, Loss: 0.05581895634531975\n",
      "Iteration 10063, Loss: 0.05577997490763664\n",
      "Iteration 10064, Loss: 0.055649321526288986\n",
      "Iteration 10065, Loss: 0.05584506317973137\n",
      "Iteration 10066, Loss: 0.05592958256602287\n",
      "Iteration 10067, Loss: 0.0558319091796875\n",
      "Iteration 10068, Loss: 0.055642012506723404\n",
      "Iteration 10069, Loss: 0.055730462074279785\n",
      "Iteration 10070, Loss: 0.05571627616882324\n",
      "Iteration 10071, Loss: 0.05562031641602516\n",
      "Iteration 10072, Loss: 0.05564550682902336\n",
      "Iteration 10073, Loss: 0.055672887712717056\n",
      "Iteration 10074, Loss: 0.055642008781433105\n",
      "Iteration 10075, Loss: 0.05573320388793945\n",
      "Iteration 10076, Loss: 0.05572656914591789\n",
      "Iteration 10077, Loss: 0.055647097527980804\n",
      "Iteration 10078, Loss: 0.055679600685834885\n",
      "Iteration 10079, Loss: 0.05563334748148918\n",
      "Iteration 10080, Loss: 0.055670104920864105\n",
      "Iteration 10081, Loss: 0.05564252659678459\n",
      "Iteration 10082, Loss: 0.055728912353515625\n",
      "Iteration 10083, Loss: 0.055702488869428635\n",
      "Iteration 10084, Loss: 0.05568715184926987\n",
      "Iteration 10085, Loss: 0.0557224377989769\n",
      "Iteration 10086, Loss: 0.05565941333770752\n",
      "Iteration 10087, Loss: 0.05574914067983627\n",
      "Iteration 10088, Loss: 0.05576201528310776\n",
      "Iteration 10089, Loss: 0.05562388896942139\n",
      "Iteration 10090, Loss: 0.05573749542236328\n",
      "Iteration 10091, Loss: 0.0557408332824707\n",
      "Iteration 10092, Loss: 0.05563926696777344\n",
      "Iteration 10093, Loss: 0.055792056024074554\n",
      "Iteration 10094, Loss: 0.055825792253017426\n",
      "Iteration 10095, Loss: 0.05569931119680405\n",
      "Iteration 10096, Loss: 0.05575740337371826\n",
      "Iteration 10097, Loss: 0.05584995076060295\n",
      "Iteration 10098, Loss: 0.05582813546061516\n",
      "Iteration 10099, Loss: 0.055708132684230804\n",
      "Iteration 10100, Loss: 0.055750928819179535\n",
      "Iteration 10101, Loss: 0.05582726374268532\n",
      "Iteration 10102, Loss: 0.05573801323771477\n",
      "Iteration 10103, Loss: 0.05570129677653313\n",
      "Iteration 10104, Loss: 0.05577155202627182\n",
      "Iteration 10105, Loss: 0.055733323097229004\n",
      "Iteration 10106, Loss: 0.055640701204538345\n",
      "Iteration 10107, Loss: 0.055730462074279785\n",
      "Iteration 10108, Loss: 0.055679284036159515\n",
      "Iteration 10109, Loss: 0.05571969598531723\n",
      "Iteration 10110, Loss: 0.055767420679330826\n",
      "Iteration 10111, Loss: 0.05571683496236801\n",
      "Iteration 10112, Loss: 0.055657826364040375\n",
      "Iteration 10113, Loss: 0.055658381432294846\n",
      "Iteration 10114, Loss: 0.05570002645254135\n",
      "Iteration 10115, Loss: 0.05571659654378891\n",
      "Iteration 10116, Loss: 0.055636487901210785\n",
      "Iteration 10117, Loss: 0.055800557136535645\n",
      "Iteration 10118, Loss: 0.05582956597208977\n",
      "Iteration 10119, Loss: 0.055683016777038574\n",
      "Iteration 10120, Loss: 0.055782757699489594\n",
      "Iteration 10121, Loss: 0.0558907613158226\n",
      "Iteration 10122, Loss: 0.05589083954691887\n",
      "Iteration 10123, Loss: 0.0557941198348999\n",
      "Iteration 10124, Loss: 0.055631641298532486\n",
      "Iteration 10125, Loss: 0.05584828183054924\n",
      "Iteration 10126, Loss: 0.05590236186981201\n",
      "Iteration 10127, Loss: 0.05578240007162094\n",
      "Iteration 10128, Loss: 0.05569358915090561\n",
      "Iteration 10129, Loss: 0.05578649044036865\n",
      "Iteration 10130, Loss: 0.055780213326215744\n",
      "Iteration 10131, Loss: 0.055680159479379654\n",
      "Iteration 10132, Loss: 0.055768292397260666\n",
      "Iteration 10133, Loss: 0.055822137743234634\n",
      "Iteration 10134, Loss: 0.05570685863494873\n",
      "Iteration 10135, Loss: 0.05574238300323486\n",
      "Iteration 10136, Loss: 0.055828969925642014\n",
      "Iteration 10137, Loss: 0.05580981820821762\n",
      "Iteration 10138, Loss: 0.0556948184967041\n",
      "Iteration 10139, Loss: 0.05576280876994133\n",
      "Iteration 10140, Loss: 0.05583234876394272\n",
      "Iteration 10141, Loss: 0.05572943016886711\n",
      "Iteration 10142, Loss: 0.055717866867780685\n",
      "Iteration 10143, Loss: 0.0557984933257103\n",
      "Iteration 10144, Loss: 0.05577341839671135\n",
      "Iteration 10145, Loss: 0.05565536022186279\n",
      "Iteration 10146, Loss: 0.05581725016236305\n",
      "Iteration 10147, Loss: 0.05588698387145996\n",
      "Iteration 10148, Loss: 0.0557815246284008\n",
      "Iteration 10149, Loss: 0.05568091198801994\n",
      "Iteration 10150, Loss: 0.055764757096767426\n",
      "Iteration 10151, Loss: 0.05574476718902588\n",
      "Iteration 10152, Loss: 0.055630289018154144\n",
      "Iteration 10153, Loss: 0.05584748834371567\n",
      "Iteration 10154, Loss: 0.055914007127285004\n",
      "Iteration 10155, Loss: 0.05580524727702141\n",
      "Iteration 10156, Loss: 0.05566740408539772\n",
      "Iteration 10157, Loss: 0.05575649067759514\n",
      "Iteration 10158, Loss: 0.05574437230825424\n",
      "Iteration 10159, Loss: 0.05563541501760483\n",
      "Iteration 10160, Loss: 0.0558340959250927\n",
      "Iteration 10161, Loss: 0.055898550897836685\n",
      "Iteration 10162, Loss: 0.05579420179128647\n",
      "Iteration 10163, Loss: 0.05567260831594467\n",
      "Iteration 10164, Loss: 0.05576137825846672\n",
      "Iteration 10165, Loss: 0.05574890226125717\n",
      "Iteration 10166, Loss: 0.055633544921875\n",
      "Iteration 10167, Loss: 0.05583878606557846\n",
      "Iteration 10168, Loss: 0.05591233819723129\n",
      "Iteration 10169, Loss: 0.0558268241584301\n",
      "Iteration 10170, Loss: 0.05565508455038071\n",
      "Iteration 10171, Loss: 0.05579312890768051\n",
      "Iteration 10172, Loss: 0.0558466911315918\n",
      "Iteration 10173, Loss: 0.055769287049770355\n",
      "Iteration 10174, Loss: 0.05564995855093002\n",
      "Iteration 10175, Loss: 0.05571592226624489\n",
      "Iteration 10176, Loss: 0.055638670921325684\n",
      "Iteration 10177, Loss: 0.05577695369720459\n",
      "Iteration 10178, Loss: 0.055834412574768066\n",
      "Iteration 10179, Loss: 0.05577767267823219\n",
      "Iteration 10180, Loss: 0.05565746873617172\n",
      "Iteration 10181, Loss: 0.055794280022382736\n",
      "Iteration 10182, Loss: 0.05582793802022934\n",
      "Iteration 10183, Loss: 0.055695176124572754\n",
      "Iteration 10184, Loss: 0.055766742676496506\n",
      "Iteration 10185, Loss: 0.05586576461791992\n",
      "Iteration 10186, Loss: 0.05585801973938942\n",
      "Iteration 10187, Loss: 0.055757008492946625\n",
      "Iteration 10188, Loss: 0.055674754083156586\n",
      "Iteration 10189, Loss: 0.05575251951813698\n",
      "Iteration 10190, Loss: 0.05568067356944084\n",
      "Iteration 10191, Loss: 0.05573161691427231\n",
      "Iteration 10192, Loss: 0.05578871816396713\n",
      "Iteration 10193, Loss: 0.05574830621480942\n",
      "Iteration 10194, Loss: 0.05562881752848625\n",
      "Iteration 10195, Loss: 0.055826585739851\n",
      "Iteration 10196, Loss: 0.05586811155080795\n",
      "Iteration 10197, Loss: 0.055741868913173676\n",
      "Iteration 10198, Loss: 0.05572545528411865\n",
      "Iteration 10199, Loss: 0.05582010746002197\n",
      "Iteration 10200, Loss: 0.055804215371608734\n",
      "Iteration 10201, Loss: 0.055686675012111664\n",
      "Iteration 10202, Loss: 0.05577639862895012\n",
      "Iteration 10203, Loss: 0.05584820359945297\n",
      "Iteration 10204, Loss: 0.055748146027326584\n",
      "Iteration 10205, Loss: 0.05570141598582268\n",
      "Iteration 10206, Loss: 0.05578053370118141\n",
      "Iteration 10207, Loss: 0.05574914067983627\n",
      "Iteration 10208, Loss: 0.05562317371368408\n",
      "Iteration 10209, Loss: 0.05584299564361572\n",
      "Iteration 10210, Loss: 0.055892229080200195\n",
      "Iteration 10211, Loss: 0.05576324462890625\n",
      "Iteration 10212, Loss: 0.055711470544338226\n",
      "Iteration 10213, Loss: 0.05580934137105942\n",
      "Iteration 10214, Loss: 0.05580095574259758\n",
      "Iteration 10215, Loss: 0.055698197335004807\n",
      "Iteration 10216, Loss: 0.0557454451918602\n",
      "Iteration 10217, Loss: 0.05579996481537819\n",
      "Iteration 10218, Loss: 0.05567709729075432\n",
      "Iteration 10219, Loss: 0.05577174946665764\n",
      "Iteration 10220, Loss: 0.05586576461791992\n",
      "Iteration 10221, Loss: 0.055853646248579025\n",
      "Iteration 10222, Loss: 0.0557476282119751\n",
      "Iteration 10223, Loss: 0.05568496510386467\n",
      "Iteration 10224, Loss: 0.05574842542409897\n",
      "Iteration 10225, Loss: 0.05564646050333977\n",
      "Iteration 10226, Loss: 0.05576721951365471\n",
      "Iteration 10227, Loss: 0.05583576485514641\n",
      "Iteration 10228, Loss: 0.055803339928388596\n",
      "Iteration 10229, Loss: 0.05567976087331772\n",
      "Iteration 10230, Loss: 0.05579523369669914\n",
      "Iteration 10231, Loss: 0.05587279796600342\n",
      "Iteration 10232, Loss: 0.05577119439840317\n",
      "Iteration 10233, Loss: 0.05568727105855942\n",
      "Iteration 10234, Loss: 0.055768728256225586\n",
      "Iteration 10235, Loss: 0.05574854463338852\n",
      "Iteration 10236, Loss: 0.0556359700858593\n",
      "Iteration 10237, Loss: 0.055840812623500824\n",
      "Iteration 10238, Loss: 0.055906932801008224\n",
      "Iteration 10239, Loss: 0.05579634755849838\n",
      "Iteration 10240, Loss: 0.05567535012960434\n",
      "Iteration 10241, Loss: 0.05576368421316147\n",
      "Iteration 10242, Loss: 0.05575096607208252\n",
      "Iteration 10243, Loss: 0.055643439292907715\n",
      "Iteration 10244, Loss: 0.05582332983613014\n",
      "Iteration 10245, Loss: 0.0558856762945652\n",
      "Iteration 10246, Loss: 0.055775485932826996\n",
      "Iteration 10247, Loss: 0.055690210312604904\n",
      "Iteration 10248, Loss: 0.05577715486288071\n",
      "Iteration 10249, Loss: 0.055761098861694336\n",
      "Iteration 10250, Loss: 0.055647771805524826\n",
      "Iteration 10251, Loss: 0.055824004113674164\n",
      "Iteration 10252, Loss: 0.05589286610484123\n",
      "Iteration 10253, Loss: 0.05579265207052231\n",
      "Iteration 10254, Loss: 0.055671576410532\n",
      "Iteration 10255, Loss: 0.055758003145456314\n",
      "Iteration 10256, Loss: 0.05574290081858635\n",
      "Iteration 10257, Loss: 0.05562480539083481\n",
      "Iteration 10258, Loss: 0.055850666016340256\n",
      "Iteration 10259, Loss: 0.05592648312449455\n",
      "Iteration 10260, Loss: 0.05584486573934555\n",
      "Iteration 10261, Loss: 0.05566108599305153\n",
      "Iteration 10262, Loss: 0.05581697076559067\n",
      "Iteration 10263, Loss: 0.05590764805674553\n",
      "Iteration 10264, Loss: 0.05586342141032219\n",
      "Iteration 10265, Loss: 0.055702488869428635\n",
      "Iteration 10266, Loss: 0.055792491883039474\n",
      "Iteration 10267, Loss: 0.05590609833598137\n",
      "Iteration 10268, Loss: 0.05586811155080795\n",
      "Iteration 10269, Loss: 0.05569903180003166\n",
      "Iteration 10270, Loss: 0.05579666420817375\n",
      "Iteration 10271, Loss: 0.05591583251953125\n",
      "Iteration 10272, Loss: 0.05589584633708\n",
      "Iteration 10273, Loss: 0.05575625225901604\n",
      "Iteration 10274, Loss: 0.05572069063782692\n",
      "Iteration 10275, Loss: 0.05582316964864731\n",
      "Iteration 10276, Loss: 0.05578359216451645\n",
      "Iteration 10277, Loss: 0.055616024881601334\n",
      "Iteration 10278, Loss: 0.05564475059509277\n",
      "Iteration 10279, Loss: 0.0556691512465477\n",
      "Iteration 10280, Loss: 0.055618766695261\n",
      "Iteration 10281, Loss: 0.05572092533111572\n",
      "Iteration 10282, Loss: 0.055661916732788086\n",
      "Iteration 10283, Loss: 0.055740199983119965\n",
      "Iteration 10284, Loss: 0.055790066719055176\n",
      "Iteration 10285, Loss: 0.05573264881968498\n",
      "Iteration 10286, Loss: 0.05565568059682846\n",
      "Iteration 10287, Loss: 0.05569998547434807\n",
      "Iteration 10288, Loss: 0.0556466206908226\n",
      "Iteration 10289, Loss: 0.05567821115255356\n",
      "Iteration 10290, Loss: 0.05565234273672104\n",
      "Iteration 10291, Loss: 0.05571945756673813\n",
      "Iteration 10292, Loss: 0.0557047538459301\n",
      "Iteration 10293, Loss: 0.05567201226949692\n",
      "Iteration 10294, Loss: 0.05569426342844963\n",
      "Iteration 10295, Loss: 0.05562242120504379\n",
      "Iteration 10296, Loss: 0.055714529007673264\n",
      "Iteration 10297, Loss: 0.055709682404994965\n",
      "Iteration 10298, Loss: 0.05563811585307121\n",
      "Iteration 10299, Loss: 0.055726371705532074\n",
      "Iteration 10300, Loss: 0.05567892640829086\n",
      "Iteration 10301, Loss: 0.05571750923991203\n",
      "Iteration 10302, Loss: 0.055762410163879395\n",
      "Iteration 10303, Loss: 0.05570904538035393\n",
      "Iteration 10304, Loss: 0.05567312240600586\n",
      "Iteration 10305, Loss: 0.05567558854818344\n",
      "Iteration 10306, Loss: 0.05568806454539299\n",
      "Iteration 10307, Loss: 0.05570511147379875\n",
      "Iteration 10308, Loss: 0.05562611669301987\n",
      "Iteration 10309, Loss: 0.055813949555158615\n",
      "Iteration 10310, Loss: 0.0558420829474926\n",
      "Iteration 10311, Loss: 0.055697087198495865\n",
      "Iteration 10312, Loss: 0.0557711161673069\n",
      "Iteration 10313, Loss: 0.05587780848145485\n",
      "Iteration 10314, Loss: 0.05587872117757797\n",
      "Iteration 10315, Loss: 0.05578470230102539\n",
      "Iteration 10316, Loss: 0.05562543869018555\n",
      "Iteration 10317, Loss: 0.05577687546610832\n",
      "Iteration 10318, Loss: 0.05575776472687721\n",
      "Iteration 10319, Loss: 0.05564785376191139\n",
      "Iteration 10320, Loss: 0.055700819939374924\n",
      "Iteration 10321, Loss: 0.055662672966718674\n",
      "Iteration 10322, Loss: 0.05572366714477539\n",
      "Iteration 10323, Loss: 0.055723272264003754\n",
      "Iteration 10324, Loss: 0.05565246194601059\n",
      "Iteration 10325, Loss: 0.05568937584757805\n",
      "Iteration 10326, Loss: 0.05563442036509514\n",
      "Iteration 10327, Loss: 0.05574158951640129\n",
      "Iteration 10328, Loss: 0.055765073746442795\n",
      "Iteration 10329, Loss: 0.05568913742899895\n",
      "Iteration 10330, Loss: 0.055726729333400726\n",
      "Iteration 10331, Loss: 0.05575649067759514\n",
      "Iteration 10332, Loss: 0.0556286983191967\n",
      "Iteration 10333, Loss: 0.05580691620707512\n",
      "Iteration 10334, Loss: 0.055896006524562836\n",
      "Iteration 10335, Loss: 0.05587991327047348\n",
      "Iteration 10336, Loss: 0.055771391838788986\n",
      "Iteration 10337, Loss: 0.055666010826826096\n",
      "Iteration 10338, Loss: 0.055757563561201096\n",
      "Iteration 10339, Loss: 0.05569462105631828\n",
      "Iteration 10340, Loss: 0.055719856172800064\n",
      "Iteration 10341, Loss: 0.055774372071027756\n",
      "Iteration 10342, Loss: 0.05573205277323723\n",
      "Iteration 10343, Loss: 0.05563656613230705\n",
      "Iteration 10344, Loss: 0.05570443719625473\n",
      "Iteration 10345, Loss: 0.055657509714365005\n",
      "Iteration 10346, Loss: 0.055717986077070236\n",
      "Iteration 10347, Loss: 0.05572613328695297\n",
      "Iteration 10348, Loss: 0.05562007427215576\n",
      "Iteration 10349, Loss: 0.05574500560760498\n",
      "Iteration 10350, Loss: 0.05570113658905029\n",
      "Iteration 10351, Loss: 0.05570225045084953\n",
      "Iteration 10352, Loss: 0.055747270584106445\n",
      "Iteration 10353, Loss: 0.055690567940473557\n",
      "Iteration 10354, Loss: 0.0557025671005249\n",
      "Iteration 10355, Loss: 0.055709123611450195\n",
      "Iteration 10356, Loss: 0.05566263571381569\n",
      "Iteration 10357, Loss: 0.0556773766875267\n",
      "Iteration 10358, Loss: 0.05563708394765854\n",
      "Iteration 10359, Loss: 0.055659376084804535\n",
      "Iteration 10360, Loss: 0.05564209073781967\n",
      "Iteration 10361, Loss: 0.05565552040934563\n",
      "Iteration 10362, Loss: 0.05566287413239479\n",
      "Iteration 10363, Loss: 0.05564407631754875\n",
      "Iteration 10364, Loss: 0.055713456124067307\n",
      "Iteration 10365, Loss: 0.055670104920864105\n",
      "Iteration 10366, Loss: 0.05572501942515373\n",
      "Iteration 10367, Loss: 0.05577151104807854\n",
      "Iteration 10368, Loss: 0.05571949854493141\n",
      "Iteration 10369, Loss: 0.055657148361206055\n",
      "Iteration 10370, Loss: 0.055659811943769455\n",
      "Iteration 10371, Loss: 0.055700063705444336\n",
      "Iteration 10372, Loss: 0.05571695417165756\n",
      "Iteration 10373, Loss: 0.05563926696777344\n",
      "Iteration 10374, Loss: 0.05579721927642822\n",
      "Iteration 10375, Loss: 0.055824439972639084\n",
      "Iteration 10376, Loss: 0.055677615106105804\n",
      "Iteration 10377, Loss: 0.05578768625855446\n",
      "Iteration 10378, Loss: 0.055895011872053146\n",
      "Iteration 10379, Loss: 0.055897198617458344\n",
      "Iteration 10380, Loss: 0.055804293602705\n",
      "Iteration 10381, Loss: 0.055628858506679535\n",
      "Iteration 10382, Loss: 0.055923860520124435\n",
      "Iteration 10383, Loss: 0.056050341576337814\n",
      "Iteration 10384, Loss: 0.0559898242354393\n",
      "Iteration 10385, Loss: 0.05576133728027344\n",
      "Iteration 10386, Loss: 0.05578025430440903\n",
      "Iteration 10387, Loss: 0.055937331169843674\n",
      "Iteration 10388, Loss: 0.0559845007956028\n",
      "Iteration 10389, Loss: 0.055931609123945236\n",
      "Iteration 10390, Loss: 0.05578979104757309\n",
      "Iteration 10391, Loss: 0.05567105859518051\n",
      "Iteration 10392, Loss: 0.0557689294219017\n",
      "Iteration 10393, Loss: 0.05568540096282959\n",
      "Iteration 10394, Loss: 0.05573984235525131\n",
      "Iteration 10395, Loss: 0.05580977723002434\n",
      "Iteration 10396, Loss: 0.0557788610458374\n",
      "Iteration 10397, Loss: 0.05565711110830307\n",
      "Iteration 10398, Loss: 0.055826425552368164\n",
      "Iteration 10399, Loss: 0.05590164661407471\n",
      "Iteration 10400, Loss: 0.05579674243927002\n",
      "Iteration 10401, Loss: 0.055672965943813324\n",
      "Iteration 10402, Loss: 0.05575696751475334\n",
      "Iteration 10403, Loss: 0.055739641189575195\n",
      "Iteration 10404, Loss: 0.055630289018154144\n",
      "Iteration 10405, Loss: 0.05584649369120598\n",
      "Iteration 10406, Loss: 0.0559084415435791\n",
      "Iteration 10407, Loss: 0.055794477462768555\n",
      "Iteration 10408, Loss: 0.05568063259124756\n",
      "Iteration 10409, Loss: 0.0557706356048584\n",
      "Iteration 10410, Loss: 0.05575919151306152\n",
      "Iteration 10411, Loss: 0.05565321445465088\n",
      "Iteration 10412, Loss: 0.05581160634756088\n",
      "Iteration 10413, Loss: 0.05587192624807358\n",
      "Iteration 10414, Loss: 0.05576050654053688\n",
      "Iteration 10415, Loss: 0.055702727288007736\n",
      "Iteration 10416, Loss: 0.055789511650800705\n",
      "Iteration 10417, Loss: 0.05577222630381584\n",
      "Iteration 10418, Loss: 0.055658500641584396\n",
      "Iteration 10419, Loss: 0.05581200495362282\n",
      "Iteration 10420, Loss: 0.05588055029511452\n",
      "Iteration 10421, Loss: 0.05577775090932846\n",
      "Iteration 10422, Loss: 0.05568365380167961\n",
      "Iteration 10423, Loss: 0.05576694384217262\n",
      "Iteration 10424, Loss: 0.05574631690979004\n",
      "Iteration 10425, Loss: 0.055626992136240005\n",
      "Iteration 10426, Loss: 0.05585750192403793\n",
      "Iteration 10427, Loss: 0.05593232437968254\n",
      "Iteration 10428, Loss: 0.05583878606557846\n",
      "Iteration 10429, Loss: 0.055648963898420334\n",
      "Iteration 10430, Loss: 0.05578780174255371\n",
      "Iteration 10431, Loss: 0.05583743378520012\n",
      "Iteration 10432, Loss: 0.055766068398952484\n",
      "Iteration 10433, Loss: 0.05564431473612785\n",
      "Iteration 10434, Loss: 0.05572188273072243\n",
      "Iteration 10435, Loss: 0.05565079301595688\n",
      "Iteration 10436, Loss: 0.055758122354745865\n",
      "Iteration 10437, Loss: 0.055817048996686935\n",
      "Iteration 10438, Loss: 0.055773936212062836\n",
      "Iteration 10439, Loss: 0.05565035343170166\n",
      "Iteration 10440, Loss: 0.05582737922668457\n",
      "Iteration 10441, Loss: 0.05588984489440918\n",
      "Iteration 10442, Loss: 0.05577409639954567\n",
      "Iteration 10443, Loss: 0.05569692701101303\n",
      "Iteration 10444, Loss: 0.05578669160604477\n",
      "Iteration 10445, Loss: 0.055773936212062836\n",
      "Iteration 10446, Loss: 0.0556696280837059\n",
      "Iteration 10447, Loss: 0.05578804016113281\n",
      "Iteration 10448, Loss: 0.05584363266825676\n",
      "Iteration 10449, Loss: 0.05572088807821274\n",
      "Iteration 10450, Loss: 0.05574023723602295\n",
      "Iteration 10451, Loss: 0.0558343343436718\n",
      "Iteration 10452, Loss: 0.05582503601908684\n",
      "Iteration 10453, Loss: 0.05572287365794182\n",
      "Iteration 10454, Loss: 0.05571329966187477\n",
      "Iteration 10455, Loss: 0.05576741695404053\n",
      "Iteration 10456, Loss: 0.055643480271101\n",
      "Iteration 10457, Loss: 0.05579805374145508\n",
      "Iteration 10458, Loss: 0.05589282512664795\n",
      "Iteration 10459, Loss: 0.05588368698954582\n",
      "Iteration 10460, Loss: 0.0557812862098217\n",
      "Iteration 10461, Loss: 0.05563469976186752\n",
      "Iteration 10462, Loss: 0.055691443383693695\n",
      "Iteration 10463, Loss: 0.055643241852521896\n",
      "Iteration 10464, Loss: 0.055632829666137695\n",
      "Iteration 10465, Loss: 0.055721085518598557\n",
      "Iteration 10466, Loss: 0.055672965943813324\n",
      "Iteration 10467, Loss: 0.05572529882192612\n",
      "Iteration 10468, Loss: 0.055772583931684494\n",
      "Iteration 10469, Loss: 0.05571810528635979\n",
      "Iteration 10470, Loss: 0.055664341896772385\n",
      "Iteration 10471, Loss: 0.05567380040884018\n",
      "Iteration 10472, Loss: 0.055685997009277344\n",
      "Iteration 10473, Loss: 0.055697839707136154\n",
      "Iteration 10474, Loss: 0.0556286983191967\n",
      "Iteration 10475, Loss: 0.055760543793439865\n",
      "Iteration 10476, Loss: 0.05574445053935051\n",
      "Iteration 10477, Loss: 0.05565289780497551\n",
      "Iteration 10478, Loss: 0.0556972436606884\n",
      "Iteration 10479, Loss: 0.05564944073557854\n",
      "Iteration 10480, Loss: 0.05574854463338852\n",
      "Iteration 10481, Loss: 0.05576125904917717\n",
      "Iteration 10482, Loss: 0.055652420967817307\n",
      "Iteration 10483, Loss: 0.05576976388692856\n",
      "Iteration 10484, Loss: 0.055811844766139984\n",
      "Iteration 10485, Loss: 0.05571727082133293\n",
      "Iteration 10486, Loss: 0.05571834370493889\n",
      "Iteration 10487, Loss: 0.05578240007162094\n",
      "Iteration 10488, Loss: 0.05571075528860092\n",
      "Iteration 10489, Loss: 0.05570455640554428\n",
      "Iteration 10490, Loss: 0.05574914067983627\n",
      "Iteration 10491, Loss: 0.05565957352519035\n",
      "Iteration 10492, Loss: 0.055769406259059906\n",
      "Iteration 10493, Loss: 0.0558219775557518\n",
      "Iteration 10494, Loss: 0.055737100541591644\n",
      "Iteration 10495, Loss: 0.05569251626729965\n",
      "Iteration 10496, Loss: 0.055754583328962326\n",
      "Iteration 10497, Loss: 0.05568452924489975\n",
      "Iteration 10498, Loss: 0.05573352426290512\n",
      "Iteration 10499, Loss: 0.05577763170003891\n",
      "Iteration 10500, Loss: 0.05569390580058098\n",
      "Iteration 10501, Loss: 0.05572899430990219\n",
      "Iteration 10502, Loss: 0.055777907371520996\n",
      "Iteration 10503, Loss: 0.0556865930557251\n",
      "Iteration 10504, Loss: 0.05574731156229973\n",
      "Iteration 10505, Loss: 0.05580870434641838\n",
      "Iteration 10506, Loss: 0.05573856830596924\n",
      "Iteration 10507, Loss: 0.05567678064107895\n",
      "Iteration 10508, Loss: 0.05573094263672829\n",
      "Iteration 10509, Loss: 0.055651191622018814\n",
      "Iteration 10510, Loss: 0.05577131360769272\n",
      "Iteration 10511, Loss: 0.05582837387919426\n",
      "Iteration 10512, Loss: 0.05576853081583977\n",
      "Iteration 10513, Loss: 0.05565878003835678\n",
      "Iteration 10514, Loss: 0.0557764396071434\n",
      "Iteration 10515, Loss: 0.05577874556183815\n",
      "Iteration 10516, Loss: 0.05563656613230705\n",
      "Iteration 10517, Loss: 0.05576527118682861\n",
      "Iteration 10518, Loss: 0.055820267647504807\n",
      "Iteration 10519, Loss: 0.05577632039785385\n",
      "Iteration 10520, Loss: 0.05564359948039055\n",
      "Iteration 10521, Loss: 0.05585793778300285\n",
      "Iteration 10522, Loss: 0.05594515800476074\n",
      "Iteration 10523, Loss: 0.05585261434316635\n",
      "Iteration 10524, Loss: 0.05563453957438469\n",
      "Iteration 10525, Loss: 0.055778782814741135\n",
      "Iteration 10526, Loss: 0.055827539414167404\n",
      "Iteration 10527, Loss: 0.055767498910427094\n",
      "Iteration 10528, Loss: 0.05563315004110336\n",
      "Iteration 10529, Loss: 0.05580051988363266\n",
      "Iteration 10530, Loss: 0.055811647325754166\n",
      "Iteration 10531, Loss: 0.055650316178798676\n",
      "Iteration 10532, Loss: 0.0558171309530735\n",
      "Iteration 10533, Loss: 0.05593351647257805\n",
      "Iteration 10534, Loss: 0.055943649262189865\n",
      "Iteration 10535, Loss: 0.05585809797048569\n",
      "Iteration 10536, Loss: 0.05568742752075195\n",
      "Iteration 10537, Loss: 0.055845022201538086\n",
      "Iteration 10538, Loss: 0.05597396939992905\n",
      "Iteration 10539, Loss: 0.0559161901473999\n",
      "Iteration 10540, Loss: 0.05569104477763176\n",
      "Iteration 10541, Loss: 0.05582992359995842\n",
      "Iteration 10542, Loss: 0.055985335260629654\n",
      "Iteration 10543, Loss: 0.0560302771627903\n",
      "Iteration 10544, Loss: 0.055975478142499924\n",
      "Iteration 10545, Loss: 0.05583159253001213\n",
      "Iteration 10546, Loss: 0.055616896599531174\n",
      "Iteration 10547, Loss: 0.05572188273072243\n",
      "Iteration 10548, Loss: 0.05564844608306885\n",
      "Iteration 10549, Loss: 0.055759113281965256\n",
      "Iteration 10550, Loss: 0.05581974983215332\n",
      "Iteration 10551, Loss: 0.055777788162231445\n",
      "Iteration 10552, Loss: 0.05564848706126213\n",
      "Iteration 10553, Loss: 0.055841170251369476\n",
      "Iteration 10554, Loss: 0.05591881275177002\n",
      "Iteration 10555, Loss: 0.0558173693716526\n",
      "Iteration 10556, Loss: 0.05565488710999489\n",
      "Iteration 10557, Loss: 0.05573725700378418\n",
      "Iteration 10558, Loss: 0.05571750923991203\n",
      "Iteration 10559, Loss: 0.055622659623622894\n",
      "Iteration 10560, Loss: 0.05565011501312256\n",
      "Iteration 10561, Loss: 0.05566736310720444\n",
      "Iteration 10562, Loss: 0.055632274597883224\n",
      "Iteration 10563, Loss: 0.05575287342071533\n",
      "Iteration 10564, Loss: 0.05575060844421387\n",
      "Iteration 10565, Loss: 0.05564026162028313\n",
      "Iteration 10566, Loss: 0.0557381734251976\n",
      "Iteration 10567, Loss: 0.05572708696126938\n",
      "Iteration 10568, Loss: 0.05564375966787338\n",
      "Iteration 10569, Loss: 0.055652301758527756\n",
      "Iteration 10570, Loss: 0.05569076910614967\n",
      "Iteration 10571, Loss: 0.055666010826826096\n",
      "Iteration 10572, Loss: 0.055705271661281586\n",
      "Iteration 10573, Loss: 0.05571087449789047\n",
      "Iteration 10574, Loss: 0.05563819408416748\n",
      "Iteration 10575, Loss: 0.05566946789622307\n",
      "Iteration 10576, Loss: 0.05565802380442619\n",
      "Iteration 10577, Loss: 0.05564352124929428\n",
      "Iteration 10578, Loss: 0.05569986626505852\n",
      "Iteration 10579, Loss: 0.0556461438536644\n",
      "Iteration 10580, Loss: 0.05574965849518776\n",
      "Iteration 10581, Loss: 0.05580099672079086\n",
      "Iteration 10582, Loss: 0.055752914398908615\n",
      "Iteration 10583, Loss: 0.05562770366668701\n",
      "Iteration 10584, Loss: 0.055820904672145844\n",
      "Iteration 10585, Loss: 0.055848997086286545\n",
      "Iteration 10586, Loss: 0.05570431798696518\n",
      "Iteration 10587, Loss: 0.05576678365468979\n",
      "Iteration 10588, Loss: 0.0558726005256176\n",
      "Iteration 10589, Loss: 0.055869702249765396\n",
      "Iteration 10590, Loss: 0.055769287049770355\n",
      "Iteration 10591, Loss: 0.05565345287322998\n",
      "Iteration 10592, Loss: 0.05572478100657463\n",
      "Iteration 10593, Loss: 0.05564010143280029\n",
      "Iteration 10594, Loss: 0.055750809609889984\n",
      "Iteration 10595, Loss: 0.05579646676778793\n",
      "Iteration 10596, Loss: 0.05574067682027817\n",
      "Iteration 10597, Loss: 0.05563807487487793\n",
      "Iteration 10598, Loss: 0.05568333715200424\n",
      "Iteration 10599, Loss: 0.05565103143453598\n",
      "Iteration 10600, Loss: 0.05565110966563225\n",
      "Iteration 10601, Loss: 0.05567697808146477\n",
      "Iteration 10602, Loss: 0.055632591247558594\n",
      "Iteration 10603, Loss: 0.05570177361369133\n",
      "Iteration 10604, Loss: 0.055673640221357346\n",
      "Iteration 10605, Loss: 0.05570121854543686\n",
      "Iteration 10606, Loss: 0.05570534989237785\n",
      "Iteration 10607, Loss: 0.05564868822693825\n",
      "Iteration 10608, Loss: 0.05565591901540756\n",
      "Iteration 10609, Loss: 0.05568885803222656\n",
      "Iteration 10610, Loss: 0.055672209709882736\n",
      "Iteration 10611, Loss: 0.055685997009277344\n",
      "Iteration 10612, Loss: 0.05567590519785881\n",
      "Iteration 10613, Loss: 0.05569052696228027\n",
      "Iteration 10614, Loss: 0.055692754685878754\n",
      "Iteration 10615, Loss: 0.0556538924574852\n",
      "Iteration 10616, Loss: 0.055659376084804535\n",
      "Iteration 10617, Loss: 0.055686235427856445\n",
      "Iteration 10618, Loss: 0.055677734315395355\n",
      "Iteration 10619, Loss: 0.05566863343119621\n",
      "Iteration 10620, Loss: 0.055652618408203125\n",
      "Iteration 10621, Loss: 0.05571186915040016\n",
      "Iteration 10622, Loss: 0.055725496262311935\n",
      "Iteration 10623, Loss: 0.05565071478486061\n",
      "Iteration 10624, Loss: 0.05576471611857414\n",
      "Iteration 10625, Loss: 0.05577520653605461\n",
      "Iteration 10626, Loss: 0.05561721324920654\n",
      "Iteration 10627, Loss: 0.055816810578107834\n",
      "Iteration 10628, Loss: 0.055910270661115646\n",
      "Iteration 10629, Loss: 0.05589902400970459\n",
      "Iteration 10630, Loss: 0.05579396337270737\n",
      "Iteration 10631, Loss: 0.055626075714826584\n",
      "Iteration 10632, Loss: 0.05577782914042473\n",
      "Iteration 10633, Loss: 0.05576372519135475\n",
      "Iteration 10634, Loss: 0.055645428597927094\n",
      "Iteration 10635, Loss: 0.055725693702697754\n",
      "Iteration 10636, Loss: 0.05571683496236801\n",
      "Iteration 10637, Loss: 0.055636487901210785\n",
      "Iteration 10638, Loss: 0.05562269687652588\n",
      "Iteration 10639, Loss: 0.05573729798197746\n",
      "Iteration 10640, Loss: 0.05572231858968735\n",
      "Iteration 10641, Loss: 0.0556543692946434\n",
      "Iteration 10642, Loss: 0.055689893662929535\n",
      "Iteration 10643, Loss: 0.05563577264547348\n",
      "Iteration 10644, Loss: 0.055698834359645844\n",
      "Iteration 10645, Loss: 0.05569605156779289\n",
      "Iteration 10646, Loss: 0.055632513016462326\n",
      "Iteration 10647, Loss: 0.055679600685834885\n",
      "Iteration 10648, Loss: 0.0556408166885376\n",
      "Iteration 10649, Loss: 0.05570920556783676\n",
      "Iteration 10650, Loss: 0.055670421570539474\n",
      "Iteration 10651, Loss: 0.05572080612182617\n",
      "Iteration 10652, Loss: 0.05575506016612053\n",
      "Iteration 10653, Loss: 0.055684011429548264\n",
      "Iteration 10654, Loss: 0.055726371705532074\n",
      "Iteration 10655, Loss: 0.05575243756175041\n",
      "Iteration 10656, Loss: 0.055619362741708755\n",
      "Iteration 10657, Loss: 0.055832307785749435\n",
      "Iteration 10658, Loss: 0.05592819303274155\n",
      "Iteration 10659, Loss: 0.05590852349996567\n",
      "Iteration 10660, Loss: 0.055790744721889496\n",
      "Iteration 10661, Loss: 0.05567840859293938\n",
      "Iteration 10662, Loss: 0.05580171197652817\n",
      "Iteration 10663, Loss: 0.05579730123281479\n",
      "Iteration 10664, Loss: 0.05565556138753891\n",
      "Iteration 10665, Loss: 0.05577552318572998\n",
      "Iteration 10666, Loss: 0.05584931746125221\n",
      "Iteration 10667, Loss: 0.05582249164581299\n",
      "Iteration 10668, Loss: 0.05570495128631592\n",
      "Iteration 10669, Loss: 0.05575831979513168\n",
      "Iteration 10670, Loss: 0.055829405784606934\n",
      "Iteration 10671, Loss: 0.05572080612182617\n",
      "Iteration 10672, Loss: 0.055731140077114105\n",
      "Iteration 10673, Loss: 0.0558164119720459\n",
      "Iteration 10674, Loss: 0.05579861253499985\n",
      "Iteration 10675, Loss: 0.055687785148620605\n",
      "Iteration 10676, Loss: 0.05577151104807854\n",
      "Iteration 10677, Loss: 0.0558345727622509\n",
      "Iteration 10678, Loss: 0.05571794509887695\n",
      "Iteration 10679, Loss: 0.055738210678100586\n",
      "Iteration 10680, Loss: 0.055828653275966644\n",
      "Iteration 10681, Loss: 0.05581478402018547\n",
      "Iteration 10682, Loss: 0.055707816034555435\n",
      "Iteration 10683, Loss: 0.0557398796081543\n",
      "Iteration 10684, Loss: 0.05579889193177223\n",
      "Iteration 10685, Loss: 0.055679045617580414\n",
      "Iteration 10686, Loss: 0.05576936528086662\n",
      "Iteration 10687, Loss: 0.05586143583059311\n",
      "Iteration 10688, Loss: 0.05584915727376938\n",
      "Iteration 10689, Loss: 0.055743854492902756\n",
      "Iteration 10690, Loss: 0.05568941682577133\n",
      "Iteration 10691, Loss: 0.05574846267700195\n",
      "Iteration 10692, Loss: 0.055637240409851074\n",
      "Iteration 10693, Loss: 0.0557810477912426\n",
      "Iteration 10694, Loss: 0.0558544397354126\n",
      "Iteration 10695, Loss: 0.055825117975473404\n",
      "Iteration 10696, Loss: 0.05570344254374504\n",
      "Iteration 10697, Loss: 0.05575994774699211\n",
      "Iteration 10698, Loss: 0.055835168808698654\n",
      "Iteration 10699, Loss: 0.05573233217000961\n",
      "Iteration 10700, Loss: 0.05571679398417473\n",
      "Iteration 10701, Loss: 0.05579841509461403\n",
      "Iteration 10702, Loss: 0.05577699467539787\n",
      "Iteration 10703, Loss: 0.0556638278067112\n",
      "Iteration 10704, Loss: 0.05580385774374008\n",
      "Iteration 10705, Loss: 0.05586950108408928\n",
      "Iteration 10706, Loss: 0.055757127702236176\n",
      "Iteration 10707, Loss: 0.05570483207702637\n",
      "Iteration 10708, Loss: 0.055792294442653656\n",
      "Iteration 10709, Loss: 0.05577695369720459\n",
      "Iteration 10710, Loss: 0.05566871538758278\n",
      "Iteration 10711, Loss: 0.05579173564910889\n",
      "Iteration 10712, Loss: 0.055852413177490234\n",
      "Iteration 10713, Loss: 0.05573618412017822\n",
      "Iteration 10714, Loss: 0.055723194032907486\n",
      "Iteration 10715, Loss: 0.05581311509013176\n",
      "Iteration 10716, Loss: 0.05579960718750954\n",
      "Iteration 10717, Loss: 0.05569307133555412\n",
      "Iteration 10718, Loss: 0.055756572633981705\n",
      "Iteration 10719, Loss: 0.055815815925598145\n",
      "Iteration 10720, Loss: 0.05569839850068092\n",
      "Iteration 10721, Loss: 0.05575172230601311\n",
      "Iteration 10722, Loss: 0.0558420829474926\n",
      "Iteration 10723, Loss: 0.05582873150706291\n",
      "Iteration 10724, Loss: 0.05572231858968735\n",
      "Iteration 10725, Loss: 0.0557175874710083\n",
      "Iteration 10726, Loss: 0.055777352303266525\n",
      "Iteration 10727, Loss: 0.05566052719950676\n",
      "Iteration 10728, Loss: 0.05577985569834709\n",
      "Iteration 10729, Loss: 0.05587029829621315\n",
      "Iteration 10730, Loss: 0.05585702508687973\n",
      "Iteration 10731, Loss: 0.05575072765350342\n",
      "Iteration 10732, Loss: 0.055679917335510254\n",
      "Iteration 10733, Loss: 0.055741868913173676\n",
      "Iteration 10734, Loss: 0.055632591247558594\n",
      "Iteration 10735, Loss: 0.055784545838832855\n",
      "Iteration 10736, Loss: 0.055859487503767014\n",
      "Iteration 10737, Loss: 0.055830955505371094\n",
      "Iteration 10738, Loss: 0.055710356682538986\n",
      "Iteration 10739, Loss: 0.05575057119131088\n",
      "Iteration 10740, Loss: 0.055824361741542816\n",
      "Iteration 10741, Loss: 0.05571750923991203\n",
      "Iteration 10742, Loss: 0.055731337517499924\n",
      "Iteration 10743, Loss: 0.05581581965088844\n",
      "Iteration 10744, Loss: 0.055796824395656586\n",
      "Iteration 10745, Loss: 0.05568639561533928\n",
      "Iteration 10746, Loss: 0.055771470069885254\n",
      "Iteration 10747, Loss: 0.055833540856838226\n",
      "Iteration 10748, Loss: 0.05571627616882324\n",
      "Iteration 10749, Loss: 0.055738966912031174\n",
      "Iteration 10750, Loss: 0.05582960695028305\n",
      "Iteration 10751, Loss: 0.05581685155630112\n",
      "Iteration 10752, Loss: 0.05571162700653076\n",
      "Iteration 10753, Loss: 0.05573086068034172\n",
      "Iteration 10754, Loss: 0.055787961930036545\n",
      "Iteration 10755, Loss: 0.05566752329468727\n",
      "Iteration 10756, Loss: 0.05577616021037102\n",
      "Iteration 10757, Loss: 0.0558677539229393\n",
      "Iteration 10758, Loss: 0.05585603043437004\n",
      "Iteration 10759, Loss: 0.05575152486562729\n",
      "Iteration 10760, Loss: 0.05567578598856926\n",
      "Iteration 10761, Loss: 0.055732809007167816\n",
      "Iteration 10762, Loss: 0.05562257766723633\n",
      "Iteration 10763, Loss: 0.055757999420166016\n",
      "Iteration 10764, Loss: 0.05579141899943352\n",
      "Iteration 10765, Loss: 0.05571866035461426\n",
      "Iteration 10766, Loss: 0.05568818375468254\n",
      "Iteration 10767, Loss: 0.05572541803121567\n",
      "Iteration 10768, Loss: 0.055628977715969086\n",
      "Iteration 10769, Loss: 0.05571306124329567\n",
      "Iteration 10770, Loss: 0.055717431008815765\n",
      "Iteration 10771, Loss: 0.05562611669301987\n",
      "Iteration 10772, Loss: 0.0558294877409935\n",
      "Iteration 10773, Loss: 0.05587160587310791\n",
      "Iteration 10774, Loss: 0.055736981332302094\n",
      "Iteration 10775, Loss: 0.055734992027282715\n",
      "Iteration 10776, Loss: 0.05583604425191879\n",
      "Iteration 10777, Loss: 0.05582968518137932\n",
      "Iteration 10778, Loss: 0.05572700873017311\n",
      "Iteration 10779, Loss: 0.055707138031721115\n",
      "Iteration 10780, Loss: 0.0557628870010376\n",
      "Iteration 10781, Loss: 0.05564276501536369\n",
      "Iteration 10782, Loss: 0.055795274674892426\n",
      "Iteration 10783, Loss: 0.055886708199977875\n",
      "Iteration 10784, Loss: 0.0558728389441967\n",
      "Iteration 10785, Loss: 0.05576547235250473\n",
      "Iteration 10786, Loss: 0.055664896965026855\n",
      "Iteration 10787, Loss: 0.055739644914865494\n",
      "Iteration 10788, Loss: 0.05565786361694336\n",
      "Iteration 10789, Loss: 0.05574635788798332\n",
      "Iteration 10790, Loss: 0.055801354348659515\n",
      "Iteration 10791, Loss: 0.05575565621256828\n",
      "Iteration 10792, Loss: 0.05561944097280502\n",
      "Iteration 10793, Loss: 0.05588607117533684\n",
      "Iteration 10794, Loss: 0.05597102642059326\n",
      "Iteration 10795, Loss: 0.055875424295663834\n",
      "Iteration 10796, Loss: 0.05562957376241684\n",
      "Iteration 10797, Loss: 0.055852536112070084\n",
      "Iteration 10798, Loss: 0.055980127304792404\n",
      "Iteration 10799, Loss: 0.05599502846598625\n",
      "Iteration 10800, Loss: 0.05590923875570297\n",
      "Iteration 10801, Loss: 0.05573837086558342\n",
      "Iteration 10802, Loss: 0.05577639862895012\n",
      "Iteration 10803, Loss: 0.055905781686306\n",
      "Iteration 10804, Loss: 0.05586012452840805\n",
      "Iteration 10805, Loss: 0.055653177201747894\n",
      "Iteration 10806, Loss: 0.055848561227321625\n",
      "Iteration 10807, Loss: 0.055993400514125824\n",
      "Iteration 10808, Loss: 0.05602709576487541\n",
      "Iteration 10809, Loss: 0.05596105381846428\n",
      "Iteration 10810, Loss: 0.05580767244100571\n",
      "Iteration 10811, Loss: 0.055674951523542404\n",
      "Iteration 10812, Loss: 0.05581625550985336\n",
      "Iteration 10813, Loss: 0.05579789727926254\n",
      "Iteration 10814, Loss: 0.055649880319833755\n",
      "Iteration 10815, Loss: 0.0557628870010376\n",
      "Iteration 10816, Loss: 0.055816177278757095\n",
      "Iteration 10817, Loss: 0.05576483532786369\n",
      "Iteration 10818, Loss: 0.05561983957886696\n",
      "Iteration 10819, Loss: 0.055896203964948654\n",
      "Iteration 10820, Loss: 0.05599180981516838\n",
      "Iteration 10821, Loss: 0.05590435117483139\n",
      "Iteration 10822, Loss: 0.05566000938415527\n",
      "Iteration 10823, Loss: 0.055864255875349045\n",
      "Iteration 10824, Loss: 0.05602654069662094\n",
      "Iteration 10825, Loss: 0.05606798455119133\n",
      "Iteration 10826, Loss: 0.055999837815761566\n",
      "Iteration 10827, Loss: 0.055838268250226974\n",
      "Iteration 10828, Loss: 0.05566573143005371\n",
      "Iteration 10829, Loss: 0.05585896968841553\n",
      "Iteration 10830, Loss: 0.055909913033246994\n",
      "Iteration 10831, Loss: 0.05578815937042236\n",
      "Iteration 10832, Loss: 0.05569843575358391\n",
      "Iteration 10833, Loss: 0.0557916983962059\n",
      "Iteration 10834, Loss: 0.055791936814785004\n",
      "Iteration 10835, Loss: 0.05570618435740471\n",
      "Iteration 10836, Loss: 0.05572398751974106\n",
      "Iteration 10837, Loss: 0.05576173588633537\n",
      "Iteration 10838, Loss: 0.05565353482961655\n",
      "Iteration 10839, Loss: 0.05575438588857651\n",
      "Iteration 10840, Loss: 0.05580803006887436\n",
      "Iteration 10841, Loss: 0.05575672909617424\n",
      "Iteration 10842, Loss: 0.05562194436788559\n",
      "Iteration 10843, Loss: 0.05580071732401848\n",
      "Iteration 10844, Loss: 0.055804770439863205\n",
      "Iteration 10845, Loss: 0.055636487901210785\n",
      "Iteration 10846, Loss: 0.0558290109038353\n",
      "Iteration 10847, Loss: 0.05594726651906967\n",
      "Iteration 10848, Loss: 0.055957358330488205\n",
      "Iteration 10849, Loss: 0.05586957931518555\n",
      "Iteration 10850, Loss: 0.05569788068532944\n",
      "Iteration 10851, Loss: 0.05583107843995094\n",
      "Iteration 10852, Loss: 0.055960141122341156\n",
      "Iteration 10853, Loss: 0.055902086198329926\n",
      "Iteration 10854, Loss: 0.05567678064107895\n",
      "Iteration 10855, Loss: 0.055839382112026215\n",
      "Iteration 10856, Loss: 0.05599455162882805\n",
      "Iteration 10857, Loss: 0.056038420647382736\n",
      "Iteration 10858, Loss: 0.055981796234846115\n",
      "Iteration 10859, Loss: 0.05583612248301506\n",
      "Iteration 10860, Loss: 0.05563076585531235\n",
      "Iteration 10861, Loss: 0.055889569222927094\n",
      "Iteration 10862, Loss: 0.055979371070861816\n",
      "Iteration 10863, Loss: 0.05589441582560539\n",
      "Iteration 10864, Loss: 0.055670976638793945\n",
      "Iteration 10865, Loss: 0.055835288017988205\n",
      "Iteration 10866, Loss: 0.055973492562770844\n",
      "Iteration 10867, Loss: 0.055996060371398926\n",
      "Iteration 10868, Loss: 0.055914442986249924\n",
      "Iteration 10869, Loss: 0.05574246495962143\n",
      "Iteration 10870, Loss: 0.05576857179403305\n",
      "Iteration 10871, Loss: 0.05589890852570534\n",
      "Iteration 10872, Loss: 0.055855512619018555\n",
      "Iteration 10873, Loss: 0.05565408989787102\n",
      "Iteration 10874, Loss: 0.055847447365522385\n",
      "Iteration 10875, Loss: 0.055988989770412445\n",
      "Iteration 10876, Loss: 0.0560150146484375\n",
      "Iteration 10877, Loss: 0.055939000099897385\n",
      "Iteration 10878, Loss: 0.05577651783823967\n",
      "Iteration 10879, Loss: 0.055725693702697754\n",
      "Iteration 10880, Loss: 0.05585412308573723\n",
      "Iteration 10881, Loss: 0.055820148438215256\n",
      "Iteration 10882, Loss: 0.055645428597927094\n",
      "Iteration 10883, Loss: 0.05579746142029762\n",
      "Iteration 10884, Loss: 0.05588706582784653\n",
      "Iteration 10885, Loss: 0.05587371438741684\n",
      "Iteration 10886, Loss: 0.055767618119716644\n",
      "Iteration 10887, Loss: 0.055657386779785156\n",
      "Iteration 10888, Loss: 0.05572164058685303\n",
      "Iteration 10889, Loss: 0.05563148111104965\n",
      "Iteration 10890, Loss: 0.05573662370443344\n",
      "Iteration 10891, Loss: 0.055751364678144455\n",
      "Iteration 10892, Loss: 0.05565663427114487\n",
      "Iteration 10893, Loss: 0.05578196048736572\n",
      "Iteration 10894, Loss: 0.05582726001739502\n",
      "Iteration 10895, Loss: 0.055709801614284515\n",
      "Iteration 10896, Loss: 0.05574337765574455\n",
      "Iteration 10897, Loss: 0.05582992359995842\n",
      "Iteration 10898, Loss: 0.05579789727926254\n",
      "Iteration 10899, Loss: 0.05566597357392311\n",
      "Iteration 10900, Loss: 0.05581244081258774\n",
      "Iteration 10901, Loss: 0.05589274689555168\n",
      "Iteration 10902, Loss: 0.05580059811472893\n",
      "Iteration 10903, Loss: 0.0556563176214695\n",
      "Iteration 10904, Loss: 0.05573856830596924\n",
      "Iteration 10905, Loss: 0.055709920823574066\n",
      "Iteration 10906, Loss: 0.05565480515360832\n",
      "Iteration 10907, Loss: 0.05564781278371811\n",
      "Iteration 10908, Loss: 0.05571385473012924\n",
      "Iteration 10909, Loss: 0.055723510682582855\n",
      "Iteration 10910, Loss: 0.05563664436340332\n",
      "Iteration 10911, Loss: 0.05577854439616203\n",
      "Iteration 10912, Loss: 0.0557861365377903\n",
      "Iteration 10913, Loss: 0.055624764412641525\n",
      "Iteration 10914, Loss: 0.0558391809463501\n",
      "Iteration 10915, Loss: 0.0559520348906517\n",
      "Iteration 10916, Loss: 0.05594853684306145\n",
      "Iteration 10917, Loss: 0.05584331601858139\n",
      "Iteration 10918, Loss: 0.05567082017660141\n",
      "Iteration 10919, Loss: 0.055863939225673676\n",
      "Iteration 10920, Loss: 0.055976711213588715\n",
      "Iteration 10921, Loss: 0.0559060201048851\n",
      "Iteration 10922, Loss: 0.05567201226949692\n",
      "Iteration 10923, Loss: 0.05585018917918205\n",
      "Iteration 10924, Loss: 0.05601000785827637\n",
      "Iteration 10925, Loss: 0.05605626106262207\n",
      "Iteration 10926, Loss: 0.055999837815761566\n",
      "Iteration 10927, Loss: 0.05585428327322006\n",
      "Iteration 10928, Loss: 0.05565234273672104\n",
      "Iteration 10929, Loss: 0.055915914475917816\n",
      "Iteration 10930, Loss: 0.05604851245880127\n",
      "Iteration 10931, Loss: 0.05599598214030266\n",
      "Iteration 10932, Loss: 0.055781006813049316\n",
      "Iteration 10933, Loss: 0.055759232491254807\n",
      "Iteration 10934, Loss: 0.055907249450683594\n",
      "Iteration 10935, Loss: 0.05594853684306145\n",
      "Iteration 10936, Loss: 0.05589139461517334\n",
      "Iteration 10937, Loss: 0.055745843797922134\n",
      "Iteration 10938, Loss: 0.0557352714240551\n",
      "Iteration 10939, Loss: 0.05583735555410385\n",
      "Iteration 10940, Loss: 0.055760543793439865\n",
      "Iteration 10941, Loss: 0.05568019673228264\n",
      "Iteration 10942, Loss: 0.05574790760874748\n",
      "Iteration 10943, Loss: 0.055717866867780685\n",
      "Iteration 10944, Loss: 0.05563696473836899\n",
      "Iteration 10945, Loss: 0.055657029151916504\n",
      "Iteration 10946, Loss: 0.05565866082906723\n",
      "Iteration 10947, Loss: 0.0556306466460228\n",
      "Iteration 10948, Loss: 0.05573606863617897\n",
      "Iteration 10949, Loss: 0.05573968216776848\n",
      "Iteration 10950, Loss: 0.05564681813120842\n",
      "Iteration 10951, Loss: 0.05576439946889877\n",
      "Iteration 10952, Loss: 0.055775683373212814\n",
      "Iteration 10953, Loss: 0.05562806501984596\n",
      "Iteration 10954, Loss: 0.05583481118083\n",
      "Iteration 10955, Loss: 0.05593634024262428\n",
      "Iteration 10956, Loss: 0.055912815034389496\n",
      "Iteration 10957, Loss: 0.05578378960490227\n",
      "Iteration 10958, Loss: 0.055690765380859375\n",
      "Iteration 10959, Loss: 0.05580373853445053\n",
      "Iteration 10960, Loss: 0.05577806755900383\n",
      "Iteration 10961, Loss: 0.05565126985311508\n",
      "Iteration 10962, Loss: 0.05572819709777832\n",
      "Iteration 10963, Loss: 0.055742621421813965\n",
      "Iteration 10964, Loss: 0.05567236989736557\n",
      "Iteration 10965, Loss: 0.05574536323547363\n",
      "Iteration 10966, Loss: 0.055762529373168945\n",
      "Iteration 10967, Loss: 0.055639706552028656\n",
      "Iteration 10968, Loss: 0.05574754998087883\n",
      "Iteration 10969, Loss: 0.05577830597758293\n",
      "Iteration 10970, Loss: 0.055700063705444336\n",
      "Iteration 10971, Loss: 0.05571524426341057\n",
      "Iteration 10972, Loss: 0.05575251951813698\n",
      "Iteration 10973, Loss: 0.0556337833404541\n",
      "Iteration 10974, Loss: 0.05580509081482887\n",
      "Iteration 10975, Loss: 0.05588265508413315\n",
      "Iteration 10976, Loss: 0.05583914369344711\n",
      "Iteration 10977, Loss: 0.05570145696401596\n",
      "Iteration 10978, Loss: 0.05577787011861801\n",
      "Iteration 10979, Loss: 0.05586783215403557\n",
      "Iteration 10980, Loss: 0.05579833313822746\n",
      "Iteration 10981, Loss: 0.05563624948263168\n",
      "Iteration 10982, Loss: 0.055692315101623535\n",
      "Iteration 10983, Loss: 0.055638790130615234\n",
      "Iteration 10984, Loss: 0.05575573816895485\n",
      "Iteration 10985, Loss: 0.05575506016612053\n",
      "Iteration 10986, Loss: 0.05563048645853996\n",
      "Iteration 10987, Loss: 0.05570026487112045\n",
      "Iteration 10988, Loss: 0.055654726922512054\n",
      "Iteration 10989, Loss: 0.0557379350066185\n",
      "Iteration 10990, Loss: 0.0557587556540966\n",
      "Iteration 10991, Loss: 0.05565623566508293\n",
      "Iteration 10992, Loss: 0.05577540770173073\n",
      "Iteration 10993, Loss: 0.055825792253017426\n",
      "Iteration 10994, Loss: 0.05573197454214096\n",
      "Iteration 10995, Loss: 0.05570399761199951\n",
      "Iteration 10996, Loss: 0.055771470069885254\n",
      "Iteration 10997, Loss: 0.05570940300822258\n",
      "Iteration 10998, Loss: 0.055695533752441406\n",
      "Iteration 10999, Loss: 0.05572943016886711\n",
      "Iteration 11000, Loss: 0.05563342571258545\n",
      "Iteration 11001, Loss: 0.0557812862098217\n",
      "Iteration 11002, Loss: 0.05581248179078102\n",
      "Iteration 11003, Loss: 0.055701375007629395\n",
      "Iteration 11004, Loss: 0.055744849145412445\n",
      "Iteration 11005, Loss: 0.05582181736826897\n",
      "Iteration 11006, Loss: 0.055770598351955414\n",
      "Iteration 11007, Loss: 0.055637918412685394\n",
      "Iteration 11008, Loss: 0.0557791031897068\n",
      "Iteration 11009, Loss: 0.05578351020812988\n",
      "Iteration 11010, Loss: 0.05562639236450195\n",
      "Iteration 11011, Loss: 0.05583890527486801\n",
      "Iteration 11012, Loss: 0.05595024675130844\n",
      "Iteration 11013, Loss: 0.05594702810049057\n",
      "Iteration 11014, Loss: 0.05584438890218735\n",
      "Iteration 11015, Loss: 0.0556764230132103\n",
      "Iteration 11016, Loss: 0.05585102364420891\n",
      "Iteration 11017, Loss: 0.05596069619059563\n",
      "Iteration 11018, Loss: 0.05589417740702629\n",
      "Iteration 11019, Loss: 0.05566708371043205\n",
      "Iteration 11020, Loss: 0.05585138127207756\n",
      "Iteration 11021, Loss: 0.05600849911570549\n",
      "Iteration 11022, Loss: 0.056053441017866135\n",
      "Iteration 11023, Loss: 0.05599788948893547\n",
      "Iteration 11024, Loss: 0.055853210389614105\n",
      "Iteration 11025, Loss: 0.05565067380666733\n",
      "Iteration 11026, Loss: 0.055914878845214844\n",
      "Iteration 11027, Loss: 0.05604644864797592\n",
      "Iteration 11028, Loss: 0.05599089711904526\n",
      "Iteration 11029, Loss: 0.05576781556010246\n",
      "Iteration 11030, Loss: 0.05577174946665764\n",
      "Iteration 11031, Loss: 0.05592592805624008\n",
      "Iteration 11032, Loss: 0.055970512330532074\n",
      "Iteration 11033, Loss: 0.05591535568237305\n",
      "Iteration 11034, Loss: 0.055770955979824066\n",
      "Iteration 11035, Loss: 0.05569760128855705\n",
      "Iteration 11036, Loss: 0.05579785630106926\n",
      "Iteration 11037, Loss: 0.055714450776576996\n",
      "Iteration 11038, Loss: 0.05571770668029785\n",
      "Iteration 11039, Loss: 0.05578812211751938\n",
      "Iteration 11040, Loss: 0.055757761001586914\n",
      "Iteration 11041, Loss: 0.05563688650727272\n",
      "Iteration 11042, Loss: 0.055851422250270844\n",
      "Iteration 11043, Loss: 0.05592537298798561\n",
      "Iteration 11044, Loss: 0.05581752583384514\n",
      "Iteration 11045, Loss: 0.055658381432294846\n",
      "Iteration 11046, Loss: 0.05574381351470947\n",
      "Iteration 11047, Loss: 0.05572756379842758\n",
      "Iteration 11048, Loss: 0.05562075227499008\n",
      "Iteration 11049, Loss: 0.055851101875305176\n",
      "Iteration 11050, Loss: 0.05590645596385002\n",
      "Iteration 11051, Loss: 0.05578538030385971\n",
      "Iteration 11052, Loss: 0.05569060891866684\n",
      "Iteration 11053, Loss: 0.055783987045288086\n",
      "Iteration 11054, Loss: 0.05577477067708969\n",
      "Iteration 11055, Loss: 0.055671773850917816\n",
      "Iteration 11056, Loss: 0.0557815246284008\n",
      "Iteration 11057, Loss: 0.05583743378520012\n",
      "Iteration 11058, Loss: 0.055719178169965744\n",
      "Iteration 11059, Loss: 0.05573614686727524\n",
      "Iteration 11060, Loss: 0.05582606792449951\n",
      "Iteration 11061, Loss: 0.05581160634756088\n",
      "Iteration 11062, Loss: 0.05570300668478012\n",
      "Iteration 11063, Loss: 0.05574588105082512\n",
      "Iteration 11064, Loss: 0.05580819025635719\n",
      "Iteration 11065, Loss: 0.055695656687021255\n",
      "Iteration 11066, Loss: 0.055750489234924316\n",
      "Iteration 11067, Loss: 0.05583719536662102\n",
      "Iteration 11068, Loss: 0.05581927299499512\n",
      "Iteration 11069, Loss: 0.05570833012461662\n",
      "Iteration 11070, Loss: 0.055741630494594574\n",
      "Iteration 11071, Loss: 0.055806439369916916\n",
      "Iteration 11072, Loss: 0.055696092545986176\n",
      "Iteration 11073, Loss: 0.05574909970164299\n",
      "Iteration 11074, Loss: 0.055835168808698654\n",
      "Iteration 11075, Loss: 0.0558171272277832\n",
      "Iteration 11076, Loss: 0.05570710077881813\n",
      "Iteration 11077, Loss: 0.05574262514710426\n",
      "Iteration 11078, Loss: 0.0558062419295311\n",
      "Iteration 11079, Loss: 0.05569414421916008\n",
      "Iteration 11080, Loss: 0.05575200170278549\n",
      "Iteration 11081, Loss: 0.0558394230902195\n",
      "Iteration 11082, Loss: 0.055823445320129395\n",
      "Iteration 11083, Loss: 0.05571524426341057\n",
      "Iteration 11084, Loss: 0.05572951212525368\n",
      "Iteration 11085, Loss: 0.05579134076833725\n",
      "Iteration 11086, Loss: 0.0556764230132103\n",
      "Iteration 11087, Loss: 0.055766742676496506\n",
      "Iteration 11088, Loss: 0.05585579201579094\n",
      "Iteration 11089, Loss: 0.05584168806672096\n",
      "Iteration 11090, Loss: 0.05573507398366928\n",
      "Iteration 11091, Loss: 0.05570077896118164\n",
      "Iteration 11092, Loss: 0.0557611808180809\n",
      "Iteration 11093, Loss: 0.05564546585083008\n",
      "Iteration 11094, Loss: 0.05578843876719475\n",
      "Iteration 11095, Loss: 0.05587689206004143\n",
      "Iteration 11096, Loss: 0.0558619499206543\n",
      "Iteration 11097, Loss: 0.05575450509786606\n",
      "Iteration 11098, Loss: 0.05567523092031479\n",
      "Iteration 11099, Loss: 0.05573531240224838\n",
      "Iteration 11100, Loss: 0.05561661720275879\n",
      "Iteration 11101, Loss: 0.05581260100007057\n",
      "Iteration 11102, Loss: 0.05590355768799782\n",
      "Iteration 11103, Loss: 0.0558902844786644\n",
      "Iteration 11104, Loss: 0.055783551186323166\n",
      "Iteration 11105, Loss: 0.05563656613230705\n",
      "Iteration 11106, Loss: 0.05571882054209709\n",
      "Iteration 11107, Loss: 0.05564391613006592\n",
      "Iteration 11108, Loss: 0.055747948586940765\n",
      "Iteration 11109, Loss: 0.05579022690653801\n",
      "Iteration 11110, Loss: 0.0557253360748291\n",
      "Iteration 11111, Loss: 0.055670857429504395\n",
      "Iteration 11112, Loss: 0.05570288747549057\n",
      "Iteration 11113, Loss: 0.05564538761973381\n",
      "Iteration 11114, Loss: 0.05565357208251953\n",
      "Iteration 11115, Loss: 0.05565532296895981\n",
      "Iteration 11116, Loss: 0.05565289780497551\n",
      "Iteration 11117, Loss: 0.05564073845744133\n",
      "Iteration 11118, Loss: 0.05567288398742676\n",
      "Iteration 11119, Loss: 0.05563616752624512\n",
      "Iteration 11120, Loss: 0.05565452575683594\n",
      "Iteration 11121, Loss: 0.05564570426940918\n",
      "Iteration 11122, Loss: 0.05564570426940918\n",
      "Iteration 11123, Loss: 0.055632710456848145\n",
      "Iteration 11124, Loss: 0.05567944049835205\n",
      "Iteration 11125, Loss: 0.055659931153059006\n",
      "Iteration 11126, Loss: 0.05569668859243393\n",
      "Iteration 11127, Loss: 0.05566171929240227\n",
      "Iteration 11128, Loss: 0.055724624544382095\n",
      "Iteration 11129, Loss: 0.05576483532786369\n",
      "Iteration 11130, Loss: 0.055705152451992035\n",
      "Iteration 11131, Loss: 0.05568496510386467\n",
      "Iteration 11132, Loss: 0.05569581314921379\n",
      "Iteration 11133, Loss: 0.05566927045583725\n",
      "Iteration 11134, Loss: 0.05568218231201172\n",
      "Iteration 11135, Loss: 0.05563366785645485\n",
      "Iteration 11136, Loss: 0.055688463151454926\n",
      "Iteration 11137, Loss: 0.05564375966787338\n",
      "Iteration 11138, Loss: 0.05572235956788063\n",
      "Iteration 11139, Loss: 0.05571584030985832\n",
      "Iteration 11140, Loss: 0.055646542459726334\n",
      "Iteration 11141, Loss: 0.0556594543159008\n",
      "Iteration 11142, Loss: 0.05568011850118637\n",
      "Iteration 11143, Loss: 0.05565989017486572\n",
      "Iteration 11144, Loss: 0.055699270218610764\n",
      "Iteration 11145, Loss: 0.05568830296397209\n",
      "Iteration 11146, Loss: 0.05567669868469238\n",
      "Iteration 11147, Loss: 0.055680714547634125\n",
      "Iteration 11148, Loss: 0.05566052719950676\n",
      "Iteration 11149, Loss: 0.055639706552028656\n",
      "Iteration 11150, Loss: 0.05573105812072754\n",
      "Iteration 11151, Loss: 0.055742423981428146\n",
      "Iteration 11152, Loss: 0.05565254017710686\n",
      "Iteration 11153, Loss: 0.05576753616333008\n",
      "Iteration 11154, Loss: 0.05579463765025139\n",
      "Iteration 11155, Loss: 0.05566386505961418\n",
      "Iteration 11156, Loss: 0.055791858583688736\n",
      "Iteration 11157, Loss: 0.05588619038462639\n",
      "Iteration 11158, Loss: 0.05586334317922592\n",
      "Iteration 11159, Loss: 0.05574194714426994\n",
      "Iteration 11160, Loss: 0.05571739003062248\n",
      "Iteration 11161, Loss: 0.05580206960439682\n",
      "Iteration 11162, Loss: 0.055732134729623795\n",
      "Iteration 11163, Loss: 0.05569803714752197\n",
      "Iteration 11164, Loss: 0.055755022913217545\n",
      "Iteration 11165, Loss: 0.055712223052978516\n",
      "Iteration 11166, Loss: 0.05566950887441635\n",
      "Iteration 11167, Loss: 0.055700384080410004\n",
      "Iteration 11168, Loss: 0.05565746873617172\n",
      "Iteration 11169, Loss: 0.0556819848716259\n",
      "Iteration 11170, Loss: 0.055667005479335785\n",
      "Iteration 11171, Loss: 0.05568910017609596\n",
      "Iteration 11172, Loss: 0.0556587390601635\n",
      "Iteration 11173, Loss: 0.05571039766073227\n",
      "Iteration 11174, Loss: 0.055728793144226074\n",
      "Iteration 11175, Loss: 0.05564066022634506\n",
      "Iteration 11176, Loss: 0.05579996481537819\n",
      "Iteration 11177, Loss: 0.0558396577835083\n",
      "Iteration 11178, Loss: 0.055713772773742676\n",
      "Iteration 11179, Loss: 0.0557456836104393\n",
      "Iteration 11180, Loss: 0.05583866685628891\n",
      "Iteration 11181, Loss: 0.05581558123230934\n",
      "Iteration 11182, Loss: 0.055691443383693695\n",
      "Iteration 11183, Loss: 0.055776119232177734\n",
      "Iteration 11184, Loss: 0.05585392564535141\n",
      "Iteration 11185, Loss: 0.055759429931640625\n",
      "Iteration 11186, Loss: 0.05568882077932358\n",
      "Iteration 11187, Loss: 0.05576439946889877\n",
      "Iteration 11188, Loss: 0.05572839826345444\n",
      "Iteration 11189, Loss: 0.05563712120056152\n",
      "Iteration 11190, Loss: 0.05568309873342514\n",
      "Iteration 11191, Loss: 0.0556468591094017\n",
      "Iteration 11192, Loss: 0.05565289780497551\n",
      "Iteration 11193, Loss: 0.05565885826945305\n",
      "Iteration 11194, Loss: 0.055648885667324066\n",
      "Iteration 11195, Loss: 0.05564006417989731\n",
      "Iteration 11196, Loss: 0.055687230080366135\n",
      "Iteration 11197, Loss: 0.05563799664378166\n",
      "Iteration 11198, Loss: 0.055732451379299164\n",
      "Iteration 11199, Loss: 0.05574111267924309\n",
      "Iteration 11200, Loss: 0.05563922971487045\n",
      "Iteration 11201, Loss: 0.05578875541687012\n",
      "Iteration 11202, Loss: 0.055822573602199554\n",
      "Iteration 11203, Loss: 0.05570109933614731\n",
      "Iteration 11204, Loss: 0.055752675980329514\n",
      "Iteration 11205, Loss: 0.05584089085459709\n",
      "Iteration 11206, Loss: 0.05581077188253403\n",
      "Iteration 11207, Loss: 0.05568154901266098\n",
      "Iteration 11208, Loss: 0.05579010769724846\n",
      "Iteration 11209, Loss: 0.05587148666381836\n",
      "Iteration 11210, Loss: 0.05578847974538803\n",
      "Iteration 11211, Loss: 0.05565556138753891\n",
      "Iteration 11212, Loss: 0.0557279996573925\n",
      "Iteration 11213, Loss: 0.055687230080366135\n",
      "Iteration 11214, Loss: 0.05569358915090561\n",
      "Iteration 11215, Loss: 0.05569843575358391\n",
      "Iteration 11216, Loss: 0.05565985292196274\n",
      "Iteration 11217, Loss: 0.05566474050283432\n",
      "Iteration 11218, Loss: 0.055672965943813324\n",
      "Iteration 11219, Loss: 0.055638473480939865\n",
      "Iteration 11220, Loss: 0.0557357482612133\n",
      "Iteration 11221, Loss: 0.055748146027326584\n",
      "Iteration 11222, Loss: 0.055649999529123306\n",
      "Iteration 11223, Loss: 0.05577925965189934\n",
      "Iteration 11224, Loss: 0.05581899732351303\n",
      "Iteration 11225, Loss: 0.055705152451992035\n",
      "Iteration 11226, Loss: 0.05574421212077141\n",
      "Iteration 11227, Loss: 0.05582686513662338\n",
      "Iteration 11228, Loss: 0.055789511650800705\n",
      "Iteration 11229, Loss: 0.055657267570495605\n",
      "Iteration 11230, Loss: 0.05581546202301979\n",
      "Iteration 11231, Loss: 0.05588909238576889\n",
      "Iteration 11232, Loss: 0.05579710379242897\n",
      "Iteration 11233, Loss: 0.055656593292951584\n",
      "Iteration 11234, Loss: 0.05573646351695061\n",
      "Iteration 11235, Loss: 0.055704955011606216\n",
      "Iteration 11236, Loss: 0.055662792176008224\n",
      "Iteration 11237, Loss: 0.05565933510661125\n",
      "Iteration 11238, Loss: 0.055700819939374924\n",
      "Iteration 11239, Loss: 0.05570852756500244\n",
      "Iteration 11240, Loss: 0.055632393807172775\n",
      "Iteration 11241, Loss: 0.05573868751525879\n",
      "Iteration 11242, Loss: 0.055698394775390625\n",
      "Iteration 11243, Loss: 0.05570114031434059\n",
      "Iteration 11244, Loss: 0.05574552342295647\n",
      "Iteration 11245, Loss: 0.05569283291697502\n",
      "Iteration 11246, Loss: 0.055692315101623535\n",
      "Iteration 11247, Loss: 0.05569271370768547\n",
      "Iteration 11248, Loss: 0.05567852780222893\n",
      "Iteration 11249, Loss: 0.05569911375641823\n",
      "Iteration 11250, Loss: 0.05562667176127434\n",
      "Iteration 11251, Loss: 0.05580174922943115\n",
      "Iteration 11252, Loss: 0.05581824108958244\n",
      "Iteration 11253, Loss: 0.05566108226776123\n",
      "Iteration 11254, Loss: 0.05580548569560051\n",
      "Iteration 11255, Loss: 0.05591869354248047\n",
      "Iteration 11256, Loss: 0.05592278763651848\n",
      "Iteration 11257, Loss: 0.055829089134931564\n",
      "Iteration 11258, Loss: 0.05565524101257324\n",
      "Iteration 11259, Loss: 0.05588750168681145\n",
      "Iteration 11260, Loss: 0.056012749671936035\n",
      "Iteration 11261, Loss: 0.05595076456665993\n",
      "Iteration 11262, Loss: 0.05572132393717766\n",
      "Iteration 11263, Loss: 0.0558096207678318\n",
      "Iteration 11264, Loss: 0.05596756935119629\n",
      "Iteration 11265, Loss: 0.056014180183410645\n",
      "Iteration 11266, Loss: 0.05596010014414787\n",
      "Iteration 11267, Loss: 0.055817048996686935\n",
      "Iteration 11268, Loss: 0.05564260855317116\n",
      "Iteration 11269, Loss: 0.055795393884181976\n",
      "Iteration 11270, Loss: 0.05578450486063957\n",
      "Iteration 11271, Loss: 0.055650513619184494\n",
      "Iteration 11272, Loss: 0.05575939267873764\n",
      "Iteration 11273, Loss: 0.05580286309123039\n",
      "Iteration 11274, Loss: 0.05573273077607155\n",
      "Iteration 11275, Loss: 0.05567077919840813\n",
      "Iteration 11276, Loss: 0.05570697784423828\n",
      "Iteration 11277, Loss: 0.055631160736083984\n",
      "Iteration 11278, Loss: 0.055616024881601334\n",
      "Iteration 11279, Loss: 0.05574282258749008\n",
      "Iteration 11280, Loss: 0.055694662034511566\n",
      "Iteration 11281, Loss: 0.05570828914642334\n",
      "Iteration 11282, Loss: 0.05575740337371826\n",
      "Iteration 11283, Loss: 0.055707696825265884\n",
      "Iteration 11284, Loss: 0.05566815659403801\n",
      "Iteration 11285, Loss: 0.055665455758571625\n",
      "Iteration 11286, Loss: 0.055700067430734634\n",
      "Iteration 11287, Loss: 0.0557221993803978\n",
      "Iteration 11288, Loss: 0.0556488074362278\n",
      "Iteration 11289, Loss: 0.05577675625681877\n",
      "Iteration 11290, Loss: 0.05579864978790283\n",
      "Iteration 11291, Loss: 0.05564562603831291\n",
      "Iteration 11292, Loss: 0.05581403151154518\n",
      "Iteration 11293, Loss: 0.055925726890563965\n",
      "Iteration 11294, Loss: 0.05593172833323479\n",
      "Iteration 11295, Loss: 0.05584216117858887\n",
      "Iteration 11296, Loss: 0.05566827580332756\n",
      "Iteration 11297, Loss: 0.0558728389441967\n",
      "Iteration 11298, Loss: 0.0560053214430809\n",
      "Iteration 11299, Loss: 0.05595088005065918\n",
      "Iteration 11300, Loss: 0.05572919175028801\n",
      "Iteration 11301, Loss: 0.05579817295074463\n",
      "Iteration 11302, Loss: 0.05595136061310768\n",
      "Iteration 11303, Loss: 0.05599471181631088\n",
      "Iteration 11304, Loss: 0.05593876168131828\n",
      "Iteration 11305, Loss: 0.055793844163417816\n",
      "Iteration 11306, Loss: 0.05566716194152832\n",
      "Iteration 11307, Loss: 0.0557682141661644\n",
      "Iteration 11308, Loss: 0.055686913430690765\n",
      "Iteration 11309, Loss: 0.05573618412017822\n",
      "Iteration 11310, Loss: 0.05580544471740723\n",
      "Iteration 11311, Loss: 0.05577365681529045\n",
      "Iteration 11312, Loss: 0.05565170571208\n",
      "Iteration 11313, Loss: 0.0558321475982666\n",
      "Iteration 11314, Loss: 0.05590784549713135\n",
      "Iteration 11315, Loss: 0.05580254644155502\n",
      "Iteration 11316, Loss: 0.05566772073507309\n",
      "Iteration 11317, Loss: 0.05575180426239967\n",
      "Iteration 11318, Loss: 0.05573427677154541\n",
      "Iteration 11319, Loss: 0.05562547966837883\n",
      "Iteration 11320, Loss: 0.05585189908742905\n",
      "Iteration 11321, Loss: 0.055913329124450684\n",
      "Iteration 11322, Loss: 0.05579733848571777\n",
      "Iteration 11323, Loss: 0.05567868798971176\n",
      "Iteration 11324, Loss: 0.05576956272125244\n",
      "Iteration 11325, Loss: 0.05575903505086899\n",
      "Iteration 11326, Loss: 0.05565464496612549\n",
      "Iteration 11327, Loss: 0.05580596253275871\n",
      "Iteration 11328, Loss: 0.0558631457388401\n",
      "Iteration 11329, Loss: 0.05574667453765869\n",
      "Iteration 11330, Loss: 0.05571508780121803\n",
      "Iteration 11331, Loss: 0.05580484867095947\n",
      "Iteration 11332, Loss: 0.05579058453440666\n",
      "Iteration 11333, Loss: 0.0556817464530468\n",
      "Iteration 11334, Loss: 0.05577472969889641\n",
      "Iteration 11335, Loss: 0.0558372363448143\n",
      "Iteration 11336, Loss: 0.05572625249624252\n",
      "Iteration 11337, Loss: 0.05572621151804924\n",
      "Iteration 11338, Loss: 0.055812202394008636\n",
      "Iteration 11339, Loss: 0.055793408304452896\n",
      "Iteration 11340, Loss: 0.05568063259124756\n",
      "Iteration 11341, Loss: 0.05578037351369858\n",
      "Iteration 11342, Loss: 0.05584665387868881\n",
      "Iteration 11343, Loss: 0.05573849007487297\n",
      "Iteration 11344, Loss: 0.05571528524160385\n",
      "Iteration 11345, Loss: 0.055799685418605804\n",
      "Iteration 11346, Loss: 0.055779580026865005\n",
      "Iteration 11347, Loss: 0.05566636845469475\n",
      "Iteration 11348, Loss: 0.05579976364970207\n",
      "Iteration 11349, Loss: 0.055866122245788574\n",
      "Iteration 11350, Loss: 0.05575720593333244\n",
      "Iteration 11351, Loss: 0.055702053010463715\n",
      "Iteration 11352, Loss: 0.05578744411468506\n",
      "Iteration 11353, Loss: 0.05576888844370842\n",
      "Iteration 11354, Loss: 0.05565667524933815\n",
      "Iteration 11355, Loss: 0.05581136792898178\n",
      "Iteration 11356, Loss: 0.055876534432172775\n",
      "Iteration 11357, Loss: 0.0557662658393383\n",
      "Iteration 11358, Loss: 0.05569656938314438\n",
      "Iteration 11359, Loss: 0.05578307434916496\n",
      "Iteration 11360, Loss: 0.05576598644256592\n",
      "Iteration 11361, Loss: 0.05565492436289787\n",
      "Iteration 11362, Loss: 0.05581279844045639\n",
      "Iteration 11363, Loss: 0.0558772087097168\n",
      "Iteration 11364, Loss: 0.055766940116882324\n",
      "Iteration 11365, Loss: 0.055695854127407074\n",
      "Iteration 11366, Loss: 0.055782441049814224\n",
      "Iteration 11367, Loss: 0.055765390396118164\n",
      "Iteration 11368, Loss: 0.0556536540389061\n",
      "Iteration 11369, Loss: 0.05581498518586159\n",
      "Iteration 11370, Loss: 0.05588042736053467\n",
      "Iteration 11371, Loss: 0.05577202886343002\n",
      "Iteration 11372, Loss: 0.055690646171569824\n",
      "Iteration 11373, Loss: 0.055776361376047134\n",
      "Iteration 11374, Loss: 0.05575831979513168\n",
      "Iteration 11375, Loss: 0.055644869804382324\n",
      "Iteration 11376, Loss: 0.05582817643880844\n",
      "Iteration 11377, Loss: 0.0558960847556591\n",
      "Iteration 11378, Loss: 0.05579209700226784\n",
      "Iteration 11379, Loss: 0.05567387863993645\n",
      "Iteration 11380, Loss: 0.05575990676879883\n",
      "Iteration 11381, Loss: 0.055744290351867676\n",
      "Iteration 11382, Loss: 0.05562945455312729\n",
      "Iteration 11383, Loss: 0.055846694856882095\n",
      "Iteration 11384, Loss: 0.05591806024312973\n",
      "Iteration 11385, Loss: 0.0558239221572876\n",
      "Iteration 11386, Loss: 0.0556536540389061\n",
      "Iteration 11387, Loss: 0.05577317997813225\n",
      "Iteration 11388, Loss: 0.055800795555114746\n",
      "Iteration 11389, Loss: 0.05570805445313454\n",
      "Iteration 11390, Loss: 0.055724941194057465\n",
      "Iteration 11391, Loss: 0.05578192323446274\n",
      "Iteration 11392, Loss: 0.055693984031677246\n",
      "Iteration 11393, Loss: 0.05573221296072006\n",
      "Iteration 11394, Loss: 0.05579189583659172\n",
      "Iteration 11395, Loss: 0.05572509765625\n",
      "Iteration 11396, Loss: 0.05568099021911621\n",
      "Iteration 11397, Loss: 0.05572132393717766\n",
      "Iteration 11398, Loss: 0.055618882179260254\n",
      "Iteration 11399, Loss: 0.05581521987915039\n",
      "Iteration 11400, Loss: 0.055889129638671875\n",
      "Iteration 11401, Loss: 0.055843353271484375\n",
      "Iteration 11402, Loss: 0.05570630356669426\n",
      "Iteration 11403, Loss: 0.055774055421352386\n",
      "Iteration 11404, Loss: 0.055863622575998306\n",
      "Iteration 11405, Loss: 0.05579495429992676\n",
      "Iteration 11406, Loss: 0.05564030259847641\n",
      "Iteration 11407, Loss: 0.05569624900817871\n",
      "Iteration 11408, Loss: 0.05565246194601059\n",
      "Iteration 11409, Loss: 0.05572867766022682\n",
      "Iteration 11410, Loss: 0.05571595951914787\n",
      "Iteration 11411, Loss: 0.05566692724823952\n",
      "Iteration 11412, Loss: 0.055691760033369064\n",
      "Iteration 11413, Loss: 0.05562381073832512\n",
      "Iteration 11414, Loss: 0.0557783842086792\n",
      "Iteration 11415, Loss: 0.05576896667480469\n",
      "Iteration 11416, Loss: 0.05563116446137428\n",
      "Iteration 11417, Loss: 0.05567324161529541\n",
      "Iteration 11418, Loss: 0.05561832711100578\n",
      "Iteration 11419, Loss: 0.05578744411468506\n",
      "Iteration 11420, Loss: 0.05579940602183342\n",
      "Iteration 11421, Loss: 0.05567074194550514\n",
      "Iteration 11422, Loss: 0.05578005313873291\n",
      "Iteration 11423, Loss: 0.05585964769124985\n",
      "Iteration 11424, Loss: 0.05580949783325195\n",
      "Iteration 11425, Loss: 0.05565397068858147\n",
      "Iteration 11426, Loss: 0.05583890527486801\n",
      "Iteration 11427, Loss: 0.055933598428964615\n",
      "Iteration 11428, Loss: 0.0558631457388401\n",
      "Iteration 11429, Loss: 0.05566243454813957\n",
      "Iteration 11430, Loss: 0.05584108829498291\n",
      "Iteration 11431, Loss: 0.05596721172332764\n",
      "Iteration 11432, Loss: 0.055954933166503906\n",
      "Iteration 11433, Loss: 0.0558222159743309\n",
      "Iteration 11434, Loss: 0.055662672966718674\n",
      "Iteration 11435, Loss: 0.05581542104482651\n",
      "Iteration 11436, Loss: 0.05583977699279785\n",
      "Iteration 11437, Loss: 0.05570387840270996\n",
      "Iteration 11438, Loss: 0.055766504257917404\n",
      "Iteration 11439, Loss: 0.05586671829223633\n",
      "Iteration 11440, Loss: 0.0558500699698925\n",
      "Iteration 11441, Loss: 0.05573594570159912\n",
      "Iteration 11442, Loss: 0.05571794509887695\n",
      "Iteration 11443, Loss: 0.05579380318522453\n",
      "Iteration 11444, Loss: 0.05571131035685539\n",
      "Iteration 11445, Loss: 0.05572466179728508\n",
      "Iteration 11446, Loss: 0.05579034611582756\n",
      "Iteration 11447, Loss: 0.055753789842128754\n",
      "Iteration 11448, Loss: 0.05565448850393295\n",
      "Iteration 11449, Loss: 0.05578462406992912\n",
      "Iteration 11450, Loss: 0.05580218881368637\n",
      "Iteration 11451, Loss: 0.055658143013715744\n",
      "Iteration 11452, Loss: 0.0557883195579052\n",
      "Iteration 11453, Loss: 0.055882178246974945\n",
      "Iteration 11454, Loss: 0.05587216466665268\n",
      "Iteration 11455, Loss: 0.05576837435364723\n",
      "Iteration 11456, Loss: 0.05565234273672104\n",
      "Iteration 11457, Loss: 0.055709682404994965\n",
      "Iteration 11458, Loss: 0.055626749992370605\n",
      "Iteration 11459, Loss: 0.05562373250722885\n",
      "Iteration 11460, Loss: 0.05571846291422844\n",
      "Iteration 11461, Loss: 0.055696092545986176\n",
      "Iteration 11462, Loss: 0.05567328259348869\n",
      "Iteration 11463, Loss: 0.055684249848127365\n",
      "Iteration 11464, Loss: 0.05565854161977768\n",
      "Iteration 11465, Loss: 0.055641889572143555\n",
      "Iteration 11466, Loss: 0.055706702172756195\n",
      "Iteration 11467, Loss: 0.05567920580506325\n",
      "Iteration 11468, Loss: 0.05569934844970703\n",
      "Iteration 11469, Loss: 0.05571814626455307\n",
      "Iteration 11470, Loss: 0.05562977120280266\n",
      "Iteration 11471, Loss: 0.055778902024030685\n",
      "Iteration 11472, Loss: 0.05578327178955078\n",
      "Iteration 11473, Loss: 0.05562790483236313\n",
      "Iteration 11474, Loss: 0.0558188371360302\n",
      "Iteration 11475, Loss: 0.05590617656707764\n",
      "Iteration 11476, Loss: 0.0558699369430542\n",
      "Iteration 11477, Loss: 0.05573209375143051\n",
      "Iteration 11478, Loss: 0.05574488639831543\n",
      "Iteration 11479, Loss: 0.05584057420492172\n",
      "Iteration 11480, Loss: 0.05577453225851059\n",
      "Iteration 11481, Loss: 0.05565742775797844\n",
      "Iteration 11482, Loss: 0.05571182817220688\n",
      "Iteration 11483, Loss: 0.05566299334168434\n",
      "Iteration 11484, Loss: 0.0557229146361351\n",
      "Iteration 11485, Loss: 0.05571866035461426\n",
      "Iteration 11486, Loss: 0.05565830320119858\n",
      "Iteration 11487, Loss: 0.055675946176052094\n",
      "Iteration 11488, Loss: 0.05563799664378166\n",
      "Iteration 11489, Loss: 0.055660367012023926\n",
      "Iteration 11490, Loss: 0.05564785376191139\n",
      "Iteration 11491, Loss: 0.05563688650727272\n",
      "Iteration 11492, Loss: 0.0557001456618309\n",
      "Iteration 11493, Loss: 0.05568011850118637\n",
      "Iteration 11494, Loss: 0.055680595338344574\n",
      "Iteration 11495, Loss: 0.05567574501037598\n",
      "Iteration 11496, Loss: 0.05568603798747063\n",
      "Iteration 11497, Loss: 0.05568564310669899\n",
      "Iteration 11498, Loss: 0.05565941333770752\n",
      "Iteration 11499, Loss: 0.055653177201747894\n",
      "Iteration 11500, Loss: 0.05570352077484131\n",
      "Iteration 11501, Loss: 0.05570396035909653\n",
      "Iteration 11502, Loss: 0.05564582347869873\n",
      "Iteration 11503, Loss: 0.055699944496154785\n",
      "Iteration 11504, Loss: 0.055641889572143555\n",
      "Iteration 11505, Loss: 0.055727601051330566\n",
      "Iteration 11506, Loss: 0.05575565621256828\n",
      "Iteration 11507, Loss: 0.05568587779998779\n",
      "Iteration 11508, Loss: 0.05572224035859108\n",
      "Iteration 11509, Loss: 0.055742744356393814\n",
      "Iteration 11510, Loss: 0.05562734976410866\n",
      "Iteration 11511, Loss: 0.05567570775747299\n",
      "Iteration 11512, Loss: 0.05561463162302971\n",
      "Iteration 11513, Loss: 0.05578836053609848\n",
      "Iteration 11514, Loss: 0.05579432100057602\n",
      "Iteration 11515, Loss: 0.05565488710999489\n",
      "Iteration 11516, Loss: 0.055794596672058105\n",
      "Iteration 11517, Loss: 0.055879633873701096\n",
      "Iteration 11518, Loss: 0.055842045694589615\n",
      "Iteration 11519, Loss: 0.055702369660139084\n",
      "Iteration 11520, Loss: 0.05577560514211655\n",
      "Iteration 11521, Loss: 0.05586954206228256\n",
      "Iteration 11522, Loss: 0.055803656578063965\n",
      "Iteration 11523, Loss: 0.055628977715969086\n",
      "Iteration 11524, Loss: 0.055723510682582855\n",
      "Iteration 11525, Loss: 0.05568905919790268\n",
      "Iteration 11526, Loss: 0.05569211766123772\n",
      "Iteration 11527, Loss: 0.055708132684230804\n",
      "Iteration 11528, Loss: 0.05563144013285637\n",
      "Iteration 11529, Loss: 0.05567852780222893\n",
      "Iteration 11530, Loss: 0.05563942715525627\n",
      "Iteration 11531, Loss: 0.055642250925302505\n",
      "Iteration 11532, Loss: 0.055666327476501465\n",
      "Iteration 11533, Loss: 0.05563557520508766\n",
      "Iteration 11534, Loss: 0.05565444752573967\n",
      "Iteration 11535, Loss: 0.055657628923654556\n",
      "Iteration 11536, Loss: 0.05563100427389145\n",
      "Iteration 11537, Loss: 0.0557178258895874\n",
      "Iteration 11538, Loss: 0.05566565319895744\n",
      "Iteration 11539, Loss: 0.055733006447553635\n",
      "Iteration 11540, Loss: 0.055783431977033615\n",
      "Iteration 11541, Loss: 0.055732809007167816\n",
      "Iteration 11542, Loss: 0.05564161390066147\n",
      "Iteration 11543, Loss: 0.05569235607981682\n",
      "Iteration 11544, Loss: 0.0556441955268383\n",
      "Iteration 11545, Loss: 0.055682938545942307\n",
      "Iteration 11546, Loss: 0.05564693734049797\n",
      "Iteration 11547, Loss: 0.055738095194101334\n",
      "Iteration 11548, Loss: 0.05574929714202881\n",
      "Iteration 11549, Loss: 0.055648766458034515\n",
      "Iteration 11550, Loss: 0.055768292397260666\n",
      "Iteration 11551, Loss: 0.05579821392893791\n",
      "Iteration 11552, Loss: 0.05568107217550278\n",
      "Iteration 11553, Loss: 0.05576924607157707\n",
      "Iteration 11554, Loss: 0.055850666016340256\n",
      "Iteration 11555, Loss: 0.05580226704478264\n",
      "Iteration 11556, Loss: 0.05565723031759262\n",
      "Iteration 11557, Loss: 0.055820345878601074\n",
      "Iteration 11558, Loss: 0.0558982715010643\n",
      "Iteration 11559, Loss: 0.055810850113630295\n",
      "Iteration 11560, Loss: 0.055643320083618164\n",
      "Iteration 11561, Loss: 0.05574222654104233\n",
      "Iteration 11562, Loss: 0.05572390928864479\n",
      "Iteration 11563, Loss: 0.05563994497060776\n",
      "Iteration 11564, Loss: 0.055643122643232346\n",
      "Iteration 11565, Loss: 0.055708251893520355\n",
      "Iteration 11566, Loss: 0.05570562928915024\n",
      "Iteration 11567, Loss: 0.05564352124929428\n",
      "Iteration 11568, Loss: 0.05568695068359375\n",
      "Iteration 11569, Loss: 0.05564054101705551\n",
      "Iteration 11570, Loss: 0.05567678064107895\n",
      "Iteration 11571, Loss: 0.055647216737270355\n",
      "Iteration 11572, Loss: 0.055728714913129807\n",
      "Iteration 11573, Loss: 0.0557200126349926\n",
      "Iteration 11574, Loss: 0.055655598640441895\n",
      "Iteration 11575, Loss: 0.05568389222025871\n",
      "Iteration 11576, Loss: 0.05563076585531235\n",
      "Iteration 11577, Loss: 0.05569148063659668\n",
      "Iteration 11578, Loss: 0.0556788444519043\n",
      "Iteration 11579, Loss: 0.05566314980387688\n",
      "Iteration 11580, Loss: 0.055635929107666016\n",
      "Iteration 11581, Loss: 0.05571671575307846\n",
      "Iteration 11582, Loss: 0.05573304742574692\n",
      "Iteration 11583, Loss: 0.05565309524536133\n",
      "Iteration 11584, Loss: 0.055777911096811295\n",
      "Iteration 11585, Loss: 0.05580814927816391\n",
      "Iteration 11586, Loss: 0.05566609278321266\n",
      "Iteration 11587, Loss: 0.05579102039337158\n",
      "Iteration 11588, Loss: 0.05589485168457031\n",
      "Iteration 11589, Loss: 0.0558924674987793\n",
      "Iteration 11590, Loss: 0.055795036256313324\n",
      "Iteration 11591, Loss: 0.05562977120280266\n",
      "Iteration 11592, Loss: 0.05585102364420891\n",
      "Iteration 11593, Loss: 0.05590514466166496\n",
      "Iteration 11594, Loss: 0.055780768394470215\n",
      "Iteration 11595, Loss: 0.055696967989206314\n",
      "Iteration 11596, Loss: 0.055792611092329025\n",
      "Iteration 11597, Loss: 0.05578538030385971\n",
      "Iteration 11598, Loss: 0.055683694779872894\n",
      "Iteration 11599, Loss: 0.05576439946889877\n",
      "Iteration 11600, Loss: 0.055818162858486176\n",
      "Iteration 11601, Loss: 0.05569450184702873\n",
      "Iteration 11602, Loss: 0.05575863644480705\n",
      "Iteration 11603, Loss: 0.05585237592458725\n",
      "Iteration 11604, Loss: 0.05583977699279785\n",
      "Iteration 11605, Loss: 0.055732131004333496\n",
      "Iteration 11606, Loss: 0.05570586770772934\n",
      "Iteration 11607, Loss: 0.05576690286397934\n",
      "Iteration 11608, Loss: 0.055652420967817307\n",
      "Iteration 11609, Loss: 0.055782876908779144\n",
      "Iteration 11610, Loss: 0.0558704137802124\n",
      "Iteration 11611, Loss: 0.05585400387644768\n",
      "Iteration 11612, Loss: 0.05574508756399155\n",
      "Iteration 11613, Loss: 0.05569108575582504\n",
      "Iteration 11614, Loss: 0.055754899978637695\n",
      "Iteration 11615, Loss: 0.05564860627055168\n",
      "Iteration 11616, Loss: 0.05577286332845688\n",
      "Iteration 11617, Loss: 0.055848121643066406\n",
      "Iteration 11618, Loss: 0.055821578949689865\n",
      "Iteration 11619, Loss: 0.05570356175303459\n",
      "Iteration 11620, Loss: 0.0557563342154026\n",
      "Iteration 11621, Loss: 0.05582813546061516\n",
      "Iteration 11622, Loss: 0.05572164058685303\n",
      "Iteration 11623, Loss: 0.05572688952088356\n",
      "Iteration 11624, Loss: 0.055811088532209396\n",
      "Iteration 11625, Loss: 0.05579241365194321\n",
      "Iteration 11626, Loss: 0.055681467056274414\n",
      "Iteration 11627, Loss: 0.05577751249074936\n",
      "Iteration 11628, Loss: 0.05584144592285156\n",
      "Iteration 11629, Loss: 0.05572764202952385\n",
      "Iteration 11630, Loss: 0.0557272844016552\n",
      "Iteration 11631, Loss: 0.05581565946340561\n",
      "Iteration 11632, Loss: 0.0558009147644043\n",
      "Iteration 11633, Loss: 0.05569338798522949\n",
      "Iteration 11634, Loss: 0.055757127702236176\n",
      "Iteration 11635, Loss: 0.05581752583384514\n",
      "Iteration 11636, Loss: 0.05570046231150627\n",
      "Iteration 11637, Loss: 0.05574933812022209\n",
      "Iteration 11638, Loss: 0.05583957955241203\n",
      "Iteration 11639, Loss: 0.055826228111982346\n",
      "Iteration 11640, Loss: 0.05571981519460678\n",
      "Iteration 11641, Loss: 0.0557200126349926\n",
      "Iteration 11642, Loss: 0.05577930063009262\n",
      "Iteration 11643, Loss: 0.05566176027059555\n",
      "Iteration 11644, Loss: 0.05577906221151352\n",
      "Iteration 11645, Loss: 0.05586985871195793\n",
      "Iteration 11646, Loss: 0.05585706606507301\n",
      "Iteration 11647, Loss: 0.05575152486562729\n",
      "Iteration 11648, Loss: 0.05567733570933342\n",
      "Iteration 11649, Loss: 0.05573825165629387\n",
      "Iteration 11650, Loss: 0.05562814325094223\n",
      "Iteration 11651, Loss: 0.05578378960490227\n",
      "Iteration 11652, Loss: 0.055854521691799164\n",
      "Iteration 11653, Loss: 0.05582154169678688\n",
      "Iteration 11654, Loss: 0.05569684877991676\n",
      "Iteration 11655, Loss: 0.05577349662780762\n",
      "Iteration 11656, Loss: 0.05585126206278801\n",
      "Iteration 11657, Loss: 0.05574818700551987\n",
      "Iteration 11658, Loss: 0.05570598691701889\n",
      "Iteration 11659, Loss: 0.05578812211751938\n",
      "Iteration 11660, Loss: 0.05576698109507561\n",
      "Iteration 11661, Loss: 0.05565532296895981\n",
      "Iteration 11662, Loss: 0.05581235885620117\n",
      "Iteration 11663, Loss: 0.05587359517812729\n",
      "Iteration 11664, Loss: 0.05575486272573471\n",
      "Iteration 11665, Loss: 0.05571107193827629\n",
      "Iteration 11666, Loss: 0.05580286309123039\n",
      "Iteration 11667, Loss: 0.055791378021240234\n",
      "Iteration 11668, Loss: 0.05568770691752434\n",
      "Iteration 11669, Loss: 0.05576062202453613\n",
      "Iteration 11670, Loss: 0.055815696716308594\n",
      "Iteration 11671, Loss: 0.0556923970580101\n",
      "Iteration 11672, Loss: 0.05575959011912346\n",
      "Iteration 11673, Loss: 0.05585388466715813\n",
      "Iteration 11674, Loss: 0.055844347923994064\n",
      "Iteration 11675, Loss: 0.055741988122463226\n",
      "Iteration 11676, Loss: 0.05568587779998779\n",
      "Iteration 11677, Loss: 0.05574067682027817\n",
      "Iteration 11678, Loss: 0.05562472343444824\n",
      "Iteration 11679, Loss: 0.05577953904867172\n",
      "Iteration 11680, Loss: 0.05584073066711426\n",
      "Iteration 11681, Loss: 0.05579710006713867\n",
      "Iteration 11682, Loss: 0.05566561222076416\n",
      "Iteration 11683, Loss: 0.055818717926740646\n",
      "Iteration 11684, Loss: 0.055900298058986664\n",
      "Iteration 11685, Loss: 0.05580560490489006\n",
      "Iteration 11686, Loss: 0.05565532296895981\n",
      "Iteration 11687, Loss: 0.05573296546936035\n",
      "Iteration 11688, Loss: 0.05570674315094948\n",
      "Iteration 11689, Loss: 0.05564407631754875\n",
      "Iteration 11690, Loss: 0.05563298985362053\n",
      "Iteration 11691, Loss: 0.055718664079904556\n",
      "Iteration 11692, Loss: 0.05573606491088867\n",
      "Iteration 11693, Loss: 0.055657509714365005\n",
      "Iteration 11694, Loss: 0.055770836770534515\n",
      "Iteration 11695, Loss: 0.05579821392893791\n",
      "Iteration 11696, Loss: 0.05564983934164047\n",
      "Iteration 11697, Loss: 0.055808067321777344\n",
      "Iteration 11698, Loss: 0.055917661637067795\n",
      "Iteration 11699, Loss: 0.05591992661356926\n",
      "Iteration 11700, Loss: 0.05582642927765846\n",
      "Iteration 11701, Loss: 0.0556514672935009\n",
      "Iteration 11702, Loss: 0.0558951310813427\n",
      "Iteration 11703, Loss: 0.05602487176656723\n",
      "Iteration 11704, Loss: 0.05596677586436272\n",
      "Iteration 11705, Loss: 0.055741194635629654\n",
      "Iteration 11706, Loss: 0.055791858583688736\n",
      "Iteration 11707, Loss: 0.055947065353393555\n",
      "Iteration 11708, Loss: 0.055992286652326584\n",
      "Iteration 11709, Loss: 0.055937688797712326\n",
      "Iteration 11710, Loss: 0.055794477462768555\n",
      "Iteration 11711, Loss: 0.055665258318185806\n",
      "Iteration 11712, Loss: 0.05576857179403305\n",
      "Iteration 11713, Loss: 0.05569557473063469\n",
      "Iteration 11714, Loss: 0.055722594261169434\n",
      "Iteration 11715, Loss: 0.055784545838832855\n",
      "Iteration 11716, Loss: 0.05574663728475571\n",
      "Iteration 11717, Loss: 0.055618803948163986\n",
      "Iteration 11718, Loss: 0.05587661266326904\n",
      "Iteration 11719, Loss: 0.055956725031137466\n",
      "Iteration 11720, Loss: 0.05586222931742668\n",
      "Iteration 11721, Loss: 0.0556417740881443\n",
      "Iteration 11722, Loss: 0.05581935495138168\n",
      "Iteration 11723, Loss: 0.05591309070587158\n",
      "Iteration 11724, Loss: 0.05588909238576889\n",
      "Iteration 11725, Loss: 0.05576229467988014\n",
      "Iteration 11726, Loss: 0.05569684877991676\n",
      "Iteration 11727, Loss: 0.05579113960266113\n",
      "Iteration 11728, Loss: 0.055729351937770844\n",
      "Iteration 11729, Loss: 0.05569235607981682\n",
      "Iteration 11730, Loss: 0.05574405565857887\n",
      "Iteration 11731, Loss: 0.05569593235850334\n",
      "Iteration 11732, Loss: 0.05568750947713852\n",
      "Iteration 11733, Loss: 0.055696964263916016\n",
      "Iteration 11734, Loss: 0.05566779896616936\n",
      "Iteration 11735, Loss: 0.055676501244306564\n",
      "Iteration 11736, Loss: 0.05565091222524643\n",
      "Iteration 11737, Loss: 0.05567721650004387\n",
      "Iteration 11738, Loss: 0.05564824864268303\n",
      "Iteration 11739, Loss: 0.05567089840769768\n",
      "Iteration 11740, Loss: 0.05563342571258545\n",
      "Iteration 11741, Loss: 0.05573320388793945\n",
      "Iteration 11742, Loss: 0.05574635788798332\n",
      "Iteration 11743, Loss: 0.055662672966718674\n",
      "Iteration 11744, Loss: 0.055757563561201096\n",
      "Iteration 11745, Loss: 0.055785179138183594\n",
      "Iteration 11746, Loss: 0.05565134808421135\n",
      "Iteration 11747, Loss: 0.05580326169729233\n",
      "Iteration 11748, Loss: 0.05590144917368889\n",
      "Iteration 11749, Loss: 0.05588686466217041\n",
      "Iteration 11750, Loss: 0.055775247514247894\n",
      "Iteration 11751, Loss: 0.055675629526376724\n",
      "Iteration 11752, Loss: 0.05578223988413811\n",
      "Iteration 11753, Loss: 0.05574878305196762\n",
      "Iteration 11754, Loss: 0.055671535432338715\n",
      "Iteration 11755, Loss: 0.05571707338094711\n",
      "Iteration 11756, Loss: 0.05569159984588623\n",
      "Iteration 11757, Loss: 0.05567443370819092\n",
      "Iteration 11758, Loss: 0.05567459389567375\n",
      "Iteration 11759, Loss: 0.05567745491862297\n",
      "Iteration 11760, Loss: 0.055680952966213226\n",
      "Iteration 11761, Loss: 0.05565059185028076\n",
      "Iteration 11762, Loss: 0.0556773766875267\n",
      "Iteration 11763, Loss: 0.055652499198913574\n",
      "Iteration 11764, Loss: 0.055690646171569824\n",
      "Iteration 11765, Loss: 0.055651627480983734\n",
      "Iteration 11766, Loss: 0.05573519319295883\n",
      "Iteration 11767, Loss: 0.055768292397260666\n",
      "Iteration 11768, Loss: 0.05569843575358391\n",
      "Iteration 11769, Loss: 0.05570853129029274\n",
      "Iteration 11770, Loss: 0.055736783891916275\n",
      "Iteration 11771, Loss: 0.055625878274440765\n",
      "Iteration 11772, Loss: 0.05572017282247543\n",
      "Iteration 11773, Loss: 0.05572915077209473\n",
      "Iteration 11774, Loss: 0.05564236640930176\n",
      "Iteration 11775, Loss: 0.05580250546336174\n",
      "Iteration 11776, Loss: 0.055839184671640396\n",
      "Iteration 11777, Loss: 0.05569974705576897\n",
      "Iteration 11778, Loss: 0.055766306817531586\n",
      "Iteration 11779, Loss: 0.055870138108730316\n",
      "Iteration 11780, Loss: 0.05586675927042961\n",
      "Iteration 11781, Loss: 0.055767178535461426\n",
      "Iteration 11782, Loss: 0.05565301701426506\n",
      "Iteration 11783, Loss: 0.05572585389018059\n",
      "Iteration 11784, Loss: 0.0556461438536644\n",
      "Iteration 11785, Loss: 0.05574468895792961\n",
      "Iteration 11786, Loss: 0.055788520723581314\n",
      "Iteration 11787, Loss: 0.055729709565639496\n",
      "Iteration 11788, Loss: 0.05565444752573967\n",
      "Iteration 11789, Loss: 0.05567542836070061\n",
      "Iteration 11790, Loss: 0.05567503347992897\n",
      "Iteration 11791, Loss: 0.05567888543009758\n",
      "Iteration 11792, Loss: 0.05564642325043678\n",
      "Iteration 11793, Loss: 0.05565210431814194\n",
      "Iteration 11794, Loss: 0.05566160008311272\n",
      "Iteration 11795, Loss: 0.05563497543334961\n",
      "Iteration 11796, Loss: 0.055738966912031174\n",
      "Iteration 11797, Loss: 0.055725693702697754\n",
      "Iteration 11798, Loss: 0.0556565523147583\n",
      "Iteration 11799, Loss: 0.05569140240550041\n",
      "Iteration 11800, Loss: 0.05563155934214592\n",
      "Iteration 11801, Loss: 0.05573916807770729\n",
      "Iteration 11802, Loss: 0.05575963109731674\n",
      "Iteration 11803, Loss: 0.05568218231201172\n",
      "Iteration 11804, Loss: 0.055734675377607346\n",
      "Iteration 11805, Loss: 0.05576412007212639\n",
      "Iteration 11806, Loss: 0.05563179776072502\n",
      "Iteration 11807, Loss: 0.05581410974264145\n",
      "Iteration 11808, Loss: 0.055911146104335785\n",
      "Iteration 11809, Loss: 0.05590057745575905\n",
      "Iteration 11810, Loss: 0.055795591324567795\n",
      "Iteration 11811, Loss: 0.05565103143453598\n",
      "Iteration 11812, Loss: 0.055816929787397385\n",
      "Iteration 11813, Loss: 0.05584120750427246\n",
      "Iteration 11814, Loss: 0.05569680780172348\n",
      "Iteration 11815, Loss: 0.055770836770534515\n",
      "Iteration 11816, Loss: 0.05587490648031235\n",
      "Iteration 11817, Loss: 0.05587379261851311\n",
      "Iteration 11818, Loss: 0.055777113884687424\n",
      "Iteration 11819, Loss: 0.055632591247558594\n",
      "Iteration 11820, Loss: 0.055682383477687836\n",
      "Iteration 11821, Loss: 0.05565270036458969\n",
      "Iteration 11822, Loss: 0.05564379692077637\n",
      "Iteration 11823, Loss: 0.055706147104501724\n",
      "Iteration 11824, Loss: 0.05565941706299782\n",
      "Iteration 11825, Loss: 0.05573340505361557\n",
      "Iteration 11826, Loss: 0.055779021233320236\n",
      "Iteration 11827, Loss: 0.055722832679748535\n",
      "Iteration 11828, Loss: 0.05565985292196274\n",
      "Iteration 11829, Loss: 0.05567725747823715\n",
      "Iteration 11830, Loss: 0.0556764230132103\n",
      "Iteration 11831, Loss: 0.055682580918073654\n",
      "Iteration 11832, Loss: 0.055640898644924164\n",
      "Iteration 11833, Loss: 0.0556766614317894\n",
      "Iteration 11834, Loss: 0.05564252659678459\n",
      "Iteration 11835, Loss: 0.05569303408265114\n",
      "Iteration 11836, Loss: 0.05564431473612785\n",
      "Iteration 11837, Loss: 0.05575088784098625\n",
      "Iteration 11838, Loss: 0.0557892732322216\n",
      "Iteration 11839, Loss: 0.05571647733449936\n",
      "Iteration 11840, Loss: 0.0556943416595459\n",
      "Iteration 11841, Loss: 0.05573360249400139\n",
      "Iteration 11842, Loss: 0.05563223361968994\n",
      "Iteration 11843, Loss: 0.05575883761048317\n",
      "Iteration 11844, Loss: 0.055807434022426605\n",
      "Iteration 11845, Loss: 0.05575716495513916\n",
      "Iteration 11846, Loss: 0.05563235655426979\n",
      "Iteration 11847, Loss: 0.05582618713378906\n",
      "Iteration 11848, Loss: 0.05586656183004379\n",
      "Iteration 11849, Loss: 0.05573729798197746\n",
      "Iteration 11850, Loss: 0.05573118105530739\n",
      "Iteration 11851, Loss: 0.05582774057984352\n",
      "Iteration 11852, Loss: 0.05581986904144287\n",
      "Iteration 11853, Loss: 0.05571635812520981\n",
      "Iteration 11854, Loss: 0.05572323128581047\n",
      "Iteration 11855, Loss: 0.05578073114156723\n",
      "Iteration 11856, Loss: 0.05566680431365967\n",
      "Iteration 11857, Loss: 0.05577222630381584\n",
      "Iteration 11858, Loss: 0.055857300758361816\n",
      "Iteration 11859, Loss: 0.055835604667663574\n",
      "Iteration 11860, Loss: 0.055719856172800064\n",
      "Iteration 11861, Loss: 0.0557330884039402\n",
      "Iteration 11862, Loss: 0.05580472946166992\n",
      "Iteration 11863, Loss: 0.055704038590192795\n",
      "Iteration 11864, Loss: 0.05573773384094238\n",
      "Iteration 11865, Loss: 0.05581732839345932\n",
      "Iteration 11866, Loss: 0.05579245463013649\n",
      "Iteration 11867, Loss: 0.05567769333720207\n",
      "Iteration 11868, Loss: 0.055786848068237305\n",
      "Iteration 11869, Loss: 0.055853210389614105\n",
      "Iteration 11870, Loss: 0.05574369803071022\n",
      "Iteration 11871, Loss: 0.055713534355163574\n",
      "Iteration 11872, Loss: 0.05579928681254387\n",
      "Iteration 11873, Loss: 0.0557812862098217\n",
      "Iteration 11874, Loss: 0.055671773850917816\n",
      "Iteration 11875, Loss: 0.05578915402293205\n",
      "Iteration 11876, Loss: 0.055850666016340256\n",
      "Iteration 11877, Loss: 0.055734872817993164\n",
      "Iteration 11878, Loss: 0.05572378635406494\n",
      "Iteration 11879, Loss: 0.05581343173980713\n",
      "Iteration 11880, Loss: 0.05579952523112297\n",
      "Iteration 11881, Loss: 0.055692873895168304\n",
      "Iteration 11882, Loss: 0.05575752258300781\n",
      "Iteration 11883, Loss: 0.05581677332520485\n",
      "Iteration 11884, Loss: 0.05569859594106674\n",
      "Iteration 11885, Loss: 0.05575243756175041\n",
      "Iteration 11886, Loss: 0.05584331601858139\n",
      "Iteration 11887, Loss: 0.05583067983388901\n",
      "Iteration 11888, Loss: 0.05572490021586418\n",
      "Iteration 11889, Loss: 0.055713534355163574\n",
      "Iteration 11890, Loss: 0.05577202886343002\n",
      "Iteration 11891, Loss: 0.055653929710388184\n",
      "Iteration 11892, Loss: 0.055785976350307465\n",
      "Iteration 11893, Loss: 0.05587701126933098\n",
      "Iteration 11894, Loss: 0.0558648519217968\n",
      "Iteration 11895, Loss: 0.055759549140930176\n",
      "Iteration 11896, Loss: 0.05566680431365967\n",
      "Iteration 11897, Loss: 0.05572795867919922\n",
      "Iteration 11898, Loss: 0.05562468618154526\n",
      "Iteration 11899, Loss: 0.055753193795681\n",
      "Iteration 11900, Loss: 0.05578581616282463\n",
      "Iteration 11901, Loss: 0.055712345987558365\n",
      "Iteration 11902, Loss: 0.05569521710276604\n",
      "Iteration 11903, Loss: 0.055726293474435806\n",
      "Iteration 11904, Loss: 0.05563056468963623\n",
      "Iteration 11905, Loss: 0.055661797523498535\n",
      "Iteration 11906, Loss: 0.055621981620788574\n",
      "Iteration 11907, Loss: 0.05573006719350815\n",
      "Iteration 11908, Loss: 0.05572176352143288\n",
      "Iteration 11909, Loss: 0.05564582720398903\n",
      "Iteration 11910, Loss: 0.05571480840444565\n",
      "Iteration 11911, Loss: 0.05566056817770004\n",
      "Iteration 11912, Loss: 0.055737853050231934\n",
      "Iteration 11913, Loss: 0.05578506365418434\n",
      "Iteration 11914, Loss: 0.05573173612356186\n",
      "Iteration 11915, Loss: 0.05565309524536133\n",
      "Iteration 11916, Loss: 0.055711232125759125\n",
      "Iteration 11917, Loss: 0.055654287338256836\n",
      "Iteration 11918, Loss: 0.05571580305695534\n",
      "Iteration 11919, Loss: 0.05573340505361557\n",
      "Iteration 11920, Loss: 0.05564630404114723\n",
      "Iteration 11921, Loss: 0.05579332634806633\n",
      "Iteration 11922, Loss: 0.05583711713552475\n",
      "Iteration 11923, Loss: 0.05572176352143288\n",
      "Iteration 11924, Loss: 0.055731456726789474\n",
      "Iteration 11925, Loss: 0.055816732347011566\n",
      "Iteration 11926, Loss: 0.0557890348136425\n",
      "Iteration 11927, Loss: 0.055660806596279144\n",
      "Iteration 11928, Loss: 0.0558168888092041\n",
      "Iteration 11929, Loss: 0.055897992104291916\n",
      "Iteration 11930, Loss: 0.055811963975429535\n",
      "Iteration 11931, Loss: 0.05564749613404274\n",
      "Iteration 11932, Loss: 0.05574842542409897\n",
      "Iteration 11933, Loss: 0.055744294077157974\n",
      "Iteration 11934, Loss: 0.055624328553676605\n",
      "Iteration 11935, Loss: 0.0558425597846508\n",
      "Iteration 11936, Loss: 0.05591229721903801\n",
      "Iteration 11937, Loss: 0.05582551285624504\n",
      "Iteration 11938, Loss: 0.0556541308760643\n",
      "Iteration 11939, Loss: 0.055795036256313324\n",
      "Iteration 11940, Loss: 0.05584621801972389\n",
      "Iteration 11941, Loss: 0.05576280876994133\n",
      "Iteration 11942, Loss: 0.05566374585032463\n",
      "Iteration 11943, Loss: 0.05572640895843506\n",
      "Iteration 11944, Loss: 0.05564991757273674\n",
      "Iteration 11945, Loss: 0.05576976388692856\n",
      "Iteration 11946, Loss: 0.05582030862569809\n",
      "Iteration 11947, Loss: 0.05574600026011467\n",
      "Iteration 11948, Loss: 0.05567646026611328\n",
      "Iteration 11949, Loss: 0.055742185562849045\n",
      "Iteration 11950, Loss: 0.05568472668528557\n",
      "Iteration 11951, Loss: 0.05572668835520744\n",
      "Iteration 11952, Loss: 0.05576980113983154\n",
      "Iteration 11953, Loss: 0.05571063607931137\n",
      "Iteration 11954, Loss: 0.05568961426615715\n",
      "Iteration 11955, Loss: 0.05572021007537842\n",
      "Iteration 11956, Loss: 0.05564801022410393\n",
      "Iteration 11957, Loss: 0.055706821382045746\n",
      "Iteration 11958, Loss: 0.05571353808045387\n",
      "Iteration 11959, Loss: 0.05563541501760483\n",
      "Iteration 11960, Loss: 0.055789828300476074\n",
      "Iteration 11961, Loss: 0.05580771341919899\n",
      "Iteration 11962, Loss: 0.055668119341135025\n",
      "Iteration 11963, Loss: 0.055786214768886566\n",
      "Iteration 11964, Loss: 0.05587836354970932\n",
      "Iteration 11965, Loss: 0.05585217475891113\n",
      "Iteration 11966, Loss: 0.05572299286723137\n",
      "Iteration 11967, Loss: 0.05574393272399902\n",
      "Iteration 11968, Loss: 0.05582913011312485\n",
      "Iteration 11969, Loss: 0.05574556440114975\n",
      "Iteration 11970, Loss: 0.05569307133555412\n",
      "Iteration 11971, Loss: 0.05576026439666748\n",
      "Iteration 11972, Loss: 0.05571623891592026\n",
      "Iteration 11973, Loss: 0.05566096305847168\n",
      "Iteration 11974, Loss: 0.05568190664052963\n",
      "Iteration 11975, Loss: 0.05566863343119621\n",
      "Iteration 11976, Loss: 0.05566803738474846\n",
      "Iteration 11977, Loss: 0.05566354840993881\n",
      "Iteration 11978, Loss: 0.055644433945417404\n",
      "Iteration 11979, Loss: 0.055689215660095215\n",
      "Iteration 11980, Loss: 0.05568504333496094\n",
      "Iteration 11981, Loss: 0.055645667016506195\n",
      "Iteration 11982, Loss: 0.055634621530771255\n",
      "Iteration 11983, Loss: 0.05567113682627678\n",
      "Iteration 11984, Loss: 0.05562051385641098\n",
      "Iteration 11985, Loss: 0.05567789077758789\n",
      "Iteration 11986, Loss: 0.05563895031809807\n",
      "Iteration 11987, Loss: 0.055624645203351974\n",
      "Iteration 11988, Loss: 0.05572152137756348\n",
      "Iteration 11989, Loss: 0.05569350719451904\n",
      "Iteration 11990, Loss: 0.05568377301096916\n",
      "Iteration 11991, Loss: 0.05570090189576149\n",
      "Iteration 11992, Loss: 0.05562905594706535\n",
      "Iteration 11993, Loss: 0.055635809898376465\n",
      "Iteration 11994, Loss: 0.05566704645752907\n",
      "Iteration 11995, Loss: 0.05563819408416748\n",
      "Iteration 11996, Loss: 0.05562635511159897\n",
      "Iteration 11997, Loss: 0.05570173263549805\n",
      "Iteration 11998, Loss: 0.05569116398692131\n",
      "Iteration 11999, Loss: 0.05565210431814194\n",
      "Iteration 12000, Loss: 0.055651549249887466\n",
      "Iteration 12001, Loss: 0.05569048970937729\n",
      "Iteration 12002, Loss: 0.05569279193878174\n",
      "Iteration 12003, Loss: 0.055630527436733246\n",
      "Iteration 12004, Loss: 0.05569203943014145\n",
      "Iteration 12005, Loss: 0.05564924329519272\n",
      "Iteration 12006, Loss: 0.05572152137756348\n",
      "Iteration 12007, Loss: 0.055718742311000824\n",
      "Iteration 12008, Loss: 0.05564209073781967\n",
      "Iteration 12009, Loss: 0.05565313622355461\n",
      "Iteration 12010, Loss: 0.055688582360744476\n",
      "Iteration 12011, Loss: 0.055670421570539474\n",
      "Iteration 12012, Loss: 0.05568961426615715\n",
      "Iteration 12013, Loss: 0.05568087100982666\n",
      "Iteration 12014, Loss: 0.055682700127363205\n",
      "Iteration 12015, Loss: 0.05568385124206543\n",
      "Iteration 12016, Loss: 0.05566100403666496\n",
      "Iteration 12017, Loss: 0.05564606189727783\n",
      "Iteration 12018, Loss: 0.055720847100019455\n",
      "Iteration 12019, Loss: 0.05572768300771713\n",
      "Iteration 12020, Loss: 0.055642448365688324\n",
      "Iteration 12021, Loss: 0.0557578019797802\n",
      "Iteration 12022, Loss: 0.05575231835246086\n",
      "Iteration 12023, Loss: 0.05563446134328842\n",
      "Iteration 12024, Loss: 0.055653851479291916\n",
      "Iteration 12025, Loss: 0.055660806596279144\n",
      "Iteration 12026, Loss: 0.05562826246023178\n",
      "Iteration 12027, Loss: 0.055657148361206055\n",
      "Iteration 12028, Loss: 0.05565190315246582\n",
      "Iteration 12029, Loss: 0.055617813020944595\n",
      "Iteration 12030, Loss: 0.05576273053884506\n",
      "Iteration 12031, Loss: 0.055734992027282715\n",
      "Iteration 12032, Loss: 0.05566565319895744\n",
      "Iteration 12033, Loss: 0.055702805519104004\n",
      "Iteration 12034, Loss: 0.05564066022634506\n",
      "Iteration 12035, Loss: 0.055775128304958344\n",
      "Iteration 12036, Loss: 0.055786650627851486\n",
      "Iteration 12037, Loss: 0.055633388459682465\n",
      "Iteration 12038, Loss: 0.05581430718302727\n",
      "Iteration 12039, Loss: 0.05590740963816643\n",
      "Iteration 12040, Loss: 0.05588337033987045\n",
      "Iteration 12041, Loss: 0.055759429931640625\n",
      "Iteration 12042, Loss: 0.055700067430734634\n",
      "Iteration 12043, Loss: 0.05579102411866188\n",
      "Iteration 12044, Loss: 0.05572263523936272\n",
      "Iteration 12045, Loss: 0.055707138031721115\n",
      "Iteration 12046, Loss: 0.05576483532786369\n",
      "Iteration 12047, Loss: 0.05572454258799553\n",
      "Iteration 12048, Loss: 0.055658578872680664\n",
      "Iteration 12049, Loss: 0.05572092905640602\n",
      "Iteration 12050, Loss: 0.055673204362392426\n",
      "Iteration 12051, Loss: 0.05571246147155762\n",
      "Iteration 12052, Loss: 0.055744171142578125\n",
      "Iteration 12053, Loss: 0.05567856878042221\n",
      "Iteration 12054, Loss: 0.055731259286403656\n",
      "Iteration 12055, Loss: 0.055750131607055664\n",
      "Iteration 12056, Loss: 0.05563509464263916\n",
      "Iteration 12057, Loss: 0.0557376965880394\n",
      "Iteration 12058, Loss: 0.05574337765574455\n",
      "Iteration 12059, Loss: 0.05563418194651604\n",
      "Iteration 12060, Loss: 0.05580707639455795\n",
      "Iteration 12061, Loss: 0.05585312843322754\n",
      "Iteration 12062, Loss: 0.05574333667755127\n",
      "Iteration 12063, Loss: 0.05571004003286362\n",
      "Iteration 12064, Loss: 0.05579296872019768\n",
      "Iteration 12065, Loss: 0.05575935170054436\n",
      "Iteration 12066, Loss: 0.05562714859843254\n",
      "Iteration 12067, Loss: 0.05583481118083\n",
      "Iteration 12068, Loss: 0.05588650703430176\n",
      "Iteration 12069, Loss: 0.05576908588409424\n",
      "Iteration 12070, Loss: 0.055698197335004807\n",
      "Iteration 12071, Loss: 0.0557887963950634\n",
      "Iteration 12072, Loss: 0.05577246472239494\n",
      "Iteration 12073, Loss: 0.05565834417939186\n",
      "Iteration 12074, Loss: 0.055809538811445236\n",
      "Iteration 12075, Loss: 0.055877409875392914\n",
      "Iteration 12076, Loss: 0.05577516555786133\n",
      "Iteration 12077, Loss: 0.055683813989162445\n",
      "Iteration 12078, Loss: 0.05576610937714577\n",
      "Iteration 12079, Loss: 0.05574334040284157\n",
      "Iteration 12080, Loss: 0.055622342973947525\n",
      "Iteration 12081, Loss: 0.055862270295619965\n",
      "Iteration 12082, Loss: 0.05593482777476311\n",
      "Iteration 12083, Loss: 0.05583568662405014\n",
      "Iteration 12084, Loss: 0.05564705654978752\n",
      "Iteration 12085, Loss: 0.05576865002512932\n",
      "Iteration 12086, Loss: 0.05579563230276108\n",
      "Iteration 12087, Loss: 0.05570884793996811\n",
      "Iteration 12088, Loss: 0.05571683496236801\n",
      "Iteration 12089, Loss: 0.055765312165021896\n",
      "Iteration 12090, Loss: 0.05566319078207016\n",
      "Iteration 12091, Loss: 0.05577067658305168\n",
      "Iteration 12092, Loss: 0.055839698761701584\n",
      "Iteration 12093, Loss: 0.055786214768886566\n",
      "Iteration 12094, Loss: 0.05565170571208\n",
      "Iteration 12095, Loss: 0.05580926313996315\n",
      "Iteration 12096, Loss: 0.055861275643110275\n",
      "Iteration 12097, Loss: 0.05574905872344971\n",
      "Iteration 12098, Loss: 0.05570976063609123\n",
      "Iteration 12099, Loss: 0.05579523369669914\n",
      "Iteration 12100, Loss: 0.055770281702280045\n",
      "Iteration 12101, Loss: 0.05565432831645012\n",
      "Iteration 12102, Loss: 0.05581045150756836\n",
      "Iteration 12103, Loss: 0.05587105080485344\n",
      "Iteration 12104, Loss: 0.055759113281965256\n",
      "Iteration 12105, Loss: 0.055702369660139084\n",
      "Iteration 12106, Loss: 0.05578911304473877\n",
      "Iteration 12107, Loss: 0.05576980113983154\n",
      "Iteration 12108, Loss: 0.055657390505075455\n",
      "Iteration 12109, Loss: 0.05580977723002434\n",
      "Iteration 12110, Loss: 0.05587371438741684\n",
      "Iteration 12111, Loss: 0.05576300993561745\n",
      "Iteration 12112, Loss: 0.05569931119680405\n",
      "Iteration 12113, Loss: 0.05578573793172836\n",
      "Iteration 12114, Loss: 0.05576781556010246\n",
      "Iteration 12115, Loss: 0.055656157433986664\n",
      "Iteration 12116, Loss: 0.055811963975429535\n",
      "Iteration 12117, Loss: 0.05587633699178696\n",
      "Iteration 12118, Loss: 0.05576571077108383\n",
      "Iteration 12119, Loss: 0.05569744110107422\n",
      "Iteration 12120, Loss: 0.05578378960490227\n",
      "Iteration 12121, Loss: 0.05576666444540024\n",
      "Iteration 12122, Loss: 0.05565524101257324\n",
      "Iteration 12123, Loss: 0.05581299588084221\n",
      "Iteration 12124, Loss: 0.055877767503261566\n",
      "Iteration 12125, Loss: 0.05576801300048828\n",
      "Iteration 12126, Loss: 0.05569521710276604\n",
      "Iteration 12127, Loss: 0.05578112602233887\n",
      "Iteration 12128, Loss: 0.05576372519135475\n",
      "Iteration 12129, Loss: 0.05565158650279045\n",
      "Iteration 12130, Loss: 0.055818915367126465\n",
      "Iteration 12131, Loss: 0.0558851957321167\n",
      "Iteration 12132, Loss: 0.05577759072184563\n",
      "Iteration 12133, Loss: 0.05568651482462883\n",
      "Iteration 12134, Loss: 0.05577179044485092\n",
      "Iteration 12135, Loss: 0.05575398728251457\n",
      "Iteration 12136, Loss: 0.05564010143280029\n",
      "Iteration 12137, Loss: 0.05583524703979492\n",
      "Iteration 12138, Loss: 0.05590371415019035\n",
      "Iteration 12139, Loss: 0.055801115930080414\n",
      "Iteration 12140, Loss: 0.05566740036010742\n",
      "Iteration 12141, Loss: 0.05575605481863022\n",
      "Iteration 12142, Loss: 0.05574421212077141\n",
      "Iteration 12143, Loss: 0.055630963295698166\n",
      "Iteration 12144, Loss: 0.05584164708852768\n",
      "Iteration 12145, Loss: 0.05591261386871338\n",
      "Iteration 12146, Loss: 0.055822890251874924\n",
      "Iteration 12147, Loss: 0.05565552040934563\n",
      "Iteration 12148, Loss: 0.05578228086233139\n",
      "Iteration 12149, Loss: 0.05581951513886452\n",
      "Iteration 12150, Loss: 0.055729787796735764\n",
      "Iteration 12151, Loss: 0.05570026487112045\n",
      "Iteration 12152, Loss: 0.05575784295797348\n",
      "Iteration 12153, Loss: 0.05567511171102524\n",
      "Iteration 12154, Loss: 0.055746279656887054\n",
      "Iteration 12155, Loss: 0.05579785630106926\n",
      "Iteration 12156, Loss: 0.05571870133280754\n",
      "Iteration 12157, Loss: 0.05570034310221672\n",
      "Iteration 12158, Loss: 0.05575132742524147\n",
      "Iteration 12159, Loss: 0.05566342920064926\n",
      "Iteration 12160, Loss: 0.05576714128255844\n",
      "Iteration 12161, Loss: 0.055826347321271896\n",
      "Iteration 12162, Loss: 0.055758558213710785\n",
      "Iteration 12163, Loss: 0.05566585063934326\n",
      "Iteration 12164, Loss: 0.05574802681803703\n",
      "Iteration 12165, Loss: 0.055714648216962814\n",
      "Iteration 12166, Loss: 0.05568647384643555\n",
      "Iteration 12167, Loss: 0.05571667477488518\n",
      "Iteration 12168, Loss: 0.05566398426890373\n",
      "Iteration 12169, Loss: 0.05572358891367912\n",
      "Iteration 12170, Loss: 0.055714331567287445\n",
      "Iteration 12171, Loss: 0.055669110268354416\n",
      "Iteration 12172, Loss: 0.055691760033369064\n",
      "Iteration 12173, Loss: 0.055639706552028656\n",
      "Iteration 12174, Loss: 0.05573825165629387\n",
      "Iteration 12175, Loss: 0.05570809170603752\n",
      "Iteration 12176, Loss: 0.055687546730041504\n",
      "Iteration 12177, Loss: 0.05572573468089104\n",
      "Iteration 12178, Loss: 0.055666886270046234\n",
      "Iteration 12179, Loss: 0.055736981332302094\n",
      "Iteration 12180, Loss: 0.05574480816721916\n",
      "Iteration 12181, Loss: 0.055637240409851074\n",
      "Iteration 12182, Loss: 0.055673085153102875\n",
      "Iteration 12183, Loss: 0.055620789527893066\n",
      "Iteration 12184, Loss: 0.05572577565908432\n",
      "Iteration 12185, Loss: 0.05572863668203354\n",
      "Iteration 12186, Loss: 0.05564340204000473\n",
      "Iteration 12187, Loss: 0.055775921791791916\n",
      "Iteration 12188, Loss: 0.05578772351145744\n",
      "Iteration 12189, Loss: 0.05562996864318848\n",
      "Iteration 12190, Loss: 0.05583178997039795\n",
      "Iteration 12191, Loss: 0.0559467077255249\n",
      "Iteration 12192, Loss: 0.055954497307538986\n",
      "Iteration 12193, Loss: 0.05586640164256096\n",
      "Iteration 12194, Loss: 0.05569513887166977\n",
      "Iteration 12195, Loss: 0.05583389848470688\n",
      "Iteration 12196, Loss: 0.055962882936000824\n",
      "Iteration 12197, Loss: 0.055907849222421646\n",
      "Iteration 12198, Loss: 0.055686794221401215\n",
      "Iteration 12199, Loss: 0.05583024024963379\n",
      "Iteration 12200, Loss: 0.055983465164899826\n",
      "Iteration 12201, Loss: 0.056026022881269455\n",
      "Iteration 12202, Loss: 0.055969201028347015\n",
      "Iteration 12203, Loss: 0.05582309141755104\n",
      "Iteration 12204, Loss: 0.055637482553720474\n",
      "Iteration 12205, Loss: 0.055803101509809494\n",
      "Iteration 12206, Loss: 0.05579721927642822\n",
      "Iteration 12207, Loss: 0.05564193055033684\n",
      "Iteration 12208, Loss: 0.05578669160604477\n",
      "Iteration 12209, Loss: 0.055858176201581955\n",
      "Iteration 12210, Loss: 0.05581601709127426\n",
      "Iteration 12211, Loss: 0.055677175521850586\n",
      "Iteration 12212, Loss: 0.05581089109182358\n",
      "Iteration 12213, Loss: 0.0559009313583374\n",
      "Iteration 12214, Loss: 0.05581474304199219\n",
      "Iteration 12215, Loss: 0.05564379692077637\n",
      "Iteration 12216, Loss: 0.055728040635585785\n",
      "Iteration 12217, Loss: 0.055702053010463715\n",
      "Iteration 12218, Loss: 0.05566076561808586\n",
      "Iteration 12219, Loss: 0.05565071105957031\n",
      "Iteration 12220, Loss: 0.0557146891951561\n",
      "Iteration 12221, Loss: 0.05572720617055893\n",
      "Iteration 12222, Loss: 0.0556410551071167\n",
      "Iteration 12223, Loss: 0.055781010538339615\n",
      "Iteration 12224, Loss: 0.055797819048166275\n",
      "Iteration 12225, Loss: 0.05564650148153305\n",
      "Iteration 12226, Loss: 0.05581764504313469\n",
      "Iteration 12227, Loss: 0.055926363915205\n",
      "Iteration 12228, Loss: 0.055919211357831955\n",
      "Iteration 12229, Loss: 0.05581101030111313\n",
      "Iteration 12230, Loss: 0.055655281990766525\n",
      "Iteration 12231, Loss: 0.055841684341430664\n",
      "Iteration 12232, Loss: 0.055899184197187424\n",
      "Iteration 12233, Loss: 0.055780213326215744\n",
      "Iteration 12234, Loss: 0.05569799989461899\n",
      "Iteration 12235, Loss: 0.05578958988189697\n",
      "Iteration 12236, Loss: 0.05578271672129631\n",
      "Iteration 12237, Loss: 0.05568850040435791\n",
      "Iteration 12238, Loss: 0.05575339123606682\n",
      "Iteration 12239, Loss: 0.055797576904296875\n",
      "Iteration 12240, Loss: 0.05567201226949692\n",
      "Iteration 12241, Loss: 0.05577341839671135\n",
      "Iteration 12242, Loss: 0.055863700807094574\n",
      "Iteration 12243, Loss: 0.05585058778524399\n",
      "Iteration 12244, Loss: 0.05574437230825424\n",
      "Iteration 12245, Loss: 0.055688463151454926\n",
      "Iteration 12246, Loss: 0.0557478666305542\n",
      "Iteration 12247, Loss: 0.055631719529628754\n",
      "Iteration 12248, Loss: 0.055798888206481934\n",
      "Iteration 12249, Loss: 0.05588456243276596\n",
      "Iteration 12250, Loss: 0.05586520954966545\n",
      "Iteration 12251, Loss: 0.05575287342071533\n",
      "Iteration 12252, Loss: 0.05568782612681389\n",
      "Iteration 12253, Loss: 0.055761098861694336\n",
      "Iteration 12254, Loss: 0.05566652864217758\n",
      "Iteration 12255, Loss: 0.05575915426015854\n",
      "Iteration 12256, Loss: 0.055833302438259125\n",
      "Iteration 12257, Loss: 0.055806320160627365\n",
      "Iteration 12258, Loss: 0.05568961426615715\n",
      "Iteration 12259, Loss: 0.05577584356069565\n",
      "Iteration 12260, Loss: 0.055845778435468674\n",
      "Iteration 12261, Loss: 0.055735789239406586\n",
      "Iteration 12262, Loss: 0.05572032928466797\n",
      "Iteration 12263, Loss: 0.05580691620707512\n",
      "Iteration 12264, Loss: 0.05579110234975815\n",
      "Iteration 12265, Loss: 0.05568329617381096\n",
      "Iteration 12266, Loss: 0.055772703140974045\n",
      "Iteration 12267, Loss: 0.05583270639181137\n",
      "Iteration 12268, Loss: 0.05571325868368149\n",
      "Iteration 12269, Loss: 0.05574266240000725\n",
      "Iteration 12270, Loss: 0.05583469197154045\n",
      "Iteration 12271, Loss: 0.055823564529418945\n",
      "Iteration 12272, Loss: 0.055719535797834396\n",
      "Iteration 12273, Loss: 0.05571882054209709\n",
      "Iteration 12274, Loss: 0.05577461048960686\n",
      "Iteration 12275, Loss: 0.05565214157104492\n",
      "Iteration 12276, Loss: 0.05578955262899399\n",
      "Iteration 12277, Loss: 0.05588313192129135\n",
      "Iteration 12278, Loss: 0.055873237550258636\n",
      "Iteration 12279, Loss: 0.055769842118024826\n",
      "Iteration 12280, Loss: 0.055649638175964355\n",
      "Iteration 12281, Loss: 0.05570519343018532\n",
      "Iteration 12282, Loss: 0.05563318729400635\n",
      "Iteration 12283, Loss: 0.055622898042201996\n",
      "Iteration 12284, Loss: 0.055733561515808105\n",
      "Iteration 12285, Loss: 0.05568981170654297\n",
      "Iteration 12286, Loss: 0.05570773407816887\n",
      "Iteration 12287, Loss: 0.05575128644704819\n",
      "Iteration 12288, Loss: 0.055693548172712326\n",
      "Iteration 12289, Loss: 0.055698953568935394\n",
      "Iteration 12290, Loss: 0.055708449333906174\n",
      "Iteration 12291, Loss: 0.055658143013715744\n",
      "Iteration 12292, Loss: 0.055669866502285004\n",
      "Iteration 12293, Loss: 0.05564848706126213\n",
      "Iteration 12294, Loss: 0.05563287064433098\n",
      "Iteration 12295, Loss: 0.0556587390601635\n",
      "Iteration 12296, Loss: 0.05564328283071518\n",
      "Iteration 12297, Loss: 0.05562543869018555\n",
      "Iteration 12298, Loss: 0.055688463151454926\n",
      "Iteration 12299, Loss: 0.05565675348043442\n",
      "Iteration 12300, Loss: 0.05571448802947998\n",
      "Iteration 12301, Loss: 0.05570562928915024\n",
      "Iteration 12302, Loss: 0.05566513538360596\n",
      "Iteration 12303, Loss: 0.05567824840545654\n",
      "Iteration 12304, Loss: 0.0556483268737793\n",
      "Iteration 12305, Loss: 0.05561765283346176\n",
      "Iteration 12306, Loss: 0.055696092545986176\n",
      "Iteration 12307, Loss: 0.055648330599069595\n",
      "Iteration 12308, Loss: 0.055735908448696136\n",
      "Iteration 12309, Loss: 0.055748146027326584\n",
      "Iteration 12310, Loss: 0.05563740059733391\n",
      "Iteration 12311, Loss: 0.05578669160604477\n",
      "Iteration 12312, Loss: 0.05582618713378906\n",
      "Iteration 12313, Loss: 0.05572045221924782\n",
      "Iteration 12314, Loss: 0.055723272264003754\n",
      "Iteration 12315, Loss: 0.055799130350351334\n",
      "Iteration 12316, Loss: 0.055748701095581055\n",
      "Iteration 12317, Loss: 0.055643320083618164\n",
      "Iteration 12318, Loss: 0.05570872873067856\n",
      "Iteration 12319, Loss: 0.05563469976186752\n",
      "Iteration 12320, Loss: 0.05575982853770256\n",
      "Iteration 12321, Loss: 0.0558139905333519\n",
      "Iteration 12322, Loss: 0.055768489837646484\n",
      "Iteration 12323, Loss: 0.05563954636454582\n",
      "Iteration 12324, Loss: 0.05584852024912834\n",
      "Iteration 12325, Loss: 0.0559212788939476\n",
      "Iteration 12326, Loss: 0.05581279844045639\n",
      "Iteration 12327, Loss: 0.055661678314208984\n",
      "Iteration 12328, Loss: 0.055748067796230316\n",
      "Iteration 12329, Loss: 0.0557326078414917\n",
      "Iteration 12330, Loss: 0.05562591552734375\n",
      "Iteration 12331, Loss: 0.055846814066171646\n",
      "Iteration 12332, Loss: 0.05590391531586647\n",
      "Iteration 12333, Loss: 0.05578140541911125\n",
      "Iteration 12334, Loss: 0.05569346994161606\n",
      "Iteration 12335, Loss: 0.055787764489650726\n",
      "Iteration 12336, Loss: 0.05577775090932846\n",
      "Iteration 12337, Loss: 0.05567396059632301\n",
      "Iteration 12338, Loss: 0.05577898398041725\n",
      "Iteration 12339, Loss: 0.05583449453115463\n",
      "Iteration 12340, Loss: 0.055711232125759125\n",
      "Iteration 12341, Loss: 0.05574611946940422\n",
      "Iteration 12342, Loss: 0.05584033578634262\n",
      "Iteration 12343, Loss: 0.05582956597208977\n",
      "Iteration 12344, Loss: 0.05572454258799553\n",
      "Iteration 12345, Loss: 0.055712103843688965\n",
      "Iteration 12346, Loss: 0.05576924607157707\n",
      "Iteration 12347, Loss: 0.055650316178798676\n",
      "Iteration 12348, Loss: 0.05578688904643059\n",
      "Iteration 12349, Loss: 0.05587729066610336\n",
      "Iteration 12350, Loss: 0.05586401745676994\n",
      "Iteration 12351, Loss: 0.05575792118906975\n",
      "Iteration 12352, Loss: 0.05566922947764397\n",
      "Iteration 12353, Loss: 0.05573130026459694\n",
      "Iteration 12354, Loss: 0.055632155388593674\n",
      "Iteration 12355, Loss: 0.05575541779398918\n",
      "Iteration 12356, Loss: 0.055798016488552094\n",
      "Iteration 12357, Loss: 0.055735789239406586\n",
      "Iteration 12358, Loss: 0.055654287338256836\n",
      "Iteration 12359, Loss: 0.05569922924041748\n",
      "Iteration 12360, Loss: 0.05564336106181145\n",
      "Iteration 12361, Loss: 0.055672965943813324\n",
      "Iteration 12362, Loss: 0.0556412935256958\n",
      "Iteration 12363, Loss: 0.055735033005476\n",
      "Iteration 12364, Loss: 0.0557255782186985\n",
      "Iteration 12365, Loss: 0.055654048919677734\n",
      "Iteration 12366, Loss: 0.055692873895168304\n",
      "Iteration 12367, Loss: 0.05563632771372795\n",
      "Iteration 12368, Loss: 0.0557478703558445\n",
      "Iteration 12369, Loss: 0.05577588081359863\n",
      "Iteration 12370, Loss: 0.05569974705576897\n",
      "Iteration 12371, Loss: 0.055713456124067307\n",
      "Iteration 12372, Loss: 0.05574945732951164\n",
      "Iteration 12373, Loss: 0.05563664808869362\n",
      "Iteration 12374, Loss: 0.05579066276550293\n",
      "Iteration 12375, Loss: 0.055869024246931076\n",
      "Iteration 12376, Loss: 0.055842041969299316\n",
      "Iteration 12377, Loss: 0.05572501942515373\n",
      "Iteration 12378, Loss: 0.055730223655700684\n",
      "Iteration 12379, Loss: 0.05580461397767067\n",
      "Iteration 12380, Loss: 0.05571051687002182\n",
      "Iteration 12381, Loss: 0.055729031562805176\n",
      "Iteration 12382, Loss: 0.055804334580898285\n",
      "Iteration 12383, Loss: 0.05577810853719711\n",
      "Iteration 12384, Loss: 0.05566664785146713\n",
      "Iteration 12385, Loss: 0.055796943604946136\n",
      "Iteration 12386, Loss: 0.055856745690107346\n",
      "Iteration 12387, Loss: 0.055739086121320724\n",
      "Iteration 12388, Loss: 0.055721960961818695\n",
      "Iteration 12389, Loss: 0.05581275746226311\n",
      "Iteration 12390, Loss: 0.05580099672079086\n",
      "Iteration 12391, Loss: 0.0556970052421093\n",
      "Iteration 12392, Loss: 0.055748067796230316\n",
      "Iteration 12393, Loss: 0.05580393597483635\n",
      "Iteration 12394, Loss: 0.055681824684143066\n",
      "Iteration 12395, Loss: 0.055766504257917404\n",
      "Iteration 12396, Loss: 0.05586020275950432\n",
      "Iteration 12397, Loss: 0.055850230157375336\n",
      "Iteration 12398, Loss: 0.05574735254049301\n",
      "Iteration 12399, Loss: 0.055678170174360275\n",
      "Iteration 12400, Loss: 0.05573364347219467\n",
      "Iteration 12401, Loss: 0.05561467260122299\n",
      "Iteration 12402, Loss: 0.05576304718852043\n",
      "Iteration 12403, Loss: 0.05579733848571777\n",
      "Iteration 12404, Loss: 0.0557246208190918\n",
      "Iteration 12405, Loss: 0.055681150406599045\n",
      "Iteration 12406, Loss: 0.05572112649679184\n",
      "Iteration 12407, Loss: 0.055637482553720474\n",
      "Iteration 12408, Loss: 0.05570574849843979\n",
      "Iteration 12409, Loss: 0.05570721998810768\n",
      "Iteration 12410, Loss: 0.055618010461330414\n",
      "Iteration 12411, Loss: 0.05579444020986557\n",
      "Iteration 12412, Loss: 0.055816493928432465\n",
      "Iteration 12413, Loss: 0.05569728463888168\n",
      "Iteration 12414, Loss: 0.05575207993388176\n",
      "Iteration 12415, Loss: 0.05583357810974121\n",
      "Iteration 12416, Loss: 0.05579014867544174\n",
      "Iteration 12417, Loss: 0.055642448365688324\n",
      "Iteration 12418, Loss: 0.05583985894918442\n",
      "Iteration 12419, Loss: 0.055924735963344574\n",
      "Iteration 12420, Loss: 0.05584720894694328\n",
      "Iteration 12421, Loss: 0.055646900087594986\n",
      "Iteration 12422, Loss: 0.055835407227277756\n",
      "Iteration 12423, Loss: 0.05594170466065407\n",
      "Iteration 12424, Loss: 0.055914126336574554\n",
      "Iteration 12425, Loss: 0.055772505700588226\n",
      "Iteration 12426, Loss: 0.0557071790099144\n",
      "Iteration 12427, Loss: 0.05581645295023918\n",
      "Iteration 12428, Loss: 0.05578506365418434\n",
      "Iteration 12429, Loss: 0.05562424659729004\n",
      "Iteration 12430, Loss: 0.05576865002512932\n",
      "Iteration 12431, Loss: 0.055819276720285416\n",
      "Iteration 12432, Loss: 0.05577031895518303\n",
      "Iteration 12433, Loss: 0.05564296618103981\n",
      "Iteration 12434, Loss: 0.0558369979262352\n",
      "Iteration 12435, Loss: 0.05590089410543442\n",
      "Iteration 12436, Loss: 0.055784862488508224\n",
      "Iteration 12437, Loss: 0.05568774789571762\n",
      "Iteration 12438, Loss: 0.05577818676829338\n",
      "Iteration 12439, Loss: 0.055766623467206955\n",
      "Iteration 12440, Loss: 0.055663228034973145\n",
      "Iteration 12441, Loss: 0.055793922394514084\n",
      "Iteration 12442, Loss: 0.055848799645900726\n",
      "Iteration 12443, Loss: 0.05572458356618881\n",
      "Iteration 12444, Loss: 0.05573618784546852\n",
      "Iteration 12445, Loss: 0.055831193923950195\n",
      "Iteration 12446, Loss: 0.055822014808654785\n",
      "Iteration 12447, Loss: 0.05571925640106201\n",
      "Iteration 12448, Loss: 0.05571591854095459\n",
      "Iteration 12449, Loss: 0.05577067658305168\n",
      "Iteration 12450, Loss: 0.0556468591094017\n",
      "Iteration 12451, Loss: 0.055793169885873795\n",
      "Iteration 12452, Loss: 0.055887382477521896\n",
      "Iteration 12453, Loss: 0.05587756633758545\n",
      "Iteration 12454, Loss: 0.0557740144431591\n",
      "Iteration 12455, Loss: 0.05564387887716293\n",
      "Iteration 12456, Loss: 0.05571119114756584\n",
      "Iteration 12457, Loss: 0.05563263222575188\n",
      "Iteration 12458, Loss: 0.05572144314646721\n",
      "Iteration 12459, Loss: 0.05572132393717766\n",
      "Iteration 12460, Loss: 0.05562702938914299\n",
      "Iteration 12461, Loss: 0.05575251579284668\n",
      "Iteration 12462, Loss: 0.055722676217556\n",
      "Iteration 12463, Loss: 0.055674634873867035\n",
      "Iteration 12464, Loss: 0.05571230500936508\n",
      "Iteration 12465, Loss: 0.05565285682678223\n",
      "Iteration 12466, Loss: 0.05575227737426758\n",
      "Iteration 12467, Loss: 0.05575883388519287\n",
      "Iteration 12468, Loss: 0.05562436580657959\n",
      "Iteration 12469, Loss: 0.05564729496836662\n",
      "Iteration 12470, Loss: 0.055662672966718674\n",
      "Iteration 12471, Loss: 0.05562933534383774\n",
      "Iteration 12472, Loss: 0.055652063339948654\n",
      "Iteration 12473, Loss: 0.055665455758571625\n",
      "Iteration 12474, Loss: 0.05564781278371811\n",
      "Iteration 12475, Loss: 0.05570828914642334\n",
      "Iteration 12476, Loss: 0.05566883459687233\n",
      "Iteration 12477, Loss: 0.05572156235575676\n",
      "Iteration 12478, Loss: 0.05576566979289055\n",
      "Iteration 12479, Loss: 0.055710673332214355\n",
      "Iteration 12480, Loss: 0.05567018315196037\n",
      "Iteration 12481, Loss: 0.055675189942121506\n",
      "Iteration 12482, Loss: 0.05568675324320793\n",
      "Iteration 12483, Loss: 0.05570324510335922\n",
      "Iteration 12484, Loss: 0.05562889575958252\n",
      "Iteration 12485, Loss: 0.05579591169953346\n",
      "Iteration 12486, Loss: 0.0558113269507885\n",
      "Iteration 12487, Loss: 0.05565711110830307\n",
      "Iteration 12488, Loss: 0.055803101509809494\n",
      "Iteration 12489, Loss: 0.055911581963300705\n",
      "Iteration 12490, Loss: 0.055913448333740234\n",
      "Iteration 12491, Loss: 0.05581927299499512\n",
      "Iteration 12492, Loss: 0.055644117295742035\n",
      "Iteration 12493, Loss: 0.05589902773499489\n",
      "Iteration 12494, Loss: 0.056024592369794846\n",
      "Iteration 12495, Loss: 0.05596470832824707\n",
      "Iteration 12496, Loss: 0.05573872849345207\n",
      "Iteration 12497, Loss: 0.055793408304452896\n",
      "Iteration 12498, Loss: 0.0559491328895092\n",
      "Iteration 12499, Loss: 0.0559949092566967\n",
      "Iteration 12500, Loss: 0.05594102665781975\n",
      "Iteration 12501, Loss: 0.055797819048166275\n",
      "Iteration 12502, Loss: 0.05565953254699707\n",
      "Iteration 12503, Loss: 0.055761538445949554\n",
      "Iteration 12504, Loss: 0.05568290129303932\n",
      "Iteration 12505, Loss: 0.05573694035410881\n",
      "Iteration 12506, Loss: 0.055804453790187836\n",
      "Iteration 12507, Loss: 0.05577143281698227\n",
      "Iteration 12508, Loss: 0.055648528039455414\n",
      "Iteration 12509, Loss: 0.05583687871694565\n",
      "Iteration 12510, Loss: 0.055913012474775314\n",
      "Iteration 12511, Loss: 0.05580770969390869\n",
      "Iteration 12512, Loss: 0.05566319078207016\n",
      "Iteration 12513, Loss: 0.05574735254049301\n",
      "Iteration 12514, Loss: 0.05572943016886711\n",
      "Iteration 12515, Loss: 0.05562051385641098\n",
      "Iteration 12516, Loss: 0.05585793778300285\n",
      "Iteration 12517, Loss: 0.05591853708028793\n",
      "Iteration 12518, Loss: 0.05579988285899162\n",
      "Iteration 12519, Loss: 0.05567781254649162\n",
      "Iteration 12520, Loss: 0.055769920349121094\n",
      "Iteration 12521, Loss: 0.055759549140930176\n",
      "Iteration 12522, Loss: 0.055657029151916504\n",
      "Iteration 12523, Loss: 0.05580044165253639\n",
      "Iteration 12524, Loss: 0.05585471913218498\n",
      "Iteration 12525, Loss: 0.05573205277323723\n",
      "Iteration 12526, Loss: 0.05572963133454323\n",
      "Iteration 12527, Loss: 0.05582316964864731\n",
      "Iteration 12528, Loss: 0.05581343546509743\n",
      "Iteration 12529, Loss: 0.055710554122924805\n",
      "Iteration 12530, Loss: 0.0557279996573925\n",
      "Iteration 12531, Loss: 0.055783193558454514\n",
      "Iteration 12532, Loss: 0.0556621178984642\n",
      "Iteration 12533, Loss: 0.05577985569834709\n",
      "Iteration 12534, Loss: 0.05587180703878403\n",
      "Iteration 12535, Loss: 0.05585980415344238\n",
      "Iteration 12536, Loss: 0.055754464119672775\n",
      "Iteration 12537, Loss: 0.05567272752523422\n",
      "Iteration 12538, Loss: 0.05573459714651108\n",
      "Iteration 12539, Loss: 0.05562683194875717\n",
      "Iteration 12540, Loss: 0.055778782814741135\n",
      "Iteration 12541, Loss: 0.05584410950541496\n",
      "Iteration 12542, Loss: 0.05580592155456543\n",
      "Iteration 12543, Loss: 0.055677179247140884\n",
      "Iteration 12544, Loss: 0.05580366030335426\n",
      "Iteration 12545, Loss: 0.05588499829173088\n",
      "Iteration 12546, Loss: 0.05578458309173584\n",
      "Iteration 12547, Loss: 0.055676620453596115\n",
      "Iteration 12548, Loss: 0.055757325142621994\n",
      "Iteration 12549, Loss: 0.05573507398366928\n",
      "Iteration 12550, Loss: 0.05562595650553703\n",
      "Iteration 12551, Loss: 0.05583389848470688\n",
      "Iteration 12552, Loss: 0.05587844178080559\n",
      "Iteration 12553, Loss: 0.05574719235301018\n",
      "Iteration 12554, Loss: 0.05572446435689926\n",
      "Iteration 12555, Loss: 0.05582316964864731\n",
      "Iteration 12556, Loss: 0.05581840127706528\n",
      "Iteration 12557, Loss: 0.05572013184428215\n",
      "Iteration 12558, Loss: 0.05570987984538078\n",
      "Iteration 12559, Loss: 0.05576014518737793\n",
      "Iteration 12560, Loss: 0.05563688278198242\n",
      "Iteration 12561, Loss: 0.05579674243927002\n",
      "Iteration 12562, Loss: 0.055885199457407\n",
      "Iteration 12563, Loss: 0.05586842820048332\n",
      "Iteration 12564, Loss: 0.05575820058584213\n",
      "Iteration 12565, Loss: 0.05567678064107895\n",
      "Iteration 12566, Loss: 0.0557510070502758\n",
      "Iteration 12567, Loss: 0.05566000938415527\n",
      "Iteration 12568, Loss: 0.05575820058584213\n",
      "Iteration 12569, Loss: 0.055828094482421875\n",
      "Iteration 12570, Loss: 0.05579761788249016\n",
      "Iteration 12571, Loss: 0.05567697808146477\n",
      "Iteration 12572, Loss: 0.05579566955566406\n",
      "Iteration 12573, Loss: 0.05586930364370346\n",
      "Iteration 12574, Loss: 0.05576205253601074\n",
      "Iteration 12575, Loss: 0.055697761476039886\n",
      "Iteration 12576, Loss: 0.05578283593058586\n",
      "Iteration 12577, Loss: 0.05576559156179428\n",
      "Iteration 12578, Loss: 0.055656593292951584\n",
      "Iteration 12579, Loss: 0.055809181183576584\n",
      "Iteration 12580, Loss: 0.05587025731801987\n",
      "Iteration 12581, Loss: 0.05575164407491684\n",
      "Iteration 12582, Loss: 0.05571313947439194\n",
      "Iteration 12583, Loss: 0.055804889649152756\n",
      "Iteration 12584, Loss: 0.0557934045791626\n",
      "Iteration 12585, Loss: 0.055689338594675064\n",
      "Iteration 12586, Loss: 0.055758677423000336\n",
      "Iteration 12587, Loss: 0.05581434816122055\n",
      "Iteration 12588, Loss: 0.055691204965114594\n",
      "Iteration 12589, Loss: 0.05576113983988762\n",
      "Iteration 12590, Loss: 0.05585543438792229\n",
      "Iteration 12591, Loss: 0.05584597587585449\n",
      "Iteration 12592, Loss: 0.055743537843227386\n",
      "Iteration 12593, Loss: 0.05568429082632065\n",
      "Iteration 12594, Loss: 0.055738769471645355\n",
      "Iteration 12595, Loss: 0.055621545761823654\n",
      "Iteration 12596, Loss: 0.0557788610458374\n",
      "Iteration 12597, Loss: 0.055835843086242676\n",
      "Iteration 12598, Loss: 0.05578788369894028\n",
      "Iteration 12599, Loss: 0.05565563961863518\n",
      "Iteration 12600, Loss: 0.0558297261595726\n",
      "Iteration 12601, Loss: 0.05590609833598137\n",
      "Iteration 12602, Loss: 0.05580596253275871\n",
      "Iteration 12603, Loss: 0.055659692734479904\n",
      "Iteration 12604, Loss: 0.05574027821421623\n",
      "Iteration 12605, Loss: 0.05571695417165756\n",
      "Iteration 12606, Loss: 0.05562790483236313\n",
      "Iteration 12607, Loss: 0.05567646026611328\n",
      "Iteration 12608, Loss: 0.05564252659678459\n",
      "Iteration 12609, Loss: 0.05565357208251953\n",
      "Iteration 12610, Loss: 0.055664900690317154\n",
      "Iteration 12611, Loss: 0.0556366853415966\n",
      "Iteration 12612, Loss: 0.05569958686828613\n",
      "Iteration 12613, Loss: 0.05564677715301514\n",
      "Iteration 12614, Loss: 0.05574909970164299\n",
      "Iteration 12615, Loss: 0.055794280022382736\n",
      "Iteration 12616, Loss: 0.05573379993438721\n",
      "Iteration 12617, Loss: 0.055661801248788834\n",
      "Iteration 12618, Loss: 0.0557151660323143\n",
      "Iteration 12619, Loss: 0.05564749240875244\n",
      "Iteration 12620, Loss: 0.05572426691651344\n",
      "Iteration 12621, Loss: 0.055750250816345215\n",
      "Iteration 12622, Loss: 0.05567840859293938\n",
      "Iteration 12623, Loss: 0.055736541748046875\n",
      "Iteration 12624, Loss: 0.0557585172355175\n",
      "Iteration 12625, Loss: 0.05562448874115944\n",
      "Iteration 12626, Loss: 0.05575263872742653\n",
      "Iteration 12627, Loss: 0.05577091500163078\n",
      "Iteration 12628, Loss: 0.055675387382507324\n",
      "Iteration 12629, Loss: 0.05575951188802719\n",
      "Iteration 12630, Loss: 0.055809855461120605\n",
      "Iteration 12631, Loss: 0.05570380017161369\n",
      "Iteration 12632, Loss: 0.0557405985891819\n",
      "Iteration 12633, Loss: 0.055818043649196625\n",
      "Iteration 12634, Loss: 0.055773019790649414\n",
      "Iteration 12635, Loss: 0.05564097687602043\n",
      "Iteration 12636, Loss: 0.055814746767282486\n",
      "Iteration 12637, Loss: 0.05585765838623047\n",
      "Iteration 12638, Loss: 0.05572851747274399\n",
      "Iteration 12639, Loss: 0.055738210678100586\n",
      "Iteration 12640, Loss: 0.0558350495994091\n",
      "Iteration 12641, Loss: 0.055820029228925705\n",
      "Iteration 12642, Loss: 0.05570809170603752\n",
      "Iteration 12643, Loss: 0.05574365705251694\n",
      "Iteration 12644, Loss: 0.05580925941467285\n",
      "Iteration 12645, Loss: 0.05570145696401596\n",
      "Iteration 12646, Loss: 0.05574552342295647\n",
      "Iteration 12647, Loss: 0.055829405784606934\n",
      "Iteration 12648, Loss: 0.055805206298828125\n",
      "Iteration 12649, Loss: 0.055689774453639984\n",
      "Iteration 12650, Loss: 0.055771827697753906\n",
      "Iteration 12651, Loss: 0.05583786964416504\n",
      "Iteration 12652, Loss: 0.05572732537984848\n",
      "Iteration 12653, Loss: 0.0557275228202343\n",
      "Iteration 12654, Loss: 0.055813830345869064\n",
      "Iteration 12655, Loss: 0.05579475685954094\n",
      "Iteration 12656, Loss: 0.05568508431315422\n",
      "Iteration 12657, Loss: 0.055772386491298676\n",
      "Iteration 12658, Loss: 0.05583254620432854\n",
      "Iteration 12659, Loss: 0.05571508780121803\n",
      "Iteration 12660, Loss: 0.055740952491760254\n",
      "Iteration 12661, Loss: 0.05583171173930168\n",
      "Iteration 12662, Loss: 0.055817846208810806\n",
      "Iteration 12663, Loss: 0.055712226778268814\n",
      "Iteration 12664, Loss: 0.05573185533285141\n",
      "Iteration 12665, Loss: 0.055789828300476074\n",
      "Iteration 12666, Loss: 0.05567213147878647\n",
      "Iteration 12667, Loss: 0.05577107518911362\n",
      "Iteration 12668, Loss: 0.05586076155304909\n",
      "Iteration 12669, Loss: 0.05584677308797836\n",
      "Iteration 12670, Loss: 0.055740837007761\n",
      "Iteration 12671, Loss: 0.05569283291697502\n",
      "Iteration 12672, Loss: 0.05575240030884743\n",
      "Iteration 12673, Loss: 0.05564054101705551\n",
      "Iteration 12674, Loss: 0.055780451744794846\n",
      "Iteration 12675, Loss: 0.055856309831142426\n",
      "Iteration 12676, Loss: 0.05582936853170395\n",
      "Iteration 12677, Loss: 0.055710237473249435\n",
      "Iteration 12678, Loss: 0.05574866384267807\n",
      "Iteration 12679, Loss: 0.055822134017944336\n",
      "Iteration 12680, Loss: 0.05571838468313217\n",
      "Iteration 12681, Loss: 0.0557279996573925\n",
      "Iteration 12682, Loss: 0.05581009387969971\n",
      "Iteration 12683, Loss: 0.0557890348136425\n",
      "Iteration 12684, Loss: 0.05567626282572746\n",
      "Iteration 12685, Loss: 0.055786848068237305\n",
      "Iteration 12686, Loss: 0.05585237592458725\n",
      "Iteration 12687, Loss: 0.0557403564453125\n",
      "Iteration 12688, Loss: 0.05571707338094711\n",
      "Iteration 12689, Loss: 0.055804334580898285\n",
      "Iteration 12690, Loss: 0.05578836053609848\n",
      "Iteration 12691, Loss: 0.055680036544799805\n",
      "Iteration 12692, Loss: 0.05577655881643295\n",
      "Iteration 12693, Loss: 0.05583791062235832\n",
      "Iteration 12694, Loss: 0.05572156235575676\n",
      "Iteration 12695, Loss: 0.05573379993438721\n",
      "Iteration 12696, Loss: 0.05582376569509506\n",
      "Iteration 12697, Loss: 0.05581005662679672\n",
      "Iteration 12698, Loss: 0.05570375919342041\n",
      "Iteration 12699, Loss: 0.05574270337820053\n",
      "Iteration 12700, Loss: 0.05580195039510727\n",
      "Iteration 12701, Loss: 0.05568397045135498\n",
      "Iteration 12702, Loss: 0.05576284974813461\n",
      "Iteration 12703, Loss: 0.05585344880819321\n",
      "Iteration 12704, Loss: 0.05584057420492172\n",
      "Iteration 12705, Loss: 0.055734872817993164\n",
      "Iteration 12706, Loss: 0.055700063705444336\n",
      "Iteration 12707, Loss: 0.05575966835021973\n",
      "Iteration 12708, Loss: 0.055642805993556976\n",
      "Iteration 12709, Loss: 0.05579189583659172\n",
      "Iteration 12710, Loss: 0.05588182061910629\n",
      "Iteration 12711, Loss: 0.05586814880371094\n",
      "Iteration 12712, Loss: 0.05576193332672119\n",
      "Iteration 12713, Loss: 0.05566366761922836\n",
      "Iteration 12714, Loss: 0.05572414770722389\n",
      "Iteration 12715, Loss: 0.055620431900024414\n",
      "Iteration 12716, Loss: 0.055719297379255295\n",
      "Iteration 12717, Loss: 0.05570896714925766\n",
      "Iteration 12718, Loss: 0.055643003433942795\n",
      "Iteration 12719, Loss: 0.055676817893981934\n",
      "Iteration 12720, Loss: 0.05565381050109863\n",
      "Iteration 12721, Loss: 0.055650077760219574\n",
      "Iteration 12722, Loss: 0.05567213147878647\n",
      "Iteration 12723, Loss: 0.055635612457990646\n",
      "Iteration 12724, Loss: 0.05567491427063942\n",
      "Iteration 12725, Loss: 0.05563275143504143\n",
      "Iteration 12726, Loss: 0.055758874863386154\n",
      "Iteration 12727, Loss: 0.05576769635081291\n",
      "Iteration 12728, Loss: 0.05564868822693825\n",
      "Iteration 12729, Loss: 0.0557832345366478\n",
      "Iteration 12730, Loss: 0.05583854764699936\n",
      "Iteration 12731, Loss: 0.055757008492946625\n",
      "Iteration 12732, Loss: 0.05567038431763649\n",
      "Iteration 12733, Loss: 0.05573916435241699\n",
      "Iteration 12734, Loss: 0.0556773766875267\n",
      "Iteration 12735, Loss: 0.05573372170329094\n",
      "Iteration 12736, Loss: 0.0557759627699852\n",
      "Iteration 12737, Loss: 0.055703047662973404\n",
      "Iteration 12738, Loss: 0.055708687752485275\n",
      "Iteration 12739, Loss: 0.05574699491262436\n",
      "Iteration 12740, Loss: 0.0556437186896801\n",
      "Iteration 12741, Loss: 0.055790744721889496\n",
      "Iteration 12742, Loss: 0.05586529150605202\n",
      "Iteration 12743, Loss: 0.05582607164978981\n",
      "Iteration 12744, Loss: 0.0556994304060936\n",
      "Iteration 12745, Loss: 0.05577242374420166\n",
      "Iteration 12746, Loss: 0.055851105600595474\n",
      "Iteration 12747, Loss: 0.05576646327972412\n",
      "Iteration 12748, Loss: 0.05567844957113266\n",
      "Iteration 12749, Loss: 0.055745601654052734\n",
      "Iteration 12750, Loss: 0.055708568543195724\n",
      "Iteration 12751, Loss: 0.05566243454813957\n",
      "Iteration 12752, Loss: 0.055685363709926605\n",
      "Iteration 12753, Loss: 0.05566354840993881\n",
      "Iteration 12754, Loss: 0.05566636845469475\n",
      "Iteration 12755, Loss: 0.055658262223005295\n",
      "Iteration 12756, Loss: 0.055663786828517914\n",
      "Iteration 12757, Loss: 0.05565381422638893\n",
      "Iteration 12758, Loss: 0.05565488710999489\n",
      "Iteration 12759, Loss: 0.055668316781520844\n",
      "Iteration 12760, Loss: 0.055642884224653244\n",
      "Iteration 12761, Loss: 0.055677615106105804\n",
      "Iteration 12762, Loss: 0.055623650550842285\n",
      "Iteration 12763, Loss: 0.05575617402791977\n",
      "Iteration 12764, Loss: 0.05579400062561035\n",
      "Iteration 12765, Loss: 0.055730901658535004\n",
      "Iteration 12766, Loss: 0.0556635856628418\n",
      "Iteration 12767, Loss: 0.055708687752485275\n",
      "Iteration 12768, Loss: 0.05564650148153305\n",
      "Iteration 12769, Loss: 0.05569661036133766\n",
      "Iteration 12770, Loss: 0.05568961426615715\n",
      "Iteration 12771, Loss: 0.05565035715699196\n",
      "Iteration 12772, Loss: 0.055634938180446625\n",
      "Iteration 12773, Loss: 0.05569025129079819\n",
      "Iteration 12774, Loss: 0.05564828962087631\n",
      "Iteration 12775, Loss: 0.05573884770274162\n",
      "Iteration 12776, Loss: 0.0557582788169384\n",
      "Iteration 12777, Loss: 0.05565901845693588\n",
      "Iteration 12778, Loss: 0.0557713508605957\n",
      "Iteration 12779, Loss: 0.055819474160671234\n",
      "Iteration 12780, Loss: 0.05572088807821274\n",
      "Iteration 12781, Loss: 0.05571826547384262\n",
      "Iteration 12782, Loss: 0.055789195001125336\n",
      "Iteration 12783, Loss: 0.05573451519012451\n",
      "Iteration 12784, Loss: 0.055660687386989594\n",
      "Iteration 12785, Loss: 0.055702608078718185\n",
      "Iteration 12786, Loss: 0.05563092231750488\n",
      "Iteration 12787, Loss: 0.0556897334754467\n",
      "Iteration 12788, Loss: 0.055674873292446136\n",
      "Iteration 12789, Loss: 0.05567216873168945\n",
      "Iteration 12790, Loss: 0.05563410371541977\n",
      "Iteration 12791, Loss: 0.05574425309896469\n",
      "Iteration 12792, Loss: 0.05578029155731201\n",
      "Iteration 12793, Loss: 0.05571361631155014\n",
      "Iteration 12794, Loss: 0.05568452924489975\n",
      "Iteration 12795, Loss: 0.0557078942656517\n",
      "Iteration 12796, Loss: 0.055651746690273285\n",
      "Iteration 12797, Loss: 0.055660367012023926\n",
      "Iteration 12798, Loss: 0.05565663427114487\n",
      "Iteration 12799, Loss: 0.05564817041158676\n",
      "Iteration 12800, Loss: 0.05565544217824936\n",
      "Iteration 12801, Loss: 0.05562834069132805\n",
      "Iteration 12802, Loss: 0.05571838468313217\n",
      "Iteration 12803, Loss: 0.05571373552083969\n",
      "Iteration 12804, Loss: 0.05564264580607414\n",
      "Iteration 12805, Loss: 0.05571838468313217\n",
      "Iteration 12806, Loss: 0.05566517636179924\n",
      "Iteration 12807, Loss: 0.05573118105530739\n",
      "Iteration 12808, Loss: 0.05577949807047844\n",
      "Iteration 12809, Loss: 0.055730026215314865\n",
      "Iteration 12810, Loss: 0.05564220994710922\n",
      "Iteration 12811, Loss: 0.05568353459239006\n",
      "Iteration 12812, Loss: 0.055648963898420334\n",
      "Iteration 12813, Loss: 0.055666886270046234\n",
      "Iteration 12814, Loss: 0.05564061924815178\n",
      "Iteration 12815, Loss: 0.05568202584981918\n",
      "Iteration 12816, Loss: 0.0556691512465477\n",
      "Iteration 12817, Loss: 0.05567479133605957\n",
      "Iteration 12818, Loss: 0.05563521757721901\n",
      "Iteration 12819, Loss: 0.055737100541591644\n",
      "Iteration 12820, Loss: 0.05577151104807854\n",
      "Iteration 12821, Loss: 0.05570821091532707\n",
      "Iteration 12822, Loss: 0.055683933198451996\n",
      "Iteration 12823, Loss: 0.05569633096456528\n",
      "Iteration 12824, Loss: 0.05566680431365967\n",
      "Iteration 12825, Loss: 0.0556793212890625\n",
      "Iteration 12826, Loss: 0.055631641298532486\n",
      "Iteration 12827, Loss: 0.055654846131801605\n",
      "Iteration 12828, Loss: 0.055633507668972015\n",
      "Iteration 12829, Loss: 0.05567900463938713\n",
      "Iteration 12830, Loss: 0.055631160736083984\n",
      "Iteration 12831, Loss: 0.05569152161478996\n",
      "Iteration 12832, Loss: 0.0556357316672802\n",
      "Iteration 12833, Loss: 0.05576237291097641\n",
      "Iteration 12834, Loss: 0.055799130350351334\n",
      "Iteration 12835, Loss: 0.05571889877319336\n",
      "Iteration 12836, Loss: 0.05570026487112045\n",
      "Iteration 12837, Loss: 0.055749695748090744\n",
      "Iteration 12838, Loss: 0.05565754696726799\n",
      "Iteration 12839, Loss: 0.05577266216278076\n",
      "Iteration 12840, Loss: 0.05584029480814934\n",
      "Iteration 12841, Loss: 0.05579455941915512\n",
      "Iteration 12842, Loss: 0.05567169189453125\n",
      "Iteration 12843, Loss: 0.055795472115278244\n",
      "Iteration 12844, Loss: 0.05585702508687973\n",
      "Iteration 12845, Loss: 0.05575200170278549\n",
      "Iteration 12846, Loss: 0.055704355239868164\n",
      "Iteration 12847, Loss: 0.05578549951314926\n",
      "Iteration 12848, Loss: 0.05576058477163315\n",
      "Iteration 12849, Loss: 0.055652063339948654\n",
      "Iteration 12850, Loss: 0.05580425262451172\n",
      "Iteration 12851, Loss: 0.055851541459560394\n",
      "Iteration 12852, Loss: 0.055723629891872406\n",
      "Iteration 12853, Loss: 0.055740516632795334\n",
      "Iteration 12854, Loss: 0.0558372363448143\n",
      "Iteration 12855, Loss: 0.0558299645781517\n",
      "Iteration 12856, Loss: 0.055729471147060394\n",
      "Iteration 12857, Loss: 0.055701255798339844\n",
      "Iteration 12858, Loss: 0.055755775421857834\n",
      "Iteration 12859, Loss: 0.05563676357269287\n",
      "Iteration 12860, Loss: 0.05579400062561035\n",
      "Iteration 12861, Loss: 0.05588142201304436\n",
      "Iteration 12862, Loss: 0.05586572736501694\n",
      "Iteration 12863, Loss: 0.05575704947113991\n",
      "Iteration 12864, Loss: 0.055672768503427505\n",
      "Iteration 12865, Loss: 0.05573372170329094\n",
      "Iteration 12866, Loss: 0.05561637878417969\n",
      "Iteration 12867, Loss: 0.055806998163461685\n",
      "Iteration 12868, Loss: 0.055892229080200195\n",
      "Iteration 12869, Loss: 0.05587407201528549\n",
      "Iteration 12870, Loss: 0.05576348677277565\n",
      "Iteration 12871, Loss: 0.055668555200099945\n",
      "Iteration 12872, Loss: 0.055738210678100586\n",
      "Iteration 12873, Loss: 0.05563843622803688\n",
      "Iteration 12874, Loss: 0.055773817002773285\n",
      "Iteration 12875, Loss: 0.055842798203229904\n",
      "Iteration 12876, Loss: 0.05580846592783928\n",
      "Iteration 12877, Loss: 0.05568277835845947\n",
      "Iteration 12878, Loss: 0.055793922394514084\n",
      "Iteration 12879, Loss: 0.05587279796600342\n",
      "Iteration 12880, Loss: 0.05577008053660393\n",
      "Iteration 12881, Loss: 0.05568981170654297\n",
      "Iteration 12882, Loss: 0.055771950632333755\n",
      "Iteration 12883, Loss: 0.05575108528137207\n",
      "Iteration 12884, Loss: 0.05563966557383537\n",
      "Iteration 12885, Loss: 0.05583357810974121\n",
      "Iteration 12886, Loss: 0.055894337594509125\n",
      "Iteration 12887, Loss: 0.05577477067708969\n",
      "Iteration 12888, Loss: 0.0556972436606884\n",
      "Iteration 12889, Loss: 0.05578958988189697\n",
      "Iteration 12890, Loss: 0.05577874183654785\n",
      "Iteration 12891, Loss: 0.05567590519785881\n",
      "Iteration 12892, Loss: 0.05577616021037102\n",
      "Iteration 12893, Loss: 0.05583035945892334\n",
      "Iteration 12894, Loss: 0.05570606514811516\n",
      "Iteration 12895, Loss: 0.05575064942240715\n",
      "Iteration 12896, Loss: 0.055845342576503754\n",
      "Iteration 12897, Loss: 0.055836718529462814\n",
      "Iteration 12898, Loss: 0.05573491379618645\n",
      "Iteration 12899, Loss: 0.055694662034511566\n",
      "Iteration 12900, Loss: 0.05574830621480942\n",
      "Iteration 12901, Loss: 0.05562698841094971\n",
      "Iteration 12902, Loss: 0.055797379463911057\n",
      "Iteration 12903, Loss: 0.055879317224025726\n",
      "Iteration 12904, Loss: 0.0558573417365551\n",
      "Iteration 12905, Loss: 0.05574282258749008\n",
      "Iteration 12906, Loss: 0.055701058357954025\n",
      "Iteration 12907, Loss: 0.05577262490987778\n",
      "Iteration 12908, Loss: 0.05567070096731186\n",
      "Iteration 12909, Loss: 0.055762093514204025\n",
      "Iteration 12910, Loss: 0.055842798203229904\n",
      "Iteration 12911, Loss: 0.055821262300014496\n",
      "Iteration 12912, Loss: 0.055708568543195724\n",
      "Iteration 12913, Loss: 0.05574389547109604\n",
      "Iteration 12914, Loss: 0.055809617042541504\n",
      "Iteration 12915, Loss: 0.05569728463888168\n",
      "Iteration 12916, Loss: 0.05574957653880119\n",
      "Iteration 12917, Loss: 0.05583719536662102\n",
      "Iteration 12918, Loss: 0.0558222159743309\n",
      "Iteration 12919, Loss: 0.05571484565734863\n",
      "Iteration 12920, Loss: 0.05572839826345444\n",
      "Iteration 12921, Loss: 0.055788200348615646\n",
      "Iteration 12922, Loss: 0.05566978454589844\n",
      "Iteration 12923, Loss: 0.05577373877167702\n",
      "Iteration 12924, Loss: 0.055865250527858734\n",
      "Iteration 12925, Loss: 0.05585325136780739\n",
      "Iteration 12926, Loss: 0.055748503655195236\n",
      "Iteration 12927, Loss: 0.05567967891693115\n",
      "Iteration 12928, Loss: 0.05573701858520508\n",
      "Iteration 12929, Loss: 0.055616818368434906\n",
      "Iteration 12930, Loss: 0.0558064803481102\n",
      "Iteration 12931, Loss: 0.0558909997344017\n",
      "Iteration 12932, Loss: 0.05587005987763405\n",
      "Iteration 12933, Loss: 0.05575605481863022\n",
      "Iteration 12934, Loss: 0.05568345636129379\n",
      "Iteration 12935, Loss: 0.05575593560934067\n",
      "Iteration 12936, Loss: 0.05565929412841797\n",
      "Iteration 12937, Loss: 0.0557611808180809\n",
      "Iteration 12938, Loss: 0.05583294481039047\n",
      "Iteration 12939, Loss: 0.05580385774374008\n",
      "Iteration 12940, Loss: 0.055684249848127365\n",
      "Iteration 12941, Loss: 0.0557858981192112\n",
      "Iteration 12942, Loss: 0.05585912987589836\n",
      "Iteration 12943, Loss: 0.055753111839294434\n",
      "Iteration 12944, Loss: 0.05570412054657936\n",
      "Iteration 12945, Loss: 0.055787961930036545\n",
      "Iteration 12946, Loss: 0.05577019974589348\n",
      "Iteration 12947, Loss: 0.05566048622131348\n",
      "Iteration 12948, Loss: 0.05580548569560051\n",
      "Iteration 12949, Loss: 0.05586807057261467\n",
      "Iteration 12950, Loss: 0.055753469467163086\n",
      "Iteration 12951, Loss: 0.05570920556783676\n",
      "Iteration 12952, Loss: 0.055798254907131195\n",
      "Iteration 12953, Loss: 0.05578446760773659\n",
      "Iteration 12954, Loss: 0.05567769333720207\n",
      "Iteration 12955, Loss: 0.055778149515390396\n",
      "Iteration 12956, Loss: 0.055837951600551605\n",
      "Iteration 12957, Loss: 0.05572148412466049\n",
      "Iteration 12958, Loss: 0.05573352426290512\n",
      "Iteration 12959, Loss: 0.05582340806722641\n",
      "Iteration 12960, Loss: 0.055809181183576584\n",
      "Iteration 12961, Loss: 0.05570181459188461\n",
      "Iteration 12962, Loss: 0.05574607849121094\n",
      "Iteration 12963, Loss: 0.055806636810302734\n",
      "Iteration 12964, Loss: 0.05569084733724594\n",
      "Iteration 12965, Loss: 0.055755894631147385\n",
      "Iteration 12966, Loss: 0.0558449849486351\n",
      "Iteration 12967, Loss: 0.055830005556344986\n",
      "Iteration 12968, Loss: 0.05572199821472168\n",
      "Iteration 12969, Loss: 0.05571973696351051\n",
      "Iteration 12970, Loss: 0.0557815246284008\n",
      "Iteration 12971, Loss: 0.05566740036010742\n",
      "Iteration 12972, Loss: 0.055773138999938965\n",
      "Iteration 12973, Loss: 0.05586179345846176\n",
      "Iteration 12974, Loss: 0.05584697052836418\n",
      "Iteration 12975, Loss: 0.05573960393667221\n",
      "Iteration 12976, Loss: 0.05569624900817871\n",
      "Iteration 12977, Loss: 0.0557585172355175\n",
      "Iteration 12978, Loss: 0.055646419525146484\n",
      "Iteration 12979, Loss: 0.05578458681702614\n",
      "Iteration 12980, Loss: 0.05587009713053703\n",
      "Iteration 12981, Loss: 0.0558524951338768\n",
      "Iteration 12982, Loss: 0.055742740631103516\n",
      "Iteration 12983, Loss: 0.05569374933838844\n",
      "Iteration 12984, Loss: 0.055756453424692154\n",
      "Iteration 12985, Loss: 0.05563974753022194\n",
      "Iteration 12986, Loss: 0.05579420179128647\n",
      "Iteration 12987, Loss: 0.055884480476379395\n",
      "Iteration 12988, Loss: 0.05587081238627434\n",
      "Iteration 12989, Loss: 0.05576447769999504\n",
      "Iteration 12990, Loss: 0.05566028878092766\n",
      "Iteration 12991, Loss: 0.05572017282247543\n",
      "Iteration 12992, Loss: 0.05562261864542961\n",
      "Iteration 12993, Loss: 0.0556948184967041\n",
      "Iteration 12994, Loss: 0.055657267570495605\n",
      "Iteration 12995, Loss: 0.05572303384542465\n",
      "Iteration 12996, Loss: 0.055727481842041016\n",
      "Iteration 12997, Loss: 0.0556364469230175\n",
      "Iteration 12998, Loss: 0.055691443383693695\n",
      "Iteration 12999, Loss: 0.05562174320220947\n",
      "Iteration 13000, Loss: 0.05576392263174057\n",
      "Iteration 13001, Loss: 0.0558096207678318\n",
      "Iteration 13002, Loss: 0.055753033608198166\n",
      "Iteration 13003, Loss: 0.05563906952738762\n",
      "Iteration 13004, Loss: 0.05577409267425537\n",
      "Iteration 13005, Loss: 0.0557631254196167\n",
      "Iteration 13006, Loss: 0.05564669892191887\n",
      "Iteration 13007, Loss: 0.05571655556559563\n",
      "Iteration 13008, Loss: 0.0557078942656517\n",
      "Iteration 13009, Loss: 0.05563608929514885\n",
      "Iteration 13010, Loss: 0.055642567574977875\n",
      "Iteration 13011, Loss: 0.0556640625\n",
      "Iteration 13012, Loss: 0.05562695115804672\n",
      "Iteration 13013, Loss: 0.05566298961639404\n",
      "Iteration 13014, Loss: 0.05562989041209221\n",
      "Iteration 13015, Loss: 0.05566609278321266\n",
      "Iteration 13016, Loss: 0.05564669892191887\n",
      "Iteration 13017, Loss: 0.05562758445739746\n",
      "Iteration 13018, Loss: 0.055730145424604416\n",
      "Iteration 13019, Loss: 0.05570010468363762\n",
      "Iteration 13020, Loss: 0.05568532273173332\n",
      "Iteration 13021, Loss: 0.055713098496198654\n",
      "Iteration 13022, Loss: 0.05562901869416237\n",
      "Iteration 13023, Loss: 0.05580922216176987\n",
      "Iteration 13024, Loss: 0.05585106462240219\n",
      "Iteration 13025, Loss: 0.055738095194101334\n",
      "Iteration 13026, Loss: 0.05571723356842995\n",
      "Iteration 13027, Loss: 0.05580127239227295\n",
      "Iteration 13028, Loss: 0.055766187608242035\n",
      "Iteration 13029, Loss: 0.05562273785471916\n",
      "Iteration 13030, Loss: 0.05586802959442139\n",
      "Iteration 13031, Loss: 0.05595044419169426\n",
      "Iteration 13032, Loss: 0.0558619499206543\n",
      "Iteration 13033, Loss: 0.05565480515360832\n",
      "Iteration 13034, Loss: 0.05583059787750244\n",
      "Iteration 13035, Loss: 0.05593709275126457\n",
      "Iteration 13036, Loss: 0.055911026895046234\n",
      "Iteration 13037, Loss: 0.05576888844370842\n",
      "Iteration 13038, Loss: 0.055707138031721115\n",
      "Iteration 13039, Loss: 0.05581307411193848\n",
      "Iteration 13040, Loss: 0.055765390396118164\n",
      "Iteration 13041, Loss: 0.05564514920115471\n",
      "Iteration 13042, Loss: 0.05568405240774155\n",
      "Iteration 13043, Loss: 0.05562639236450195\n",
      "Iteration 13044, Loss: 0.05571242421865463\n",
      "Iteration 13045, Loss: 0.0556463822722435\n",
      "Iteration 13046, Loss: 0.05575315281748772\n",
      "Iteration 13047, Loss: 0.05580989643931389\n",
      "Iteration 13048, Loss: 0.05576670542359352\n",
      "Iteration 13049, Loss: 0.055634740740060806\n",
      "Iteration 13050, Loss: 0.05586576461791992\n",
      "Iteration 13051, Loss: 0.05595036596059799\n",
      "Iteration 13052, Loss: 0.05585336685180664\n",
      "Iteration 13053, Loss: 0.055626433342695236\n",
      "Iteration 13054, Loss: 0.05572740361094475\n",
      "Iteration 13055, Loss: 0.05572597309947014\n",
      "Iteration 13056, Loss: 0.05562838166952133\n",
      "Iteration 13057, Loss: 0.05582582950592041\n",
      "Iteration 13058, Loss: 0.05586898326873779\n",
      "Iteration 13059, Loss: 0.05573884770274162\n",
      "Iteration 13060, Loss: 0.055729787796735764\n",
      "Iteration 13061, Loss: 0.05582769960165024\n",
      "Iteration 13062, Loss: 0.05582018941640854\n",
      "Iteration 13063, Loss: 0.05571802705526352\n",
      "Iteration 13064, Loss: 0.05571794882416725\n",
      "Iteration 13065, Loss: 0.05577357858419418\n",
      "Iteration 13066, Loss: 0.05565492436289787\n",
      "Iteration 13067, Loss: 0.05578545853495598\n",
      "Iteration 13068, Loss: 0.05587593838572502\n",
      "Iteration 13069, Loss: 0.055861394852399826\n",
      "Iteration 13070, Loss: 0.0557534322142601\n",
      "Iteration 13071, Loss: 0.05568079277873039\n",
      "Iteration 13072, Loss: 0.05575045198202133\n",
      "Iteration 13073, Loss: 0.055654168128967285\n",
      "Iteration 13074, Loss: 0.055765192955732346\n",
      "Iteration 13075, Loss: 0.055837392807006836\n",
      "Iteration 13076, Loss: 0.05580810829997063\n",
      "Iteration 13077, Loss: 0.05568830296397209\n",
      "Iteration 13078, Loss: 0.05578025430440903\n",
      "Iteration 13079, Loss: 0.05585360899567604\n",
      "Iteration 13080, Loss: 0.055745724588632584\n",
      "Iteration 13081, Loss: 0.05571087449789047\n",
      "Iteration 13082, Loss: 0.055796265602111816\n",
      "Iteration 13083, Loss: 0.055779021233320236\n",
      "Iteration 13084, Loss: 0.055669866502285004\n",
      "Iteration 13085, Loss: 0.05579233542084694\n",
      "Iteration 13086, Loss: 0.05585360527038574\n",
      "Iteration 13087, Loss: 0.05573507398366928\n",
      "Iteration 13088, Loss: 0.05572577565908432\n",
      "Iteration 13089, Loss: 0.055817484855651855\n",
      "Iteration 13090, Loss: 0.05580580234527588\n",
      "Iteration 13091, Loss: 0.05570141598582268\n",
      "Iteration 13092, Loss: 0.05574313923716545\n",
      "Iteration 13093, Loss: 0.0557992085814476\n",
      "Iteration 13094, Loss: 0.05567646399140358\n",
      "Iteration 13095, Loss: 0.05577131360769272\n",
      "Iteration 13096, Loss: 0.05586520954966545\n",
      "Iteration 13097, Loss: 0.05585535615682602\n",
      "Iteration 13098, Loss: 0.05575207993388176\n",
      "Iteration 13099, Loss: 0.055672768503427505\n",
      "Iteration 13100, Loss: 0.055728040635585785\n",
      "Iteration 13101, Loss: 0.05562126636505127\n",
      "Iteration 13102, Loss: 0.055719099938869476\n",
      "Iteration 13103, Loss: 0.05570761486887932\n",
      "Iteration 13104, Loss: 0.055645350366830826\n",
      "Iteration 13105, Loss: 0.05567217245697975\n",
      "Iteration 13106, Loss: 0.055661242455244064\n",
      "Iteration 13107, Loss: 0.05565246194601059\n",
      "Iteration 13108, Loss: 0.05568230152130127\n",
      "Iteration 13109, Loss: 0.055630605667829514\n",
      "Iteration 13110, Loss: 0.055744413286447525\n",
      "Iteration 13111, Loss: 0.05578303709626198\n",
      "Iteration 13112, Loss: 0.05572402849793434\n",
      "Iteration 13113, Loss: 0.05565774440765381\n",
      "Iteration 13114, Loss: 0.05566485971212387\n",
      "Iteration 13115, Loss: 0.05569418519735336\n",
      "Iteration 13116, Loss: 0.05571091175079346\n",
      "Iteration 13117, Loss: 0.05563291162252426\n",
      "Iteration 13118, Loss: 0.0558045320212841\n",
      "Iteration 13119, Loss: 0.05583123490214348\n",
      "Iteration 13120, Loss: 0.0556817464530468\n",
      "Iteration 13121, Loss: 0.055785179138183594\n",
      "Iteration 13122, Loss: 0.055895209312438965\n",
      "Iteration 13123, Loss: 0.055899541825056076\n",
      "Iteration 13124, Loss: 0.055808864533901215\n",
      "Iteration 13125, Loss: 0.05563458055257797\n",
      "Iteration 13126, Loss: 0.055919092148542404\n",
      "Iteration 13127, Loss: 0.05605113506317139\n",
      "Iteration 13128, Loss: 0.05599586293101311\n",
      "Iteration 13129, Loss: 0.055773377418518066\n",
      "Iteration 13130, Loss: 0.05576670169830322\n",
      "Iteration 13131, Loss: 0.05592024698853493\n",
      "Iteration 13132, Loss: 0.05596423149108887\n",
      "Iteration 13133, Loss: 0.05590931698679924\n",
      "Iteration 13134, Loss: 0.05576547235250473\n",
      "Iteration 13135, Loss: 0.055704593658447266\n",
      "Iteration 13136, Loss: 0.05580409616231918\n",
      "Iteration 13137, Loss: 0.055722080171108246\n",
      "Iteration 13138, Loss: 0.055710796266794205\n",
      "Iteration 13139, Loss: 0.05578009411692619\n",
      "Iteration 13140, Loss: 0.05574886128306389\n",
      "Iteration 13141, Loss: 0.05562710762023926\n",
      "Iteration 13142, Loss: 0.055864930152893066\n",
      "Iteration 13143, Loss: 0.05594051256775856\n",
      "Iteration 13144, Loss: 0.0558372363448143\n",
      "Iteration 13145, Loss: 0.055643677711486816\n",
      "Iteration 13146, Loss: 0.0557408332824707\n",
      "Iteration 13147, Loss: 0.05574047565460205\n",
      "Iteration 13148, Loss: 0.05564141646027565\n",
      "Iteration 13149, Loss: 0.05581521987915039\n",
      "Iteration 13150, Loss: 0.055869463831186295\n",
      "Iteration 13151, Loss: 0.055756889283657074\n",
      "Iteration 13152, Loss: 0.055703841149806976\n",
      "Iteration 13153, Loss: 0.05579086393117905\n",
      "Iteration 13154, Loss: 0.055770955979824066\n",
      "Iteration 13155, Loss: 0.0556519441306591\n",
      "Iteration 13156, Loss: 0.05582265183329582\n",
      "Iteration 13157, Loss: 0.05589707940816879\n",
      "Iteration 13158, Loss: 0.055803220719099045\n",
      "Iteration 13159, Loss: 0.055659811943769455\n",
      "Iteration 13160, Loss: 0.05575057119131088\n",
      "Iteration 13161, Loss: 0.05574103444814682\n",
      "Iteration 13162, Loss: 0.05562230199575424\n",
      "Iteration 13163, Loss: 0.05585118383169174\n",
      "Iteration 13164, Loss: 0.05592934414744377\n",
      "Iteration 13165, Loss: 0.05585360527038574\n",
      "Iteration 13166, Loss: 0.05567070096731186\n",
      "Iteration 13167, Loss: 0.05582082271575928\n",
      "Iteration 13168, Loss: 0.055926959961652756\n",
      "Iteration 13169, Loss: 0.05589473247528076\n",
      "Iteration 13170, Loss: 0.05574099346995354\n",
      "Iteration 13171, Loss: 0.05574600026011467\n",
      "Iteration 13172, Loss: 0.05585726350545883\n",
      "Iteration 13173, Loss: 0.05582165718078613\n",
      "Iteration 13174, Loss: 0.0556560754776001\n",
      "Iteration 13175, Loss: 0.055835604667663574\n",
      "Iteration 13176, Loss: 0.05594754219055176\n",
      "Iteration 13177, Loss: 0.05591559410095215\n",
      "Iteration 13178, Loss: 0.05576479434967041\n",
      "Iteration 13179, Loss: 0.055727045983076096\n",
      "Iteration 13180, Loss: 0.0558401383459568\n",
      "Iteration 13181, Loss: 0.055818162858486176\n",
      "Iteration 13182, Loss: 0.05566168203949928\n",
      "Iteration 13183, Loss: 0.05582841485738754\n",
      "Iteration 13184, Loss: 0.055940985679626465\n",
      "Iteration 13185, Loss: 0.0559215173125267\n",
      "Iteration 13186, Loss: 0.05579467862844467\n",
      "Iteration 13187, Loss: 0.05569720268249512\n",
      "Iteration 13188, Loss: 0.05581236258149147\n",
      "Iteration 13189, Loss: 0.055817484855651855\n",
      "Iteration 13190, Loss: 0.05568321794271469\n",
      "Iteration 13191, Loss: 0.05578327178955078\n",
      "Iteration 13192, Loss: 0.055876851081848145\n",
      "Iteration 13193, Loss: 0.05586302652955055\n",
      "Iteration 13194, Loss: 0.05575970932841301\n",
      "Iteration 13195, Loss: 0.055692195892333984\n",
      "Iteration 13196, Loss: 0.05577914044260979\n",
      "Iteration 13197, Loss: 0.05573248863220215\n",
      "Iteration 13198, Loss: 0.055693745613098145\n",
      "Iteration 13199, Loss: 0.05574055761098862\n",
      "Iteration 13200, Loss: 0.05570987984538078\n",
      "Iteration 13201, Loss: 0.05566788092255592\n",
      "Iteration 13202, Loss: 0.0557025671005249\n",
      "Iteration 13203, Loss: 0.055667441338300705\n",
      "Iteration 13204, Loss: 0.055693428963422775\n",
      "Iteration 13205, Loss: 0.055689774453639984\n",
      "Iteration 13206, Loss: 0.05566271394491196\n",
      "Iteration 13207, Loss: 0.05565536394715309\n",
      "Iteration 13208, Loss: 0.055674951523542404\n",
      "Iteration 13209, Loss: 0.05564646050333977\n",
      "Iteration 13210, Loss: 0.05573141574859619\n",
      "Iteration 13211, Loss: 0.05573801323771477\n",
      "Iteration 13212, Loss: 0.05564793199300766\n",
      "Iteration 13213, Loss: 0.05575263500213623\n",
      "Iteration 13214, Loss: 0.05575692653656006\n",
      "Iteration 13215, Loss: 0.05561594292521477\n",
      "Iteration 13216, Loss: 0.05572931095957756\n",
      "Iteration 13217, Loss: 0.0556894950568676\n",
      "Iteration 13218, Loss: 0.05570065975189209\n",
      "Iteration 13219, Loss: 0.055730026215314865\n",
      "Iteration 13220, Loss: 0.055643122643232346\n",
      "Iteration 13221, Loss: 0.055790744721889496\n",
      "Iteration 13222, Loss: 0.055831193923950195\n",
      "Iteration 13223, Loss: 0.05571616068482399\n",
      "Iteration 13224, Loss: 0.05573654547333717\n",
      "Iteration 13225, Loss: 0.05581998825073242\n",
      "Iteration 13226, Loss: 0.05578160285949707\n",
      "Iteration 13227, Loss: 0.05564066022634506\n",
      "Iteration 13228, Loss: 0.055845264345407486\n",
      "Iteration 13229, Loss: 0.05592533200979233\n",
      "Iteration 13230, Loss: 0.05583389848470688\n",
      "Iteration 13231, Loss: 0.05564383789896965\n",
      "Iteration 13232, Loss: 0.05578116700053215\n",
      "Iteration 13233, Loss: 0.055816054344177246\n",
      "Iteration 13234, Loss: 0.055724821984767914\n",
      "Iteration 13235, Loss: 0.05570574849843979\n",
      "Iteration 13236, Loss: 0.055763088166713715\n",
      "Iteration 13237, Loss: 0.05567503347992897\n",
      "Iteration 13238, Loss: 0.055752240121364594\n",
      "Iteration 13239, Loss: 0.05581045523285866\n",
      "Iteration 13240, Loss: 0.05573972314596176\n",
      "Iteration 13241, Loss: 0.05567427724599838\n",
      "Iteration 13242, Loss: 0.05572772026062012\n",
      "Iteration 13243, Loss: 0.055644236505031586\n",
      "Iteration 13244, Loss: 0.05577627941966057\n",
      "Iteration 13245, Loss: 0.05583664029836655\n",
      "Iteration 13246, Loss: 0.055782876908779144\n",
      "Iteration 13247, Loss: 0.0556594543159008\n",
      "Iteration 13248, Loss: 0.05580361932516098\n",
      "Iteration 13249, Loss: 0.05584697052836418\n",
      "Iteration 13250, Loss: 0.05571850389242172\n",
      "Iteration 13251, Loss: 0.055748146027326584\n",
      "Iteration 13252, Loss: 0.05584466829895973\n",
      "Iteration 13253, Loss: 0.055833183228969574\n",
      "Iteration 13254, Loss: 0.055728279054164886\n",
      "Iteration 13255, Loss: 0.05571413040161133\n",
      "Iteration 13256, Loss: 0.055775128304958344\n",
      "Iteration 13257, Loss: 0.05567137524485588\n",
      "Iteration 13258, Loss: 0.05576181784272194\n",
      "Iteration 13259, Loss: 0.055839262902736664\n",
      "Iteration 13260, Loss: 0.05581502243876457\n",
      "Iteration 13261, Loss: 0.05570141598582268\n",
      "Iteration 13262, Loss: 0.055757563561201096\n",
      "Iteration 13263, Loss: 0.055823326110839844\n",
      "Iteration 13264, Loss: 0.05571158975362778\n",
      "Iteration 13265, Loss: 0.055739883333444595\n",
      "Iteration 13266, Loss: 0.055826667696237564\n",
      "Iteration 13267, Loss: 0.055811285972595215\n",
      "Iteration 13268, Loss: 0.05570455640554428\n",
      "Iteration 13269, Loss: 0.055744294077157974\n",
      "Iteration 13270, Loss: 0.05580318346619606\n",
      "Iteration 13271, Loss: 0.055685125291347504\n",
      "Iteration 13272, Loss: 0.05576241388916969\n",
      "Iteration 13273, Loss: 0.055852532386779785\n",
      "Iteration 13274, Loss: 0.0558396577835083\n",
      "Iteration 13275, Loss: 0.055734001100063324\n",
      "Iteration 13276, Loss: 0.05570177361369133\n",
      "Iteration 13277, Loss: 0.05575975030660629\n",
      "Iteration 13278, Loss: 0.055641017854213715\n",
      "Iteration 13279, Loss: 0.05579448118805885\n",
      "Iteration 13280, Loss: 0.05588380619883537\n",
      "Iteration 13281, Loss: 0.05586926266551018\n",
      "Iteration 13282, Loss: 0.055761776864528656\n",
      "Iteration 13283, Loss: 0.05566835403442383\n",
      "Iteration 13284, Loss: 0.055734794586896896\n",
      "Iteration 13285, Loss: 0.05563298985362053\n",
      "Iteration 13286, Loss: 0.055774491280317307\n",
      "Iteration 13287, Loss: 0.05583842843770981\n",
      "Iteration 13288, Loss: 0.055799126625061035\n",
      "Iteration 13289, Loss: 0.05566927045583725\n",
      "Iteration 13290, Loss: 0.05581768602132797\n",
      "Iteration 13291, Loss: 0.05589978024363518\n",
      "Iteration 13292, Loss: 0.05580004304647446\n",
      "Iteration 13293, Loss: 0.05566636845469475\n",
      "Iteration 13294, Loss: 0.05574679374694824\n",
      "Iteration 13295, Loss: 0.05572446435689926\n",
      "Iteration 13296, Loss: 0.05562170594930649\n",
      "Iteration 13297, Loss: 0.05576237291097641\n",
      "Iteration 13298, Loss: 0.05574337765574455\n",
      "Iteration 13299, Loss: 0.05565345287322998\n",
      "Iteration 13300, Loss: 0.055699508637189865\n",
      "Iteration 13301, Loss: 0.05565035343170166\n",
      "Iteration 13302, Loss: 0.05574874207377434\n",
      "Iteration 13303, Loss: 0.05576614663004875\n",
      "Iteration 13304, Loss: 0.055661045014858246\n",
      "Iteration 13305, Loss: 0.055768173187971115\n",
      "Iteration 13306, Loss: 0.055818796157836914\n",
      "Iteration 13307, Loss: 0.05573121830821037\n",
      "Iteration 13308, Loss: 0.05569911375641823\n",
      "Iteration 13309, Loss: 0.05575982853770256\n",
      "Iteration 13310, Loss: 0.05568651482462883\n",
      "Iteration 13311, Loss: 0.05573034659028053\n",
      "Iteration 13312, Loss: 0.055774569511413574\n",
      "Iteration 13313, Loss: 0.05568611994385719\n",
      "Iteration 13314, Loss: 0.05574003979563713\n",
      "Iteration 13315, Loss: 0.0557934045791626\n",
      "Iteration 13316, Loss: 0.05570908635854721\n",
      "Iteration 13317, Loss: 0.05571715161204338\n",
      "Iteration 13318, Loss: 0.05577242746949196\n",
      "Iteration 13319, Loss: 0.055694662034511566\n",
      "Iteration 13320, Loss: 0.055723629891872406\n",
      "Iteration 13321, Loss: 0.05577031895518303\n",
      "Iteration 13322, Loss: 0.05568047612905502\n",
      "Iteration 13323, Loss: 0.05574945732951164\n",
      "Iteration 13324, Loss: 0.05580703541636467\n",
      "Iteration 13325, Loss: 0.05573030561208725\n",
      "Iteration 13326, Loss: 0.05568957328796387\n",
      "Iteration 13327, Loss: 0.055743180215358734\n",
      "Iteration 13328, Loss: 0.055662792176008224\n",
      "Iteration 13329, Loss: 0.055763404816389084\n",
      "Iteration 13330, Loss: 0.05581768602132797\n",
      "Iteration 13331, Loss: 0.05574818700551987\n",
      "Iteration 13332, Loss: 0.05567391961812973\n",
      "Iteration 13333, Loss: 0.0557425431907177\n",
      "Iteration 13334, Loss: 0.05569303035736084\n",
      "Iteration 13335, Loss: 0.05571548268198967\n",
      "Iteration 13336, Loss: 0.05575418844819069\n",
      "Iteration 13337, Loss: 0.055696092545986176\n",
      "Iteration 13338, Loss: 0.05570213124155998\n",
      "Iteration 13339, Loss: 0.0557204894721508\n",
      "Iteration 13340, Loss: 0.055648963898420334\n",
      "Iteration 13341, Loss: 0.05568111315369606\n",
      "Iteration 13342, Loss: 0.0556594543159008\n",
      "Iteration 13343, Loss: 0.055700741708278656\n",
      "Iteration 13344, Loss: 0.05566549673676491\n",
      "Iteration 13345, Loss: 0.055717311799526215\n",
      "Iteration 13346, Loss: 0.055751245468854904\n",
      "Iteration 13347, Loss: 0.05568389222025871\n",
      "Iteration 13348, Loss: 0.05572402477264404\n",
      "Iteration 13349, Loss: 0.05574222654104233\n",
      "Iteration 13350, Loss: 0.05562996864318848\n",
      "Iteration 13351, Loss: 0.05566060543060303\n",
      "Iteration 13352, Loss: 0.055646222084760666\n",
      "Iteration 13353, Loss: 0.05564165115356445\n",
      "Iteration 13354, Loss: 0.05564038082957268\n",
      "Iteration 13355, Loss: 0.055671773850917816\n",
      "Iteration 13356, Loss: 0.055649083107709885\n",
      "Iteration 13357, Loss: 0.05571635812520981\n",
      "Iteration 13358, Loss: 0.05568516254425049\n",
      "Iteration 13359, Loss: 0.055705152451992035\n",
      "Iteration 13360, Loss: 0.05574270337820053\n",
      "Iteration 13361, Loss: 0.05568043515086174\n",
      "Iteration 13362, Loss: 0.055722080171108246\n",
      "Iteration 13363, Loss: 0.05573344603180885\n",
      "Iteration 13364, Loss: 0.05564085766673088\n",
      "Iteration 13365, Loss: 0.055653613060712814\n",
      "Iteration 13366, Loss: 0.055667996406555176\n",
      "Iteration 13367, Loss: 0.05562150478363037\n",
      "Iteration 13368, Loss: 0.05566537380218506\n",
      "Iteration 13369, Loss: 0.05565563961863518\n",
      "Iteration 13370, Loss: 0.055637359619140625\n",
      "Iteration 13371, Loss: 0.05572644993662834\n",
      "Iteration 13372, Loss: 0.055687665939331055\n",
      "Iteration 13373, Loss: 0.05570876598358154\n",
      "Iteration 13374, Loss: 0.05575287342071533\n",
      "Iteration 13375, Loss: 0.055698953568935394\n",
      "Iteration 13376, Loss: 0.05568651482462883\n",
      "Iteration 13377, Loss: 0.0556875504553318\n",
      "Iteration 13378, Loss: 0.0556824617087841\n",
      "Iteration 13379, Loss: 0.05570264905691147\n",
      "Iteration 13380, Loss: 0.05562826246023178\n",
      "Iteration 13381, Loss: 0.05580715462565422\n",
      "Iteration 13382, Loss: 0.0558294877409935\n",
      "Iteration 13383, Loss: 0.055677175521850586\n",
      "Iteration 13384, Loss: 0.055791378021240234\n",
      "Iteration 13385, Loss: 0.05590280145406723\n",
      "Iteration 13386, Loss: 0.05590880289673805\n",
      "Iteration 13387, Loss: 0.05581967160105705\n",
      "Iteration 13388, Loss: 0.055646102875471115\n",
      "Iteration 13389, Loss: 0.05590466782450676\n",
      "Iteration 13390, Loss: 0.05603691190481186\n",
      "Iteration 13391, Loss: 0.05598203465342522\n",
      "Iteration 13392, Loss: 0.05576002597808838\n",
      "Iteration 13393, Loss: 0.05577659606933594\n",
      "Iteration 13394, Loss: 0.05592989921569824\n",
      "Iteration 13395, Loss: 0.055973611772060394\n",
      "Iteration 13396, Loss: 0.05591810122132301\n",
      "Iteration 13397, Loss: 0.05577389523386955\n",
      "Iteration 13398, Loss: 0.05569494143128395\n",
      "Iteration 13399, Loss: 0.05579491704702377\n",
      "Iteration 13400, Loss: 0.055713336914777756\n",
      "Iteration 13401, Loss: 0.055717986077070236\n",
      "Iteration 13402, Loss: 0.05578724667429924\n",
      "Iteration 13403, Loss: 0.055756013840436935\n",
      "Iteration 13404, Loss: 0.05563390254974365\n",
      "Iteration 13405, Loss: 0.05585746094584465\n",
      "Iteration 13406, Loss: 0.055933039635419846\n",
      "Iteration 13407, Loss: 0.05582857131958008\n",
      "Iteration 13408, Loss: 0.05564983934164047\n",
      "Iteration 13409, Loss: 0.05573701858520508\n",
      "Iteration 13410, Loss: 0.05572466179728508\n",
      "Iteration 13411, Loss: 0.05561904236674309\n",
      "Iteration 13412, Loss: 0.055849432945251465\n",
      "Iteration 13413, Loss: 0.055906735360622406\n",
      "Iteration 13414, Loss: 0.05579575151205063\n",
      "Iteration 13415, Loss: 0.055677056312561035\n",
      "Iteration 13416, Loss: 0.05576896667480469\n",
      "Iteration 13417, Loss: 0.05575895681977272\n",
      "Iteration 13418, Loss: 0.055646419525146484\n",
      "Iteration 13419, Loss: 0.05582241341471672\n",
      "Iteration 13420, Loss: 0.0558927096426487\n",
      "Iteration 13421, Loss: 0.05580206960439682\n",
      "Iteration 13422, Loss: 0.05566219612956047\n",
      "Iteration 13423, Loss: 0.05576137825846672\n",
      "Iteration 13424, Loss: 0.05576292797923088\n",
      "Iteration 13425, Loss: 0.055644791573286057\n",
      "Iteration 13426, Loss: 0.055820904672145844\n",
      "Iteration 13427, Loss: 0.055900733917951584\n",
      "Iteration 13428, Loss: 0.05583536624908447\n",
      "Iteration 13429, Loss: 0.05567149445414543\n",
      "Iteration 13430, Loss: 0.05581291764974594\n",
      "Iteration 13431, Loss: 0.05590435117483139\n",
      "Iteration 13432, Loss: 0.055848997086286545\n",
      "Iteration 13433, Loss: 0.055666446685791016\n",
      "Iteration 13434, Loss: 0.055840812623500824\n",
      "Iteration 13435, Loss: 0.055966418236494064\n",
      "Iteration 13436, Loss: 0.05594535917043686\n",
      "Iteration 13437, Loss: 0.05579749867320061\n",
      "Iteration 13438, Loss: 0.05569494143128395\n",
      "Iteration 13439, Loss: 0.055815935134887695\n",
      "Iteration 13440, Loss: 0.05580592527985573\n",
      "Iteration 13441, Loss: 0.055656515061855316\n",
      "Iteration 13442, Loss: 0.05582595244050026\n",
      "Iteration 13443, Loss: 0.055934034287929535\n",
      "Iteration 13444, Loss: 0.05590895935893059\n",
      "Iteration 13445, Loss: 0.0557740144431591\n",
      "Iteration 13446, Loss: 0.055709127336740494\n",
      "Iteration 13447, Loss: 0.05581653490662575\n",
      "Iteration 13448, Loss: 0.055787645280361176\n",
      "Iteration 13449, Loss: 0.05564018338918686\n",
      "Iteration 13450, Loss: 0.055757444351911545\n",
      "Iteration 13451, Loss: 0.055800002068281174\n",
      "Iteration 13452, Loss: 0.05574604123830795\n",
      "Iteration 13453, Loss: 0.055647414177656174\n",
      "Iteration 13454, Loss: 0.05575740337371826\n",
      "Iteration 13455, Loss: 0.05573614686727524\n",
      "Iteration 13456, Loss: 0.05566764250397682\n",
      "Iteration 13457, Loss: 0.05571119114756584\n",
      "Iteration 13458, Loss: 0.05567646026611328\n",
      "Iteration 13459, Loss: 0.05570729821920395\n",
      "Iteration 13460, Loss: 0.05570296570658684\n",
      "Iteration 13461, Loss: 0.05566783994436264\n",
      "Iteration 13462, Loss: 0.05568810552358627\n",
      "Iteration 13463, Loss: 0.05563656613230705\n",
      "Iteration 13464, Loss: 0.05570042133331299\n",
      "Iteration 13465, Loss: 0.05569100379943848\n",
      "Iteration 13466, Loss: 0.055655401200056076\n",
      "Iteration 13467, Loss: 0.05566704273223877\n",
      "Iteration 13468, Loss: 0.05567086115479469\n",
      "Iteration 13469, Loss: 0.05566541478037834\n",
      "Iteration 13470, Loss: 0.05567077919840813\n",
      "Iteration 13471, Loss: 0.055639270693063736\n",
      "Iteration 13472, Loss: 0.055700503289699554\n",
      "Iteration 13473, Loss: 0.055698197335004807\n",
      "Iteration 13474, Loss: 0.05563263222575188\n",
      "Iteration 13475, Loss: 0.05564161390066147\n",
      "Iteration 13476, Loss: 0.05567757412791252\n",
      "Iteration 13477, Loss: 0.05564912408590317\n",
      "Iteration 13478, Loss: 0.05572470277547836\n",
      "Iteration 13479, Loss: 0.05571337789297104\n",
      "Iteration 13480, Loss: 0.05566319078207016\n",
      "Iteration 13481, Loss: 0.05568305775523186\n",
      "Iteration 13482, Loss: 0.05563676357269287\n",
      "Iteration 13483, Loss: 0.055643122643232346\n",
      "Iteration 13484, Loss: 0.05564165115356445\n",
      "Iteration 13485, Loss: 0.055670659989118576\n",
      "Iteration 13486, Loss: 0.05565027520060539\n",
      "Iteration 13487, Loss: 0.05571150779724121\n",
      "Iteration 13488, Loss: 0.05567511171102524\n",
      "Iteration 13489, Loss: 0.05571615695953369\n",
      "Iteration 13490, Loss: 0.05575863644480705\n",
      "Iteration 13491, Loss: 0.05570336431264877\n",
      "Iteration 13492, Loss: 0.05568190664052963\n",
      "Iteration 13493, Loss: 0.055684369057416916\n",
      "Iteration 13494, Loss: 0.05568385124206543\n",
      "Iteration 13495, Loss: 0.05570312589406967\n",
      "Iteration 13496, Loss: 0.05562766641378403\n",
      "Iteration 13497, Loss: 0.05580965802073479\n",
      "Iteration 13498, Loss: 0.05583382025361061\n",
      "Iteration 13499, Loss: 0.055682502686977386\n",
      "Iteration 13500, Loss: 0.055786650627851486\n",
      "Iteration 13501, Loss: 0.05589767545461655\n",
      "Iteration 13502, Loss: 0.0559033565223217\n",
      "Iteration 13503, Loss: 0.05581367015838623\n",
      "Iteration 13504, Loss: 0.05563986301422119\n",
      "Iteration 13505, Loss: 0.05591297522187233\n",
      "Iteration 13506, Loss: 0.056045614182949066\n",
      "Iteration 13507, Loss: 0.055990658700466156\n",
      "Iteration 13508, Loss: 0.05576813593506813\n",
      "Iteration 13509, Loss: 0.05577107518911362\n",
      "Iteration 13510, Loss: 0.05592461675405502\n",
      "Iteration 13511, Loss: 0.05596844479441643\n",
      "Iteration 13512, Loss: 0.05591341108083725\n",
      "Iteration 13513, Loss: 0.05576956272125244\n",
      "Iteration 13514, Loss: 0.05570018291473389\n",
      "Iteration 13515, Loss: 0.05579988285899162\n",
      "Iteration 13516, Loss: 0.05571802705526352\n",
      "Iteration 13517, Loss: 0.05571456998586655\n",
      "Iteration 13518, Loss: 0.055783990770578384\n",
      "Iteration 13519, Loss: 0.055752675980329514\n",
      "Iteration 13520, Loss: 0.055631160736083984\n",
      "Iteration 13521, Loss: 0.05586076155304909\n",
      "Iteration 13522, Loss: 0.055936139076948166\n",
      "Iteration 13523, Loss: 0.05583226680755615\n",
      "Iteration 13524, Loss: 0.05564737319946289\n",
      "Iteration 13525, Loss: 0.055738650262355804\n",
      "Iteration 13526, Loss: 0.05573141574859619\n",
      "Iteration 13527, Loss: 0.05562814325094223\n",
      "Iteration 13528, Loss: 0.05583779141306877\n",
      "Iteration 13529, Loss: 0.05589568614959717\n",
      "Iteration 13530, Loss: 0.055785100907087326\n",
      "Iteration 13531, Loss: 0.055682700127363205\n",
      "Iteration 13532, Loss: 0.05577190965414047\n",
      "Iteration 13533, Loss: 0.05575796216726303\n",
      "Iteration 13534, Loss: 0.055643320083618164\n",
      "Iteration 13535, Loss: 0.05582793802022934\n",
      "Iteration 13536, Loss: 0.05589946359395981\n",
      "Iteration 13537, Loss: 0.055806875228881836\n",
      "Iteration 13538, Loss: 0.05565901845693588\n",
      "Iteration 13539, Loss: 0.05575959011912346\n",
      "Iteration 13540, Loss: 0.05576316639780998\n",
      "Iteration 13541, Loss: 0.055649999529123306\n",
      "Iteration 13542, Loss: 0.05581061169505119\n",
      "Iteration 13543, Loss: 0.055885039269924164\n",
      "Iteration 13544, Loss: 0.05581101030111313\n",
      "Iteration 13545, Loss: 0.055657826364040375\n",
      "Iteration 13546, Loss: 0.05579380318522453\n",
      "Iteration 13547, Loss: 0.05584267899394035\n",
      "Iteration 13548, Loss: 0.05574802681803703\n",
      "Iteration 13549, Loss: 0.055690765380859375\n",
      "Iteration 13550, Loss: 0.05575883761048317\n",
      "Iteration 13551, Loss: 0.055696964263916016\n",
      "Iteration 13552, Loss: 0.05570511147379875\n",
      "Iteration 13553, Loss: 0.05573726072907448\n",
      "Iteration 13554, Loss: 0.055632077157497406\n",
      "Iteration 13555, Loss: 0.05580262467265129\n",
      "Iteration 13556, Loss: 0.055859290063381195\n",
      "Iteration 13557, Loss: 0.05577695369720459\n",
      "Iteration 13558, Loss: 0.05566330999135971\n",
      "Iteration 13559, Loss: 0.05575649067759514\n",
      "Iteration 13560, Loss: 0.05573292821645737\n",
      "Iteration 13561, Loss: 0.055655401200056076\n",
      "Iteration 13562, Loss: 0.05567721650004387\n",
      "Iteration 13563, Loss: 0.05564805120229721\n",
      "Iteration 13564, Loss: 0.0556771382689476\n",
      "Iteration 13565, Loss: 0.05564562603831291\n",
      "Iteration 13566, Loss: 0.05566549301147461\n",
      "Iteration 13567, Loss: 0.055631279945373535\n",
      "Iteration 13568, Loss: 0.055736660957336426\n",
      "Iteration 13569, Loss: 0.0557403564453125\n",
      "Iteration 13570, Loss: 0.055647771805524826\n",
      "Iteration 13571, Loss: 0.05576268956065178\n",
      "Iteration 13572, Loss: 0.05577441304922104\n",
      "Iteration 13573, Loss: 0.055628422647714615\n",
      "Iteration 13574, Loss: 0.05583401769399643\n",
      "Iteration 13575, Loss: 0.055935464799404144\n",
      "Iteration 13576, Loss: 0.05591130256652832\n",
      "Iteration 13577, Loss: 0.05578120797872543\n",
      "Iteration 13578, Loss: 0.055692993104457855\n",
      "Iteration 13579, Loss: 0.05580401420593262\n",
      "Iteration 13580, Loss: 0.05577576160430908\n",
      "Iteration 13581, Loss: 0.05565158650279045\n",
      "Iteration 13582, Loss: 0.05572275444865227\n",
      "Iteration 13583, Loss: 0.05573050305247307\n",
      "Iteration 13584, Loss: 0.055659178644418716\n",
      "Iteration 13585, Loss: 0.05576018616557121\n",
      "Iteration 13586, Loss: 0.05577417463064194\n",
      "Iteration 13587, Loss: 0.05564379692077637\n",
      "Iteration 13588, Loss: 0.05576721951365471\n",
      "Iteration 13589, Loss: 0.0558219775557518\n",
      "Iteration 13590, Loss: 0.0557657890021801\n",
      "Iteration 13591, Loss: 0.05563275143504143\n",
      "Iteration 13592, Loss: 0.055804572999477386\n",
      "Iteration 13593, Loss: 0.055822890251874924\n",
      "Iteration 13594, Loss: 0.055668752640485764\n",
      "Iteration 13595, Loss: 0.055798254907131195\n",
      "Iteration 13596, Loss: 0.055910829454660416\n",
      "Iteration 13597, Loss: 0.05591794103384018\n",
      "Iteration 13598, Loss: 0.05582960695028305\n",
      "Iteration 13599, Loss: 0.055658064782619476\n",
      "Iteration 13600, Loss: 0.05588245391845703\n",
      "Iteration 13601, Loss: 0.056010447442531586\n",
      "Iteration 13602, Loss: 0.0559520348906517\n",
      "Iteration 13603, Loss: 0.05572652816772461\n",
      "Iteration 13604, Loss: 0.05580226704478264\n",
      "Iteration 13605, Loss: 0.05595799535512924\n",
      "Iteration 13606, Loss: 0.05600345507264137\n",
      "Iteration 13607, Loss: 0.055949293076992035\n",
      "Iteration 13608, Loss: 0.05580604076385498\n",
      "Iteration 13609, Loss: 0.05564801022410393\n",
      "Iteration 13610, Loss: 0.05574818700551987\n",
      "Iteration 13611, Loss: 0.05566593259572983\n",
      "Iteration 13612, Loss: 0.05575207993388176\n",
      "Iteration 13613, Loss: 0.05582193657755852\n",
      "Iteration 13614, Loss: 0.055790744721889496\n",
      "Iteration 13615, Loss: 0.05566927045583725\n",
      "Iteration 13616, Loss: 0.055807631462812424\n",
      "Iteration 13617, Loss: 0.05588257685303688\n",
      "Iteration 13618, Loss: 0.05577651783823967\n",
      "Iteration 13619, Loss: 0.05568675324320793\n",
      "Iteration 13620, Loss: 0.055770836770534515\n",
      "Iteration 13621, Loss: 0.05575331300497055\n",
      "Iteration 13622, Loss: 0.055644236505031586\n",
      "Iteration 13623, Loss: 0.05582642927765846\n",
      "Iteration 13624, Loss: 0.0558878593146801\n",
      "Iteration 13625, Loss: 0.05576976388692856\n",
      "Iteration 13626, Loss: 0.055699508637189865\n",
      "Iteration 13627, Loss: 0.055790625512599945\n",
      "Iteration 13628, Loss: 0.05577918142080307\n",
      "Iteration 13629, Loss: 0.05567551031708717\n",
      "Iteration 13630, Loss: 0.055777233093976974\n",
      "Iteration 13631, Loss: 0.05583314225077629\n",
      "Iteration 13632, Loss: 0.055711232125759125\n",
      "Iteration 13633, Loss: 0.05574464797973633\n",
      "Iteration 13634, Loss: 0.05583767220377922\n",
      "Iteration 13635, Loss: 0.055827461183071136\n",
      "Iteration 13636, Loss: 0.05572414770722389\n",
      "Iteration 13637, Loss: 0.05571047589182854\n",
      "Iteration 13638, Loss: 0.055766187608242035\n",
      "Iteration 13639, Loss: 0.05564475059509277\n",
      "Iteration 13640, Loss: 0.055793408304452896\n",
      "Iteration 13641, Loss: 0.055885594338178635\n",
      "Iteration 13642, Loss: 0.05587403103709221\n",
      "Iteration 13643, Loss: 0.05576908960938454\n",
      "Iteration 13644, Loss: 0.05565321817994118\n",
      "Iteration 13645, Loss: 0.05572100728750229\n",
      "Iteration 13646, Loss: 0.05563000962138176\n",
      "Iteration 13647, Loss: 0.05574604123830795\n",
      "Iteration 13648, Loss: 0.05577608197927475\n",
      "Iteration 13649, Loss: 0.05570026487112045\n",
      "Iteration 13650, Loss: 0.05571115389466286\n",
      "Iteration 13651, Loss: 0.055742185562849045\n",
      "Iteration 13652, Loss: 0.055614035576581955\n",
      "Iteration 13653, Loss: 0.05561407655477524\n",
      "Iteration 13654, Loss: 0.055731773376464844\n",
      "Iteration 13655, Loss: 0.055747270584106445\n",
      "Iteration 13656, Loss: 0.055667243897914886\n",
      "Iteration 13657, Loss: 0.05575947090983391\n",
      "Iteration 13658, Loss: 0.0557883195579052\n",
      "Iteration 13659, Loss: 0.055641692131757736\n",
      "Iteration 13660, Loss: 0.05581355094909668\n",
      "Iteration 13661, Loss: 0.05592203512787819\n",
      "Iteration 13662, Loss: 0.055923186242580414\n",
      "Iteration 13663, Loss: 0.055828336626291275\n",
      "Iteration 13664, Loss: 0.05565345659852028\n",
      "Iteration 13665, Loss: 0.055892348289489746\n",
      "Iteration 13666, Loss: 0.056020818650722504\n",
      "Iteration 13667, Loss: 0.055962007492780685\n",
      "Iteration 13668, Loss: 0.055736105889081955\n",
      "Iteration 13669, Loss: 0.05579594895243645\n",
      "Iteration 13670, Loss: 0.0559513196349144\n",
      "Iteration 13671, Loss: 0.05599657818675041\n",
      "Iteration 13672, Loss: 0.05594206228852272\n",
      "Iteration 13673, Loss: 0.05579901114106178\n",
      "Iteration 13674, Loss: 0.0556594543159008\n",
      "Iteration 13675, Loss: 0.05576467514038086\n",
      "Iteration 13676, Loss: 0.05569593235850334\n",
      "Iteration 13677, Loss: 0.055719535797834396\n",
      "Iteration 13678, Loss: 0.0557786226272583\n",
      "Iteration 13679, Loss: 0.05573789402842522\n",
      "Iteration 13680, Loss: 0.05561864376068115\n",
      "Iteration 13681, Loss: 0.05566287040710449\n",
      "Iteration 13682, Loss: 0.055642008781433105\n",
      "Iteration 13683, Loss: 0.05564459413290024\n",
      "Iteration 13684, Loss: 0.05567511171102524\n",
      "Iteration 13685, Loss: 0.055660925805568695\n",
      "Iteration 13686, Loss: 0.055687110871076584\n",
      "Iteration 13687, Loss: 0.055644117295742035\n",
      "Iteration 13688, Loss: 0.05574099346995354\n",
      "Iteration 13689, Loss: 0.05578625202178955\n",
      "Iteration 13690, Loss: 0.05573340505361557\n",
      "Iteration 13691, Loss: 0.055636726319789886\n",
      "Iteration 13692, Loss: 0.055637482553720474\n",
      "Iteration 13693, Loss: 0.0557175874710083\n",
      "Iteration 13694, Loss: 0.05573689937591553\n",
      "Iteration 13695, Loss: 0.055660925805568695\n",
      "Iteration 13696, Loss: 0.055762968957424164\n",
      "Iteration 13697, Loss: 0.05578812211751938\n",
      "Iteration 13698, Loss: 0.05563914775848389\n",
      "Iteration 13699, Loss: 0.05581530183553696\n",
      "Iteration 13700, Loss: 0.05592402070760727\n",
      "Iteration 13701, Loss: 0.05592716112732887\n",
      "Iteration 13702, Loss: 0.05583488941192627\n",
      "Iteration 13703, Loss: 0.055659692734479904\n",
      "Iteration 13704, Loss: 0.05588511750102043\n",
      "Iteration 13705, Loss: 0.05601811781525612\n",
      "Iteration 13706, Loss: 0.05596451088786125\n",
      "Iteration 13707, Loss: 0.05574345588684082\n",
      "Iteration 13708, Loss: 0.05578669160604477\n",
      "Iteration 13709, Loss: 0.05593927949666977\n",
      "Iteration 13710, Loss: 0.05598251149058342\n",
      "Iteration 13711, Loss: 0.0559261254966259\n",
      "Iteration 13712, Loss: 0.05578112602233887\n",
      "Iteration 13713, Loss: 0.05568405240774155\n",
      "Iteration 13714, Loss: 0.055785417556762695\n",
      "Iteration 13715, Loss: 0.05570467561483383\n",
      "Iteration 13716, Loss: 0.055722516030073166\n",
      "Iteration 13717, Loss: 0.05579153820872307\n",
      "Iteration 13718, Loss: 0.05575966835021973\n",
      "Iteration 13719, Loss: 0.05563783645629883\n",
      "Iteration 13720, Loss: 0.055850110948085785\n",
      "Iteration 13721, Loss: 0.05592477694153786\n",
      "Iteration 13722, Loss: 0.05581831932067871\n",
      "Iteration 13723, Loss: 0.055656395852565765\n",
      "Iteration 13724, Loss: 0.05574135109782219\n",
      "Iteration 13725, Loss: 0.055724821984767914\n",
      "Iteration 13726, Loss: 0.05561697855591774\n",
      "Iteration 13727, Loss: 0.05585769936442375\n",
      "Iteration 13728, Loss: 0.055915676057338715\n",
      "Iteration 13729, Loss: 0.05579841136932373\n",
      "Iteration 13730, Loss: 0.05567821115255356\n",
      "Iteration 13731, Loss: 0.05577055737376213\n",
      "Iteration 13732, Loss: 0.05576157942414284\n",
      "Iteration 13733, Loss: 0.055657386779785156\n",
      "Iteration 13734, Loss: 0.055801551789045334\n",
      "Iteration 13735, Loss: 0.055859487503767014\n",
      "Iteration 13736, Loss: 0.05574635788798332\n",
      "Iteration 13737, Loss: 0.055712103843688965\n",
      "Iteration 13738, Loss: 0.05579936504364014\n",
      "Iteration 13739, Loss: 0.05578164383769035\n",
      "Iteration 13740, Loss: 0.05566827580332756\n",
      "Iteration 13741, Loss: 0.05579674243927002\n",
      "Iteration 13742, Loss: 0.055864691734313965\n",
      "Iteration 13743, Loss: 0.055760663002729416\n",
      "Iteration 13744, Loss: 0.05569513887166977\n",
      "Iteration 13745, Loss: 0.055777471512556076\n",
      "Iteration 13746, Loss: 0.05575494095683098\n",
      "Iteration 13747, Loss: 0.055636171251535416\n",
      "Iteration 13748, Loss: 0.05584323778748512\n",
      "Iteration 13749, Loss: 0.05591519922018051\n",
      "Iteration 13750, Loss: 0.0558147057890892\n",
      "Iteration 13751, Loss: 0.05565603822469711\n",
      "Iteration 13752, Loss: 0.05574953556060791\n",
      "Iteration 13753, Loss: 0.05574381351470947\n",
      "Iteration 13754, Loss: 0.055633943527936935\n",
      "Iteration 13755, Loss: 0.0558323860168457\n",
      "Iteration 13756, Loss: 0.0559004545211792\n",
      "Iteration 13757, Loss: 0.055807750672101974\n",
      "Iteration 13758, Loss: 0.05566028878092766\n",
      "Iteration 13759, Loss: 0.05576535314321518\n",
      "Iteration 13760, Loss: 0.0557735376060009\n",
      "Iteration 13761, Loss: 0.05565997213125229\n",
      "Iteration 13762, Loss: 0.055799249559640884\n",
      "Iteration 13763, Loss: 0.055875103920698166\n",
      "Iteration 13764, Loss: 0.055806320160627365\n",
      "Iteration 13765, Loss: 0.05565846338868141\n",
      "Iteration 13766, Loss: 0.05579463765025139\n",
      "Iteration 13767, Loss: 0.05584239959716797\n",
      "Iteration 13768, Loss: 0.05574226379394531\n",
      "Iteration 13769, Loss: 0.05570320412516594\n",
      "Iteration 13770, Loss: 0.0557762011885643\n",
      "Iteration 13771, Loss: 0.055721960961818695\n",
      "Iteration 13772, Loss: 0.055673085153102875\n",
      "Iteration 13773, Loss: 0.055705904960632324\n",
      "Iteration 13774, Loss: 0.055632274597883224\n",
      "Iteration 13775, Loss: 0.05566283315420151\n",
      "Iteration 13776, Loss: 0.055626075714826584\n",
      "Iteration 13777, Loss: 0.055752359330654144\n",
      "Iteration 13778, Loss: 0.05573928356170654\n",
      "Iteration 13779, Loss: 0.055649202316999435\n",
      "Iteration 13780, Loss: 0.05569903180003166\n",
      "Iteration 13781, Loss: 0.05564749240875244\n",
      "Iteration 13782, Loss: 0.05575251951813698\n",
      "Iteration 13783, Loss: 0.055780768394470215\n",
      "Iteration 13784, Loss: 0.05568941682577133\n",
      "Iteration 13785, Loss: 0.05573868751525879\n",
      "Iteration 13786, Loss: 0.055789511650800705\n",
      "Iteration 13787, Loss: 0.05569569393992424\n",
      "Iteration 13788, Loss: 0.0557405948638916\n",
      "Iteration 13789, Loss: 0.05580612272024155\n",
      "Iteration 13790, Loss: 0.05574409291148186\n",
      "Iteration 13791, Loss: 0.0556645430624485\n",
      "Iteration 13792, Loss: 0.055725179612636566\n",
      "Iteration 13793, Loss: 0.05565647408366203\n",
      "Iteration 13794, Loss: 0.055751364678144455\n",
      "Iteration 13795, Loss: 0.05580242723226547\n",
      "Iteration 13796, Loss: 0.05574822425842285\n",
      "Iteration 13797, Loss: 0.05565444752573967\n",
      "Iteration 13798, Loss: 0.055755577981472015\n",
      "Iteration 13799, Loss: 0.05572907254099846\n",
      "Iteration 13800, Loss: 0.05567654222249985\n",
      "Iteration 13801, Loss: 0.05571544170379639\n",
      "Iteration 13802, Loss: 0.055675506591796875\n",
      "Iteration 13803, Loss: 0.05570634454488754\n",
      "Iteration 13804, Loss: 0.055693428963422775\n",
      "Iteration 13805, Loss: 0.055685125291347504\n",
      "Iteration 13806, Loss: 0.055710434913635254\n",
      "Iteration 13807, Loss: 0.05564574524760246\n",
      "Iteration 13808, Loss: 0.05577000230550766\n",
      "Iteration 13809, Loss: 0.05578406900167465\n",
      "Iteration 13810, Loss: 0.055646978318691254\n",
      "Iteration 13811, Loss: 0.05578915402293205\n",
      "Iteration 13812, Loss: 0.05586628243327141\n",
      "Iteration 13813, Loss: 0.05582805722951889\n",
      "Iteration 13814, Loss: 0.05569279193878174\n",
      "Iteration 13815, Loss: 0.055785659700632095\n",
      "Iteration 13816, Loss: 0.05587446689605713\n",
      "Iteration 13817, Loss: 0.055797379463911057\n",
      "Iteration 13818, Loss: 0.05564534664154053\n",
      "Iteration 13819, Loss: 0.05571591854095459\n",
      "Iteration 13820, Loss: 0.05567312240600586\n",
      "Iteration 13821, Loss: 0.05571242421865463\n",
      "Iteration 13822, Loss: 0.05571877956390381\n",
      "Iteration 13823, Loss: 0.055644117295742035\n",
      "Iteration 13824, Loss: 0.0556669645011425\n",
      "Iteration 13825, Loss: 0.05565595626831055\n",
      "Iteration 13826, Loss: 0.05563628673553467\n",
      "Iteration 13827, Loss: 0.05568234249949455\n",
      "Iteration 13828, Loss: 0.055616024881601334\n",
      "Iteration 13829, Loss: 0.055695775896310806\n",
      "Iteration 13830, Loss: 0.055663347244262695\n",
      "Iteration 13831, Loss: 0.05570892617106438\n",
      "Iteration 13832, Loss: 0.0557025708258152\n",
      "Iteration 13833, Loss: 0.05566537380218506\n",
      "Iteration 13834, Loss: 0.05567558854818344\n",
      "Iteration 13835, Loss: 0.05565456673502922\n",
      "Iteration 13836, Loss: 0.055615346878767014\n",
      "Iteration 13837, Loss: 0.05576324462890625\n",
      "Iteration 13838, Loss: 0.05579730123281479\n",
      "Iteration 13839, Loss: 0.05572724714875221\n",
      "Iteration 13840, Loss: 0.055678289383649826\n",
      "Iteration 13841, Loss: 0.05572148412466049\n",
      "Iteration 13842, Loss: 0.05564137548208237\n",
      "Iteration 13843, Loss: 0.055721163749694824\n",
      "Iteration 13844, Loss: 0.05573972314596176\n",
      "Iteration 13845, Loss: 0.05566239729523659\n",
      "Iteration 13846, Loss: 0.05576503649353981\n",
      "Iteration 13847, Loss: 0.055792372673749924\n",
      "Iteration 13848, Loss: 0.0556483268737793\n",
      "Iteration 13849, Loss: 0.05580473318696022\n",
      "Iteration 13850, Loss: 0.0559060201048851\n",
      "Iteration 13851, Loss: 0.05589592456817627\n",
      "Iteration 13852, Loss: 0.05578732490539551\n",
      "Iteration 13853, Loss: 0.05565126985311508\n",
      "Iteration 13854, Loss: 0.05578315258026123\n",
      "Iteration 13855, Loss: 0.0557636059820652\n",
      "Iteration 13856, Loss: 0.05565448850393295\n",
      "Iteration 13857, Loss: 0.05571568012237549\n",
      "Iteration 13858, Loss: 0.05570685863494873\n",
      "Iteration 13859, Loss: 0.055639587342739105\n",
      "Iteration 13860, Loss: 0.05568898096680641\n",
      "Iteration 13861, Loss: 0.0556621178984642\n",
      "Iteration 13862, Loss: 0.05569712445139885\n",
      "Iteration 13863, Loss: 0.055674754083156586\n",
      "Iteration 13864, Loss: 0.05570594593882561\n",
      "Iteration 13865, Loss: 0.055727601051330566\n",
      "Iteration 13866, Loss: 0.055651187896728516\n",
      "Iteration 13867, Loss: 0.055763207376003265\n",
      "Iteration 13868, Loss: 0.05577715486288071\n",
      "Iteration 13869, Loss: 0.055624447762966156\n",
      "Iteration 13870, Loss: 0.05583656206727028\n",
      "Iteration 13871, Loss: 0.05594369024038315\n",
      "Iteration 13872, Loss: 0.05593172833323479\n",
      "Iteration 13873, Loss: 0.055817048996686935\n",
      "Iteration 13874, Loss: 0.055663347244262695\n",
      "Iteration 13875, Loss: 0.05583862587809563\n",
      "Iteration 13876, Loss: 0.05589517205953598\n",
      "Iteration 13877, Loss: 0.05577755346894264\n",
      "Iteration 13878, Loss: 0.05570030212402344\n",
      "Iteration 13879, Loss: 0.05579078197479248\n",
      "Iteration 13880, Loss: 0.05578235909342766\n",
      "Iteration 13881, Loss: 0.055688779801130295\n",
      "Iteration 13882, Loss: 0.055752795189619064\n",
      "Iteration 13883, Loss: 0.05579523369669914\n",
      "Iteration 13884, Loss: 0.05566946789622307\n",
      "Iteration 13885, Loss: 0.05577433109283447\n",
      "Iteration 13886, Loss: 0.05586385726928711\n",
      "Iteration 13887, Loss: 0.055850349366664886\n",
      "Iteration 13888, Loss: 0.05574437230825424\n",
      "Iteration 13889, Loss: 0.05568854138255119\n",
      "Iteration 13890, Loss: 0.05574747174978256\n",
      "Iteration 13891, Loss: 0.05563346669077873\n",
      "Iteration 13892, Loss: 0.055789828300476074\n",
      "Iteration 13893, Loss: 0.055867116898298264\n",
      "Iteration 13894, Loss: 0.05583930015563965\n",
      "Iteration 13895, Loss: 0.055718861520290375\n",
      "Iteration 13896, Loss: 0.05574003979563713\n",
      "Iteration 13897, Loss: 0.05581625550985336\n",
      "Iteration 13898, Loss: 0.05571893975138664\n",
      "Iteration 13899, Loss: 0.055724263191223145\n",
      "Iteration 13900, Loss: 0.05580199137330055\n",
      "Iteration 13901, Loss: 0.05577616021037102\n",
      "Iteration 13902, Loss: 0.05566171929240227\n",
      "Iteration 13903, Loss: 0.05580667778849602\n",
      "Iteration 13904, Loss: 0.055870652198791504\n",
      "Iteration 13905, Loss: 0.05575720593333244\n",
      "Iteration 13906, Loss: 0.05570626258850098\n",
      "Iteration 13907, Loss: 0.055794477462768555\n",
      "Iteration 13908, Loss: 0.05577969551086426\n",
      "Iteration 13909, Loss: 0.05567280575633049\n",
      "Iteration 13910, Loss: 0.05578526109457016\n",
      "Iteration 13911, Loss: 0.0558445081114769\n",
      "Iteration 13912, Loss: 0.05572585389018059\n",
      "Iteration 13913, Loss: 0.05573252961039543\n",
      "Iteration 13914, Loss: 0.05582404136657715\n",
      "Iteration 13915, Loss: 0.055812083184719086\n",
      "Iteration 13916, Loss: 0.05570729821920395\n",
      "Iteration 13917, Loss: 0.05573658272624016\n",
      "Iteration 13918, Loss: 0.055793922394514084\n",
      "Iteration 13919, Loss: 0.05567312240600586\n",
      "Iteration 13920, Loss: 0.05577167123556137\n",
      "Iteration 13921, Loss: 0.05586373805999756\n",
      "Iteration 13922, Loss: 0.055852293968200684\n",
      "Iteration 13923, Loss: 0.05574778839945793\n",
      "Iteration 13924, Loss: 0.055679839104413986\n",
      "Iteration 13925, Loss: 0.05573837086558342\n",
      "Iteration 13926, Loss: 0.0556231364607811\n",
      "Iteration 13927, Loss: 0.05579245463013649\n",
      "Iteration 13928, Loss: 0.055868905037641525\n",
      "Iteration 13929, Loss: 0.05584121122956276\n",
      "Iteration 13930, Loss: 0.05572144314646721\n",
      "Iteration 13931, Loss: 0.055734358727931976\n",
      "Iteration 13932, Loss: 0.055807869881391525\n",
      "Iteration 13933, Loss: 0.055701933801174164\n",
      "Iteration 13934, Loss: 0.05574158951640129\n",
      "Iteration 13935, Loss: 0.05582507699728012\n",
      "Iteration 13936, Loss: 0.055805329233407974\n",
      "Iteration 13937, Loss: 0.055694662034511566\n",
      "Iteration 13938, Loss: 0.05575963109731674\n",
      "Iteration 13939, Loss: 0.05582253262400627\n",
      "Iteration 13940, Loss: 0.055706463754177094\n",
      "Iteration 13941, Loss: 0.055744729936122894\n",
      "Iteration 13942, Loss: 0.055834412574768066\n",
      "Iteration 13943, Loss: 0.05582110211253166\n",
      "Iteration 13944, Loss: 0.05571560189127922\n",
      "Iteration 13945, Loss: 0.05572522059082985\n",
      "Iteration 13946, Loss: 0.05578279495239258\n",
      "Iteration 13947, Loss: 0.05566362664103508\n",
      "Iteration 13948, Loss: 0.0557764396071434\n",
      "Iteration 13949, Loss: 0.05586683750152588\n",
      "Iteration 13950, Loss: 0.055854082107543945\n",
      "Iteration 13951, Loss: 0.05574830621480942\n",
      "Iteration 13952, Loss: 0.055679917335510254\n",
      "Iteration 13953, Loss: 0.0557379350066185\n",
      "Iteration 13954, Loss: 0.05562238022685051\n",
      "Iteration 13955, Loss: 0.05578824132680893\n",
      "Iteration 13956, Loss: 0.055856507271528244\n",
      "Iteration 13957, Loss: 0.05581963434815407\n",
      "Iteration 13958, Loss: 0.05569251626729965\n",
      "Iteration 13959, Loss: 0.05578009411692619\n",
      "Iteration 13960, Loss: 0.055860720574855804\n",
      "Iteration 13961, Loss: 0.055767498910427094\n",
      "Iteration 13962, Loss: 0.05568297952413559\n",
      "Iteration 13963, Loss: 0.055758360773324966\n",
      "Iteration 13964, Loss: 0.05572966858744621\n",
      "Iteration 13965, Loss: 0.05562750622630119\n",
      "Iteration 13966, Loss: 0.05576217547059059\n",
      "Iteration 13967, Loss: 0.05573869124054909\n",
      "Iteration 13968, Loss: 0.05566072463989258\n",
      "Iteration 13969, Loss: 0.0557001456618309\n",
      "Iteration 13970, Loss: 0.0556466206908226\n",
      "Iteration 13971, Loss: 0.05575573444366455\n",
      "Iteration 13972, Loss: 0.05576249212026596\n",
      "Iteration 13973, Loss: 0.05563950538635254\n",
      "Iteration 13974, Loss: 0.055757127702236176\n",
      "Iteration 13975, Loss: 0.05578247830271721\n",
      "Iteration 13976, Loss: 0.055681709200143814\n",
      "Iteration 13977, Loss: 0.055757444351911545\n",
      "Iteration 13978, Loss: 0.055819593369960785\n",
      "Iteration 13979, Loss: 0.055736225098371506\n",
      "Iteration 13980, Loss: 0.05569136515259743\n",
      "Iteration 13981, Loss: 0.0557534322142601\n",
      "Iteration 13982, Loss: 0.055688463151454926\n",
      "Iteration 13983, Loss: 0.055721282958984375\n",
      "Iteration 13984, Loss: 0.055757325142621994\n",
      "Iteration 13985, Loss: 0.055664900690317154\n",
      "Iteration 13986, Loss: 0.055757761001586914\n",
      "Iteration 13987, Loss: 0.0558062419295311\n",
      "Iteration 13988, Loss: 0.055715762078762054\n",
      "Iteration 13989, Loss: 0.05571413040161133\n",
      "Iteration 13990, Loss: 0.0557757243514061\n",
      "Iteration 13991, Loss: 0.055704474449157715\n",
      "Iteration 13992, Loss: 0.055706463754177094\n",
      "Iteration 13993, Loss: 0.05574929714202881\n",
      "Iteration 13994, Loss: 0.055655043572187424\n",
      "Iteration 13995, Loss: 0.05577671527862549\n",
      "Iteration 13996, Loss: 0.05583548545837402\n",
      "Iteration 13997, Loss: 0.055759672075510025\n",
      "Iteration 13998, Loss: 0.05566835403442383\n",
      "Iteration 13999, Loss: 0.05574524402618408\n",
      "Iteration 14000, Loss: 0.05570352450013161\n",
      "Iteration 14001, Loss: 0.0556974820792675\n",
      "Iteration 14002, Loss: 0.0557279996573925\n",
      "Iteration 14003, Loss: 0.05566426366567612\n",
      "Iteration 14004, Loss: 0.055731695145368576\n",
      "Iteration 14005, Loss: 0.05573789402842522\n",
      "Iteration 14006, Loss: 0.05563263222575188\n",
      "Iteration 14007, Loss: 0.055653613060712814\n",
      "Iteration 14008, Loss: 0.055644791573286057\n",
      "Iteration 14009, Loss: 0.05566028878092766\n",
      "Iteration 14010, Loss: 0.05563374608755112\n",
      "Iteration 14011, Loss: 0.05571071431040764\n",
      "Iteration 14012, Loss: 0.05565480515360832\n",
      "Iteration 14013, Loss: 0.05574238300323486\n",
      "Iteration 14014, Loss: 0.05579591169953346\n",
      "Iteration 14015, Loss: 0.05575045198202133\n",
      "Iteration 14016, Loss: 0.055621109902858734\n",
      "Iteration 14017, Loss: 0.05585094541311264\n",
      "Iteration 14018, Loss: 0.05590447038412094\n",
      "Iteration 14019, Loss: 0.05577969551086426\n",
      "Iteration 14020, Loss: 0.055696647614240646\n",
      "Iteration 14021, Loss: 0.055792491883039474\n",
      "Iteration 14022, Loss: 0.05578351020812988\n",
      "Iteration 14023, Loss: 0.05567876622080803\n",
      "Iteration 14024, Loss: 0.0557732991874218\n",
      "Iteration 14025, Loss: 0.05583028122782707\n",
      "Iteration 14026, Loss: 0.0557103157043457\n",
      "Iteration 14027, Loss: 0.05574401468038559\n",
      "Iteration 14028, Loss: 0.05583588406443596\n",
      "Iteration 14029, Loss: 0.05582038685679436\n",
      "Iteration 14030, Loss: 0.055710118263959885\n",
      "Iteration 14031, Loss: 0.0557379350066185\n",
      "Iteration 14032, Loss: 0.0558011569082737\n",
      "Iteration 14033, Loss: 0.055687230080366135\n",
      "Iteration 14034, Loss: 0.05575835704803467\n",
      "Iteration 14035, Loss: 0.055846814066171646\n",
      "Iteration 14036, Loss: 0.05582916736602783\n",
      "Iteration 14037, Loss: 0.05571893975138664\n",
      "Iteration 14038, Loss: 0.05572708696126938\n",
      "Iteration 14039, Loss: 0.055790822952985764\n",
      "Iteration 14040, Loss: 0.05567892640829086\n",
      "Iteration 14041, Loss: 0.05576225370168686\n",
      "Iteration 14042, Loss: 0.055848680436611176\n",
      "Iteration 14043, Loss: 0.05583127588033676\n",
      "Iteration 14044, Loss: 0.0557224377989769\n",
      "Iteration 14045, Loss: 0.05572104826569557\n",
      "Iteration 14046, Loss: 0.05578303337097168\n",
      "Iteration 14047, Loss: 0.05566990375518799\n",
      "Iteration 14048, Loss: 0.05576781556010246\n",
      "Iteration 14049, Loss: 0.055853765457868576\n",
      "Iteration 14050, Loss: 0.055836956948041916\n",
      "Iteration 14051, Loss: 0.05572832003235817\n",
      "Iteration 14052, Loss: 0.05571151152253151\n",
      "Iteration 14053, Loss: 0.05577262490987778\n",
      "Iteration 14054, Loss: 0.055657386779785156\n",
      "Iteration 14055, Loss: 0.055777352303266525\n",
      "Iteration 14056, Loss: 0.05586457625031471\n",
      "Iteration 14057, Loss: 0.055848561227321625\n",
      "Iteration 14058, Loss: 0.05573999881744385\n",
      "Iteration 14059, Loss: 0.0556952990591526\n",
      "Iteration 14060, Loss: 0.05575704947113991\n",
      "Iteration 14061, Loss: 0.055641058832407\n",
      "Iteration 14062, Loss: 0.05579253286123276\n",
      "Iteration 14063, Loss: 0.05588233470916748\n",
      "Iteration 14064, Loss: 0.05586842820048332\n",
      "Iteration 14065, Loss: 0.05576169490814209\n",
      "Iteration 14066, Loss: 0.055664341896772385\n",
      "Iteration 14067, Loss: 0.055727921426296234\n",
      "Iteration 14068, Loss: 0.055625203996896744\n",
      "Iteration 14069, Loss: 0.055761419236660004\n",
      "Iteration 14070, Loss: 0.05580536648631096\n",
      "Iteration 14071, Loss: 0.05574377626180649\n",
      "Iteration 14072, Loss: 0.05564634129405022\n",
      "Iteration 14073, Loss: 0.05571691319346428\n",
      "Iteration 14074, Loss: 0.05565110966563225\n",
      "Iteration 14075, Loss: 0.05573316663503647\n",
      "Iteration 14076, Loss: 0.05577190965414047\n",
      "Iteration 14077, Loss: 0.055710434913635254\n",
      "Iteration 14078, Loss: 0.05568035691976547\n",
      "Iteration 14079, Loss: 0.0556941032409668\n",
      "Iteration 14080, Loss: 0.05566469952464104\n",
      "Iteration 14081, Loss: 0.055673640221357346\n",
      "Iteration 14082, Loss: 0.055644869804382324\n",
      "Iteration 14083, Loss: 0.055632829666137695\n",
      "Iteration 14084, Loss: 0.05566028878092766\n",
      "Iteration 14085, Loss: 0.05563589185476303\n",
      "Iteration 14086, Loss: 0.055638592690229416\n",
      "Iteration 14087, Loss: 0.055672965943813324\n",
      "Iteration 14088, Loss: 0.05564729496836662\n",
      "Iteration 14089, Loss: 0.05571901798248291\n",
      "Iteration 14090, Loss: 0.05569656938314438\n",
      "Iteration 14091, Loss: 0.05568564310669899\n",
      "Iteration 14092, Loss: 0.055714331567287445\n",
      "Iteration 14093, Loss: 0.055641137063503265\n",
      "Iteration 14094, Loss: 0.05578124523162842\n",
      "Iteration 14095, Loss: 0.05580449104309082\n",
      "Iteration 14096, Loss: 0.05566287040710449\n",
      "Iteration 14097, Loss: 0.05579300969839096\n",
      "Iteration 14098, Loss: 0.05589362233877182\n",
      "Iteration 14099, Loss: 0.05588340759277344\n",
      "Iteration 14100, Loss: 0.05577564239501953\n",
      "Iteration 14101, Loss: 0.0556621178984642\n",
      "Iteration 14102, Loss: 0.055766187608242035\n",
      "Iteration 14103, Loss: 0.055720530450344086\n",
      "Iteration 14104, Loss: 0.05569195747375488\n",
      "Iteration 14105, Loss: 0.055738531053066254\n",
      "Iteration 14106, Loss: 0.05569581314921379\n",
      "Iteration 14107, Loss: 0.05568075552582741\n",
      "Iteration 14108, Loss: 0.05568047612905502\n",
      "Iteration 14109, Loss: 0.055683813989162445\n",
      "Iteration 14110, Loss: 0.055699072778224945\n",
      "Iteration 14111, Loss: 0.055635612457990646\n",
      "Iteration 14112, Loss: 0.05576388165354729\n",
      "Iteration 14113, Loss: 0.05575975030660629\n",
      "Iteration 14114, Loss: 0.05564165115356445\n",
      "Iteration 14115, Loss: 0.05572764202952385\n",
      "Iteration 14116, Loss: 0.055721405893564224\n",
      "Iteration 14117, Loss: 0.055628661066293716\n",
      "Iteration 14118, Loss: 0.05562480539083481\n",
      "Iteration 14119, Loss: 0.055722951889038086\n",
      "Iteration 14120, Loss: 0.05573674291372299\n",
      "Iteration 14121, Loss: 0.055655401200056076\n",
      "Iteration 14122, Loss: 0.05577461048960686\n",
      "Iteration 14123, Loss: 0.05580318346619606\n",
      "Iteration 14124, Loss: 0.05565643683075905\n",
      "Iteration 14125, Loss: 0.05580250546336174\n",
      "Iteration 14126, Loss: 0.05591082572937012\n",
      "Iteration 14127, Loss: 0.05591186136007309\n",
      "Iteration 14128, Loss: 0.05581625550985336\n",
      "Iteration 14129, Loss: 0.055643998086452484\n",
      "Iteration 14130, Loss: 0.05589493364095688\n",
      "Iteration 14131, Loss: 0.05601247400045395\n",
      "Iteration 14132, Loss: 0.05594416707754135\n",
      "Iteration 14133, Loss: 0.055711351335048676\n",
      "Iteration 14134, Loss: 0.05581768602132797\n",
      "Iteration 14135, Loss: 0.055976592004299164\n",
      "Iteration 14136, Loss: 0.05602530762553215\n",
      "Iteration 14137, Loss: 0.055973730981349945\n",
      "Iteration 14138, Loss: 0.05583298206329346\n",
      "Iteration 14139, Loss: 0.055619001388549805\n",
      "Iteration 14140, Loss: 0.055922072380781174\n",
      "Iteration 14141, Loss: 0.056041400879621506\n",
      "Iteration 14142, Loss: 0.05598370358347893\n",
      "Iteration 14143, Loss: 0.05577234551310539\n",
      "Iteration 14144, Loss: 0.055761419236660004\n",
      "Iteration 14145, Loss: 0.05590900033712387\n",
      "Iteration 14146, Loss: 0.055945079773664474\n",
      "Iteration 14147, Loss: 0.05587757006287575\n",
      "Iteration 14148, Loss: 0.05571715161204338\n",
      "Iteration 14149, Loss: 0.05578561872243881\n",
      "Iteration 14150, Loss: 0.055902961641550064\n",
      "Iteration 14151, Loss: 0.05584565922617912\n",
      "Iteration 14152, Loss: 0.055640459060668945\n",
      "Iteration 14153, Loss: 0.05584494397044182\n",
      "Iteration 14154, Loss: 0.055969201028347015\n",
      "Iteration 14155, Loss: 0.05597245693206787\n",
      "Iteration 14156, Loss: 0.055870138108730316\n",
      "Iteration 14157, Loss: 0.05569235607981682\n",
      "Iteration 14158, Loss: 0.05583819001913071\n",
      "Iteration 14159, Loss: 0.055964354425668716\n",
      "Iteration 14160, Loss: 0.055919013917446136\n",
      "Iteration 14161, Loss: 0.05571647733449936\n",
      "Iteration 14162, Loss: 0.05579892918467522\n",
      "Iteration 14163, Loss: 0.05594225972890854\n",
      "Iteration 14164, Loss: 0.0559697151184082\n",
      "Iteration 14165, Loss: 0.05589485168457031\n",
      "Iteration 14166, Loss: 0.05573531240224838\n",
      "Iteration 14167, Loss: 0.0557686910033226\n",
      "Iteration 14168, Loss: 0.05588666722178459\n",
      "Iteration 14169, Loss: 0.055833302438259125\n",
      "Iteration 14170, Loss: 0.055627547204494476\n",
      "Iteration 14171, Loss: 0.05583381652832031\n",
      "Iteration 14172, Loss: 0.05594980716705322\n",
      "Iteration 14173, Loss: 0.05595974251627922\n",
      "Iteration 14174, Loss: 0.05587395280599594\n",
      "Iteration 14175, Loss: 0.055703405290842056\n",
      "Iteration 14176, Loss: 0.05582054704427719\n",
      "Iteration 14177, Loss: 0.05594949051737785\n",
      "Iteration 14178, Loss: 0.05589195340871811\n",
      "Iteration 14179, Loss: 0.055667243897914886\n",
      "Iteration 14180, Loss: 0.05584506317973137\n",
      "Iteration 14181, Loss: 0.056000351905822754\n",
      "Iteration 14182, Loss: 0.0560452938079834\n",
      "Iteration 14183, Loss: 0.055990301072597504\n",
      "Iteration 14184, Loss: 0.055845897644758224\n",
      "Iteration 14185, Loss: 0.05562293529510498\n",
      "Iteration 14186, Loss: 0.055994074791669846\n",
      "Iteration 14187, Loss: 0.05618099495768547\n",
      "Iteration 14188, Loss: 0.05617424100637436\n",
      "Iteration 14189, Loss: 0.055993519723415375\n",
      "Iteration 14190, Loss: 0.05566203594207764\n",
      "Iteration 14191, Loss: 0.05591662973165512\n",
      "Iteration 14192, Loss: 0.056131601333618164\n",
      "Iteration 14193, Loss: 0.056229911744594574\n",
      "Iteration 14194, Loss: 0.05622200295329094\n",
      "Iteration 14195, Loss: 0.05611920356750488\n",
      "Iteration 14196, Loss: 0.05593109130859375\n",
      "Iteration 14197, Loss: 0.055668555200099945\n",
      "Iteration 14198, Loss: 0.05597865581512451\n",
      "Iteration 14199, Loss: 0.056208133697509766\n",
      "Iteration 14200, Loss: 0.0562409982085228\n",
      "Iteration 14201, Loss: 0.0560968741774559\n",
      "Iteration 14202, Loss: 0.055796068161726\n",
      "Iteration 14203, Loss: 0.0558011531829834\n",
      "Iteration 14204, Loss: 0.05600166693329811\n",
      "Iteration 14205, Loss: 0.05608785152435303\n",
      "Iteration 14206, Loss: 0.05607008934020996\n",
      "Iteration 14207, Loss: 0.05595862865447998\n",
      "Iteration 14208, Loss: 0.05576372146606445\n",
      "Iteration 14209, Loss: 0.05576741695404053\n",
      "Iteration 14210, Loss: 0.05592282861471176\n",
      "Iteration 14211, Loss: 0.05589127913117409\n",
      "Iteration 14212, Loss: 0.05569235607981682\n",
      "Iteration 14213, Loss: 0.055809181183576584\n",
      "Iteration 14214, Loss: 0.05594770237803459\n",
      "Iteration 14215, Loss: 0.05597734823822975\n",
      "Iteration 14216, Loss: 0.055908601731061935\n",
      "Iteration 14217, Loss: 0.05575231835246086\n",
      "Iteration 14218, Loss: 0.05573658272624016\n",
      "Iteration 14219, Loss: 0.055850863456726074\n",
      "Iteration 14220, Loss: 0.055783312767744064\n",
      "Iteration 14221, Loss: 0.055655281990766525\n",
      "Iteration 14222, Loss: 0.05571595951914787\n",
      "Iteration 14223, Loss: 0.05567697808146477\n",
      "Iteration 14224, Loss: 0.05569612979888916\n",
      "Iteration 14225, Loss: 0.055682383477687836\n",
      "Iteration 14226, Loss: 0.05569394677877426\n",
      "Iteration 14227, Loss: 0.0557224377989769\n",
      "Iteration 14228, Loss: 0.05565432831645012\n",
      "Iteration 14229, Loss: 0.05576149746775627\n",
      "Iteration 14230, Loss: 0.05577770993113518\n",
      "Iteration 14231, Loss: 0.05562182515859604\n",
      "Iteration 14232, Loss: 0.05582817643880844\n",
      "Iteration 14233, Loss: 0.05593514442443848\n",
      "Iteration 14234, Loss: 0.055935148149728775\n",
      "Iteration 14235, Loss: 0.05584001541137695\n",
      "Iteration 14236, Loss: 0.05566481873393059\n",
      "Iteration 14237, Loss: 0.05587522312998772\n",
      "Iteration 14238, Loss: 0.05600380897521973\n",
      "Iteration 14239, Loss: 0.05594813823699951\n",
      "Iteration 14240, Loss: 0.05572628974914551\n",
      "Iteration 14241, Loss: 0.055800121277570724\n",
      "Iteration 14242, Loss: 0.055953823029994965\n",
      "Iteration 14243, Loss: 0.05599729344248772\n",
      "Iteration 14244, Loss: 0.055941104888916016\n",
      "Iteration 14245, Loss: 0.05579598993062973\n",
      "Iteration 14246, Loss: 0.05566521733999252\n",
      "Iteration 14247, Loss: 0.05577345937490463\n",
      "Iteration 14248, Loss: 0.05570332333445549\n",
      "Iteration 14249, Loss: 0.05571651831269264\n",
      "Iteration 14250, Loss: 0.05577874183654785\n",
      "Iteration 14251, Loss: 0.055741630494594574\n",
      "Iteration 14252, Loss: 0.05562099069356918\n",
      "Iteration 14253, Loss: 0.05583298206329346\n",
      "Iteration 14254, Loss: 0.05587335675954819\n",
      "Iteration 14255, Loss: 0.055743180215358734\n",
      "Iteration 14256, Loss: 0.055726610124111176\n",
      "Iteration 14257, Loss: 0.055824242532253265\n",
      "Iteration 14258, Loss: 0.0558118037879467\n",
      "Iteration 14259, Loss: 0.05569887161254883\n",
      "Iteration 14260, Loss: 0.05575454235076904\n",
      "Iteration 14261, Loss: 0.05582138150930405\n",
      "Iteration 14262, Loss: 0.05571448802947998\n",
      "Iteration 14263, Loss: 0.05573197454214096\n",
      "Iteration 14264, Loss: 0.0558144673705101\n",
      "Iteration 14265, Loss: 0.055786728858947754\n",
      "Iteration 14266, Loss: 0.055664341896772385\n",
      "Iteration 14267, Loss: 0.05580878257751465\n",
      "Iteration 14268, Loss: 0.0558808259665966\n",
      "Iteration 14269, Loss: 0.055775128304958344\n",
      "Iteration 14270, Loss: 0.05568587779998779\n",
      "Iteration 14271, Loss: 0.05576964467763901\n",
      "Iteration 14272, Loss: 0.055746279656887054\n",
      "Iteration 14273, Loss: 0.05563116446137428\n",
      "Iteration 14274, Loss: 0.05583767220377922\n",
      "Iteration 14275, Loss: 0.0558924674987793\n",
      "Iteration 14276, Loss: 0.055767618119716644\n",
      "Iteration 14277, Loss: 0.055705152451992035\n",
      "Iteration 14278, Loss: 0.05580051988363266\n",
      "Iteration 14279, Loss: 0.055791061371564865\n",
      "Iteration 14280, Loss: 0.05568830296397209\n",
      "Iteration 14281, Loss: 0.05575792118906975\n",
      "Iteration 14282, Loss: 0.05581200495362282\n",
      "Iteration 14283, Loss: 0.055687546730041504\n",
      "Iteration 14284, Loss: 0.05576392263174057\n",
      "Iteration 14285, Loss: 0.05585889145731926\n",
      "Iteration 14286, Loss: 0.055848997086286545\n",
      "Iteration 14287, Loss: 0.05574556440114975\n",
      "Iteration 14288, Loss: 0.0556824617087841\n",
      "Iteration 14289, Loss: 0.0557405948638916\n",
      "Iteration 14290, Loss: 0.055633269250392914\n",
      "Iteration 14291, Loss: 0.05576881021261215\n",
      "Iteration 14292, Loss: 0.05582837387919426\n",
      "Iteration 14293, Loss: 0.05578526109457016\n",
      "Iteration 14294, Loss: 0.055652741342782974\n",
      "Iteration 14295, Loss: 0.05583763122558594\n",
      "Iteration 14296, Loss: 0.055921636521816254\n",
      "Iteration 14297, Loss: 0.05582718178629875\n",
      "Iteration 14298, Loss: 0.055641256272792816\n",
      "Iteration 14299, Loss: 0.05572621151804924\n",
      "Iteration 14300, Loss: 0.055709563195705414\n",
      "Iteration 14301, Loss: 0.055630844086408615\n",
      "Iteration 14302, Loss: 0.05562388896942139\n",
      "Iteration 14303, Loss: 0.05570407956838608\n",
      "Iteration 14304, Loss: 0.055678170174360275\n",
      "Iteration 14305, Loss: 0.05568861961364746\n",
      "Iteration 14306, Loss: 0.055687785148620605\n",
      "Iteration 14307, Loss: 0.05566883459687233\n",
      "Iteration 14308, Loss: 0.05566565319895744\n",
      "Iteration 14309, Loss: 0.05568110942840576\n",
      "Iteration 14310, Loss: 0.055660247802734375\n",
      "Iteration 14311, Loss: 0.0557098388671875\n",
      "Iteration 14312, Loss: 0.055718980729579926\n",
      "Iteration 14313, Loss: 0.05563334748148918\n",
      "Iteration 14314, Loss: 0.05573805421590805\n",
      "Iteration 14315, Loss: 0.05570296570658684\n",
      "Iteration 14316, Loss: 0.055692993104457855\n",
      "Iteration 14317, Loss: 0.055731456726789474\n",
      "Iteration 14318, Loss: 0.0556696280837059\n",
      "Iteration 14319, Loss: 0.05573185533285141\n",
      "Iteration 14320, Loss: 0.055741749703884125\n",
      "Iteration 14321, Loss: 0.05563334748148918\n",
      "Iteration 14322, Loss: 0.05564618483185768\n",
      "Iteration 14323, Loss: 0.05567439645528793\n",
      "Iteration 14324, Loss: 0.05561606213450432\n",
      "Iteration 14325, Loss: 0.05567093938589096\n",
      "Iteration 14326, Loss: 0.05564955994486809\n",
      "Iteration 14327, Loss: 0.05563108250498772\n",
      "Iteration 14328, Loss: 0.05573296546936035\n",
      "Iteration 14329, Loss: 0.055694542825222015\n",
      "Iteration 14330, Loss: 0.05570165440440178\n",
      "Iteration 14331, Loss: 0.05574548617005348\n",
      "Iteration 14332, Loss: 0.05569152161478996\n",
      "Iteration 14333, Loss: 0.05569418519735336\n",
      "Iteration 14334, Loss: 0.0556952990591526\n",
      "Iteration 14335, Loss: 0.055674951523542404\n",
      "Iteration 14336, Loss: 0.05569545552134514\n",
      "Iteration 14337, Loss: 0.055620789527893066\n",
      "Iteration 14338, Loss: 0.055815499275922775\n",
      "Iteration 14339, Loss: 0.055838704109191895\n",
      "Iteration 14340, Loss: 0.05568695068359375\n",
      "Iteration 14341, Loss: 0.05578164383769035\n",
      "Iteration 14342, Loss: 0.05589254945516586\n",
      "Iteration 14343, Loss: 0.05589791387319565\n",
      "Iteration 14344, Loss: 0.05580870434641838\n",
      "Iteration 14345, Loss: 0.05563458055257797\n",
      "Iteration 14346, Loss: 0.05591758340597153\n",
      "Iteration 14347, Loss: 0.05604998394846916\n",
      "Iteration 14348, Loss: 0.05599486827850342\n",
      "Iteration 14349, Loss: 0.05577234551310539\n",
      "Iteration 14350, Loss: 0.05576610565185547\n",
      "Iteration 14351, Loss: 0.05591968819499016\n",
      "Iteration 14352, Loss: 0.05596383661031723\n",
      "Iteration 14353, Loss: 0.05590864270925522\n",
      "Iteration 14354, Loss: 0.055764637887477875\n",
      "Iteration 14355, Loss: 0.05570463463664055\n",
      "Iteration 14356, Loss: 0.055804293602705\n",
      "Iteration 14357, Loss: 0.055722080171108246\n",
      "Iteration 14358, Loss: 0.055709920823574066\n",
      "Iteration 14359, Loss: 0.05577961727976799\n",
      "Iteration 14360, Loss: 0.05574854463338852\n",
      "Iteration 14361, Loss: 0.05562714859843254\n",
      "Iteration 14362, Loss: 0.05586386099457741\n",
      "Iteration 14363, Loss: 0.055939119309186935\n",
      "Iteration 14364, Loss: 0.05583461374044418\n",
      "Iteration 14365, Loss: 0.05564427748322487\n",
      "Iteration 14366, Loss: 0.05573558807373047\n",
      "Iteration 14367, Loss: 0.055727921426296234\n",
      "Iteration 14368, Loss: 0.055624764412641525\n",
      "Iteration 14369, Loss: 0.05584140866994858\n",
      "Iteration 14370, Loss: 0.05589906498789787\n",
      "Iteration 14371, Loss: 0.05578676983714104\n",
      "Iteration 14372, Loss: 0.055682145059108734\n",
      "Iteration 14373, Loss: 0.05577210709452629\n",
      "Iteration 14374, Loss: 0.055759232491254807\n",
      "Iteration 14375, Loss: 0.05564669892191887\n",
      "Iteration 14376, Loss: 0.05582141876220703\n",
      "Iteration 14377, Loss: 0.0558905228972435\n",
      "Iteration 14378, Loss: 0.055794160813093185\n",
      "Iteration 14379, Loss: 0.055667124688625336\n",
      "Iteration 14380, Loss: 0.055756133049726486\n",
      "Iteration 14381, Loss: 0.05574425309896469\n",
      "Iteration 14382, Loss: 0.05562369152903557\n",
      "Iteration 14383, Loss: 0.055845461785793304\n",
      "Iteration 14384, Loss: 0.05592445656657219\n",
      "Iteration 14385, Loss: 0.05585404485464096\n",
      "Iteration 14386, Loss: 0.05567809194326401\n",
      "Iteration 14387, Loss: 0.05581534281373024\n",
      "Iteration 14388, Loss: 0.055922746658325195\n",
      "Iteration 14389, Loss: 0.05588821694254875\n",
      "Iteration 14390, Loss: 0.055728279054164886\n",
      "Iteration 14391, Loss: 0.055762529373168945\n",
      "Iteration 14392, Loss: 0.05587760731577873\n",
      "Iteration 14393, Loss: 0.05584736913442612\n",
      "Iteration 14394, Loss: 0.055690567940473557\n",
      "Iteration 14395, Loss: 0.05579563230276108\n",
      "Iteration 14396, Loss: 0.05590478703379631\n",
      "Iteration 14397, Loss: 0.05586922541260719\n",
      "Iteration 14398, Loss: 0.05571262165904045\n",
      "Iteration 14399, Loss: 0.055774133652448654\n",
      "Iteration 14400, Loss: 0.05588337033987045\n",
      "Iteration 14401, Loss: 0.05584804341197014\n",
      "Iteration 14402, Loss: 0.05568353459239006\n",
      "Iteration 14403, Loss: 0.05580981820821762\n",
      "Iteration 14404, Loss: 0.05592568963766098\n",
      "Iteration 14405, Loss: 0.05589723587036133\n",
      "Iteration 14406, Loss: 0.05574886128306389\n",
      "Iteration 14407, Loss: 0.055735908448696136\n",
      "Iteration 14408, Loss: 0.05584363266825676\n",
      "Iteration 14409, Loss: 0.055812638252973557\n",
      "Iteration 14410, Loss: 0.05564785376191139\n",
      "Iteration 14411, Loss: 0.055847328156232834\n",
      "Iteration 14412, Loss: 0.055965546518564224\n",
      "Iteration 14413, Loss: 0.0559459924697876\n",
      "Iteration 14414, Loss: 0.05581279844045639\n",
      "Iteration 14415, Loss: 0.05569124221801758\n",
      "Iteration 14416, Loss: 0.055819276720285416\n",
      "Iteration 14417, Loss: 0.0558474101126194\n",
      "Iteration 14418, Loss: 0.0557306632399559\n",
      "Iteration 14419, Loss: 0.05573682114481926\n",
      "Iteration 14420, Loss: 0.05582189932465553\n",
      "Iteration 14421, Loss: 0.05579734221100807\n",
      "Iteration 14422, Loss: 0.055693626403808594\n",
      "Iteration 14423, Loss: 0.05575752258300781\n",
      "Iteration 14424, Loss: 0.05581077188253403\n",
      "Iteration 14425, Loss: 0.05570455640554428\n",
      "Iteration 14426, Loss: 0.05574365705251694\n",
      "Iteration 14427, Loss: 0.055824559181928635\n",
      "Iteration 14428, Loss: 0.055801551789045334\n",
      "Iteration 14429, Loss: 0.055692754685878754\n",
      "Iteration 14430, Loss: 0.055761538445949554\n",
      "Iteration 14431, Loss: 0.055822014808654785\n",
      "Iteration 14432, Loss: 0.055711232125759125\n",
      "Iteration 14433, Loss: 0.055738966912031174\n",
      "Iteration 14434, Loss: 0.05582507699728012\n",
      "Iteration 14435, Loss: 0.05580807104706764\n",
      "Iteration 14436, Loss: 0.05570101737976074\n",
      "Iteration 14437, Loss: 0.05574754998087883\n",
      "Iteration 14438, Loss: 0.05580770969390869\n",
      "Iteration 14439, Loss: 0.055692434310913086\n",
      "Iteration 14440, Loss: 0.05575454607605934\n",
      "Iteration 14441, Loss: 0.05584355443716049\n",
      "Iteration 14442, Loss: 0.055829647928476334\n",
      "Iteration 14443, Loss: 0.055723827332258224\n",
      "Iteration 14444, Loss: 0.05571417137980461\n",
      "Iteration 14445, Loss: 0.055773377418518066\n",
      "Iteration 14446, Loss: 0.055656276643276215\n",
      "Iteration 14447, Loss: 0.0557810477912426\n",
      "Iteration 14448, Loss: 0.05587069317698479\n",
      "Iteration 14449, Loss: 0.055857304483652115\n",
      "Iteration 14450, Loss: 0.05575108528137207\n",
      "Iteration 14451, Loss: 0.05567670240998268\n",
      "Iteration 14452, Loss: 0.05573539063334465\n",
      "Iteration 14453, Loss: 0.05561765283346176\n",
      "Iteration 14454, Loss: 0.05579781532287598\n",
      "Iteration 14455, Loss: 0.055873673409223557\n",
      "Iteration 14456, Loss: 0.05584339424967766\n",
      "Iteration 14457, Loss: 0.055720847100019455\n",
      "Iteration 14458, Loss: 0.05573912709951401\n",
      "Iteration 14459, Loss: 0.05581597611308098\n",
      "Iteration 14460, Loss: 0.05571524426341057\n",
      "Iteration 14461, Loss: 0.055728793144226074\n",
      "Iteration 14462, Loss: 0.055808745324611664\n",
      "Iteration 14463, Loss: 0.055785298347473145\n",
      "Iteration 14464, Loss: 0.05567328259348869\n",
      "Iteration 14465, Loss: 0.05578947439789772\n",
      "Iteration 14466, Loss: 0.05585126206278801\n",
      "Iteration 14467, Loss: 0.05573407933115959\n",
      "Iteration 14468, Loss: 0.055725496262311935\n",
      "Iteration 14469, Loss: 0.055815938860177994\n",
      "Iteration 14470, Loss: 0.05580314248800278\n",
      "Iteration 14471, Loss: 0.055698834359645844\n",
      "Iteration 14472, Loss: 0.05574631690979004\n",
      "Iteration 14473, Loss: 0.05580183118581772\n",
      "Iteration 14474, Loss: 0.05568047612905502\n",
      "Iteration 14475, Loss: 0.055766306817531586\n",
      "Iteration 14476, Loss: 0.05585881322622299\n",
      "Iteration 14477, Loss: 0.055847685784101486\n",
      "Iteration 14478, Loss: 0.05574401468038559\n",
      "Iteration 14479, Loss: 0.055683933198451996\n",
      "Iteration 14480, Loss: 0.0557403564453125\n",
      "Iteration 14481, Loss: 0.05562722682952881\n",
      "Iteration 14482, Loss: 0.055777549743652344\n",
      "Iteration 14483, Loss: 0.05583878606557846\n",
      "Iteration 14484, Loss: 0.05579531565308571\n",
      "Iteration 14485, Loss: 0.05566354840993881\n",
      "Iteration 14486, Loss: 0.05582066625356674\n",
      "Iteration 14487, Loss: 0.05590224638581276\n",
      "Iteration 14488, Loss: 0.055807631462812424\n",
      "Iteration 14489, Loss: 0.055653613060712814\n",
      "Iteration 14490, Loss: 0.055731695145368576\n",
      "Iteration 14491, Loss: 0.055705904960632324\n",
      "Iteration 14492, Loss: 0.05564439669251442\n",
      "Iteration 14493, Loss: 0.05563021078705788\n",
      "Iteration 14494, Loss: 0.055724382400512695\n",
      "Iteration 14495, Loss: 0.05574576184153557\n",
      "Iteration 14496, Loss: 0.05567117780447006\n",
      "Iteration 14497, Loss: 0.05574675649404526\n",
      "Iteration 14498, Loss: 0.055769842118024826\n",
      "Iteration 14499, Loss: 0.05561785027384758\n",
      "Iteration 14500, Loss: 0.05583357810974121\n",
      "Iteration 14501, Loss: 0.05594504252076149\n",
      "Iteration 14502, Loss: 0.05594980716705322\n",
      "Iteration 14503, Loss: 0.05585845559835434\n",
      "Iteration 14504, Loss: 0.05568341538310051\n",
      "Iteration 14505, Loss: 0.05585312843322754\n",
      "Iteration 14506, Loss: 0.05598624795675278\n",
      "Iteration 14507, Loss: 0.05593141168355942\n",
      "Iteration 14508, Loss: 0.05570896714925766\n",
      "Iteration 14509, Loss: 0.05581287667155266\n",
      "Iteration 14510, Loss: 0.05596650019288063\n",
      "Iteration 14511, Loss: 0.056009769439697266\n",
      "Iteration 14512, Loss: 0.05595310777425766\n",
      "Iteration 14513, Loss: 0.05580803006887436\n",
      "Iteration 14514, Loss: 0.05564967915415764\n",
      "Iteration 14515, Loss: 0.055762529373168945\n",
      "Iteration 14516, Loss: 0.055702924728393555\n",
      "Iteration 14517, Loss: 0.05570797249674797\n",
      "Iteration 14518, Loss: 0.05576161667704582\n",
      "Iteration 14519, Loss: 0.05571647733449936\n",
      "Iteration 14520, Loss: 0.055652499198913574\n",
      "Iteration 14521, Loss: 0.05565023422241211\n",
      "Iteration 14522, Loss: 0.055704277008771896\n",
      "Iteration 14523, Loss: 0.05571699142456055\n",
      "Iteration 14524, Loss: 0.05563275143504143\n",
      "Iteration 14525, Loss: 0.05580262467265129\n",
      "Iteration 14526, Loss: 0.055831633508205414\n",
      "Iteration 14527, Loss: 0.05568893998861313\n",
      "Iteration 14528, Loss: 0.05577433481812477\n",
      "Iteration 14529, Loss: 0.05587927624583244\n",
      "Iteration 14530, Loss: 0.05587760731577873\n",
      "Iteration 14531, Loss: 0.055780649185180664\n",
      "Iteration 14532, Loss: 0.05563601106405258\n",
      "Iteration 14533, Loss: 0.05576368421316147\n",
      "Iteration 14534, Loss: 0.05572954937815666\n",
      "Iteration 14535, Loss: 0.055675387382507324\n",
      "Iteration 14536, Loss: 0.055718183517456055\n",
      "Iteration 14537, Loss: 0.05566839501261711\n",
      "Iteration 14538, Loss: 0.055724143981933594\n",
      "Iteration 14539, Loss: 0.0557231530547142\n",
      "Iteration 14540, Loss: 0.05565599724650383\n",
      "Iteration 14541, Loss: 0.05568289756774902\n",
      "Iteration 14542, Loss: 0.05562707036733627\n",
      "Iteration 14543, Loss: 0.05574210733175278\n",
      "Iteration 14544, Loss: 0.05574282258749008\n",
      "Iteration 14545, Loss: 0.05565023422241211\n",
      "Iteration 14546, Loss: 0.05575517937541008\n",
      "Iteration 14547, Loss: 0.05576571077108383\n",
      "Iteration 14548, Loss: 0.055623214691877365\n",
      "Iteration 14549, Loss: 0.05583775043487549\n",
      "Iteration 14550, Loss: 0.055939238518476486\n",
      "Iteration 14551, Loss: 0.05591992661356926\n",
      "Iteration 14552, Loss: 0.05579928681254387\n",
      "Iteration 14553, Loss: 0.05568051338195801\n",
      "Iteration 14554, Loss: 0.05580977723002434\n",
      "Iteration 14555, Loss: 0.055824123322963715\n",
      "Iteration 14556, Loss: 0.05568603798747063\n",
      "Iteration 14557, Loss: 0.055777035653591156\n",
      "Iteration 14558, Loss: 0.055874548852443695\n",
      "Iteration 14559, Loss: 0.05586870759725571\n",
      "Iteration 14560, Loss: 0.05577174946665764\n",
      "Iteration 14561, Loss: 0.05566044896841049\n",
      "Iteration 14562, Loss: 0.055772703140974045\n",
      "Iteration 14563, Loss: 0.05574552342295647\n",
      "Iteration 14564, Loss: 0.05567018315196037\n",
      "Iteration 14565, Loss: 0.055718742311000824\n",
      "Iteration 14566, Loss: 0.0556948184967041\n",
      "Iteration 14567, Loss: 0.055671654641628265\n",
      "Iteration 14568, Loss: 0.05566906929016113\n",
      "Iteration 14569, Loss: 0.05567634105682373\n",
      "Iteration 14570, Loss: 0.055673956871032715\n",
      "Iteration 14571, Loss: 0.055667243897914886\n",
      "Iteration 14572, Loss: 0.05564320459961891\n",
      "Iteration 14573, Loss: 0.05569533631205559\n",
      "Iteration 14574, Loss: 0.055676303803920746\n",
      "Iteration 14575, Loss: 0.05568786710500717\n",
      "Iteration 14576, Loss: 0.055682580918073654\n",
      "Iteration 14577, Loss: 0.055674515664577484\n",
      "Iteration 14578, Loss: 0.05567435547709465\n",
      "Iteration 14579, Loss: 0.055671654641628265\n",
      "Iteration 14580, Loss: 0.055650632828474045\n",
      "Iteration 14581, Loss: 0.05571611970663071\n",
      "Iteration 14582, Loss: 0.055720530450344086\n",
      "Iteration 14583, Loss: 0.05563386529684067\n",
      "Iteration 14584, Loss: 0.05572235956788063\n",
      "Iteration 14585, Loss: 0.05567145720124245\n",
      "Iteration 14586, Loss: 0.055728040635585785\n",
      "Iteration 14587, Loss: 0.055775921791791916\n",
      "Iteration 14588, Loss: 0.05572104454040527\n",
      "Iteration 14589, Loss: 0.05566298961639404\n",
      "Iteration 14590, Loss: 0.055692195892333984\n",
      "Iteration 14591, Loss: 0.05565830320119858\n",
      "Iteration 14592, Loss: 0.0556662492454052\n",
      "Iteration 14593, Loss: 0.055651985108852386\n",
      "Iteration 14594, Loss: 0.0556764230132103\n",
      "Iteration 14595, Loss: 0.055648963898420334\n",
      "Iteration 14596, Loss: 0.05569549649953842\n",
      "Iteration 14597, Loss: 0.05566239356994629\n",
      "Iteration 14598, Loss: 0.05572168156504631\n",
      "Iteration 14599, Loss: 0.05574576184153557\n",
      "Iteration 14600, Loss: 0.05566156283020973\n",
      "Iteration 14601, Loss: 0.055758874863386154\n",
      "Iteration 14602, Loss: 0.05579054355621338\n",
      "Iteration 14603, Loss: 0.05566394329071045\n",
      "Iteration 14604, Loss: 0.05578911304473877\n",
      "Iteration 14605, Loss: 0.05587991327047348\n",
      "Iteration 14606, Loss: 0.05584776774048805\n",
      "Iteration 14607, Loss: 0.05571413412690163\n",
      "Iteration 14608, Loss: 0.055758677423000336\n",
      "Iteration 14609, Loss: 0.055847328156232834\n",
      "Iteration 14610, Loss: 0.055771034210920334\n",
      "Iteration 14611, Loss: 0.055666010826826096\n",
      "Iteration 14612, Loss: 0.05572720617055893\n",
      "Iteration 14613, Loss: 0.05567709729075432\n",
      "Iteration 14614, Loss: 0.05571087449789047\n",
      "Iteration 14615, Loss: 0.05571603775024414\n",
      "Iteration 14616, Loss: 0.05565107241272926\n",
      "Iteration 14617, Loss: 0.0556594543159008\n",
      "Iteration 14618, Loss: 0.05567002296447754\n",
      "Iteration 14619, Loss: 0.05562083050608635\n",
      "Iteration 14620, Loss: 0.05576777830719948\n",
      "Iteration 14621, Loss: 0.055799685418605804\n",
      "Iteration 14622, Loss: 0.05571679398417473\n",
      "Iteration 14623, Loss: 0.05570423603057861\n",
      "Iteration 14624, Loss: 0.05575315281748772\n",
      "Iteration 14625, Loss: 0.05565234273672104\n",
      "Iteration 14626, Loss: 0.05578303709626198\n",
      "Iteration 14627, Loss: 0.055857542902231216\n",
      "Iteration 14628, Loss: 0.05581597611308098\n",
      "Iteration 14629, Loss: 0.05568619817495346\n",
      "Iteration 14630, Loss: 0.055789947509765625\n",
      "Iteration 14631, Loss: 0.05586647987365723\n",
      "Iteration 14632, Loss: 0.05577198788523674\n",
      "Iteration 14633, Loss: 0.05568178743124008\n",
      "Iteration 14634, Loss: 0.055756449699401855\n",
      "Iteration 14635, Loss: 0.05572299286723137\n",
      "Iteration 14636, Loss: 0.055645983666181564\n",
      "Iteration 14637, Loss: 0.055711470544338226\n",
      "Iteration 14638, Loss: 0.05565357208251953\n",
      "Iteration 14639, Loss: 0.05572625249624252\n",
      "Iteration 14640, Loss: 0.05575831979513168\n",
      "Iteration 14641, Loss: 0.05568905919790268\n",
      "Iteration 14642, Loss: 0.055718421936035156\n",
      "Iteration 14643, Loss: 0.055741310119628906\n",
      "Iteration 14644, Loss: 0.05562647432088852\n",
      "Iteration 14645, Loss: 0.05570213124155998\n",
      "Iteration 14646, Loss: 0.055659256875514984\n",
      "Iteration 14647, Loss: 0.055728793144226074\n",
      "Iteration 14648, Loss: 0.05574778839945793\n",
      "Iteration 14649, Loss: 0.05564574524760246\n",
      "Iteration 14650, Loss: 0.05578247830271721\n",
      "Iteration 14651, Loss: 0.05582491680979729\n",
      "Iteration 14652, Loss: 0.05571969598531723\n",
      "Iteration 14653, Loss: 0.0557246208190918\n",
      "Iteration 14654, Loss: 0.05579996109008789\n",
      "Iteration 14655, Loss: 0.05574747174978256\n",
      "Iteration 14656, Loss: 0.05564602464437485\n",
      "Iteration 14657, Loss: 0.05569680780172348\n",
      "Iteration 14658, Loss: 0.055629294365644455\n",
      "Iteration 14659, Loss: 0.055678289383649826\n",
      "Iteration 14660, Loss: 0.055648092180490494\n",
      "Iteration 14661, Loss: 0.05572676658630371\n",
      "Iteration 14662, Loss: 0.0557103157043457\n",
      "Iteration 14663, Loss: 0.05567169189453125\n",
      "Iteration 14664, Loss: 0.05569792166352272\n",
      "Iteration 14665, Loss: 0.05562174320220947\n",
      "Iteration 14666, Loss: 0.05581279844045639\n",
      "Iteration 14667, Loss: 0.05584395304322243\n",
      "Iteration 14668, Loss: 0.055714212357997894\n",
      "Iteration 14669, Loss: 0.05574723333120346\n",
      "Iteration 14670, Loss: 0.05584180727601051\n",
      "Iteration 14671, Loss: 0.05582503601908684\n",
      "Iteration 14672, Loss: 0.05570809170603752\n",
      "Iteration 14673, Loss: 0.05574747174978256\n",
      "Iteration 14674, Loss: 0.05582074448466301\n",
      "Iteration 14675, Loss: 0.05572688952088356\n",
      "Iteration 14676, Loss: 0.05571317672729492\n",
      "Iteration 14677, Loss: 0.05578669160604477\n",
      "Iteration 14678, Loss: 0.0557510070502758\n",
      "Iteration 14679, Loss: 0.055633544921875\n",
      "Iteration 14680, Loss: 0.055811088532209396\n",
      "Iteration 14681, Loss: 0.05583922192454338\n",
      "Iteration 14682, Loss: 0.055692754685878754\n",
      "Iteration 14683, Loss: 0.05577560514211655\n",
      "Iteration 14684, Loss: 0.055883608758449554\n",
      "Iteration 14685, Loss: 0.05588666722178459\n",
      "Iteration 14686, Loss: 0.05579507350921631\n",
      "Iteration 14687, Loss: 0.05562404915690422\n",
      "Iteration 14688, Loss: 0.05590585991740227\n",
      "Iteration 14689, Loss: 0.05601048469543457\n",
      "Iteration 14690, Loss: 0.05592918395996094\n",
      "Iteration 14691, Loss: 0.05568361282348633\n",
      "Iteration 14692, Loss: 0.05584804341197014\n",
      "Iteration 14693, Loss: 0.05601577088236809\n",
      "Iteration 14694, Loss: 0.05606897920370102\n",
      "Iteration 14695, Loss: 0.05601787939667702\n",
      "Iteration 14696, Loss: 0.055874865502119064\n",
      "Iteration 14697, Loss: 0.055659692734479904\n",
      "Iteration 14698, Loss: 0.05593311786651611\n",
      "Iteration 14699, Loss: 0.05610279366374016\n",
      "Iteration 14700, Loss: 0.05608022212982178\n",
      "Iteration 14701, Loss: 0.05588531494140625\n",
      "Iteration 14702, Loss: 0.05566906929016113\n",
      "Iteration 14703, Loss: 0.05580803006887436\n",
      "Iteration 14704, Loss: 0.055843912065029144\n",
      "Iteration 14705, Loss: 0.05578303709626198\n",
      "Iteration 14706, Loss: 0.05563938617706299\n",
      "Iteration 14707, Loss: 0.0558701753616333\n",
      "Iteration 14708, Loss: 0.05596340075135231\n",
      "Iteration 14709, Loss: 0.05587860196828842\n",
      "Iteration 14710, Loss: 0.055651865899562836\n",
      "Iteration 14711, Loss: 0.05584164708852768\n",
      "Iteration 14712, Loss: 0.055972736328840256\n",
      "Iteration 14713, Loss: 0.0559893473982811\n",
      "Iteration 14714, Loss: 0.05590319633483887\n",
      "Iteration 14715, Loss: 0.05572950839996338\n",
      "Iteration 14716, Loss: 0.055787842720746994\n",
      "Iteration 14717, Loss: 0.055919092148542404\n",
      "Iteration 14718, Loss: 0.05587538331747055\n",
      "Iteration 14719, Loss: 0.05567217245697975\n",
      "Iteration 14720, Loss: 0.05583349987864494\n",
      "Iteration 14721, Loss: 0.055976711213588715\n",
      "Iteration 14722, Loss: 0.05600563809275627\n",
      "Iteration 14723, Loss: 0.05593296140432358\n",
      "Iteration 14724, Loss: 0.05577385425567627\n",
      "Iteration 14725, Loss: 0.055722516030073166\n",
      "Iteration 14726, Loss: 0.05584681034088135\n",
      "Iteration 14727, Loss: 0.055805087089538574\n",
      "Iteration 14728, Loss: 0.055641137063503265\n",
      "Iteration 14729, Loss: 0.055750489234924316\n",
      "Iteration 14730, Loss: 0.0557866096496582\n",
      "Iteration 14731, Loss: 0.055725179612636566\n",
      "Iteration 14732, Loss: 0.055662594735622406\n",
      "Iteration 14733, Loss: 0.05567709729075432\n",
      "Iteration 14734, Loss: 0.05567765235900879\n",
      "Iteration 14735, Loss: 0.05568655580282211\n",
      "Iteration 14736, Loss: 0.05562925338745117\n",
      "Iteration 14737, Loss: 0.0556541308760643\n",
      "Iteration 14738, Loss: 0.05562524124979973\n",
      "Iteration 14739, Loss: 0.0557078942656517\n",
      "Iteration 14740, Loss: 0.055665258318185806\n",
      "Iteration 14741, Loss: 0.05571822449564934\n",
      "Iteration 14742, Loss: 0.05574015900492668\n",
      "Iteration 14743, Loss: 0.05564248561859131\n",
      "Iteration 14744, Loss: 0.05579594895243645\n",
      "Iteration 14745, Loss: 0.0558442696928978\n",
      "Iteration 14746, Loss: 0.055739086121320724\n",
      "Iteration 14747, Loss: 0.05570948123931885\n",
      "Iteration 14748, Loss: 0.05578815937042236\n",
      "Iteration 14749, Loss: 0.055745165795087814\n",
      "Iteration 14750, Loss: 0.05563533678650856\n",
      "Iteration 14751, Loss: 0.05566640943288803\n",
      "Iteration 14752, Loss: 0.05566990375518799\n",
      "Iteration 14753, Loss: 0.05565953254699707\n",
      "Iteration 14754, Loss: 0.05568615719676018\n",
      "Iteration 14755, Loss: 0.0556468591094017\n",
      "Iteration 14756, Loss: 0.0557398796081543\n",
      "Iteration 14757, Loss: 0.05577806755900383\n",
      "Iteration 14758, Loss: 0.05571194738149643\n",
      "Iteration 14759, Loss: 0.05568758770823479\n",
      "Iteration 14760, Loss: 0.05571385473012924\n",
      "Iteration 14761, Loss: 0.0556461438536644\n",
      "Iteration 14762, Loss: 0.05566362664103508\n",
      "Iteration 14763, Loss: 0.05564216896891594\n",
      "Iteration 14764, Loss: 0.055692873895168304\n",
      "Iteration 14765, Loss: 0.05566000938415527\n",
      "Iteration 14766, Loss: 0.055708371102809906\n",
      "Iteration 14767, Loss: 0.05570952221751213\n",
      "Iteration 14768, Loss: 0.05564224720001221\n",
      "Iteration 14769, Loss: 0.05563875287771225\n",
      "Iteration 14770, Loss: 0.0557154044508934\n",
      "Iteration 14771, Loss: 0.05571293830871582\n",
      "Iteration 14772, Loss: 0.05564344301819801\n",
      "Iteration 14773, Loss: 0.05570896714925766\n",
      "Iteration 14774, Loss: 0.05565011501312256\n",
      "Iteration 14775, Loss: 0.05574306100606918\n",
      "Iteration 14776, Loss: 0.05579201504588127\n",
      "Iteration 14777, Loss: 0.055743101984262466\n",
      "Iteration 14778, Loss: 0.05562722682952881\n",
      "Iteration 14779, Loss: 0.05574393644928932\n",
      "Iteration 14780, Loss: 0.05570987984538078\n",
      "Iteration 14781, Loss: 0.055683933198451996\n",
      "Iteration 14782, Loss: 0.055719297379255295\n",
      "Iteration 14783, Loss: 0.05564960092306137\n",
      "Iteration 14784, Loss: 0.05577123165130615\n",
      "Iteration 14785, Loss: 0.05579964444041252\n",
      "Iteration 14786, Loss: 0.05567912384867668\n",
      "Iteration 14787, Loss: 0.055766861885786057\n",
      "Iteration 14788, Loss: 0.05584637448191643\n",
      "Iteration 14789, Loss: 0.05580099672079086\n",
      "Iteration 14790, Loss: 0.05565277859568596\n",
      "Iteration 14791, Loss: 0.0558374747633934\n",
      "Iteration 14792, Loss: 0.055927716195583344\n",
      "Iteration 14793, Loss: 0.05584883689880371\n",
      "Iteration 14794, Loss: 0.05564459413290024\n",
      "Iteration 14795, Loss: 0.05583513155579567\n",
      "Iteration 14796, Loss: 0.05593729019165039\n",
      "Iteration 14797, Loss: 0.055904190987348557\n",
      "Iteration 14798, Loss: 0.055754661560058594\n",
      "Iteration 14799, Loss: 0.0557299479842186\n",
      "Iteration 14800, Loss: 0.05583946034312248\n",
      "Iteration 14801, Loss: 0.05579765886068344\n",
      "Iteration 14802, Loss: 0.05561939999461174\n",
      "Iteration 14803, Loss: 0.05584319680929184\n",
      "Iteration 14804, Loss: 0.0559215173125267\n",
      "Iteration 14805, Loss: 0.05585503578186035\n",
      "Iteration 14806, Loss: 0.05568202584981918\n",
      "Iteration 14807, Loss: 0.05581609532237053\n",
      "Iteration 14808, Loss: 0.055926643311977386\n",
      "Iteration 14809, Loss: 0.05588948726654053\n",
      "Iteration 14810, Loss: 0.055722277611494064\n",
      "Iteration 14811, Loss: 0.05577429383993149\n",
      "Iteration 14812, Loss: 0.05589437484741211\n",
      "Iteration 14813, Loss: 0.05587196350097656\n",
      "Iteration 14814, Loss: 0.05572466179728508\n",
      "Iteration 14815, Loss: 0.05575597286224365\n",
      "Iteration 14816, Loss: 0.0558604821562767\n",
      "Iteration 14817, Loss: 0.0558188371360302\n",
      "Iteration 14818, Loss: 0.05565544217824936\n",
      "Iteration 14819, Loss: 0.05583071708679199\n",
      "Iteration 14820, Loss: 0.05593355745077133\n",
      "Iteration 14821, Loss: 0.0558883361518383\n",
      "Iteration 14822, Loss: 0.05571822449564934\n",
      "Iteration 14823, Loss: 0.055780570954084396\n",
      "Iteration 14824, Loss: 0.05590160936117172\n",
      "Iteration 14825, Loss: 0.055880188941955566\n",
      "Iteration 14826, Loss: 0.05573296919465065\n",
      "Iteration 14827, Loss: 0.055747631937265396\n",
      "Iteration 14828, Loss: 0.05585277080535889\n",
      "Iteration 14829, Loss: 0.055813949555158615\n",
      "Iteration 14830, Loss: 0.05565067380666733\n",
      "Iteration 14831, Loss: 0.05583953857421875\n",
      "Iteration 14832, Loss: 0.055945396423339844\n",
      "Iteration 14833, Loss: 0.05590284243226051\n",
      "Iteration 14834, Loss: 0.05573884770274162\n",
      "Iteration 14835, Loss: 0.05575891584157944\n",
      "Iteration 14836, Loss: 0.05587633699178696\n",
      "Iteration 14837, Loss: 0.055854879319667816\n",
      "Iteration 14838, Loss: 0.05570312589406967\n",
      "Iteration 14839, Loss: 0.0557839497923851\n",
      "Iteration 14840, Loss: 0.05589238926768303\n",
      "Iteration 14841, Loss: 0.055858295410871506\n",
      "Iteration 14842, Loss: 0.05571142956614494\n",
      "Iteration 14843, Loss: 0.05576976388692856\n",
      "Iteration 14844, Loss: 0.05586942285299301\n",
      "Iteration 14845, Loss: 0.05582432076334953\n",
      "Iteration 14846, Loss: 0.05564693734049797\n",
      "Iteration 14847, Loss: 0.055857423692941666\n",
      "Iteration 14848, Loss: 0.0559820756316185\n",
      "Iteration 14849, Loss: 0.05596407502889633\n",
      "Iteration 14850, Loss: 0.055826786905527115\n",
      "Iteration 14851, Loss: 0.05568739026784897\n",
      "Iteration 14852, Loss: 0.0558248795568943\n",
      "Iteration 14853, Loss: 0.05586854740977287\n",
      "Iteration 14854, Loss: 0.055765509605407715\n",
      "Iteration 14855, Loss: 0.05569605156779289\n",
      "Iteration 14856, Loss: 0.055771827697753906\n",
      "Iteration 14857, Loss: 0.05574027821421623\n",
      "Iteration 14858, Loss: 0.055663228034973145\n",
      "Iteration 14859, Loss: 0.05574731156229973\n",
      "Iteration 14860, Loss: 0.05573252961039543\n",
      "Iteration 14861, Loss: 0.05566652864217758\n",
      "Iteration 14862, Loss: 0.05569998547434807\n",
      "Iteration 14863, Loss: 0.055665455758571625\n",
      "Iteration 14864, Loss: 0.05570805445313454\n",
      "Iteration 14865, Loss: 0.05568361282348633\n",
      "Iteration 14866, Loss: 0.05569922924041748\n",
      "Iteration 14867, Loss: 0.05573081970214844\n",
      "Iteration 14868, Loss: 0.05566716194152832\n",
      "Iteration 14869, Loss: 0.05574170872569084\n",
      "Iteration 14870, Loss: 0.05575386807322502\n",
      "Iteration 14871, Loss: 0.05563024803996086\n",
      "Iteration 14872, Loss: 0.05569637194275856\n",
      "Iteration 14873, Loss: 0.055661123245954514\n",
      "Iteration 14874, Loss: 0.05571989342570305\n",
      "Iteration 14875, Loss: 0.055721282958984375\n",
      "Iteration 14876, Loss: 0.05564606562256813\n",
      "Iteration 14877, Loss: 0.05568055436015129\n",
      "Iteration 14878, Loss: 0.05563422292470932\n",
      "Iteration 14879, Loss: 0.055679045617580414\n",
      "Iteration 14880, Loss: 0.05565885826945305\n",
      "Iteration 14881, Loss: 0.0556994304060936\n",
      "Iteration 14882, Loss: 0.055664461106061935\n",
      "Iteration 14883, Loss: 0.05572156235575676\n",
      "Iteration 14884, Loss: 0.055761538445949554\n",
      "Iteration 14885, Loss: 0.05570165440440178\n",
      "Iteration 14886, Loss: 0.0556894950568676\n",
      "Iteration 14887, Loss: 0.055698513984680176\n",
      "Iteration 14888, Loss: 0.05566819757223129\n",
      "Iteration 14889, Loss: 0.055682502686977386\n",
      "Iteration 14890, Loss: 0.055628661066293716\n",
      "Iteration 14891, Loss: 0.055681824684143066\n",
      "Iteration 14892, Loss: 0.055638473480939865\n",
      "Iteration 14893, Loss: 0.05572398751974106\n",
      "Iteration 14894, Loss: 0.05570133775472641\n",
      "Iteration 14895, Loss: 0.05567912384867668\n",
      "Iteration 14896, Loss: 0.055700186640024185\n",
      "Iteration 14897, Loss: 0.05562317371368408\n",
      "Iteration 14898, Loss: 0.05573273077607155\n",
      "Iteration 14899, Loss: 0.0556795597076416\n",
      "Iteration 14900, Loss: 0.055722832679748535\n",
      "Iteration 14901, Loss: 0.05577492713928223\n",
      "Iteration 14902, Loss: 0.05572839826345444\n",
      "Iteration 14903, Loss: 0.055636487901210785\n",
      "Iteration 14904, Loss: 0.055639151483774185\n",
      "Iteration 14905, Loss: 0.0557071790099144\n",
      "Iteration 14906, Loss: 0.05571361631155014\n",
      "Iteration 14907, Loss: 0.05562746524810791\n",
      "Iteration 14908, Loss: 0.05579773709177971\n",
      "Iteration 14909, Loss: 0.05581045150756836\n",
      "Iteration 14910, Loss: 0.05564955994486809\n",
      "Iteration 14911, Loss: 0.055816613137722015\n",
      "Iteration 14912, Loss: 0.0559336356818676\n",
      "Iteration 14913, Loss: 0.05594436451792717\n",
      "Iteration 14914, Loss: 0.055859409272670746\n",
      "Iteration 14915, Loss: 0.05568917840719223\n",
      "Iteration 14916, Loss: 0.05584017559885979\n",
      "Iteration 14917, Loss: 0.05596884340047836\n",
      "Iteration 14918, Loss: 0.055910829454660416\n",
      "Iteration 14919, Loss: 0.05568603798747063\n",
      "Iteration 14920, Loss: 0.055831633508205414\n",
      "Iteration 14921, Loss: 0.055986564606428146\n",
      "Iteration 14922, Loss: 0.05603114888072014\n",
      "Iteration 14923, Loss: 0.05597587674856186\n",
      "Iteration 14924, Loss: 0.05583159253001213\n",
      "Iteration 14925, Loss: 0.0556158646941185\n",
      "Iteration 14926, Loss: 0.05577429383993149\n",
      "Iteration 14927, Loss: 0.05574933812022209\n",
      "Iteration 14928, Loss: 0.05565452575683594\n",
      "Iteration 14929, Loss: 0.055694740265607834\n",
      "Iteration 14930, Loss: 0.05563775822520256\n",
      "Iteration 14931, Loss: 0.05577198788523674\n",
      "Iteration 14932, Loss: 0.05578327178955078\n",
      "Iteration 14933, Loss: 0.0556490421295166\n",
      "Iteration 14934, Loss: 0.055789630860090256\n",
      "Iteration 14935, Loss: 0.05586111918091774\n",
      "Iteration 14936, Loss: 0.05580659955739975\n",
      "Iteration 14937, Loss: 0.05565500259399414\n",
      "Iteration 14938, Loss: 0.05583469197154045\n",
      "Iteration 14939, Loss: 0.0559212788939476\n",
      "Iteration 14940, Loss: 0.05583870783448219\n",
      "Iteration 14941, Loss: 0.0556359700858593\n",
      "Iteration 14942, Loss: 0.05581192299723625\n",
      "Iteration 14943, Loss: 0.05587848275899887\n",
      "Iteration 14944, Loss: 0.05581120774149895\n",
      "Iteration 14945, Loss: 0.05564872547984123\n",
      "Iteration 14946, Loss: 0.0558340959250927\n",
      "Iteration 14947, Loss: 0.055913329124450684\n",
      "Iteration 14948, Loss: 0.05582880973815918\n",
      "Iteration 14949, Loss: 0.05563875287771225\n",
      "Iteration 14950, Loss: 0.05579507350921631\n",
      "Iteration 14951, Loss: 0.05583854764699936\n",
      "Iteration 14952, Loss: 0.05574647709727287\n",
      "Iteration 14953, Loss: 0.055687665939331055\n",
      "Iteration 14954, Loss: 0.05575358867645264\n",
      "Iteration 14955, Loss: 0.05568234249949455\n",
      "Iteration 14956, Loss: 0.05573419854044914\n",
      "Iteration 14957, Loss: 0.055779580026865005\n",
      "Iteration 14958, Loss: 0.055696211755275726\n",
      "Iteration 14959, Loss: 0.05572656914591789\n",
      "Iteration 14960, Loss: 0.05577612295746803\n",
      "Iteration 14961, Loss: 0.055684804916381836\n",
      "Iteration 14962, Loss: 0.05574754998087883\n",
      "Iteration 14963, Loss: 0.055808667093515396\n",
      "Iteration 14964, Loss: 0.05573789402842522\n",
      "Iteration 14965, Loss: 0.05567701905965805\n",
      "Iteration 14966, Loss: 0.05573054403066635\n",
      "Iteration 14967, Loss: 0.05564912408590317\n",
      "Iteration 14968, Loss: 0.05577389523386955\n",
      "Iteration 14969, Loss: 0.055832069367170334\n",
      "Iteration 14970, Loss: 0.05577230453491211\n",
      "Iteration 14971, Loss: 0.05565845966339111\n",
      "Iteration 14972, Loss: 0.0557815246284008\n",
      "Iteration 14973, Loss: 0.05579209327697754\n",
      "Iteration 14974, Loss: 0.05564364045858383\n",
      "Iteration 14975, Loss: 0.05580409616231918\n",
      "Iteration 14976, Loss: 0.055900853127241135\n",
      "Iteration 14977, Loss: 0.05589183419942856\n",
      "Iteration 14978, Loss: 0.055789314210414886\n",
      "Iteration 14979, Loss: 0.05564308166503906\n",
      "Iteration 14980, Loss: 0.055801115930080414\n",
      "Iteration 14981, Loss: 0.0558091402053833\n",
      "Iteration 14982, Loss: 0.055669985711574554\n",
      "Iteration 14983, Loss: 0.055777668952941895\n",
      "Iteration 14984, Loss: 0.0558631457388401\n",
      "Iteration 14985, Loss: 0.055838506668806076\n",
      "Iteration 14986, Loss: 0.055714648216962814\n",
      "Iteration 14987, Loss: 0.055748581886291504\n",
      "Iteration 14988, Loss: 0.0558290109038353\n",
      "Iteration 14989, Loss: 0.05574127286672592\n",
      "Iteration 14990, Loss: 0.05569831654429436\n",
      "Iteration 14991, Loss: 0.05576857179403305\n",
      "Iteration 14992, Loss: 0.055730223655700684\n",
      "Iteration 14993, Loss: 0.055638037621974945\n",
      "Iteration 14994, Loss: 0.05569470301270485\n",
      "Iteration 14995, Loss: 0.05563756078481674\n",
      "Iteration 14996, Loss: 0.055689334869384766\n",
      "Iteration 14997, Loss: 0.055659376084804535\n",
      "Iteration 14998, Loss: 0.05571810528635979\n",
      "Iteration 14999, Loss: 0.05571611970663071\n",
      "Iteration 15000, Loss: 0.055652063339948654\n",
      "Iteration 15001, Loss: 0.05567765235900879\n",
      "Iteration 15002, Loss: 0.05564574524760246\n",
      "Iteration 15003, Loss: 0.05565599724650383\n",
      "Iteration 15004, Loss: 0.05563513562083244\n",
      "Iteration 15005, Loss: 0.05571174621582031\n",
      "Iteration 15006, Loss: 0.05567721650004387\n",
      "Iteration 15007, Loss: 0.055703721940517426\n",
      "Iteration 15008, Loss: 0.05572474002838135\n",
      "Iteration 15009, Loss: 0.05562881752848625\n",
      "Iteration 15010, Loss: 0.055814824998378754\n",
      "Iteration 15011, Loss: 0.0558600053191185\n",
      "Iteration 15012, Loss: 0.05574711412191391\n",
      "Iteration 15013, Loss: 0.055711034685373306\n",
      "Iteration 15014, Loss: 0.05579618737101555\n",
      "Iteration 15015, Loss: 0.055763762444257736\n",
      "Iteration 15016, Loss: 0.0556233748793602\n",
      "Iteration 15017, Loss: 0.05586763471364975\n",
      "Iteration 15018, Loss: 0.05594877526164055\n",
      "Iteration 15019, Loss: 0.055859051644802094\n",
      "Iteration 15020, Loss: 0.05565301701426506\n",
      "Iteration 15021, Loss: 0.05582718178629875\n",
      "Iteration 15022, Loss: 0.0559261254966259\n",
      "Iteration 15023, Loss: 0.05589350312948227\n",
      "Iteration 15024, Loss: 0.05574552342295647\n",
      "Iteration 15025, Loss: 0.055737100541591644\n",
      "Iteration 15026, Loss: 0.05584323778748512\n",
      "Iteration 15027, Loss: 0.055792491883039474\n",
      "Iteration 15028, Loss: 0.05562857910990715\n",
      "Iteration 15029, Loss: 0.05575263500213623\n",
      "Iteration 15030, Loss: 0.05574151128530502\n",
      "Iteration 15031, Loss: 0.05563497543334961\n",
      "Iteration 15032, Loss: 0.055694542825222015\n",
      "Iteration 15033, Loss: 0.05562182515859604\n",
      "Iteration 15034, Loss: 0.05578017234802246\n",
      "Iteration 15035, Loss: 0.0558343343436718\n",
      "Iteration 15036, Loss: 0.05577953904867172\n",
      "Iteration 15037, Loss: 0.055651746690273285\n",
      "Iteration 15038, Loss: 0.05581863969564438\n",
      "Iteration 15039, Loss: 0.05586886778473854\n",
      "Iteration 15040, Loss: 0.0557425431907177\n",
      "Iteration 15041, Loss: 0.055728159844875336\n",
      "Iteration 15042, Loss: 0.05582380294799805\n",
      "Iteration 15043, Loss: 0.05581478402018547\n",
      "Iteration 15044, Loss: 0.05571381375193596\n",
      "Iteration 15045, Loss: 0.05572601407766342\n",
      "Iteration 15046, Loss: 0.055779337882995605\n",
      "Iteration 15047, Loss: 0.05566064640879631\n",
      "Iteration 15048, Loss: 0.0557783842086792\n",
      "Iteration 15049, Loss: 0.05586572736501694\n",
      "Iteration 15050, Loss: 0.05585039034485817\n",
      "Iteration 15051, Loss: 0.05574297904968262\n",
      "Iteration 15052, Loss: 0.0556926354765892\n",
      "Iteration 15053, Loss: 0.05575339123606682\n",
      "Iteration 15054, Loss: 0.055641692131757736\n",
      "Iteration 15055, Loss: 0.05578207969665527\n",
      "Iteration 15056, Loss: 0.05585964769124985\n",
      "Iteration 15057, Loss: 0.05583393573760986\n",
      "Iteration 15058, Loss: 0.055716000497341156\n",
      "Iteration 15059, Loss: 0.05574091523885727\n",
      "Iteration 15060, Loss: 0.05581359192728996\n",
      "Iteration 15061, Loss: 0.05570928379893303\n",
      "Iteration 15062, Loss: 0.055736105889081955\n",
      "Iteration 15063, Loss: 0.055818360298871994\n",
      "Iteration 15064, Loss: 0.055797576904296875\n",
      "Iteration 15065, Loss: 0.05568504333496094\n",
      "Iteration 15066, Loss: 0.0557759627699852\n",
      "Iteration 15067, Loss: 0.05584132671356201\n",
      "Iteration 15068, Loss: 0.05572950839996338\n",
      "Iteration 15069, Loss: 0.05572609230875969\n",
      "Iteration 15070, Loss: 0.055813197046518326\n",
      "Iteration 15071, Loss: 0.05579742044210434\n",
      "Iteration 15072, Loss: 0.05568921938538551\n",
      "Iteration 15073, Loss: 0.055765312165021896\n",
      "Iteration 15074, Loss: 0.05582619085907936\n",
      "Iteration 15075, Loss: 0.05570971965789795\n",
      "Iteration 15076, Loss: 0.05574357509613037\n",
      "Iteration 15077, Loss: 0.05583333969116211\n",
      "Iteration 15078, Loss: 0.05581995099782944\n",
      "Iteration 15079, Loss: 0.05571381375193596\n",
      "Iteration 15080, Loss: 0.05572975054383278\n",
      "Iteration 15081, Loss: 0.05578867718577385\n",
      "Iteration 15082, Loss: 0.055670540779829025\n",
      "Iteration 15083, Loss: 0.055773936212062836\n",
      "Iteration 15084, Loss: 0.05586501210927963\n",
      "Iteration 15085, Loss: 0.05585245415568352\n",
      "Iteration 15086, Loss: 0.05574707314372063\n",
      "Iteration 15087, Loss: 0.055684130638837814\n",
      "Iteration 15088, Loss: 0.055743735283613205\n",
      "Iteration 15089, Loss: 0.05562949553132057\n",
      "Iteration 15090, Loss: 0.05579336732625961\n",
      "Iteration 15091, Loss: 0.05587395280599594\n",
      "Iteration 15092, Loss: 0.055850982666015625\n",
      "Iteration 15093, Loss: 0.055735550820827484\n",
      "Iteration 15094, Loss: 0.05571206659078598\n",
      "Iteration 15095, Loss: 0.05578112602233887\n",
      "Iteration 15096, Loss: 0.05567121505737305\n",
      "Iteration 15097, Loss: 0.055767737329006195\n",
      "Iteration 15098, Loss: 0.05585328862071037\n",
      "Iteration 15099, Loss: 0.055835966020822525\n",
      "Iteration 15100, Loss: 0.05572665110230446\n",
      "Iteration 15101, Loss: 0.05571695417165756\n",
      "Iteration 15102, Loss: 0.05577914044260979\n",
      "Iteration 15103, Loss: 0.05566374585032463\n",
      "Iteration 15104, Loss: 0.05577552318572998\n",
      "Iteration 15105, Loss: 0.05586374178528786\n",
      "Iteration 15106, Loss: 0.055848877876996994\n",
      "Iteration 15107, Loss: 0.05574151128530502\n",
      "Iteration 15108, Loss: 0.05569382756948471\n",
      "Iteration 15109, Loss: 0.0557534322142601\n",
      "Iteration 15110, Loss: 0.05563632771372795\n",
      "Iteration 15111, Loss: 0.055795393884181976\n",
      "Iteration 15112, Loss: 0.055882375687360764\n",
      "Iteration 15113, Loss: 0.05586572736501694\n",
      "Iteration 15114, Loss: 0.055756568908691406\n",
      "Iteration 15115, Loss: 0.05567694082856178\n",
      "Iteration 15116, Loss: 0.05574306100606918\n",
      "Iteration 15117, Loss: 0.055636368691921234\n",
      "Iteration 15118, Loss: 0.055785100907087326\n",
      "Iteration 15119, Loss: 0.055862944573163986\n",
      "Iteration 15120, Loss: 0.055838070809841156\n",
      "Iteration 15121, Loss: 0.05572124570608139\n",
      "Iteration 15122, Loss: 0.05573276802897453\n",
      "Iteration 15123, Loss: 0.05580270290374756\n",
      "Iteration 15124, Loss: 0.055692158639431\n",
      "Iteration 15125, Loss: 0.0557527169585228\n",
      "Iteration 15126, Loss: 0.055839382112026215\n",
      "Iteration 15127, Loss: 0.055822692811489105\n",
      "Iteration 15128, Loss: 0.05571393296122551\n",
      "Iteration 15129, Loss: 0.055733006447553635\n",
      "Iteration 15130, Loss: 0.055794041603803635\n",
      "Iteration 15131, Loss: 0.055676501244306564\n",
      "Iteration 15132, Loss: 0.05576884746551514\n",
      "Iteration 15133, Loss: 0.05585932731628418\n",
      "Iteration 15134, Loss: 0.055846571922302246\n",
      "Iteration 15135, Loss: 0.055741194635629654\n",
      "Iteration 15136, Loss: 0.0556923970580101\n",
      "Iteration 15137, Loss: 0.05575021356344223\n",
      "Iteration 15138, Loss: 0.055633269250392914\n",
      "Iteration 15139, Loss: 0.05579312890768051\n",
      "Iteration 15140, Loss: 0.05587514489889145\n",
      "Iteration 15141, Loss: 0.05585400387644768\n",
      "Iteration 15142, Loss: 0.05574027821421623\n",
      "Iteration 15143, Loss: 0.05570407956838608\n",
      "Iteration 15144, Loss: 0.05577310174703598\n",
      "Iteration 15145, Loss: 0.055666010826826096\n",
      "Iteration 15146, Loss: 0.055770039558410645\n",
      "Iteration 15147, Loss: 0.055853962898254395\n",
      "Iteration 15148, Loss: 0.05583556741476059\n",
      "Iteration 15149, Loss: 0.05572521686553955\n",
      "Iteration 15150, Loss: 0.05571989342570305\n",
      "Iteration 15151, Loss: 0.05578359216451645\n",
      "Iteration 15152, Loss: 0.05567002668976784\n",
      "Iteration 15153, Loss: 0.05577123165130615\n",
      "Iteration 15154, Loss: 0.05585964769124985\n",
      "Iteration 15155, Loss: 0.055844902992248535\n",
      "Iteration 15156, Loss: 0.0557379350066185\n",
      "Iteration 15157, Loss: 0.055698275566101074\n",
      "Iteration 15158, Loss: 0.05575815960764885\n",
      "Iteration 15159, Loss: 0.05564042180776596\n",
      "Iteration 15160, Loss: 0.05579531192779541\n",
      "Iteration 15161, Loss: 0.055885594338178635\n",
      "Iteration 15162, Loss: 0.0558723621070385\n",
      "Iteration 15163, Loss: 0.05576610565185547\n",
      "Iteration 15164, Loss: 0.05565885826945305\n",
      "Iteration 15165, Loss: 0.055717747658491135\n",
      "Iteration 15166, Loss: 0.055623333901166916\n",
      "Iteration 15167, Loss: 0.05565687268972397\n",
      "Iteration 15168, Loss: 0.05564972013235092\n",
      "Iteration 15169, Loss: 0.05562905594706535\n",
      "Iteration 15170, Loss: 0.055673323571681976\n",
      "Iteration 15171, Loss: 0.055631838738918304\n",
      "Iteration 15172, Loss: 0.05566505715250969\n",
      "Iteration 15173, Loss: 0.055649083107709885\n",
      "Iteration 15174, Loss: 0.05562810227274895\n",
      "Iteration 15175, Loss: 0.0557197742164135\n",
      "Iteration 15176, Loss: 0.05566386505961418\n",
      "Iteration 15177, Loss: 0.05573618784546852\n",
      "Iteration 15178, Loss: 0.055789075791835785\n",
      "Iteration 15179, Loss: 0.055742423981428146\n",
      "Iteration 15180, Loss: 0.05562059208750725\n",
      "Iteration 15181, Loss: 0.055701613426208496\n",
      "Iteration 15182, Loss: 0.05563827604055405\n",
      "Iteration 15183, Loss: 0.05574437230825424\n",
      "Iteration 15184, Loss: 0.05576245114207268\n",
      "Iteration 15185, Loss: 0.05566096305847168\n",
      "Iteration 15186, Loss: 0.055778902024030685\n",
      "Iteration 15187, Loss: 0.05583338066935539\n",
      "Iteration 15188, Loss: 0.05573650449514389\n",
      "Iteration 15189, Loss: 0.05570491403341293\n",
      "Iteration 15190, Loss: 0.05577671900391579\n",
      "Iteration 15191, Loss: 0.05572386831045151\n",
      "Iteration 15192, Loss: 0.05566958710551262\n",
      "Iteration 15193, Loss: 0.05569390580058098\n",
      "Iteration 15194, Loss: 0.05564681813120842\n",
      "Iteration 15195, Loss: 0.05563187971711159\n",
      "Iteration 15196, Loss: 0.05573379993438721\n",
      "Iteration 15197, Loss: 0.05572017282247543\n",
      "Iteration 15198, Loss: 0.055656593292951584\n",
      "Iteration 15199, Loss: 0.055686794221401215\n",
      "Iteration 15200, Loss: 0.055636607110500336\n",
      "Iteration 15201, Loss: 0.05568341538310051\n",
      "Iteration 15202, Loss: 0.055667679756879807\n",
      "Iteration 15203, Loss: 0.055682066828012466\n",
      "Iteration 15204, Loss: 0.055642370134592056\n",
      "Iteration 15205, Loss: 0.05573900789022446\n",
      "Iteration 15206, Loss: 0.055779341608285904\n",
      "Iteration 15207, Loss: 0.05572104454040527\n",
      "Iteration 15208, Loss: 0.055662356317043304\n",
      "Iteration 15209, Loss: 0.05567058175802231\n",
      "Iteration 15210, Loss: 0.05568981170654297\n",
      "Iteration 15211, Loss: 0.055705271661281586\n",
      "Iteration 15212, Loss: 0.05562770739197731\n",
      "Iteration 15213, Loss: 0.055806320160627365\n",
      "Iteration 15214, Loss: 0.05582750216126442\n",
      "Iteration 15215, Loss: 0.05567578598856926\n",
      "Iteration 15216, Loss: 0.05579110234975815\n",
      "Iteration 15217, Loss: 0.05590101331472397\n",
      "Iteration 15218, Loss: 0.055905263870954514\n",
      "Iteration 15219, Loss: 0.05581426993012428\n",
      "Iteration 15220, Loss: 0.05563954636454582\n",
      "Iteration 15221, Loss: 0.05591174215078354\n",
      "Iteration 15222, Loss: 0.056042592972517014\n",
      "Iteration 15223, Loss: 0.05598680302500725\n",
      "Iteration 15224, Loss: 0.05576388165354729\n",
      "Iteration 15225, Loss: 0.055774372071027756\n",
      "Iteration 15226, Loss: 0.05592819303274155\n",
      "Iteration 15227, Loss: 0.055972300469875336\n",
      "Iteration 15228, Loss: 0.05591706559062004\n",
      "Iteration 15229, Loss: 0.0557730607688427\n",
      "Iteration 15230, Loss: 0.05569545552134514\n",
      "Iteration 15231, Loss: 0.05579555407166481\n",
      "Iteration 15232, Loss: 0.055713653564453125\n",
      "Iteration 15233, Loss: 0.05571802705526352\n",
      "Iteration 15234, Loss: 0.055787406861782074\n",
      "Iteration 15235, Loss: 0.05575605481863022\n",
      "Iteration 15236, Loss: 0.055634502321481705\n",
      "Iteration 15237, Loss: 0.05585634708404541\n",
      "Iteration 15238, Loss: 0.05593077465891838\n",
      "Iteration 15239, Loss: 0.055824678391218185\n",
      "Iteration 15240, Loss: 0.05565289780497551\n",
      "Iteration 15241, Loss: 0.05573868751525879\n",
      "Iteration 15242, Loss: 0.05572358891367912\n",
      "Iteration 15243, Loss: 0.05561673641204834\n",
      "Iteration 15244, Loss: 0.05585145950317383\n",
      "Iteration 15245, Loss: 0.05590641498565674\n",
      "Iteration 15246, Loss: 0.0557916983962059\n",
      "Iteration 15247, Loss: 0.055682502686977386\n",
      "Iteration 15248, Loss: 0.05577417463064194\n",
      "Iteration 15249, Loss: 0.05576400086283684\n",
      "Iteration 15250, Loss: 0.05565357580780983\n",
      "Iteration 15251, Loss: 0.05581283941864967\n",
      "Iteration 15252, Loss: 0.055879633873701096\n",
      "Iteration 15253, Loss: 0.055781565606594086\n",
      "Iteration 15254, Loss: 0.05567765235900879\n",
      "Iteration 15255, Loss: 0.05576125904917717\n",
      "Iteration 15256, Loss: 0.05574139207601547\n",
      "Iteration 15257, Loss: 0.05561677739024162\n",
      "Iteration 15258, Loss: 0.055836401879787445\n",
      "Iteration 15259, Loss: 0.05590566247701645\n",
      "Iteration 15260, Loss: 0.05583349987864494\n",
      "Iteration 15261, Loss: 0.05567121505737305\n",
      "Iteration 15262, Loss: 0.05581430718302727\n",
      "Iteration 15263, Loss: 0.05590144917368889\n",
      "Iteration 15264, Loss: 0.05583775416016579\n",
      "Iteration 15265, Loss: 0.055645547807216644\n",
      "Iteration 15266, Loss: 0.05586286634206772\n",
      "Iteration 15267, Loss: 0.055989108979701996\n",
      "Iteration 15268, Loss: 0.05596888065338135\n",
      "Iteration 15269, Loss: 0.055822573602199554\n",
      "Iteration 15270, Loss: 0.05568142980337143\n",
      "Iteration 15271, Loss: 0.0558195523917675\n",
      "Iteration 15272, Loss: 0.05584101006388664\n",
      "Iteration 15273, Loss: 0.0557146891951561\n",
      "Iteration 15274, Loss: 0.05575442314147949\n",
      "Iteration 15275, Loss: 0.05584617704153061\n",
      "Iteration 15276, Loss: 0.05581367388367653\n",
      "Iteration 15277, Loss: 0.05568806454539299\n",
      "Iteration 15278, Loss: 0.05578049272298813\n",
      "Iteration 15279, Loss: 0.05585285276174545\n",
      "Iteration 15280, Loss: 0.05576217547059059\n",
      "Iteration 15281, Loss: 0.055685363709926605\n",
      "Iteration 15282, Loss: 0.05575581640005112\n",
      "Iteration 15283, Loss: 0.05571293830871582\n",
      "Iteration 15284, Loss: 0.05566660687327385\n",
      "Iteration 15285, Loss: 0.05568750947713852\n",
      "Iteration 15286, Loss: 0.055664896965026855\n",
      "Iteration 15287, Loss: 0.05566469952464104\n",
      "Iteration 15288, Loss: 0.05566537380218506\n",
      "Iteration 15289, Loss: 0.055645108222961426\n",
      "Iteration 15290, Loss: 0.0556894950568676\n",
      "Iteration 15291, Loss: 0.05568552017211914\n",
      "Iteration 15292, Loss: 0.055645111948251724\n",
      "Iteration 15293, Loss: 0.05564260482788086\n",
      "Iteration 15294, Loss: 0.05566342920064926\n",
      "Iteration 15295, Loss: 0.055616896599531174\n",
      "Iteration 15296, Loss: 0.05569060891866684\n",
      "Iteration 15297, Loss: 0.05566128343343735\n",
      "Iteration 15298, Loss: 0.05570710077881813\n",
      "Iteration 15299, Loss: 0.05569076910614967\n",
      "Iteration 15300, Loss: 0.055687230080366135\n",
      "Iteration 15301, Loss: 0.055709682404994965\n",
      "Iteration 15302, Loss: 0.05563271418213844\n",
      "Iteration 15303, Loss: 0.05578788369894028\n",
      "Iteration 15304, Loss: 0.055801473557949066\n",
      "Iteration 15305, Loss: 0.05564669892191887\n",
      "Iteration 15306, Loss: 0.055815499275922775\n",
      "Iteration 15307, Loss: 0.055925093591213226\n",
      "Iteration 15308, Loss: 0.05592576786875725\n",
      "Iteration 15309, Loss: 0.055829647928476334\n",
      "Iteration 15310, Loss: 0.05565786734223366\n",
      "Iteration 15311, Loss: 0.055877409875392914\n",
      "Iteration 15312, Loss: 0.05599502846598625\n",
      "Iteration 15313, Loss: 0.05592958256602287\n",
      "Iteration 15314, Loss: 0.0556996688246727\n",
      "Iteration 15315, Loss: 0.05582726374268532\n",
      "Iteration 15316, Loss: 0.05598581209778786\n",
      "Iteration 15317, Loss: 0.05603349208831787\n",
      "Iteration 15318, Loss: 0.05598076432943344\n",
      "Iteration 15319, Loss: 0.05583886429667473\n",
      "Iteration 15320, Loss: 0.05563044548034668\n",
      "Iteration 15321, Loss: 0.055920958518981934\n",
      "Iteration 15322, Loss: 0.056039296090602875\n",
      "Iteration 15323, Loss: 0.055970631539821625\n",
      "Iteration 15324, Loss: 0.055735670030117035\n",
      "Iteration 15325, Loss: 0.055803459137678146\n",
      "Iteration 15326, Loss: 0.05596514791250229\n",
      "Iteration 15327, Loss: 0.05601402372121811\n",
      "Iteration 15328, Loss: 0.055960457772016525\n",
      "Iteration 15329, Loss: 0.05581585690379143\n",
      "Iteration 15330, Loss: 0.055643320083618164\n",
      "Iteration 15331, Loss: 0.05578025430440903\n",
      "Iteration 15332, Loss: 0.055747151374816895\n",
      "Iteration 15333, Loss: 0.05566565319895744\n",
      "Iteration 15334, Loss: 0.055712465196847916\n",
      "Iteration 15335, Loss: 0.05567431449890137\n",
      "Iteration 15336, Loss: 0.05570737645030022\n",
      "Iteration 15337, Loss: 0.05569811910390854\n",
      "Iteration 15338, Loss: 0.05567741394042969\n",
      "Iteration 15339, Loss: 0.05569986626505852\n",
      "Iteration 15340, Loss: 0.05562814325094223\n",
      "Iteration 15341, Loss: 0.05579086393117905\n",
      "Iteration 15342, Loss: 0.0558164119720459\n",
      "Iteration 15343, Loss: 0.05569839850068092\n",
      "Iteration 15344, Loss: 0.055751800537109375\n",
      "Iteration 15345, Loss: 0.05583306401968002\n",
      "Iteration 15346, Loss: 0.05579141899943352\n",
      "Iteration 15347, Loss: 0.05564582347869873\n",
      "Iteration 15348, Loss: 0.05584009736776352\n",
      "Iteration 15349, Loss: 0.05592688173055649\n",
      "Iteration 15350, Loss: 0.05585062503814697\n",
      "Iteration 15351, Loss: 0.055649321526288986\n",
      "Iteration 15352, Loss: 0.05583842843770981\n",
      "Iteration 15353, Loss: 0.05594957247376442\n",
      "Iteration 15354, Loss: 0.05592775344848633\n",
      "Iteration 15355, Loss: 0.05579189583659172\n",
      "Iteration 15356, Loss: 0.05568873882293701\n",
      "Iteration 15357, Loss: 0.05580552667379379\n",
      "Iteration 15358, Loss: 0.05578958988189697\n",
      "Iteration 15359, Loss: 0.05563744157552719\n",
      "Iteration 15360, Loss: 0.05580087751150131\n",
      "Iteration 15361, Loss: 0.055886149406433105\n",
      "Iteration 15362, Loss: 0.05586684122681618\n",
      "Iteration 15363, Loss: 0.055756568908691406\n",
      "Iteration 15364, Loss: 0.055688343942165375\n",
      "Iteration 15365, Loss: 0.05576936528086662\n",
      "Iteration 15366, Loss: 0.055696528404951096\n",
      "Iteration 15367, Loss: 0.05572609230875969\n",
      "Iteration 15368, Loss: 0.05578605458140373\n",
      "Iteration 15369, Loss: 0.05574989318847656\n",
      "Iteration 15370, Loss: 0.055638354271650314\n",
      "Iteration 15371, Loss: 0.055818043649196625\n",
      "Iteration 15372, Loss: 0.055856864899396896\n",
      "Iteration 15373, Loss: 0.05572589486837387\n",
      "Iteration 15374, Loss: 0.055741749703884125\n",
      "Iteration 15375, Loss: 0.055838942527770996\n",
      "Iteration 15376, Loss: 0.055827025324106216\n",
      "Iteration 15377, Loss: 0.055716078728437424\n",
      "Iteration 15378, Loss: 0.05573316663503647\n",
      "Iteration 15379, Loss: 0.05579809471964836\n",
      "Iteration 15380, Loss: 0.055688582360744476\n",
      "Iteration 15381, Loss: 0.055755894631147385\n",
      "Iteration 15382, Loss: 0.055840253829956055\n",
      "Iteration 15383, Loss: 0.0558147057890892\n",
      "Iteration 15384, Loss: 0.05569521710276604\n",
      "Iteration 15385, Loss: 0.05577012151479721\n",
      "Iteration 15386, Loss: 0.05584224313497543\n",
      "Iteration 15387, Loss: 0.05573737993836403\n",
      "Iteration 15388, Loss: 0.05571639537811279\n",
      "Iteration 15389, Loss: 0.055798809975385666\n",
      "Iteration 15390, Loss: 0.05577461048960686\n",
      "Iteration 15391, Loss: 0.055660687386989594\n",
      "Iteration 15392, Loss: 0.055807750672101974\n",
      "Iteration 15393, Loss: 0.055868905037641525\n",
      "Iteration 15394, Loss: 0.0557510070502758\n",
      "Iteration 15395, Loss: 0.05571504682302475\n",
      "Iteration 15396, Loss: 0.055805884301662445\n",
      "Iteration 15397, Loss: 0.05579213425517082\n",
      "Iteration 15398, Loss: 0.05568619817495346\n",
      "Iteration 15399, Loss: 0.05576745793223381\n",
      "Iteration 15400, Loss: 0.055824439972639084\n",
      "Iteration 15401, Loss: 0.05570296570658684\n",
      "Iteration 15402, Loss: 0.055752478539943695\n",
      "Iteration 15403, Loss: 0.05584554001688957\n",
      "Iteration 15404, Loss: 0.05583417788147926\n",
      "Iteration 15405, Loss: 0.0557299479842186\n",
      "Iteration 15406, Loss: 0.0557069405913353\n",
      "Iteration 15407, Loss: 0.05576400086283684\n",
      "Iteration 15408, Loss: 0.05564781278371811\n",
      "Iteration 15409, Loss: 0.055784229189157486\n",
      "Iteration 15410, Loss: 0.05586886405944824\n",
      "Iteration 15411, Loss: 0.055850863456726074\n",
      "Iteration 15412, Loss: 0.05574067682027817\n",
      "Iteration 15413, Loss: 0.055698197335004807\n",
      "Iteration 15414, Loss: 0.05576121807098389\n",
      "Iteration 15415, Loss: 0.05564634129405022\n",
      "Iteration 15416, Loss: 0.05578843876719475\n",
      "Iteration 15417, Loss: 0.05587613955140114\n",
      "Iteration 15418, Loss: 0.05586063861846924\n",
      "Iteration 15419, Loss: 0.055752359330654144\n",
      "Iteration 15420, Loss: 0.05568051338195801\n",
      "Iteration 15421, Loss: 0.05574345588684082\n",
      "Iteration 15422, Loss: 0.055631123483181\n",
      "Iteration 15423, Loss: 0.05579455941915512\n",
      "Iteration 15424, Loss: 0.055877964943647385\n",
      "Iteration 15425, Loss: 0.055858056992292404\n",
      "Iteration 15426, Loss: 0.055745720863342285\n",
      "Iteration 15427, Loss: 0.05569382756948471\n",
      "Iteration 15428, Loss: 0.055758994072675705\n",
      "Iteration 15429, Loss: 0.05564574524760246\n",
      "Iteration 15430, Loss: 0.05578768253326416\n",
      "Iteration 15431, Loss: 0.05587494373321533\n",
      "Iteration 15432, Loss: 0.05585936829447746\n",
      "Iteration 15433, Loss: 0.055751048028469086\n",
      "Iteration 15434, Loss: 0.05568154901266098\n",
      "Iteration 15435, Loss: 0.05574222654104233\n",
      "Iteration 15436, Loss: 0.0556257963180542\n",
      "Iteration 15437, Loss: 0.05580063909292221\n",
      "Iteration 15438, Loss: 0.05588483810424805\n",
      "Iteration 15439, Loss: 0.05586564540863037\n",
      "Iteration 15440, Loss: 0.05575386807322502\n",
      "Iteration 15441, Loss: 0.05568389222025871\n",
      "Iteration 15442, Loss: 0.055753469467163086\n",
      "Iteration 15443, Loss: 0.05565107241272926\n",
      "Iteration 15444, Loss: 0.05577488988637924\n",
      "Iteration 15445, Loss: 0.05585356801748276\n",
      "Iteration 15446, Loss: 0.05583024024963379\n",
      "Iteration 15447, Loss: 0.05571528524160385\n",
      "Iteration 15448, Loss: 0.05573884770274162\n",
      "Iteration 15449, Loss: 0.05580679699778557\n",
      "Iteration 15450, Loss: 0.055694662034511566\n",
      "Iteration 15451, Loss: 0.05575203895568848\n",
      "Iteration 15452, Loss: 0.0558398962020874\n",
      "Iteration 15453, Loss: 0.055824439972639084\n",
      "Iteration 15454, Loss: 0.055716753005981445\n",
      "Iteration 15455, Loss: 0.05572740361094475\n",
      "Iteration 15456, Loss: 0.05578736588358879\n",
      "Iteration 15457, Loss: 0.05566827580332756\n",
      "Iteration 15458, Loss: 0.055775683373212814\n",
      "Iteration 15459, Loss: 0.055867474526166916\n",
      "Iteration 15460, Loss: 0.05585567280650139\n",
      "Iteration 15461, Loss: 0.05575112625956535\n",
      "Iteration 15462, Loss: 0.055677179247140884\n",
      "Iteration 15463, Loss: 0.05573427677154541\n",
      "Iteration 15464, Loss: 0.05561904236674309\n",
      "Iteration 15465, Loss: 0.055767618119716644\n",
      "Iteration 15466, Loss: 0.05580783262848854\n",
      "Iteration 15467, Loss: 0.055742304772138596\n",
      "Iteration 15468, Loss: 0.0556592158973217\n",
      "Iteration 15469, Loss: 0.05572474002838135\n",
      "Iteration 15470, Loss: 0.05565778538584709\n",
      "Iteration 15471, Loss: 0.055736981332302094\n",
      "Iteration 15472, Loss: 0.055783115327358246\n",
      "Iteration 15473, Loss: 0.05573165416717529\n",
      "Iteration 15474, Loss: 0.05564161390066147\n",
      "Iteration 15475, Loss: 0.05566521733999252\n",
      "Iteration 15476, Loss: 0.05567061901092529\n",
      "Iteration 15477, Loss: 0.055662594735622406\n",
      "Iteration 15478, Loss: 0.055684369057416916\n",
      "Iteration 15479, Loss: 0.055644672363996506\n",
      "Iteration 15480, Loss: 0.05573570728302002\n",
      "Iteration 15481, Loss: 0.055765990167856216\n",
      "Iteration 15482, Loss: 0.0556892566382885\n",
      "Iteration 15483, Loss: 0.055726490914821625\n",
      "Iteration 15484, Loss: 0.055757682770490646\n",
      "Iteration 15485, Loss: 0.05562460422515869\n",
      "Iteration 15486, Loss: 0.05582551285624504\n",
      "Iteration 15487, Loss: 0.055918656289577484\n",
      "Iteration 15488, Loss: 0.05589083954691887\n",
      "Iteration 15489, Loss: 0.05576201528310776\n",
      "Iteration 15490, Loss: 0.05570781230926514\n",
      "Iteration 15491, Loss: 0.05580536648631096\n",
      "Iteration 15492, Loss: 0.0557502917945385\n",
      "Iteration 15493, Loss: 0.055680595338344574\n",
      "Iteration 15494, Loss: 0.055730823427438736\n",
      "Iteration 15495, Loss: 0.05569490045309067\n",
      "Iteration 15496, Loss: 0.05568218231201172\n",
      "Iteration 15497, Loss: 0.055687230080366135\n",
      "Iteration 15498, Loss: 0.05567864701151848\n",
      "Iteration 15499, Loss: 0.05568631738424301\n",
      "Iteration 15500, Loss: 0.05565520375967026\n",
      "Iteration 15501, Loss: 0.05569867417216301\n",
      "Iteration 15502, Loss: 0.05565913766622543\n",
      "Iteration 15503, Loss: 0.055708885192871094\n",
      "Iteration 15504, Loss: 0.05572342872619629\n",
      "Iteration 15505, Loss: 0.05562993139028549\n",
      "Iteration 15506, Loss: 0.0558188371360302\n",
      "Iteration 15507, Loss: 0.055868230760097504\n",
      "Iteration 15508, Loss: 0.055758994072675705\n",
      "Iteration 15509, Loss: 0.0556994304060936\n",
      "Iteration 15510, Loss: 0.05578446388244629\n",
      "Iteration 15511, Loss: 0.0557582788169384\n",
      "Iteration 15512, Loss: 0.0556262731552124\n",
      "Iteration 15513, Loss: 0.05586230754852295\n",
      "Iteration 15514, Loss: 0.05594658851623535\n",
      "Iteration 15515, Loss: 0.05586624518036842\n",
      "Iteration 15516, Loss: 0.05566283315420151\n",
      "Iteration 15517, Loss: 0.055833183228969574\n",
      "Iteration 15518, Loss: 0.05595255270600319\n",
      "Iteration 15519, Loss: 0.05594329163432121\n",
      "Iteration 15520, Loss: 0.05582042783498764\n",
      "Iteration 15521, Loss: 0.0556594543159008\n",
      "Iteration 15522, Loss: 0.05582217499613762\n",
      "Iteration 15523, Loss: 0.055861830711364746\n",
      "Iteration 15524, Loss: 0.05573900789022446\n",
      "Iteration 15525, Loss: 0.05572887510061264\n",
      "Iteration 15526, Loss: 0.05582106485962868\n",
      "Iteration 15527, Loss: 0.05580473318696022\n",
      "Iteration 15528, Loss: 0.055698078125715256\n",
      "Iteration 15529, Loss: 0.05575239658355713\n",
      "Iteration 15530, Loss: 0.05581244081258774\n",
      "Iteration 15531, Loss: 0.05570264905691147\n",
      "Iteration 15532, Loss: 0.05574468895792961\n",
      "Iteration 15533, Loss: 0.0558290109038353\n",
      "Iteration 15534, Loss: 0.05580862611532211\n",
      "Iteration 15535, Loss: 0.05569816008210182\n",
      "Iteration 15536, Loss: 0.05575482174754143\n",
      "Iteration 15537, Loss: 0.05581844225525856\n",
      "Iteration 15538, Loss: 0.05570841208100319\n",
      "Iteration 15539, Loss: 0.05574015900492668\n",
      "Iteration 15540, Loss: 0.05582595244050026\n",
      "Iteration 15541, Loss: 0.05580846592783928\n",
      "Iteration 15542, Loss: 0.05569978803396225\n",
      "Iteration 15543, Loss: 0.05575064942240715\n",
      "Iteration 15544, Loss: 0.055812399834394455\n",
      "Iteration 15545, Loss: 0.05569792166352272\n",
      "Iteration 15546, Loss: 0.05575041100382805\n",
      "Iteration 15547, Loss: 0.05583922192454338\n",
      "Iteration 15548, Loss: 0.05582491680979729\n",
      "Iteration 15549, Loss: 0.05571838468313217\n",
      "Iteration 15550, Loss: 0.0557229146361351\n",
      "Iteration 15551, Loss: 0.05578283593058586\n",
      "Iteration 15552, Loss: 0.05566596984863281\n",
      "Iteration 15553, Loss: 0.05577508732676506\n",
      "Iteration 15554, Loss: 0.055865328758955\n",
      "Iteration 15555, Loss: 0.05585237592458725\n",
      "Iteration 15556, Loss: 0.055746953934431076\n",
      "Iteration 15557, Loss: 0.05568265914916992\n",
      "Iteration 15558, Loss: 0.05574194714426994\n",
      "Iteration 15559, Loss: 0.05562647432088852\n",
      "Iteration 15560, Loss: 0.055794280022382736\n",
      "Iteration 15561, Loss: 0.055874548852443695\n",
      "Iteration 15562, Loss: 0.0558498315513134\n",
      "Iteration 15563, Loss: 0.0557326078414917\n",
      "Iteration 15564, Loss: 0.05571671575307846\n",
      "Iteration 15565, Loss: 0.055787764489650726\n",
      "Iteration 15566, Loss: 0.055681150406599045\n",
      "Iteration 15567, Loss: 0.05575748533010483\n",
      "Iteration 15568, Loss: 0.055841051042079926\n",
      "Iteration 15569, Loss: 0.055821698158979416\n",
      "Iteration 15570, Loss: 0.055710792541503906\n",
      "Iteration 15571, Loss: 0.05573868751525879\n",
      "Iteration 15572, Loss: 0.0558018684387207\n",
      "Iteration 15573, Loss: 0.055687110871076584\n",
      "Iteration 15574, Loss: 0.055757761001586914\n",
      "Iteration 15575, Loss: 0.05584617704153061\n",
      "Iteration 15576, Loss: 0.055831752717494965\n",
      "Iteration 15577, Loss: 0.05572513863444328\n",
      "Iteration 15578, Loss: 0.05571389198303223\n",
      "Iteration 15579, Loss: 0.055772822350263596\n",
      "Iteration 15580, Loss: 0.05565532296895981\n",
      "Iteration 15581, Loss: 0.055780768394470215\n",
      "Iteration 15582, Loss: 0.05586906522512436\n",
      "Iteration 15583, Loss: 0.055854205042123795\n",
      "Iteration 15584, Loss: 0.055746834725141525\n",
      "Iteration 15585, Loss: 0.055684927850961685\n",
      "Iteration 15586, Loss: 0.055745046585798264\n",
      "Iteration 15587, Loss: 0.055627547204494476\n",
      "Iteration 15588, Loss: 0.05580330267548561\n",
      "Iteration 15589, Loss: 0.05589306727051735\n",
      "Iteration 15590, Loss: 0.055879078805446625\n",
      "Iteration 15591, Loss: 0.05577242374420166\n",
      "Iteration 15592, Loss: 0.05565166845917702\n",
      "Iteration 15593, Loss: 0.05572398751974106\n",
      "Iteration 15594, Loss: 0.055635254830121994\n",
      "Iteration 15595, Loss: 0.05575581640005112\n",
      "Iteration 15596, Loss: 0.05580175295472145\n",
      "Iteration 15597, Loss: 0.05574202537536621\n",
      "Iteration 15598, Loss: 0.055643998086452484\n",
      "Iteration 15599, Loss: 0.055703163146972656\n",
      "Iteration 15600, Loss: 0.05563954636454582\n",
      "Iteration 15601, Loss: 0.0557071790099144\n",
      "Iteration 15602, Loss: 0.05570431798696518\n",
      "Iteration 15603, Loss: 0.0556306466460228\n",
      "Iteration 15604, Loss: 0.055640578269958496\n",
      "Iteration 15605, Loss: 0.055687110871076584\n",
      "Iteration 15606, Loss: 0.05567721650004387\n",
      "Iteration 15607, Loss: 0.055664144456386566\n",
      "Iteration 15608, Loss: 0.05562559887766838\n",
      "Iteration 15609, Loss: 0.05575362965464592\n",
      "Iteration 15610, Loss: 0.05579380318522453\n",
      "Iteration 15611, Loss: 0.05573252961039543\n",
      "Iteration 15612, Loss: 0.05565623566508293\n",
      "Iteration 15613, Loss: 0.05569875240325928\n",
      "Iteration 15614, Loss: 0.05564900487661362\n",
      "Iteration 15615, Loss: 0.055676184594631195\n",
      "Iteration 15616, Loss: 0.05564749613404274\n",
      "Iteration 15617, Loss: 0.055725377053022385\n",
      "Iteration 15618, Loss: 0.05571655556559563\n",
      "Iteration 15619, Loss: 0.05565723031759262\n",
      "Iteration 15620, Loss: 0.05568917840719223\n",
      "Iteration 15621, Loss: 0.05563453957438469\n",
      "Iteration 15622, Loss: 0.05571635812520981\n",
      "Iteration 15623, Loss: 0.055724941194057465\n",
      "Iteration 15624, Loss: 0.05564634129405022\n",
      "Iteration 15625, Loss: 0.055775564163923264\n",
      "Iteration 15626, Loss: 0.05578947067260742\n",
      "Iteration 15627, Loss: 0.05563187971711159\n",
      "Iteration 15628, Loss: 0.05581676959991455\n",
      "Iteration 15629, Loss: 0.055922627449035645\n",
      "Iteration 15630, Loss: 0.05592366307973862\n",
      "Iteration 15631, Loss: 0.055829886347055435\n",
      "Iteration 15632, Loss: 0.055651865899562836\n",
      "Iteration 15633, Loss: 0.05589783191680908\n",
      "Iteration 15634, Loss: 0.05603504553437233\n",
      "Iteration 15635, Loss: 0.0559844970703125\n",
      "Iteration 15636, Loss: 0.05576654523611069\n",
      "Iteration 15637, Loss: 0.055766504257917404\n",
      "Iteration 15638, Loss: 0.055917344987392426\n",
      "Iteration 15639, Loss: 0.05595894902944565\n",
      "Iteration 15640, Loss: 0.055901527404785156\n",
      "Iteration 15641, Loss: 0.055755775421857834\n",
      "Iteration 15642, Loss: 0.0557173527777195\n",
      "Iteration 15643, Loss: 0.05581927299499512\n",
      "Iteration 15644, Loss: 0.055739086121320724\n",
      "Iteration 15645, Loss: 0.05569537729024887\n",
      "Iteration 15646, Loss: 0.05576400086283684\n",
      "Iteration 15647, Loss: 0.05573221296072006\n",
      "Iteration 15648, Loss: 0.05561177060008049\n",
      "Iteration 15649, Loss: 0.05571004003286362\n",
      "Iteration 15650, Loss: 0.05566903203725815\n",
      "Iteration 15651, Loss: 0.05571115016937256\n",
      "Iteration 15652, Loss: 0.05572688952088356\n",
      "Iteration 15653, Loss: 0.05562456697225571\n",
      "Iteration 15654, Loss: 0.055775802582502365\n",
      "Iteration 15655, Loss: 0.05578112602233887\n",
      "Iteration 15656, Loss: 0.0556388720870018\n",
      "Iteration 15657, Loss: 0.05580636113882065\n",
      "Iteration 15658, Loss: 0.0558883361518383\n",
      "Iteration 15659, Loss: 0.05584574118256569\n",
      "Iteration 15660, Loss: 0.05570344254374504\n",
      "Iteration 15661, Loss: 0.055775921791791916\n",
      "Iteration 15662, Loss: 0.055871330201625824\n",
      "Iteration 15663, Loss: 0.055808983743190765\n",
      "Iteration 15664, Loss: 0.05561916157603264\n",
      "Iteration 15665, Loss: 0.055742744356393814\n",
      "Iteration 15666, Loss: 0.05572434514760971\n",
      "Iteration 15667, Loss: 0.055650196969509125\n",
      "Iteration 15668, Loss: 0.05568397045135498\n",
      "Iteration 15669, Loss: 0.055635932832956314\n",
      "Iteration 15670, Loss: 0.05566823482513428\n",
      "Iteration 15671, Loss: 0.05564038082957268\n",
      "Iteration 15672, Loss: 0.05572986602783203\n",
      "Iteration 15673, Loss: 0.055703602731227875\n",
      "Iteration 15674, Loss: 0.05568448826670647\n",
      "Iteration 15675, Loss: 0.05571870133280754\n",
      "Iteration 15676, Loss: 0.055652499198913574\n",
      "Iteration 15677, Loss: 0.05576082319021225\n",
      "Iteration 15678, Loss: 0.055777668952941895\n",
      "Iteration 15679, Loss: 0.05563068762421608\n",
      "Iteration 15680, Loss: 0.05580767244100571\n",
      "Iteration 15681, Loss: 0.055892471224069595\n",
      "Iteration 15682, Loss: 0.05585920810699463\n",
      "Iteration 15683, Loss: 0.05572676658630371\n",
      "Iteration 15684, Loss: 0.055742423981428146\n",
      "Iteration 15685, Loss: 0.055831555277109146\n",
      "Iteration 15686, Loss: 0.055753111839294434\n",
      "Iteration 15687, Loss: 0.05568353459239006\n",
      "Iteration 15688, Loss: 0.055746953934431076\n",
      "Iteration 15689, Loss: 0.0557047538459301\n",
      "Iteration 15690, Loss: 0.055670421570539474\n",
      "Iteration 15691, Loss: 0.055684011429548264\n",
      "Iteration 15692, Loss: 0.05567149445414543\n",
      "Iteration 15693, Loss: 0.05567535012960434\n",
      "Iteration 15694, Loss: 0.055654048919677734\n",
      "Iteration 15695, Loss: 0.05566736310720444\n",
      "Iteration 15696, Loss: 0.055656276643276215\n",
      "Iteration 15697, Loss: 0.0556538924574852\n",
      "Iteration 15698, Loss: 0.05566934868693352\n",
      "Iteration 15699, Loss: 0.055641334503889084\n",
      "Iteration 15700, Loss: 0.055670302361249924\n",
      "Iteration 15701, Loss: 0.05562412738800049\n",
      "Iteration 15702, Loss: 0.055761657655239105\n",
      "Iteration 15703, Loss: 0.0557861365377903\n",
      "Iteration 15704, Loss: 0.05569478124380112\n",
      "Iteration 15705, Loss: 0.05573209375143051\n",
      "Iteration 15706, Loss: 0.055783312767744064\n",
      "Iteration 15707, Loss: 0.05568508431315422\n",
      "Iteration 15708, Loss: 0.05575215816497803\n",
      "Iteration 15709, Loss: 0.05582309141755104\n",
      "Iteration 15710, Loss: 0.05576968565583229\n",
      "Iteration 15711, Loss: 0.05564960092306137\n",
      "Iteration 15712, Loss: 0.05578180402517319\n",
      "Iteration 15713, Loss: 0.05579444020986557\n",
      "Iteration 15714, Loss: 0.055642448365688324\n",
      "Iteration 15715, Loss: 0.05581784248352051\n",
      "Iteration 15716, Loss: 0.05592723935842514\n",
      "Iteration 15717, Loss: 0.05592489242553711\n",
      "Iteration 15718, Loss: 0.05582404509186745\n",
      "Iteration 15719, Loss: 0.05565889924764633\n",
      "Iteration 15720, Loss: 0.0558648519217968\n",
      "Iteration 15721, Loss: 0.05596423149108887\n",
      "Iteration 15722, Loss: 0.0558805875480175\n",
      "Iteration 15723, Loss: 0.05564403533935547\n",
      "Iteration 15724, Loss: 0.05584971234202385\n",
      "Iteration 15725, Loss: 0.05599093809723854\n",
      "Iteration 15726, Loss: 0.056023161858320236\n",
      "Iteration 15727, Loss: 0.05595708265900612\n",
      "Iteration 15728, Loss: 0.05580294504761696\n",
      "Iteration 15729, Loss: 0.05566410347819328\n",
      "Iteration 15730, Loss: 0.05577560514211655\n",
      "Iteration 15731, Loss: 0.05570411682128906\n",
      "Iteration 15732, Loss: 0.05571512505412102\n",
      "Iteration 15733, Loss: 0.05577806755900383\n",
      "Iteration 15734, Loss: 0.05574091523885727\n",
      "Iteration 15735, Loss: 0.05561594292521477\n",
      "Iteration 15736, Loss: 0.05585614964365959\n",
      "Iteration 15737, Loss: 0.05591019243001938\n",
      "Iteration 15738, Loss: 0.05578502267599106\n",
      "Iteration 15739, Loss: 0.055691879242658615\n",
      "Iteration 15740, Loss: 0.055788081139326096\n",
      "Iteration 15741, Loss: 0.05577953904867172\n",
      "Iteration 15742, Loss: 0.05567610263824463\n",
      "Iteration 15743, Loss: 0.0557735376060009\n",
      "Iteration 15744, Loss: 0.05582849308848381\n",
      "Iteration 15745, Loss: 0.0557052306830883\n",
      "Iteration 15746, Loss: 0.055749259889125824\n",
      "Iteration 15747, Loss: 0.05584323778748512\n",
      "Iteration 15748, Loss: 0.05583135411143303\n",
      "Iteration 15749, Loss: 0.05572509765625\n",
      "Iteration 15750, Loss: 0.05571186542510986\n",
      "Iteration 15751, Loss: 0.0557713508605957\n",
      "Iteration 15752, Loss: 0.05565440654754639\n",
      "Iteration 15753, Loss: 0.055781763046979904\n",
      "Iteration 15754, Loss: 0.055870890617370605\n",
      "Iteration 15755, Loss: 0.05585598945617676\n",
      "Iteration 15756, Loss: 0.05574854463338852\n",
      "Iteration 15757, Loss: 0.05568289756774902\n",
      "Iteration 15758, Loss: 0.0557459220290184\n",
      "Iteration 15759, Loss: 0.05564193055033684\n",
      "Iteration 15760, Loss: 0.055769167840480804\n",
      "Iteration 15761, Loss: 0.055837154388427734\n",
      "Iteration 15762, Loss: 0.05580298230051994\n",
      "Iteration 15763, Loss: 0.05567757412791252\n",
      "Iteration 15764, Loss: 0.05579749867320061\n",
      "Iteration 15765, Loss: 0.055877093225717545\n",
      "Iteration 15766, Loss: 0.055778585374355316\n",
      "Iteration 15767, Loss: 0.05567781254649162\n",
      "Iteration 15768, Loss: 0.055757444351911545\n",
      "Iteration 15769, Loss: 0.055734675377607346\n",
      "Iteration 15770, Loss: 0.05561935901641846\n",
      "Iteration 15771, Loss: 0.05586151406168938\n",
      "Iteration 15772, Loss: 0.05592668056488037\n",
      "Iteration 15773, Loss: 0.055813711136579514\n",
      "Iteration 15774, Loss: 0.05566295236349106\n",
      "Iteration 15775, Loss: 0.055753231048583984\n",
      "Iteration 15776, Loss: 0.05574246495962143\n",
      "Iteration 15777, Loss: 0.05563787743449211\n",
      "Iteration 15778, Loss: 0.05582619085907936\n",
      "Iteration 15779, Loss: 0.055884283035993576\n",
      "Iteration 15780, Loss: 0.05576980113983154\n",
      "Iteration 15781, Loss: 0.05569501966238022\n",
      "Iteration 15782, Loss: 0.05578402802348137\n",
      "Iteration 15783, Loss: 0.05576988309621811\n",
      "Iteration 15784, Loss: 0.05565973371267319\n",
      "Iteration 15785, Loss: 0.05580254644155502\n",
      "Iteration 15786, Loss: 0.0558672770857811\n",
      "Iteration 15787, Loss: 0.05576082319021225\n",
      "Iteration 15788, Loss: 0.05569533631205559\n",
      "Iteration 15789, Loss: 0.0557791031897068\n",
      "Iteration 15790, Loss: 0.0557582788169384\n",
      "Iteration 15791, Loss: 0.05564066022634506\n",
      "Iteration 15792, Loss: 0.055834852159023285\n",
      "Iteration 15793, Loss: 0.055907171219587326\n",
      "Iteration 15794, Loss: 0.05580862611532211\n",
      "Iteration 15795, Loss: 0.05565810576081276\n",
      "Iteration 15796, Loss: 0.05575041100382805\n",
      "Iteration 15797, Loss: 0.05574369430541992\n",
      "Iteration 15798, Loss: 0.055631160736083984\n",
      "Iteration 15799, Loss: 0.05583536624908447\n",
      "Iteration 15800, Loss: 0.0559062585234642\n",
      "Iteration 15801, Loss: 0.0558193139731884\n",
      "Iteration 15802, Loss: 0.055654726922512054\n",
      "Iteration 15803, Loss: 0.05578378960490227\n",
      "Iteration 15804, Loss: 0.05582305043935776\n",
      "Iteration 15805, Loss: 0.05573161691427231\n",
      "Iteration 15806, Loss: 0.05569855496287346\n",
      "Iteration 15807, Loss: 0.055759113281965256\n",
      "Iteration 15808, Loss: 0.0556817464530468\n",
      "Iteration 15809, Loss: 0.055733323097229004\n",
      "Iteration 15810, Loss: 0.05578073114156723\n",
      "Iteration 15811, Loss: 0.05569573491811752\n",
      "Iteration 15812, Loss: 0.05572565644979477\n",
      "Iteration 15813, Loss: 0.055778346955776215\n",
      "Iteration 15814, Loss: 0.0556926354765892\n",
      "Iteration 15815, Loss: 0.05573141574859619\n",
      "Iteration 15816, Loss: 0.05578688904643059\n",
      "Iteration 15817, Loss: 0.055710237473249435\n",
      "Iteration 15818, Loss: 0.05570431798696518\n",
      "Iteration 15819, Loss: 0.05575239658355713\n",
      "Iteration 15820, Loss: 0.05566307157278061\n",
      "Iteration 15821, Loss: 0.055765870958566666\n",
      "Iteration 15822, Loss: 0.05582507699728012\n",
      "Iteration 15823, Loss: 0.055753592401742935\n",
      "Iteration 15824, Loss: 0.05566819757223129\n",
      "Iteration 15825, Loss: 0.05574210733175278\n",
      "Iteration 15826, Loss: 0.055697083473205566\n",
      "Iteration 15827, Loss: 0.05570534989237785\n",
      "Iteration 15828, Loss: 0.055739883333444595\n",
      "Iteration 15829, Loss: 0.055677853524684906\n",
      "Iteration 15830, Loss: 0.055719297379255295\n",
      "Iteration 15831, Loss: 0.0557326078414917\n",
      "Iteration 15832, Loss: 0.0556337870657444\n",
      "Iteration 15833, Loss: 0.05567316338419914\n",
      "Iteration 15834, Loss: 0.0556412972509861\n",
      "Iteration 15835, Loss: 0.05573217198252678\n",
      "Iteration 15836, Loss: 0.05570833012461662\n",
      "Iteration 15837, Loss: 0.055679600685834885\n",
      "Iteration 15838, Loss: 0.05571258068084717\n",
      "Iteration 15839, Loss: 0.05564546957612038\n",
      "Iteration 15840, Loss: 0.0557715930044651\n",
      "Iteration 15841, Loss: 0.05579026788473129\n",
      "Iteration 15842, Loss: 0.055645983666181564\n",
      "Iteration 15843, Loss: 0.05580254644155502\n",
      "Iteration 15844, Loss: 0.05589616298675537\n",
      "Iteration 15845, Loss: 0.05587172508239746\n",
      "Iteration 15846, Loss: 0.05574576184153557\n",
      "Iteration 15847, Loss: 0.05571294203400612\n",
      "Iteration 15848, Loss: 0.05579988285899162\n",
      "Iteration 15849, Loss: 0.05572275444865227\n",
      "Iteration 15850, Loss: 0.05570872873067856\n",
      "Iteration 15851, Loss: 0.05577163025736809\n",
      "Iteration 15852, Loss: 0.05573054403066635\n",
      "Iteration 15853, Loss: 0.05565150827169418\n",
      "Iteration 15854, Loss: 0.055731337517499924\n",
      "Iteration 15855, Loss: 0.05568699166178703\n",
      "Iteration 15856, Loss: 0.05570749565958977\n",
      "Iteration 15857, Loss: 0.05574822425842285\n",
      "Iteration 15858, Loss: 0.05569490045309067\n",
      "Iteration 15859, Loss: 0.05569152161478996\n",
      "Iteration 15860, Loss: 0.055694978684186935\n",
      "Iteration 15861, Loss: 0.05567137524485588\n",
      "Iteration 15862, Loss: 0.0556892566382885\n",
      "Iteration 15863, Loss: 0.055624961853027344\n",
      "Iteration 15864, Loss: 0.055751919746398926\n",
      "Iteration 15865, Loss: 0.05574818700551987\n",
      "Iteration 15866, Loss: 0.05564546585083008\n",
      "Iteration 15867, Loss: 0.055747829377651215\n",
      "Iteration 15868, Loss: 0.05575088784098625\n",
      "Iteration 15869, Loss: 0.05561622232198715\n",
      "Iteration 15870, Loss: 0.05578700825572014\n",
      "Iteration 15871, Loss: 0.05580341815948486\n",
      "Iteration 15872, Loss: 0.055679284036159515\n",
      "Iteration 15873, Loss: 0.05577008053660393\n",
      "Iteration 15874, Loss: 0.05585261434316635\n",
      "Iteration 15875, Loss: 0.055806756019592285\n",
      "Iteration 15876, Loss: 0.055658578872680664\n",
      "Iteration 15877, Loss: 0.05582249537110329\n",
      "Iteration 15878, Loss: 0.055911701172590256\n",
      "Iteration 15879, Loss: 0.055841170251369476\n",
      "Iteration 15880, Loss: 0.05564252659678459\n",
      "Iteration 15881, Loss: 0.05584510415792465\n",
      "Iteration 15882, Loss: 0.055957913398742676\n",
      "Iteration 15883, Loss: 0.05593351647257805\n",
      "Iteration 15884, Loss: 0.05579336732625961\n",
      "Iteration 15885, Loss: 0.05569279193878174\n",
      "Iteration 15886, Loss: 0.05581212416291237\n",
      "Iteration 15887, Loss: 0.05580496788024902\n",
      "Iteration 15888, Loss: 0.05565723031759262\n",
      "Iteration 15889, Loss: 0.05580993741750717\n",
      "Iteration 15890, Loss: 0.05591265484690666\n",
      "Iteration 15891, Loss: 0.05590137094259262\n",
      "Iteration 15892, Loss: 0.05579396337270737\n",
      "Iteration 15893, Loss: 0.05567535012960434\n",
      "Iteration 15894, Loss: 0.05580655857920647\n",
      "Iteration 15895, Loss: 0.05582535266876221\n",
      "Iteration 15896, Loss: 0.05568854138255119\n",
      "Iteration 15897, Loss: 0.05577079579234123\n",
      "Iteration 15898, Loss: 0.055866919457912445\n",
      "Iteration 15899, Loss: 0.05586131662130356\n",
      "Iteration 15900, Loss: 0.055763959884643555\n",
      "Iteration 15901, Loss: 0.05565742775797844\n",
      "Iteration 15902, Loss: 0.05573956295847893\n",
      "Iteration 15903, Loss: 0.055676817893981934\n",
      "Iteration 15904, Loss: 0.05572223663330078\n",
      "Iteration 15905, Loss: 0.05576777458190918\n",
      "Iteration 15906, Loss: 0.055709682404994965\n",
      "Iteration 15907, Loss: 0.05567833036184311\n",
      "Iteration 15908, Loss: 0.05568917840719223\n",
      "Iteration 15909, Loss: 0.055668674409389496\n",
      "Iteration 15910, Loss: 0.055678486824035645\n",
      "Iteration 15911, Loss: 0.05563906952738762\n",
      "Iteration 15912, Loss: 0.05563410371541977\n",
      "Iteration 15913, Loss: 0.05565313622355461\n",
      "Iteration 15914, Loss: 0.05565381050109863\n",
      "Iteration 15915, Loss: 0.05562536045908928\n",
      "Iteration 15916, Loss: 0.05575060844421387\n",
      "Iteration 15917, Loss: 0.05572402477264404\n",
      "Iteration 15918, Loss: 0.05567077919840813\n",
      "Iteration 15919, Loss: 0.05570698156952858\n",
      "Iteration 15920, Loss: 0.05564375966787338\n",
      "Iteration 15921, Loss: 0.05576884746551514\n",
      "Iteration 15922, Loss: 0.055780887603759766\n",
      "Iteration 15923, Loss: 0.05562639608979225\n",
      "Iteration 15924, Loss: 0.055814504623413086\n",
      "Iteration 15925, Loss: 0.05590478703379631\n",
      "Iteration 15926, Loss: 0.05587879940867424\n",
      "Iteration 15927, Loss: 0.05575339123606682\n",
      "Iteration 15928, Loss: 0.055705390870571136\n",
      "Iteration 15929, Loss: 0.05579483509063721\n",
      "Iteration 15930, Loss: 0.05572342872619629\n",
      "Iteration 15931, Loss: 0.05570630356669426\n",
      "Iteration 15932, Loss: 0.0557660274207592\n",
      "Iteration 15933, Loss: 0.055726807564496994\n",
      "Iteration 15934, Loss: 0.05565488710999489\n",
      "Iteration 15935, Loss: 0.055728357285261154\n",
      "Iteration 15936, Loss: 0.05568572133779526\n",
      "Iteration 15937, Loss: 0.0557052306830883\n",
      "Iteration 15938, Loss: 0.05574202537536621\n",
      "Iteration 15939, Loss: 0.05568460747599602\n",
      "Iteration 15940, Loss: 0.05571138858795166\n",
      "Iteration 15941, Loss: 0.05572017282247543\n",
      "Iteration 15942, Loss: 0.05565115064382553\n",
      "Iteration 15943, Loss: 0.055677615106105804\n",
      "Iteration 15944, Loss: 0.055630605667829514\n",
      "Iteration 15945, Loss: 0.05572311207652092\n",
      "Iteration 15946, Loss: 0.0557204894721508\n",
      "Iteration 15947, Loss: 0.055644791573286057\n",
      "Iteration 15948, Loss: 0.05573264881968498\n",
      "Iteration 15949, Loss: 0.055699270218610764\n",
      "Iteration 15950, Loss: 0.05569549649953842\n",
      "Iteration 15951, Loss: 0.05573276802897453\n",
      "Iteration 15952, Loss: 0.0556744746863842\n",
      "Iteration 15953, Loss: 0.05572080612182617\n",
      "Iteration 15954, Loss: 0.0557255782186985\n",
      "Iteration 15955, Loss: 0.055650316178798676\n",
      "Iteration 15956, Loss: 0.05566974729299545\n",
      "Iteration 15957, Loss: 0.05563728138804436\n",
      "Iteration 15958, Loss: 0.055680517107248306\n",
      "Iteration 15959, Loss: 0.05564284324645996\n",
      "Iteration 15960, Loss: 0.05571440979838371\n",
      "Iteration 15961, Loss: 0.055693864822387695\n",
      "Iteration 15962, Loss: 0.05568067356944084\n",
      "Iteration 15963, Loss: 0.055696845054626465\n",
      "Iteration 15964, Loss: 0.05563036724925041\n",
      "Iteration 15965, Loss: 0.05568087100982666\n",
      "Iteration 15966, Loss: 0.05563632771372795\n",
      "Iteration 15967, Loss: 0.05565563961863518\n",
      "Iteration 15968, Loss: 0.055640142410993576\n",
      "Iteration 15969, Loss: 0.055656079202890396\n",
      "Iteration 15970, Loss: 0.05561908334493637\n",
      "Iteration 15971, Loss: 0.05576865002512932\n",
      "Iteration 15972, Loss: 0.05576133728027344\n",
      "Iteration 15973, Loss: 0.05563589185476303\n",
      "Iteration 15974, Loss: 0.055734794586896896\n",
      "Iteration 15975, Loss: 0.055729907006025314\n",
      "Iteration 15976, Loss: 0.05562595650553703\n",
      "Iteration 15977, Loss: 0.05569998547434807\n",
      "Iteration 15978, Loss: 0.05562106892466545\n",
      "Iteration 15979, Loss: 0.05577743053436279\n",
      "Iteration 15980, Loss: 0.05584101006388664\n",
      "Iteration 15981, Loss: 0.05580401420593262\n",
      "Iteration 15982, Loss: 0.0556771382689476\n",
      "Iteration 15983, Loss: 0.055800918489694595\n",
      "Iteration 15984, Loss: 0.05588154122233391\n",
      "Iteration 15985, Loss: 0.05578116700053215\n",
      "Iteration 15986, Loss: 0.055677931755781174\n",
      "Iteration 15987, Loss: 0.055758558213710785\n",
      "Iteration 15988, Loss: 0.055737655609846115\n",
      "Iteration 15989, Loss: 0.05562559887766838\n",
      "Iteration 15990, Loss: 0.055853407829999924\n",
      "Iteration 15991, Loss: 0.055917542427778244\n",
      "Iteration 15992, Loss: 0.055801987648010254\n",
      "Iteration 15993, Loss: 0.05567292496562004\n",
      "Iteration 15994, Loss: 0.055763088166713715\n",
      "Iteration 15995, Loss: 0.05575088784098625\n",
      "Iteration 15996, Loss: 0.055646780878305435\n",
      "Iteration 15997, Loss: 0.055814746767282486\n",
      "Iteration 15998, Loss: 0.05587097257375717\n",
      "Iteration 15999, Loss: 0.05574965476989746\n",
      "Iteration 16000, Loss: 0.0557146891951561\n",
      "Iteration 16001, Loss: 0.05580759048461914\n",
      "Iteration 16002, Loss: 0.055797457695007324\n",
      "Iteration 16003, Loss: 0.05569438263773918\n",
      "Iteration 16004, Loss: 0.05574890226125717\n",
      "Iteration 16005, Loss: 0.05580441281199455\n",
      "Iteration 16006, Loss: 0.055683497339487076\n",
      "Iteration 16007, Loss: 0.0557628870010376\n",
      "Iteration 16008, Loss: 0.05585483834147453\n",
      "Iteration 16009, Loss: 0.055843155831098557\n",
      "Iteration 16010, Loss: 0.0557381734251976\n",
      "Iteration 16011, Loss: 0.055692195892333984\n",
      "Iteration 16012, Loss: 0.05575108900666237\n",
      "Iteration 16013, Loss: 0.055633388459682465\n",
      "Iteration 16014, Loss: 0.05579785630106926\n",
      "Iteration 16015, Loss: 0.055888377130031586\n",
      "Iteration 16016, Loss: 0.05587538331747055\n",
      "Iteration 16017, Loss: 0.05576976388692856\n",
      "Iteration 16018, Loss: 0.05565067380666733\n",
      "Iteration 16019, Loss: 0.05571460723876953\n",
      "Iteration 16020, Loss: 0.05562508478760719\n",
      "Iteration 16021, Loss: 0.05570944398641586\n",
      "Iteration 16022, Loss: 0.055692993104457855\n",
      "Iteration 16023, Loss: 0.05566108226776123\n",
      "Iteration 16024, Loss: 0.05565134808421135\n",
      "Iteration 16025, Loss: 0.05571119114756584\n",
      "Iteration 16026, Loss: 0.055721085518598557\n",
      "Iteration 16027, Loss: 0.05563926696777344\n",
      "Iteration 16028, Loss: 0.055763959884643555\n",
      "Iteration 16029, Loss: 0.05576181784272194\n",
      "Iteration 16030, Loss: 0.05562500283122063\n",
      "Iteration 16031, Loss: 0.05564574524760246\n",
      "Iteration 16032, Loss: 0.0556635856628418\n",
      "Iteration 16033, Loss: 0.05563044920563698\n",
      "Iteration 16034, Loss: 0.05563982576131821\n",
      "Iteration 16035, Loss: 0.05567634105682373\n",
      "Iteration 16036, Loss: 0.055657267570495605\n",
      "Iteration 16037, Loss: 0.05569899082183838\n",
      "Iteration 16038, Loss: 0.055667441338300705\n",
      "Iteration 16039, Loss: 0.05571639537811279\n",
      "Iteration 16040, Loss: 0.05575096607208252\n",
      "Iteration 16041, Loss: 0.05568333715200424\n",
      "Iteration 16042, Loss: 0.05572180077433586\n",
      "Iteration 16043, Loss: 0.05574071407318115\n",
      "Iteration 16044, Loss: 0.05562623590230942\n",
      "Iteration 16045, Loss: 0.05563267320394516\n",
      "Iteration 16046, Loss: 0.05569855496287346\n",
      "Iteration 16047, Loss: 0.0556333065032959\n",
      "Iteration 16048, Loss: 0.05576503276824951\n",
      "Iteration 16049, Loss: 0.055822573602199554\n",
      "Iteration 16050, Loss: 0.055776916444301605\n",
      "Iteration 16051, Loss: 0.055646102875471115\n",
      "Iteration 16052, Loss: 0.055843472480773926\n",
      "Iteration 16053, Loss: 0.05591984838247299\n",
      "Iteration 16054, Loss: 0.05581442639231682\n",
      "Iteration 16055, Loss: 0.05565870180726051\n",
      "Iteration 16056, Loss: 0.0557430200278759\n",
      "Iteration 16057, Loss: 0.05572652816772461\n",
      "Iteration 16058, Loss: 0.05562412738800049\n",
      "Iteration 16059, Loss: 0.05582074448466301\n",
      "Iteration 16060, Loss: 0.05585479736328125\n",
      "Iteration 16061, Loss: 0.055721282958984375\n",
      "Iteration 16062, Loss: 0.055743973702192307\n",
      "Iteration 16063, Loss: 0.05584251880645752\n",
      "Iteration 16064, Loss: 0.055834416300058365\n",
      "Iteration 16065, Loss: 0.05572986602783203\n",
      "Iteration 16066, Loss: 0.05570511147379875\n",
      "Iteration 16067, Loss: 0.05576511472463608\n",
      "Iteration 16068, Loss: 0.05565357580780983\n",
      "Iteration 16069, Loss: 0.055781763046979904\n",
      "Iteration 16070, Loss: 0.05586731433868408\n",
      "Iteration 16071, Loss: 0.055846571922302246\n",
      "Iteration 16072, Loss: 0.05573272705078125\n",
      "Iteration 16073, Loss: 0.055714093148708344\n",
      "Iteration 16074, Loss: 0.05578573793172836\n",
      "Iteration 16075, Loss: 0.05568782612681389\n",
      "Iteration 16076, Loss: 0.05574754998087883\n",
      "Iteration 16077, Loss: 0.055825233459472656\n",
      "Iteration 16078, Loss: 0.05580063909292221\n",
      "Iteration 16079, Loss: 0.05568718910217285\n",
      "Iteration 16080, Loss: 0.055773019790649414\n",
      "Iteration 16081, Loss: 0.05583902448415756\n",
      "Iteration 16082, Loss: 0.055727921426296234\n",
      "Iteration 16083, Loss: 0.05572553724050522\n",
      "Iteration 16084, Loss: 0.05581232160329819\n",
      "Iteration 16085, Loss: 0.05579666420817375\n",
      "Iteration 16086, Loss: 0.0556892566382885\n",
      "Iteration 16087, Loss: 0.05576249212026596\n",
      "Iteration 16088, Loss: 0.05582209676504135\n",
      "Iteration 16089, Loss: 0.05570411682128906\n",
      "Iteration 16090, Loss: 0.05574731156229973\n",
      "Iteration 16091, Loss: 0.05583854764699936\n",
      "Iteration 16092, Loss: 0.05582670494914055\n",
      "Iteration 16093, Loss: 0.05572203919291496\n",
      "Iteration 16094, Loss: 0.05571460723876953\n",
      "Iteration 16095, Loss: 0.05577167123556137\n",
      "Iteration 16096, Loss: 0.05565134808421135\n",
      "Iteration 16097, Loss: 0.055788278579711914\n",
      "Iteration 16098, Loss: 0.05588078871369362\n",
      "Iteration 16099, Loss: 0.05586997792124748\n",
      "Iteration 16100, Loss: 0.055765870958566666\n",
      "Iteration 16101, Loss: 0.05565556138753891\n",
      "Iteration 16102, Loss: 0.05571357533335686\n",
      "Iteration 16103, Loss: 0.055626075714826584\n",
      "Iteration 16104, Loss: 0.05565393343567848\n",
      "Iteration 16105, Loss: 0.05564995855093002\n",
      "Iteration 16106, Loss: 0.055635809898376465\n",
      "Iteration 16107, Loss: 0.055652737617492676\n",
      "Iteration 16108, Loss: 0.05565778538584709\n",
      "Iteration 16109, Loss: 0.05563155934214592\n",
      "Iteration 16110, Loss: 0.05574357509613037\n",
      "Iteration 16111, Loss: 0.0557173490524292\n",
      "Iteration 16112, Loss: 0.055676184594631195\n",
      "Iteration 16113, Loss: 0.05571107193827629\n",
      "Iteration 16114, Loss: 0.05564526841044426\n",
      "Iteration 16115, Loss: 0.05577222630381584\n",
      "Iteration 16116, Loss: 0.05578812211751938\n",
      "Iteration 16117, Loss: 0.0556388720870018\n",
      "Iteration 16118, Loss: 0.055812202394008636\n",
      "Iteration 16119, Loss: 0.05590840429067612\n",
      "Iteration 16120, Loss: 0.05588778108358383\n",
      "Iteration 16121, Loss: 0.05576622486114502\n",
      "Iteration 16122, Loss: 0.0556897334754467\n",
      "Iteration 16123, Loss: 0.05578335374593735\n",
      "Iteration 16124, Loss: 0.05572136491537094\n",
      "Iteration 16125, Loss: 0.055704038590192795\n",
      "Iteration 16126, Loss: 0.05575835704803467\n",
      "Iteration 16127, Loss: 0.05571747198700905\n",
      "Iteration 16128, Loss: 0.05566311255097389\n",
      "Iteration 16129, Loss: 0.0557071790099144\n",
      "Iteration 16130, Loss: 0.055660687386989594\n",
      "Iteration 16131, Loss: 0.05570157617330551\n",
      "Iteration 16132, Loss: 0.05570833012461662\n",
      "Iteration 16133, Loss: 0.05562416836619377\n",
      "Iteration 16134, Loss: 0.05575386807322502\n",
      "Iteration 16135, Loss: 0.055766504257917404\n",
      "Iteration 16136, Loss: 0.05567137524485588\n",
      "Iteration 16137, Loss: 0.0557587556540966\n",
      "Iteration 16138, Loss: 0.05580242723226547\n",
      "Iteration 16139, Loss: 0.05569219961762428\n",
      "Iteration 16140, Loss: 0.05575541779398918\n",
      "Iteration 16141, Loss: 0.055835407227277756\n",
      "Iteration 16142, Loss: 0.05579165741801262\n",
      "Iteration 16143, Loss: 0.055657386779785156\n",
      "Iteration 16144, Loss: 0.055813632905483246\n",
      "Iteration 16145, Loss: 0.05587999150156975\n",
      "Iteration 16146, Loss: 0.055776797235012054\n",
      "Iteration 16147, Loss: 0.05568110942840576\n",
      "Iteration 16148, Loss: 0.055761855095624924\n",
      "Iteration 16149, Loss: 0.05572926998138428\n",
      "Iteration 16150, Loss: 0.05563589185476303\n",
      "Iteration 16151, Loss: 0.055702608078718185\n",
      "Iteration 16152, Loss: 0.05563585087656975\n",
      "Iteration 16153, Loss: 0.05572887510061264\n",
      "Iteration 16154, Loss: 0.05574759095907211\n",
      "Iteration 16155, Loss: 0.05566263198852539\n",
      "Iteration 16156, Loss: 0.05576721951365471\n",
      "Iteration 16157, Loss: 0.05580540746450424\n",
      "Iteration 16158, Loss: 0.05568031594157219\n",
      "Iteration 16159, Loss: 0.055771153420209885\n",
      "Iteration 16160, Loss: 0.05586231127381325\n",
      "Iteration 16161, Loss: 0.05584112927317619\n",
      "Iteration 16162, Loss: 0.055722951889038086\n",
      "Iteration 16163, Loss: 0.05573233217000961\n",
      "Iteration 16164, Loss: 0.05580834671854973\n",
      "Iteration 16165, Loss: 0.05571882054209709\n",
      "Iteration 16166, Loss: 0.055718861520290375\n",
      "Iteration 16167, Loss: 0.05579002946615219\n",
      "Iteration 16168, Loss: 0.055755019187927246\n",
      "Iteration 16169, Loss: 0.055644433945417404\n",
      "Iteration 16170, Loss: 0.055803459137678146\n",
      "Iteration 16171, Loss: 0.05583560839295387\n",
      "Iteration 16172, Loss: 0.05569346994161606\n",
      "Iteration 16173, Loss: 0.05577222630381584\n",
      "Iteration 16174, Loss: 0.055877767503261566\n",
      "Iteration 16175, Loss: 0.05587844178080559\n",
      "Iteration 16176, Loss: 0.05578526109457016\n",
      "Iteration 16177, Loss: 0.0556233748793602\n",
      "Iteration 16178, Loss: 0.055794600397348404\n",
      "Iteration 16179, Loss: 0.05579575151205063\n",
      "Iteration 16180, Loss: 0.055648449808359146\n",
      "Iteration 16181, Loss: 0.055793486535549164\n",
      "Iteration 16182, Loss: 0.055872682482004166\n",
      "Iteration 16183, Loss: 0.05583091825246811\n",
      "Iteration 16184, Loss: 0.05568758770823479\n",
      "Iteration 16185, Loss: 0.05579861253499985\n",
      "Iteration 16186, Loss: 0.05589358136057854\n",
      "Iteration 16187, Loss: 0.055818717926740646\n",
      "Iteration 16188, Loss: 0.05563334748148918\n",
      "Iteration 16189, Loss: 0.05575704574584961\n",
      "Iteration 16190, Loss: 0.05576225370168686\n",
      "Iteration 16191, Loss: 0.05564431473612785\n",
      "Iteration 16192, Loss: 0.05581037327647209\n",
      "Iteration 16193, Loss: 0.055877964943647385\n",
      "Iteration 16194, Loss: 0.05579495429992676\n",
      "Iteration 16195, Loss: 0.05565842241048813\n",
      "Iteration 16196, Loss: 0.0557662658393383\n",
      "Iteration 16197, Loss: 0.055768292397260666\n",
      "Iteration 16198, Loss: 0.05563477799296379\n",
      "Iteration 16199, Loss: 0.05583012476563454\n",
      "Iteration 16200, Loss: 0.05592505261301994\n",
      "Iteration 16201, Loss: 0.05589274689555168\n",
      "Iteration 16202, Loss: 0.05575903505086899\n",
      "Iteration 16203, Loss: 0.05572271719574928\n",
      "Iteration 16204, Loss: 0.05582340806722641\n",
      "Iteration 16205, Loss: 0.055787526071071625\n",
      "Iteration 16206, Loss: 0.05564308166503906\n",
      "Iteration 16207, Loss: 0.05574898049235344\n",
      "Iteration 16208, Loss: 0.05578402802348137\n",
      "Iteration 16209, Loss: 0.05572597309947014\n",
      "Iteration 16210, Loss: 0.05566469952464104\n",
      "Iteration 16211, Loss: 0.05570507049560547\n",
      "Iteration 16212, Loss: 0.05565508455038071\n",
      "Iteration 16213, Loss: 0.055691007524728775\n",
      "Iteration 16214, Loss: 0.055676303803920746\n",
      "Iteration 16215, Loss: 0.05568313971161842\n",
      "Iteration 16216, Loss: 0.055663466453552246\n",
      "Iteration 16217, Loss: 0.05570010468363762\n",
      "Iteration 16218, Loss: 0.05570892617106438\n",
      "Iteration 16219, Loss: 0.055621031671762466\n",
      "Iteration 16220, Loss: 0.05569704622030258\n",
      "Iteration 16221, Loss: 0.055624090135097504\n",
      "Iteration 16222, Loss: 0.055730901658535004\n",
      "Iteration 16223, Loss: 0.05572986602783203\n",
      "Iteration 16224, Loss: 0.05563613027334213\n",
      "Iteration 16225, Loss: 0.05576304718852043\n",
      "Iteration 16226, Loss: 0.05575660988688469\n",
      "Iteration 16227, Loss: 0.05563092231750488\n",
      "Iteration 16228, Loss: 0.05565337464213371\n",
      "Iteration 16229, Loss: 0.055660367012023926\n",
      "Iteration 16230, Loss: 0.05562659353017807\n",
      "Iteration 16231, Loss: 0.05566176027059555\n",
      "Iteration 16232, Loss: 0.05565500631928444\n",
      "Iteration 16233, Loss: 0.055634938180446625\n",
      "Iteration 16234, Loss: 0.055730465799570084\n",
      "Iteration 16235, Loss: 0.05569382756948471\n",
      "Iteration 16236, Loss: 0.05570141598582268\n",
      "Iteration 16237, Loss: 0.055743735283613205\n",
      "Iteration 16238, Loss: 0.05568830296397209\n",
      "Iteration 16239, Loss: 0.055701736360788345\n",
      "Iteration 16240, Loss: 0.055704712867736816\n",
      "Iteration 16241, Loss: 0.055667243897914886\n",
      "Iteration 16242, Loss: 0.05568615719676018\n",
      "Iteration 16243, Loss: 0.05561379715800285\n",
      "Iteration 16244, Loss: 0.05566772073507309\n",
      "Iteration 16245, Loss: 0.05562707036733627\n",
      "Iteration 16246, Loss: 0.05576654523611069\n",
      "Iteration 16247, Loss: 0.05575239658355713\n",
      "Iteration 16248, Loss: 0.05564415827393532\n",
      "Iteration 16249, Loss: 0.055673997849226\n",
      "Iteration 16250, Loss: 0.05561665818095207\n",
      "Iteration 16251, Loss: 0.055684130638837814\n",
      "Iteration 16252, Loss: 0.05565301701426506\n",
      "Iteration 16253, Loss: 0.055717747658491135\n",
      "Iteration 16254, Loss: 0.05569637194275856\n",
      "Iteration 16255, Loss: 0.05568810552358627\n",
      "Iteration 16256, Loss: 0.05571766942739487\n",
      "Iteration 16257, Loss: 0.05564642325043678\n",
      "Iteration 16258, Loss: 0.05577317997813225\n",
      "Iteration 16259, Loss: 0.055791061371564865\n",
      "Iteration 16260, Loss: 0.05563867464661598\n",
      "Iteration 16261, Loss: 0.05582066625356674\n",
      "Iteration 16262, Loss: 0.055928390473127365\n",
      "Iteration 16263, Loss: 0.055921874940395355\n",
      "Iteration 16264, Loss: 0.05581530183553696\n",
      "Iteration 16265, Loss: 0.055652182549238205\n",
      "Iteration 16266, Loss: 0.055857181549072266\n",
      "Iteration 16267, Loss: 0.05593351647257805\n",
      "Iteration 16268, Loss: 0.05582992359995842\n",
      "Iteration 16269, Loss: 0.05565377324819565\n",
      "Iteration 16270, Loss: 0.055747389793395996\n",
      "Iteration 16271, Loss: 0.05575188249349594\n",
      "Iteration 16272, Loss: 0.05566621199250221\n",
      "Iteration 16273, Loss: 0.055770400911569595\n",
      "Iteration 16274, Loss: 0.05580691620707512\n",
      "Iteration 16275, Loss: 0.05567733570933342\n",
      "Iteration 16276, Loss: 0.055772386491298676\n",
      "Iteration 16277, Loss: 0.05586457625031471\n",
      "Iteration 16278, Loss: 0.05584963411092758\n",
      "Iteration 16279, Loss: 0.05573884770274162\n",
      "Iteration 16280, Loss: 0.055701933801174164\n",
      "Iteration 16281, Loss: 0.05576956644654274\n",
      "Iteration 16282, Loss: 0.05566565319895744\n",
      "Iteration 16283, Loss: 0.055768728256225586\n",
      "Iteration 16284, Loss: 0.055850349366664886\n",
      "Iteration 16285, Loss: 0.055827658623456955\n",
      "Iteration 16286, Loss: 0.055713336914777756\n",
      "Iteration 16287, Loss: 0.05573984235525131\n",
      "Iteration 16288, Loss: 0.05580870434641838\n",
      "Iteration 16289, Loss: 0.05570344254374504\n",
      "Iteration 16290, Loss: 0.055741071701049805\n",
      "Iteration 16291, Loss: 0.055823925882577896\n",
      "Iteration 16292, Loss: 0.055803779512643814\n",
      "Iteration 16293, Loss: 0.05569291114807129\n",
      "Iteration 16294, Loss: 0.05576268956065178\n",
      "Iteration 16295, Loss: 0.05582630634307861\n",
      "Iteration 16296, Loss: 0.05571305751800537\n",
      "Iteration 16297, Loss: 0.055738650262355804\n",
      "Iteration 16298, Loss: 0.05582670494914055\n",
      "Iteration 16299, Loss: 0.055811844766139984\n",
      "Iteration 16300, Loss: 0.05570463463664055\n",
      "Iteration 16301, Loss: 0.05574226379394531\n",
      "Iteration 16302, Loss: 0.05580214783549309\n",
      "Iteration 16303, Loss: 0.05568460747599602\n",
      "Iteration 16304, Loss: 0.05576225370168686\n",
      "Iteration 16305, Loss: 0.05585316941142082\n",
      "Iteration 16306, Loss: 0.05584057420492172\n",
      "Iteration 16307, Loss: 0.05573543161153793\n",
      "Iteration 16308, Loss: 0.05569839850068092\n",
      "Iteration 16309, Loss: 0.05575684830546379\n",
      "Iteration 16310, Loss: 0.055638592690229416\n",
      "Iteration 16311, Loss: 0.05579523369669914\n",
      "Iteration 16312, Loss: 0.055885475128889084\n",
      "Iteration 16313, Loss: 0.05587192624807358\n",
      "Iteration 16314, Loss: 0.055765550583601\n",
      "Iteration 16315, Loss: 0.05565845966339111\n",
      "Iteration 16316, Loss: 0.05571723356842995\n",
      "Iteration 16317, Loss: 0.05562206357717514\n",
      "Iteration 16318, Loss: 0.05562448874115944\n",
      "Iteration 16319, Loss: 0.055713534355163574\n",
      "Iteration 16320, Loss: 0.05569736286997795\n",
      "Iteration 16321, Loss: 0.05566251650452614\n",
      "Iteration 16322, Loss: 0.05566760152578354\n",
      "Iteration 16323, Loss: 0.05568544194102287\n",
      "Iteration 16324, Loss: 0.05567952245473862\n",
      "Iteration 16325, Loss: 0.05566660687327385\n",
      "Iteration 16326, Loss: 0.05565401166677475\n",
      "Iteration 16327, Loss: 0.05570908635854721\n",
      "Iteration 16328, Loss: 0.055716872215270996\n",
      "Iteration 16329, Loss: 0.05564359948039055\n",
      "Iteration 16330, Loss: 0.05574886128306389\n",
      "Iteration 16331, Loss: 0.05573117733001709\n",
      "Iteration 16332, Loss: 0.05566287413239479\n",
      "Iteration 16333, Loss: 0.05569370836019516\n",
      "Iteration 16334, Loss: 0.055638156831264496\n",
      "Iteration 16335, Loss: 0.0557585172355175\n",
      "Iteration 16336, Loss: 0.05574902147054672\n",
      "Iteration 16337, Loss: 0.055645547807216644\n",
      "Iteration 16338, Loss: 0.05568341538310051\n",
      "Iteration 16339, Loss: 0.05563398450613022\n",
      "Iteration 16340, Loss: 0.05576146021485329\n",
      "Iteration 16341, Loss: 0.055765628814697266\n",
      "Iteration 16342, Loss: 0.05564491078257561\n",
      "Iteration 16343, Loss: 0.05577031895518303\n",
      "Iteration 16344, Loss: 0.05581037327647209\n",
      "Iteration 16345, Loss: 0.05571957677602768\n",
      "Iteration 16346, Loss: 0.05570996180176735\n",
      "Iteration 16347, Loss: 0.05576976388692856\n",
      "Iteration 16348, Loss: 0.055689774453639984\n",
      "Iteration 16349, Loss: 0.05573225021362305\n",
      "Iteration 16350, Loss: 0.055784426629543304\n",
      "Iteration 16351, Loss: 0.0557078942656517\n",
      "Iteration 16352, Loss: 0.05570777505636215\n",
      "Iteration 16353, Loss: 0.055754225701093674\n",
      "Iteration 16354, Loss: 0.055661361664533615\n",
      "Iteration 16355, Loss: 0.05577254667878151\n",
      "Iteration 16356, Loss: 0.05583576485514641\n",
      "Iteration 16357, Loss: 0.055771034210920334\n",
      "Iteration 16358, Loss: 0.055659931153059006\n",
      "Iteration 16359, Loss: 0.05576467514038086\n",
      "Iteration 16360, Loss: 0.05576062202453613\n",
      "Iteration 16361, Loss: 0.05563247203826904\n",
      "Iteration 16362, Loss: 0.055719535797834396\n",
      "Iteration 16363, Loss: 0.05572887510061264\n",
      "Iteration 16364, Loss: 0.055645983666181564\n",
      "Iteration 16365, Loss: 0.05579201504588127\n",
      "Iteration 16366, Loss: 0.055822812020778656\n",
      "Iteration 16367, Loss: 0.055679164826869965\n",
      "Iteration 16368, Loss: 0.05578327551484108\n",
      "Iteration 16369, Loss: 0.05588865652680397\n",
      "Iteration 16370, Loss: 0.055885832756757736\n",
      "Iteration 16371, Loss: 0.0557863749563694\n",
      "Iteration 16372, Loss: 0.055633943527936935\n",
      "Iteration 16373, Loss: 0.05578303709626198\n",
      "Iteration 16374, Loss: 0.0557708740234375\n",
      "Iteration 16375, Loss: 0.05564415454864502\n",
      "Iteration 16376, Loss: 0.05573086068034172\n",
      "Iteration 16377, Loss: 0.05573702231049538\n",
      "Iteration 16378, Loss: 0.05563235655426979\n",
      "Iteration 16379, Loss: 0.05582531541585922\n",
      "Iteration 16380, Loss: 0.05588766187429428\n",
      "Iteration 16381, Loss: 0.05579396337270737\n",
      "Iteration 16382, Loss: 0.05566585063934326\n",
      "Iteration 16383, Loss: 0.0557611808180809\n",
      "Iteration 16384, Loss: 0.055753193795681\n",
      "Iteration 16385, Loss: 0.05562416836619377\n",
      "Iteration 16386, Loss: 0.055842362344264984\n",
      "Iteration 16387, Loss: 0.05593077465891838\n",
      "Iteration 16388, Loss: 0.0558803491294384\n",
      "Iteration 16389, Loss: 0.055722080171108246\n",
      "Iteration 16390, Loss: 0.05577349662780762\n",
      "Iteration 16391, Loss: 0.055884163826704025\n",
      "Iteration 16392, Loss: 0.055847249925136566\n",
      "Iteration 16393, Loss: 0.05567288398742676\n",
      "Iteration 16394, Loss: 0.05582861229777336\n",
      "Iteration 16395, Loss: 0.05595378205180168\n",
      "Iteration 16396, Loss: 0.05593892186880112\n",
      "Iteration 16397, Loss: 0.055804334580898285\n",
      "Iteration 16398, Loss: 0.05568420886993408\n",
      "Iteration 16399, Loss: 0.05581045523285866\n",
      "Iteration 16400, Loss: 0.05581418797373772\n",
      "Iteration 16401, Loss: 0.05567014589905739\n",
      "Iteration 16402, Loss: 0.05580183118581772\n",
      "Iteration 16403, Loss: 0.055904947221279144\n",
      "Iteration 16404, Loss: 0.05588905140757561\n",
      "Iteration 16405, Loss: 0.0557740144431591\n",
      "Iteration 16406, Loss: 0.05569037050008774\n",
      "Iteration 16407, Loss: 0.05579328536987305\n",
      "Iteration 16408, Loss: 0.055762529373168945\n",
      "Iteration 16409, Loss: 0.05566883087158203\n",
      "Iteration 16410, Loss: 0.05572148412466049\n",
      "Iteration 16411, Loss: 0.055716197937726974\n",
      "Iteration 16412, Loss: 0.055657945573329926\n",
      "Iteration 16413, Loss: 0.05573956295847893\n",
      "Iteration 16414, Loss: 0.055729907006025314\n",
      "Iteration 16415, Loss: 0.05566410347819328\n",
      "Iteration 16416, Loss: 0.05570785328745842\n",
      "Iteration 16417, Loss: 0.05567511171102524\n",
      "Iteration 16418, Loss: 0.0557096004486084\n",
      "Iteration 16419, Loss: 0.05571397393941879\n",
      "Iteration 16420, Loss: 0.05565440654754639\n",
      "Iteration 16421, Loss: 0.05569803714752197\n",
      "Iteration 16422, Loss: 0.055648207664489746\n",
      "Iteration 16423, Loss: 0.05573960393667221\n",
      "Iteration 16424, Loss: 0.05577242746949196\n",
      "Iteration 16425, Loss: 0.05570288747549057\n",
      "Iteration 16426, Loss: 0.055704515427351\n",
      "Iteration 16427, Loss: 0.055732809007167816\n",
      "Iteration 16428, Loss: 0.055634502321481705\n",
      "Iteration 16429, Loss: 0.055706147104501724\n",
      "Iteration 16430, Loss: 0.05570542812347412\n",
      "Iteration 16431, Loss: 0.055619560182094574\n",
      "Iteration 16432, Loss: 0.055747393518686295\n",
      "Iteration 16433, Loss: 0.05573881044983864\n",
      "Iteration 16434, Loss: 0.05564634129405022\n",
      "Iteration 16435, Loss: 0.05573264881968498\n",
      "Iteration 16436, Loss: 0.05570852756500244\n",
      "Iteration 16437, Loss: 0.05567800998687744\n",
      "Iteration 16438, Loss: 0.055700384080410004\n",
      "Iteration 16439, Loss: 0.055636804550886154\n",
      "Iteration 16440, Loss: 0.05573594942688942\n",
      "Iteration 16441, Loss: 0.055699825286865234\n",
      "Iteration 16442, Loss: 0.05569791793823242\n",
      "Iteration 16443, Loss: 0.05573928356170654\n",
      "Iteration 16444, Loss: 0.055683694779872894\n",
      "Iteration 16445, Loss: 0.0557078942656517\n",
      "Iteration 16446, Loss: 0.0557103157043457\n",
      "Iteration 16447, Loss: 0.05566438287496567\n",
      "Iteration 16448, Loss: 0.05568389222025871\n",
      "Iteration 16449, Loss: 0.05562663450837135\n",
      "Iteration 16450, Loss: 0.05572907254099846\n",
      "Iteration 16451, Loss: 0.05569831654429436\n",
      "Iteration 16452, Loss: 0.055685125291347504\n",
      "Iteration 16453, Loss: 0.05571174621582031\n",
      "Iteration 16454, Loss: 0.055625997483730316\n",
      "Iteration 16455, Loss: 0.055811524391174316\n",
      "Iteration 16456, Loss: 0.05586020275950432\n",
      "Iteration 16457, Loss: 0.055762212723493576\n",
      "Iteration 16458, Loss: 0.055687349289655685\n",
      "Iteration 16459, Loss: 0.055768173187971115\n",
      "Iteration 16460, Loss: 0.05573348328471184\n",
      "Iteration 16461, Loss: 0.05564860627055168\n",
      "Iteration 16462, Loss: 0.055663228034973145\n",
      "Iteration 16463, Loss: 0.05566760152578354\n",
      "Iteration 16464, Loss: 0.05564204975962639\n",
      "Iteration 16465, Loss: 0.05573328584432602\n",
      "Iteration 16466, Loss: 0.055739641189575195\n",
      "Iteration 16467, Loss: 0.05564379692077637\n",
      "Iteration 16468, Loss: 0.05576137825846672\n",
      "Iteration 16469, Loss: 0.0557703971862793\n",
      "Iteration 16470, Loss: 0.05562766641378403\n",
      "Iteration 16471, Loss: 0.05582861229777336\n",
      "Iteration 16472, Loss: 0.055916666984558105\n",
      "Iteration 16473, Loss: 0.05587339401245117\n",
      "Iteration 16474, Loss: 0.05572442337870598\n",
      "Iteration 16475, Loss: 0.055762968957424164\n",
      "Iteration 16476, Loss: 0.05586696043610573\n",
      "Iteration 16477, Loss: 0.05581542104482651\n",
      "Iteration 16478, Loss: 0.05562099069356918\n",
      "Iteration 16479, Loss: 0.05588054656982422\n",
      "Iteration 16480, Loss: 0.056006234139204025\n",
      "Iteration 16481, Loss: 0.05599125474691391\n",
      "Iteration 16482, Loss: 0.055855631828308105\n",
      "Iteration 16483, Loss: 0.05567896366119385\n",
      "Iteration 16484, Loss: 0.05584649369120598\n",
      "Iteration 16485, Loss: 0.055934708565473557\n",
      "Iteration 16486, Loss: 0.05586012452840805\n",
      "Iteration 16487, Loss: 0.05563652515411377\n",
      "Iteration 16488, Loss: 0.055888693779706955\n",
      "Iteration 16489, Loss: 0.05604855343699455\n",
      "Iteration 16490, Loss: 0.05607791990041733\n",
      "Iteration 16491, Loss: 0.05599169060587883\n",
      "Iteration 16492, Loss: 0.05581335350871086\n",
      "Iteration 16493, Loss: 0.05571826547384262\n",
      "Iteration 16494, Loss: 0.055871330201625824\n",
      "Iteration 16495, Loss: 0.05588364601135254\n",
      "Iteration 16496, Loss: 0.055738888680934906\n",
      "Iteration 16497, Loss: 0.05575243756175041\n",
      "Iteration 16498, Loss: 0.05585611239075661\n",
      "Iteration 16499, Loss: 0.05585432052612305\n",
      "Iteration 16500, Loss: 0.0557628870010376\n",
      "Iteration 16501, Loss: 0.05568162724375725\n",
      "Iteration 16502, Loss: 0.055773697793483734\n",
      "Iteration 16503, Loss: 0.055744532495737076\n",
      "Iteration 16504, Loss: 0.05568154901266098\n",
      "Iteration 16505, Loss: 0.055726490914821625\n",
      "Iteration 16506, Loss: 0.05571329593658447\n",
      "Iteration 16507, Loss: 0.05565858259797096\n",
      "Iteration 16508, Loss: 0.05572076886892319\n",
      "Iteration 16509, Loss: 0.055706463754177094\n",
      "Iteration 16510, Loss: 0.055672287940979004\n",
      "Iteration 16511, Loss: 0.055699270218610764\n",
      "Iteration 16512, Loss: 0.05565158650279045\n",
      "Iteration 16513, Loss: 0.05571882054209709\n",
      "Iteration 16514, Loss: 0.05573364347219467\n",
      "Iteration 16515, Loss: 0.055662792176008224\n",
      "Iteration 16516, Loss: 0.05574830621480942\n",
      "Iteration 16517, Loss: 0.05575668811798096\n",
      "Iteration 16518, Loss: 0.0556306466460228\n",
      "Iteration 16519, Loss: 0.05569462105631828\n",
      "Iteration 16520, Loss: 0.055674754083156586\n",
      "Iteration 16521, Loss: 0.05568047612905502\n",
      "Iteration 16522, Loss: 0.05565202608704567\n",
      "Iteration 16523, Loss: 0.05572343245148659\n",
      "Iteration 16524, Loss: 0.05575307458639145\n",
      "Iteration 16525, Loss: 0.05568087100982666\n",
      "Iteration 16526, Loss: 0.05573050305247307\n",
      "Iteration 16527, Loss: 0.05575573444366455\n",
      "Iteration 16528, Loss: 0.05561808869242668\n",
      "Iteration 16529, Loss: 0.05581025406718254\n",
      "Iteration 16530, Loss: 0.05587999150156975\n",
      "Iteration 16531, Loss: 0.055830638855695724\n",
      "Iteration 16532, Loss: 0.05569195747375488\n",
      "Iteration 16533, Loss: 0.05578970909118652\n",
      "Iteration 16534, Loss: 0.05587732791900635\n",
      "Iteration 16535, Loss: 0.055803459137678146\n",
      "Iteration 16536, Loss: 0.05563628673553467\n",
      "Iteration 16537, Loss: 0.05569605156779289\n",
      "Iteration 16538, Loss: 0.055646102875471115\n",
      "Iteration 16539, Loss: 0.0557456836104393\n",
      "Iteration 16540, Loss: 0.05574560537934303\n",
      "Iteration 16541, Loss: 0.0556340217590332\n",
      "Iteration 16542, Loss: 0.05567992106080055\n",
      "Iteration 16543, Loss: 0.055614154785871506\n",
      "Iteration 16544, Loss: 0.055748146027326584\n",
      "Iteration 16545, Loss: 0.05577163025736809\n",
      "Iteration 16546, Loss: 0.05569462105631828\n",
      "Iteration 16547, Loss: 0.05571969598531723\n",
      "Iteration 16548, Loss: 0.055748824030160904\n",
      "Iteration 16549, Loss: 0.05561892315745354\n",
      "Iteration 16550, Loss: 0.0557355098426342\n",
      "Iteration 16551, Loss: 0.05575486272573471\n",
      "Iteration 16552, Loss: 0.055677175521850586\n",
      "Iteration 16553, Loss: 0.05574294179677963\n",
      "Iteration 16554, Loss: 0.055771272629499435\n",
      "Iteration 16555, Loss: 0.05562921613454819\n",
      "Iteration 16556, Loss: 0.05581855773925781\n",
      "Iteration 16557, Loss: 0.055919211357831955\n",
      "Iteration 16558, Loss: 0.055911820381879807\n",
      "Iteration 16559, Loss: 0.05580858513712883\n",
      "Iteration 16560, Loss: 0.055646222084760666\n",
      "Iteration 16561, Loss: 0.05585698410868645\n",
      "Iteration 16562, Loss: 0.055930059403181076\n",
      "Iteration 16563, Loss: 0.05582340806722641\n",
      "Iteration 16564, Loss: 0.05565635487437248\n",
      "Iteration 16565, Loss: 0.05574532598257065\n",
      "Iteration 16566, Loss: 0.055737219750881195\n",
      "Iteration 16567, Loss: 0.055639348924160004\n",
      "Iteration 16568, Loss: 0.05581732839345932\n",
      "Iteration 16569, Loss: 0.055863142013549805\n",
      "Iteration 16570, Loss: 0.05573419854044914\n",
      "Iteration 16571, Loss: 0.05573304742574692\n",
      "Iteration 16572, Loss: 0.05583016201853752\n",
      "Iteration 16573, Loss: 0.0558197908103466\n",
      "Iteration 16574, Loss: 0.05571206659078598\n",
      "Iteration 16575, Loss: 0.05573276802897453\n",
      "Iteration 16576, Loss: 0.05579352751374245\n",
      "Iteration 16577, Loss: 0.05567789450287819\n",
      "Iteration 16578, Loss: 0.05576690286397934\n",
      "Iteration 16579, Loss: 0.05585567280650139\n",
      "Iteration 16580, Loss: 0.055836521089076996\n",
      "Iteration 16581, Loss: 0.055722951889038086\n",
      "Iteration 16582, Loss: 0.05572676658630371\n",
      "Iteration 16583, Loss: 0.05579448118805885\n",
      "Iteration 16584, Loss: 0.055687468498945236\n",
      "Iteration 16585, Loss: 0.055754583328962326\n",
      "Iteration 16586, Loss: 0.05583822727203369\n",
      "Iteration 16587, Loss: 0.05581720918416977\n",
      "Iteration 16588, Loss: 0.055705390870571136\n",
      "Iteration 16589, Loss: 0.05574854463338852\n",
      "Iteration 16590, Loss: 0.05581244081258774\n",
      "Iteration 16591, Loss: 0.05569958686828613\n",
      "Iteration 16592, Loss: 0.055748701095581055\n",
      "Iteration 16593, Loss: 0.05583588406443596\n",
      "Iteration 16594, Loss: 0.055820029228925705\n",
      "Iteration 16595, Loss: 0.05571246147155762\n",
      "Iteration 16596, Loss: 0.055733125656843185\n",
      "Iteration 16597, Loss: 0.05579257011413574\n",
      "Iteration 16598, Loss: 0.05567622557282448\n",
      "Iteration 16599, Loss: 0.05576654523611069\n",
      "Iteration 16600, Loss: 0.0558551549911499\n",
      "Iteration 16601, Loss: 0.055840931832790375\n",
      "Iteration 16602, Loss: 0.055734675377607346\n",
      "Iteration 16603, Loss: 0.055701375007629395\n",
      "Iteration 16604, Loss: 0.05576038733124733\n",
      "Iteration 16605, Loss: 0.05564491078257561\n",
      "Iteration 16606, Loss: 0.05578514188528061\n",
      "Iteration 16607, Loss: 0.05586938187479973\n",
      "Iteration 16608, Loss: 0.055850349366664886\n",
      "Iteration 16609, Loss: 0.05573884770274162\n",
      "Iteration 16610, Loss: 0.055701494216918945\n",
      "Iteration 16611, Loss: 0.055767301470041275\n",
      "Iteration 16612, Loss: 0.0556560754776001\n",
      "Iteration 16613, Loss: 0.05577930063009262\n",
      "Iteration 16614, Loss: 0.05586647987365723\n",
      "Iteration 16615, Loss: 0.055850230157375336\n",
      "Iteration 16616, Loss: 0.05574178695678711\n",
      "Iteration 16617, Loss: 0.0556941032409668\n",
      "Iteration 16618, Loss: 0.05575672909617424\n",
      "Iteration 16619, Loss: 0.055643241852521896\n",
      "Iteration 16620, Loss: 0.055788081139326096\n",
      "Iteration 16621, Loss: 0.0558747872710228\n",
      "Iteration 16622, Loss: 0.05585845559835434\n",
      "Iteration 16623, Loss: 0.055749617516994476\n",
      "Iteration 16624, Loss: 0.055683497339487076\n",
      "Iteration 16625, Loss: 0.05574508756399155\n",
      "Iteration 16626, Loss: 0.055627427995204926\n",
      "Iteration 16627, Loss: 0.05580413341522217\n",
      "Iteration 16628, Loss: 0.05589497089385986\n",
      "Iteration 16629, Loss: 0.055882178246974945\n",
      "Iteration 16630, Loss: 0.0557764396071434\n",
      "Iteration 16631, Loss: 0.055643480271101\n",
      "Iteration 16632, Loss: 0.05570225045084953\n",
      "Iteration 16633, Loss: 0.055632710456848145\n",
      "Iteration 16634, Loss: 0.05563756078481674\n",
      "Iteration 16635, Loss: 0.05568985268473625\n",
      "Iteration 16636, Loss: 0.05564403906464577\n",
      "Iteration 16637, Loss: 0.055736105889081955\n",
      "Iteration 16638, Loss: 0.055753353983163834\n",
      "Iteration 16639, Loss: 0.055654168128967285\n",
      "Iteration 16640, Loss: 0.0557791031897068\n",
      "Iteration 16641, Loss: 0.05582750216126442\n",
      "Iteration 16642, Loss: 0.055725179612636566\n",
      "Iteration 16643, Loss: 0.055717628449201584\n",
      "Iteration 16644, Loss: 0.055792491883039474\n",
      "Iteration 16645, Loss: 0.05574508756399155\n",
      "Iteration 16646, Loss: 0.05564296245574951\n",
      "Iteration 16647, Loss: 0.05571170896291733\n",
      "Iteration 16648, Loss: 0.05564030259847641\n",
      "Iteration 16649, Loss: 0.05575506016612053\n",
      "Iteration 16650, Loss: 0.05580862611532211\n",
      "Iteration 16651, Loss: 0.055763326585292816\n",
      "Iteration 16652, Loss: 0.055632710456848145\n",
      "Iteration 16653, Loss: 0.055861275643110275\n",
      "Iteration 16654, Loss: 0.05593749135732651\n",
      "Iteration 16655, Loss: 0.05583159253001213\n",
      "Iteration 16656, Loss: 0.05564773082733154\n",
      "Iteration 16657, Loss: 0.05573507398366928\n",
      "Iteration 16658, Loss: 0.05572088807821274\n",
      "Iteration 16659, Loss: 0.055613838136196136\n",
      "Iteration 16660, Loss: 0.055852096527814865\n",
      "Iteration 16661, Loss: 0.05590439215302467\n",
      "Iteration 16662, Loss: 0.05578450486063957\n",
      "Iteration 16663, Loss: 0.05569016933441162\n",
      "Iteration 16664, Loss: 0.055784426629543304\n",
      "Iteration 16665, Loss: 0.055771034210920334\n",
      "Iteration 16666, Loss: 0.055655043572187424\n",
      "Iteration 16667, Loss: 0.05581565946340561\n",
      "Iteration 16668, Loss: 0.055886153131723404\n",
      "Iteration 16669, Loss: 0.05578633397817612\n",
      "Iteration 16670, Loss: 0.05567511171102524\n",
      "Iteration 16671, Loss: 0.05576078221201897\n",
      "Iteration 16672, Loss: 0.05574015900492668\n",
      "Iteration 16673, Loss: 0.055614713579416275\n",
      "Iteration 16674, Loss: 0.055804453790187836\n",
      "Iteration 16675, Loss: 0.055851858109235764\n",
      "Iteration 16676, Loss: 0.05577246472239494\n",
      "Iteration 16677, Loss: 0.05567058175802231\n",
      "Iteration 16678, Loss: 0.05576543137431145\n",
      "Iteration 16679, Loss: 0.055753033608198166\n",
      "Iteration 16680, Loss: 0.055644553154706955\n",
      "Iteration 16681, Loss: 0.05570455640554428\n",
      "Iteration 16682, Loss: 0.05569879338145256\n",
      "Iteration 16683, Loss: 0.05564391613006592\n",
      "Iteration 16684, Loss: 0.05570876598358154\n",
      "Iteration 16685, Loss: 0.05566982552409172\n",
      "Iteration 16686, Loss: 0.05570678040385246\n",
      "Iteration 16687, Loss: 0.05572454258799553\n",
      "Iteration 16688, Loss: 0.05562571808695793\n",
      "Iteration 16689, Loss: 0.05582531541585922\n",
      "Iteration 16690, Loss: 0.055882178246974945\n",
      "Iteration 16691, Loss: 0.05578522011637688\n",
      "Iteration 16692, Loss: 0.055674873292446136\n",
      "Iteration 16693, Loss: 0.05576670169830322\n",
      "Iteration 16694, Loss: 0.0557527169585228\n",
      "Iteration 16695, Loss: 0.05562559887766838\n",
      "Iteration 16696, Loss: 0.05578096956014633\n",
      "Iteration 16697, Loss: 0.05583123490214348\n",
      "Iteration 16698, Loss: 0.05577286332845688\n",
      "Iteration 16699, Loss: 0.0556587390601635\n",
      "Iteration 16700, Loss: 0.05578991025686264\n",
      "Iteration 16701, Loss: 0.0558144673705101\n",
      "Iteration 16702, Loss: 0.05567312240600586\n",
      "Iteration 16703, Loss: 0.055786848068237305\n",
      "Iteration 16704, Loss: 0.05588933080434799\n",
      "Iteration 16705, Loss: 0.05588758364319801\n",
      "Iteration 16706, Loss: 0.05579213425517082\n",
      "Iteration 16707, Loss: 0.05563577264547348\n",
      "Iteration 16708, Loss: 0.055840253829956055\n",
      "Iteration 16709, Loss: 0.05588774010539055\n",
      "Iteration 16710, Loss: 0.055760305374860764\n",
      "Iteration 16711, Loss: 0.05571448802947998\n",
      "Iteration 16712, Loss: 0.05581124871969223\n",
      "Iteration 16713, Loss: 0.055803775787353516\n",
      "Iteration 16714, Loss: 0.055699508637189865\n",
      "Iteration 16715, Loss: 0.05574667453765869\n",
      "Iteration 16716, Loss: 0.0558038167655468\n",
      "Iteration 16717, Loss: 0.055686239153146744\n",
      "Iteration 16718, Loss: 0.05576086416840553\n",
      "Iteration 16719, Loss: 0.0558498315513134\n",
      "Iteration 16720, Loss: 0.055829647928476334\n",
      "Iteration 16721, Loss: 0.05571393296122551\n",
      "Iteration 16722, Loss: 0.05573999881744385\n",
      "Iteration 16723, Loss: 0.055809538811445236\n",
      "Iteration 16724, Loss: 0.05570264905691147\n",
      "Iteration 16725, Loss: 0.0557430200278759\n",
      "Iteration 16726, Loss: 0.0558268241584301\n",
      "Iteration 16727, Loss: 0.05580409616231918\n",
      "Iteration 16728, Loss: 0.05568969249725342\n",
      "Iteration 16729, Loss: 0.055770717561244965\n",
      "Iteration 16730, Loss: 0.05583624169230461\n",
      "Iteration 16731, Loss: 0.05572358891367912\n",
      "Iteration 16732, Loss: 0.05573097988963127\n",
      "Iteration 16733, Loss: 0.055818360298871994\n",
      "Iteration 16734, Loss: 0.055801115930080414\n",
      "Iteration 16735, Loss: 0.05569255352020264\n",
      "Iteration 16736, Loss: 0.05576082319021225\n",
      "Iteration 16737, Loss: 0.05582078546285629\n",
      "Iteration 16738, Loss: 0.055702727288007736\n",
      "Iteration 16739, Loss: 0.055749695748090744\n",
      "Iteration 16740, Loss: 0.05584041401743889\n",
      "Iteration 16741, Loss: 0.055827222764492035\n",
      "Iteration 16742, Loss: 0.05572164058685303\n",
      "Iteration 16743, Loss: 0.055718421936035156\n",
      "Iteration 16744, Loss: 0.055776678025722504\n",
      "Iteration 16745, Loss: 0.055660050362348557\n",
      "Iteration 16746, Loss: 0.05577715486288071\n",
      "Iteration 16747, Loss: 0.05586457625031471\n",
      "Iteration 16748, Loss: 0.05584919452667236\n",
      "Iteration 16749, Loss: 0.05574151128530502\n",
      "Iteration 16750, Loss: 0.05569335073232651\n",
      "Iteration 16751, Loss: 0.05575370788574219\n",
      "Iteration 16752, Loss: 0.055638790130615234\n",
      "Iteration 16753, Loss: 0.05578867718577385\n",
      "Iteration 16754, Loss: 0.05587180703878403\n",
      "Iteration 16755, Loss: 0.055851344019174576\n",
      "Iteration 16756, Loss: 0.05573829263448715\n",
      "Iteration 16757, Loss: 0.05570407956838608\n",
      "Iteration 16758, Loss: 0.05577202886343002\n",
      "Iteration 16759, Loss: 0.055664025247097015\n",
      "Iteration 16760, Loss: 0.05577147379517555\n",
      "Iteration 16761, Loss: 0.05585626885294914\n",
      "Iteration 16762, Loss: 0.05583842843770981\n",
      "Iteration 16763, Loss: 0.05572863668203354\n",
      "Iteration 16764, Loss: 0.055713336914777756\n",
      "Iteration 16765, Loss: 0.05577707290649414\n",
      "Iteration 16766, Loss: 0.05566366761922836\n",
      "Iteration 16767, Loss: 0.05577472969889641\n",
      "Iteration 16768, Loss: 0.0558626689016819\n",
      "Iteration 16769, Loss: 0.055847883224487305\n",
      "Iteration 16770, Loss: 0.055740635842084885\n",
      "Iteration 16771, Loss: 0.055693745613098145\n",
      "Iteration 16772, Loss: 0.055753909051418304\n",
      "Iteration 16773, Loss: 0.05563640594482422\n",
      "Iteration 16774, Loss: 0.05579646676778793\n",
      "Iteration 16775, Loss: 0.055886272341012955\n",
      "Iteration 16776, Loss: 0.055872201919555664\n",
      "Iteration 16777, Loss: 0.05576515197753906\n",
      "Iteration 16778, Loss: 0.055660370737314224\n",
      "Iteration 16779, Loss: 0.05572080612182617\n",
      "Iteration 16780, Loss: 0.05562162399291992\n",
      "Iteration 16781, Loss: 0.05570010468363762\n",
      "Iteration 16782, Loss: 0.05566815659403801\n",
      "Iteration 16783, Loss: 0.055705390870571136\n",
      "Iteration 16784, Loss: 0.05570566654205322\n",
      "Iteration 16785, Loss: 0.055653415620326996\n",
      "Iteration 16786, Loss: 0.055657826364040375\n",
      "Iteration 16787, Loss: 0.05568560212850571\n",
      "Iteration 16788, Loss: 0.05565973371267319\n",
      "Iteration 16789, Loss: 0.055709801614284515\n",
      "Iteration 16790, Loss: 0.055716875940561295\n",
      "Iteration 16791, Loss: 0.05563116446137428\n",
      "Iteration 16792, Loss: 0.05571194738149643\n",
      "Iteration 16793, Loss: 0.05564729496836662\n",
      "Iteration 16794, Loss: 0.05575438588857651\n",
      "Iteration 16795, Loss: 0.05581188574433327\n",
      "Iteration 16796, Loss: 0.05576908588409424\n",
      "Iteration 16797, Loss: 0.05564121529459953\n",
      "Iteration 16798, Loss: 0.05584661290049553\n",
      "Iteration 16799, Loss: 0.055919330567121506\n",
      "Iteration 16800, Loss: 0.055811408907175064\n",
      "Iteration 16801, Loss: 0.05566271394491196\n",
      "Iteration 16802, Loss: 0.055748265236616135\n",
      "Iteration 16803, Loss: 0.05573173612356186\n",
      "Iteration 16804, Loss: 0.05562428757548332\n",
      "Iteration 16805, Loss: 0.05585082620382309\n",
      "Iteration 16806, Loss: 0.055908363312482834\n",
      "Iteration 16807, Loss: 0.05578593537211418\n",
      "Iteration 16808, Loss: 0.055691126734018326\n",
      "Iteration 16809, Loss: 0.055785298347473145\n",
      "Iteration 16810, Loss: 0.05577588081359863\n",
      "Iteration 16811, Loss: 0.055673323571681976\n",
      "Iteration 16812, Loss: 0.05577906221151352\n",
      "Iteration 16813, Loss: 0.05583270639181137\n",
      "Iteration 16814, Loss: 0.05570737645030022\n",
      "Iteration 16815, Loss: 0.05575045198202133\n",
      "Iteration 16816, Loss: 0.05584597587585449\n",
      "Iteration 16817, Loss: 0.055837396532297134\n",
      "Iteration 16818, Loss: 0.05573539063334465\n",
      "Iteration 16819, Loss: 0.055694662034511566\n",
      "Iteration 16820, Loss: 0.05574890226125717\n",
      "Iteration 16821, Loss: 0.05562857910990715\n",
      "Iteration 16822, Loss: 0.05579519644379616\n",
      "Iteration 16823, Loss: 0.05587732791900635\n",
      "Iteration 16824, Loss: 0.05585654824972153\n",
      "Iteration 16825, Loss: 0.05574322119355202\n",
      "Iteration 16826, Loss: 0.05569815635681152\n",
      "Iteration 16827, Loss: 0.055766306817531586\n",
      "Iteration 16828, Loss: 0.055657707154750824\n",
      "Iteration 16829, Loss: 0.055775921791791916\n",
      "Iteration 16830, Loss: 0.055861156433820724\n",
      "Iteration 16831, Loss: 0.05584351345896721\n",
      "Iteration 16832, Loss: 0.05573384091258049\n",
      "Iteration 16833, Loss: 0.05570594593882561\n",
      "Iteration 16834, Loss: 0.05576900765299797\n",
      "Iteration 16835, Loss: 0.05565440654754639\n",
      "Iteration 16836, Loss: 0.05578220263123512\n",
      "Iteration 16837, Loss: 0.055871132761240005\n",
      "Iteration 16838, Loss: 0.05585682392120361\n",
      "Iteration 16839, Loss: 0.055749695748090744\n",
      "Iteration 16840, Loss: 0.05568055436015129\n",
      "Iteration 16841, Loss: 0.05573999881744385\n",
      "Iteration 16842, Loss: 0.055621188133955\n",
      "Iteration 16843, Loss: 0.0558086633682251\n",
      "Iteration 16844, Loss: 0.055899303406476974\n",
      "Iteration 16845, Loss: 0.05588551610708237\n",
      "Iteration 16846, Loss: 0.055778663605451584\n",
      "Iteration 16847, Loss: 0.05564344301819801\n",
      "Iteration 16848, Loss: 0.055716197937726974\n",
      "Iteration 16849, Loss: 0.05563442036509514\n",
      "Iteration 16850, Loss: 0.05574099346995354\n",
      "Iteration 16851, Loss: 0.05576590821146965\n",
      "Iteration 16852, Loss: 0.055682938545942307\n",
      "Iteration 16853, Loss: 0.05573900789022446\n",
      "Iteration 16854, Loss: 0.05577830597758293\n",
      "Iteration 16855, Loss: 0.05565889924764633\n",
      "Iteration 16856, Loss: 0.0557868517935276\n",
      "Iteration 16857, Loss: 0.055872559547424316\n",
      "Iteration 16858, Loss: 0.05584200471639633\n",
      "Iteration 16859, Loss: 0.055715203285217285\n",
      "Iteration 16860, Loss: 0.05575176328420639\n",
      "Iteration 16861, Loss: 0.05583524703979492\n",
      "Iteration 16862, Loss: 0.05575693026185036\n",
      "Iteration 16863, Loss: 0.05568007752299309\n",
      "Iteration 16864, Loss: 0.055742066353559494\n",
      "Iteration 16865, Loss: 0.05569835752248764\n",
      "Iteration 16866, Loss: 0.05567968264222145\n",
      "Iteration 16867, Loss: 0.0556875504553318\n",
      "Iteration 16868, Loss: 0.055673997849226\n",
      "Iteration 16869, Loss: 0.05568047612905502\n",
      "Iteration 16870, Loss: 0.05564972013235092\n",
      "Iteration 16871, Loss: 0.055672407150268555\n",
      "Iteration 16872, Loss: 0.055652979761362076\n",
      "Iteration 16873, Loss: 0.055655598640441895\n",
      "Iteration 16874, Loss: 0.055660687386989594\n",
      "Iteration 16875, Loss: 0.05565166473388672\n",
      "Iteration 16876, Loss: 0.05564491078257561\n",
      "Iteration 16877, Loss: 0.05567821115255356\n",
      "Iteration 16878, Loss: 0.05563334748148918\n",
      "Iteration 16879, Loss: 0.05569811910390854\n",
      "Iteration 16880, Loss: 0.055663589388132095\n",
      "Iteration 16881, Loss: 0.055717431008815765\n",
      "Iteration 16882, Loss: 0.05573026463389397\n",
      "Iteration 16883, Loss: 0.05563577264547348\n",
      "Iteration 16884, Loss: 0.05575720593333244\n",
      "Iteration 16885, Loss: 0.055754464119672775\n",
      "Iteration 16886, Loss: 0.055625639855861664\n",
      "Iteration 16887, Loss: 0.05569756403565407\n",
      "Iteration 16888, Loss: 0.055638235062360764\n",
      "Iteration 16889, Loss: 0.05576388165354729\n",
      "Iteration 16890, Loss: 0.055798254907131195\n",
      "Iteration 16891, Loss: 0.055707577615976334\n",
      "Iteration 16892, Loss: 0.05572100728750229\n",
      "Iteration 16893, Loss: 0.055777113884687424\n",
      "Iteration 16894, Loss: 0.055690448731184006\n",
      "Iteration 16895, Loss: 0.05573948472738266\n",
      "Iteration 16896, Loss: 0.05579972267150879\n",
      "Iteration 16897, Loss: 0.0557326078414917\n",
      "Iteration 16898, Loss: 0.05567745491862297\n",
      "Iteration 16899, Loss: 0.0557272844016552\n",
      "Iteration 16900, Loss: 0.05564375966787338\n",
      "Iteration 16901, Loss: 0.05577186867594719\n",
      "Iteration 16902, Loss: 0.05583024024963379\n",
      "Iteration 16903, Loss: 0.05577763170003891\n",
      "Iteration 16904, Loss: 0.055657386779785156\n",
      "Iteration 16905, Loss: 0.05580238625407219\n",
      "Iteration 16906, Loss: 0.05584196373820305\n",
      "Iteration 16907, Loss: 0.05570884793996811\n",
      "Iteration 16908, Loss: 0.05575716495513916\n",
      "Iteration 16909, Loss: 0.05585635080933571\n",
      "Iteration 16910, Loss: 0.05584915727376938\n",
      "Iteration 16911, Loss: 0.05574890226125717\n",
      "Iteration 16912, Loss: 0.05568210408091545\n",
      "Iteration 16913, Loss: 0.055748384445905685\n",
      "Iteration 16914, Loss: 0.055664658546447754\n",
      "Iteration 16915, Loss: 0.05574246495962143\n",
      "Iteration 16916, Loss: 0.05579785630106926\n",
      "Iteration 16917, Loss: 0.05575462430715561\n",
      "Iteration 16918, Loss: 0.05562317371368408\n",
      "Iteration 16919, Loss: 0.055869344621896744\n",
      "Iteration 16920, Loss: 0.05594849959015846\n",
      "Iteration 16921, Loss: 0.05585638806223869\n",
      "Iteration 16922, Loss: 0.05564781278371811\n",
      "Iteration 16923, Loss: 0.05581387132406235\n",
      "Iteration 16924, Loss: 0.05590160936117172\n",
      "Iteration 16925, Loss: 0.05586898699402809\n",
      "Iteration 16926, Loss: 0.05573137849569321\n",
      "Iteration 16927, Loss: 0.055741630494594574\n",
      "Iteration 16928, Loss: 0.05583779141306877\n",
      "Iteration 16929, Loss: 0.055774133652448654\n",
      "Iteration 16930, Loss: 0.05565027520060539\n",
      "Iteration 16931, Loss: 0.05570407956838608\n",
      "Iteration 16932, Loss: 0.05564276501536369\n",
      "Iteration 16933, Loss: 0.05576348304748535\n",
      "Iteration 16934, Loss: 0.05578116700053215\n",
      "Iteration 16935, Loss: 0.05565234273672104\n",
      "Iteration 16936, Loss: 0.055793046951293945\n",
      "Iteration 16937, Loss: 0.05587267875671387\n",
      "Iteration 16938, Loss: 0.05582758039236069\n",
      "Iteration 16939, Loss: 0.05568365380167961\n",
      "Iteration 16940, Loss: 0.05579908937215805\n",
      "Iteration 16941, Loss: 0.05589143559336662\n",
      "Iteration 16942, Loss: 0.0558241605758667\n",
      "Iteration 16943, Loss: 0.05562667176127434\n",
      "Iteration 16944, Loss: 0.05583048239350319\n",
      "Iteration 16945, Loss: 0.05590729042887688\n",
      "Iteration 16946, Loss: 0.055846333503723145\n",
      "Iteration 16947, Loss: 0.0556846484541893\n",
      "Iteration 16948, Loss: 0.055808186531066895\n",
      "Iteration 16949, Loss: 0.0559111051261425\n",
      "Iteration 16950, Loss: 0.05586278438568115\n",
      "Iteration 16951, Loss: 0.05567912384867668\n",
      "Iteration 16952, Loss: 0.05582742020487785\n",
      "Iteration 16953, Loss: 0.05595799535512924\n",
      "Iteration 16954, Loss: 0.05595064163208008\n",
      "Iteration 16955, Loss: 0.05582614988088608\n",
      "Iteration 16956, Loss: 0.05567566677927971\n",
      "Iteration 16957, Loss: 0.055822890251874924\n",
      "Iteration 16958, Loss: 0.05587407201528549\n",
      "Iteration 16959, Loss: 0.055770277976989746\n",
      "Iteration 16960, Loss: 0.05569072812795639\n",
      "Iteration 16961, Loss: 0.0557689294219017\n",
      "Iteration 16962, Loss: 0.055739760398864746\n",
      "Iteration 16963, Loss: 0.05565452575683594\n",
      "Iteration 16964, Loss: 0.05575815960764885\n",
      "Iteration 16965, Loss: 0.055749695748090744\n",
      "Iteration 16966, Loss: 0.055650196969509125\n",
      "Iteration 16967, Loss: 0.0556943453848362\n",
      "Iteration 16968, Loss: 0.055665772408246994\n",
      "Iteration 16969, Loss: 0.055704038590192795\n",
      "Iteration 16970, Loss: 0.055680714547634125\n",
      "Iteration 16971, Loss: 0.05569879338145256\n",
      "Iteration 16972, Loss: 0.055727362632751465\n",
      "Iteration 16973, Loss: 0.05565520375967026\n",
      "Iteration 16974, Loss: 0.05576670542359352\n",
      "Iteration 16975, Loss: 0.05579026788473129\n",
      "Iteration 16976, Loss: 0.055648528039455414\n",
      "Iteration 16977, Loss: 0.055802904069423676\n",
      "Iteration 16978, Loss: 0.05589834973216057\n",
      "Iteration 16979, Loss: 0.05587732791900635\n",
      "Iteration 16980, Loss: 0.055755458772182465\n",
      "Iteration 16981, Loss: 0.05569982901215553\n",
      "Iteration 16982, Loss: 0.0557866096496582\n",
      "Iteration 16983, Loss: 0.055711787194013596\n",
      "Iteration 16984, Loss: 0.055718183517456055\n",
      "Iteration 16985, Loss: 0.055780015885829926\n",
      "Iteration 16986, Loss: 0.05574055761098862\n",
      "Iteration 16987, Loss: 0.05564940348267555\n",
      "Iteration 16988, Loss: 0.05576261132955551\n",
      "Iteration 16989, Loss: 0.055749814957380295\n",
      "Iteration 16990, Loss: 0.05565647408366203\n",
      "Iteration 16991, Loss: 0.055706899613142014\n",
      "Iteration 16992, Loss: 0.05568476766347885\n",
      "Iteration 16993, Loss: 0.05567880719900131\n",
      "Iteration 16994, Loss: 0.055663906037807465\n",
      "Iteration 16995, Loss: 0.05569450184702873\n",
      "Iteration 16996, Loss: 0.0557025708258152\n",
      "Iteration 16997, Loss: 0.05562031641602516\n",
      "Iteration 16998, Loss: 0.05564745515584946\n",
      "Iteration 16999, Loss: 0.05566537380218506\n",
      "Iteration 17000, Loss: 0.05562528222799301\n",
      "Iteration 17001, Loss: 0.05576062574982643\n",
      "Iteration 17002, Loss: 0.05575788393616676\n",
      "Iteration 17003, Loss: 0.0556388720870018\n",
      "Iteration 17004, Loss: 0.055753909051418304\n",
      "Iteration 17005, Loss: 0.05576324462890625\n",
      "Iteration 17006, Loss: 0.0556388720870018\n",
      "Iteration 17007, Loss: 0.05581625550985336\n",
      "Iteration 17008, Loss: 0.055891554802656174\n",
      "Iteration 17009, Loss: 0.05582531541585922\n",
      "Iteration 17010, Loss: 0.05566751956939697\n",
      "Iteration 17011, Loss: 0.05581188574433327\n",
      "Iteration 17012, Loss: 0.05589529126882553\n",
      "Iteration 17013, Loss: 0.05582849308848381\n",
      "Iteration 17014, Loss: 0.05563318729400635\n",
      "Iteration 17015, Loss: 0.05586850643157959\n",
      "Iteration 17016, Loss: 0.05598720163106918\n",
      "Iteration 17017, Loss: 0.055960021913051605\n",
      "Iteration 17018, Loss: 0.0558113269507885\n",
      "Iteration 17019, Loss: 0.05569688603281975\n",
      "Iteration 17020, Loss: 0.05582539364695549\n",
      "Iteration 17021, Loss: 0.05584287643432617\n",
      "Iteration 17022, Loss: 0.055721525102853775\n",
      "Iteration 17023, Loss: 0.05574754998087883\n",
      "Iteration 17024, Loss: 0.05583469197154045\n",
      "Iteration 17025, Loss: 0.055802226066589355\n",
      "Iteration 17026, Loss: 0.055688463151454926\n",
      "Iteration 17027, Loss: 0.05577000230550766\n",
      "Iteration 17028, Loss: 0.05582952871918678\n",
      "Iteration 17029, Loss: 0.0557326078414917\n",
      "Iteration 17030, Loss: 0.05571524426341057\n",
      "Iteration 17031, Loss: 0.05578915402293205\n",
      "Iteration 17032, Loss: 0.05575474351644516\n",
      "Iteration 17033, Loss: 0.055653929710388184\n",
      "Iteration 17034, Loss: 0.05578148365020752\n",
      "Iteration 17035, Loss: 0.05579996481537819\n",
      "Iteration 17036, Loss: 0.0556512288749218\n",
      "Iteration 17037, Loss: 0.05580361932516098\n",
      "Iteration 17038, Loss: 0.055908285081386566\n",
      "Iteration 17039, Loss: 0.0559084415435791\n",
      "Iteration 17040, Loss: 0.055814146995544434\n",
      "Iteration 17041, Loss: 0.05563724413514137\n",
      "Iteration 17042, Loss: 0.055917780846357346\n",
      "Iteration 17043, Loss: 0.05605078116059303\n",
      "Iteration 17044, Loss: 0.05599534511566162\n",
      "Iteration 17045, Loss: 0.05577067658305168\n",
      "Iteration 17046, Loss: 0.05576944351196289\n",
      "Iteration 17047, Loss: 0.055924657732248306\n",
      "Iteration 17048, Loss: 0.05596916005015373\n",
      "Iteration 17049, Loss: 0.05591380596160889\n",
      "Iteration 17050, Loss: 0.055769406259059906\n",
      "Iteration 17051, Loss: 0.05570018291473389\n",
      "Iteration 17052, Loss: 0.05580071732401848\n",
      "Iteration 17053, Loss: 0.055719099938869476\n",
      "Iteration 17054, Loss: 0.05571337789297104\n",
      "Iteration 17055, Loss: 0.05578283593058586\n",
      "Iteration 17056, Loss: 0.055751919746398926\n",
      "Iteration 17057, Loss: 0.05563243478536606\n",
      "Iteration 17058, Loss: 0.055852413177490234\n",
      "Iteration 17059, Loss: 0.055922310799360275\n",
      "Iteration 17060, Loss: 0.0558137521147728\n",
      "Iteration 17061, Loss: 0.05566283315420151\n",
      "Iteration 17062, Loss: 0.055752161890268326\n",
      "Iteration 17063, Loss: 0.055742502212524414\n",
      "Iteration 17064, Loss: 0.05563930794596672\n",
      "Iteration 17065, Loss: 0.055824361741542816\n",
      "Iteration 17066, Loss: 0.05588126182556152\n",
      "Iteration 17067, Loss: 0.055768489837646484\n",
      "Iteration 17068, Loss: 0.05569648742675781\n",
      "Iteration 17069, Loss: 0.05578470602631569\n",
      "Iteration 17070, Loss: 0.05576900765299797\n",
      "Iteration 17071, Loss: 0.055655401200056076\n",
      "Iteration 17072, Loss: 0.0558137521147728\n",
      "Iteration 17073, Loss: 0.055882930755615234\n",
      "Iteration 17074, Loss: 0.05578351393342018\n",
      "Iteration 17075, Loss: 0.05567634478211403\n",
      "Iteration 17076, Loss: 0.05575978755950928\n",
      "Iteration 17077, Loss: 0.05574027821421623\n",
      "Iteration 17078, Loss: 0.05561872571706772\n",
      "Iteration 17079, Loss: 0.05586012452840805\n",
      "Iteration 17080, Loss: 0.05593884363770485\n",
      "Iteration 17081, Loss: 0.05585933104157448\n",
      "Iteration 17082, Loss: 0.05566807836294174\n",
      "Iteration 17083, Loss: 0.0558243989944458\n",
      "Iteration 17084, Loss: 0.05593494698405266\n",
      "Iteration 17085, Loss: 0.05591130629181862\n",
      "Iteration 17086, Loss: 0.055769287049770355\n",
      "Iteration 17087, Loss: 0.055707257241010666\n",
      "Iteration 17088, Loss: 0.0558137521147728\n",
      "Iteration 17089, Loss: 0.05577453225851059\n",
      "Iteration 17090, Loss: 0.05562595650553703\n",
      "Iteration 17091, Loss: 0.05565544217824936\n",
      "Iteration 17092, Loss: 0.05566132068634033\n",
      "Iteration 17093, Loss: 0.05561566352844238\n",
      "Iteration 17094, Loss: 0.0557611808180809\n",
      "Iteration 17095, Loss: 0.05574452877044678\n",
      "Iteration 17096, Loss: 0.05564745515584946\n",
      "Iteration 17097, Loss: 0.055700063705444336\n",
      "Iteration 17098, Loss: 0.05564793199300766\n",
      "Iteration 17099, Loss: 0.05575172230601311\n",
      "Iteration 17100, Loss: 0.05578017607331276\n",
      "Iteration 17101, Loss: 0.05569016933441162\n",
      "Iteration 17102, Loss: 0.05573571100831032\n",
      "Iteration 17103, Loss: 0.055785421282052994\n",
      "Iteration 17104, Loss: 0.05569291487336159\n",
      "Iteration 17105, Loss: 0.055742304772138596\n",
      "Iteration 17106, Loss: 0.055807195603847504\n",
      "Iteration 17107, Loss: 0.055747270584106445\n",
      "Iteration 17108, Loss: 0.05566342920064926\n",
      "Iteration 17109, Loss: 0.055734436959028244\n",
      "Iteration 17110, Loss: 0.05568528175354004\n",
      "Iteration 17111, Loss: 0.05571882054209709\n",
      "Iteration 17112, Loss: 0.055759310722351074\n",
      "Iteration 17113, Loss: 0.05570336431264877\n",
      "Iteration 17114, Loss: 0.0556899718940258\n",
      "Iteration 17115, Loss: 0.055709801614284515\n",
      "Iteration 17116, Loss: 0.05565591901540756\n",
      "Iteration 17117, Loss: 0.055675070732831955\n",
      "Iteration 17118, Loss: 0.055652301758527756\n",
      "Iteration 17119, Loss: 0.055702291429042816\n",
      "Iteration 17120, Loss: 0.0556640662252903\n",
      "Iteration 17121, Loss: 0.055714331567287445\n",
      "Iteration 17122, Loss: 0.0557405948638916\n",
      "Iteration 17123, Loss: 0.055659811943769455\n",
      "Iteration 17124, Loss: 0.0557686910033226\n",
      "Iteration 17125, Loss: 0.05580385774374008\n",
      "Iteration 17126, Loss: 0.05567697808146477\n",
      "Iteration 17127, Loss: 0.05577484890818596\n",
      "Iteration 17128, Loss: 0.05586489289999008\n",
      "Iteration 17129, Loss: 0.05583568662405014\n",
      "Iteration 17130, Loss: 0.0557052306830883\n",
      "Iteration 17131, Loss: 0.05576543137431145\n",
      "Iteration 17132, Loss: 0.05585002899169922\n",
      "Iteration 17133, Loss: 0.05576523393392563\n",
      "Iteration 17134, Loss: 0.05567725747823715\n",
      "Iteration 17135, Loss: 0.05574564263224602\n",
      "Iteration 17136, Loss: 0.0557020902633667\n",
      "Iteration 17137, Loss: 0.05567491054534912\n",
      "Iteration 17138, Loss: 0.055679917335510254\n",
      "Iteration 17139, Loss: 0.05568142980337143\n",
      "Iteration 17140, Loss: 0.0556894950568676\n",
      "Iteration 17141, Loss: 0.055640220642089844\n",
      "Iteration 17142, Loss: 0.05568210408091545\n",
      "Iteration 17143, Loss: 0.05564121529459953\n",
      "Iteration 17144, Loss: 0.055676501244306564\n",
      "Iteration 17145, Loss: 0.055631041526794434\n",
      "Iteration 17146, Loss: 0.055759310722351074\n",
      "Iteration 17147, Loss: 0.05578303709626198\n",
      "Iteration 17148, Loss: 0.055686913430690765\n",
      "Iteration 17149, Loss: 0.055743854492902756\n",
      "Iteration 17150, Loss: 0.05579865351319313\n",
      "Iteration 17151, Loss: 0.05570785328745842\n",
      "Iteration 17152, Loss: 0.05572379007935524\n",
      "Iteration 17153, Loss: 0.05578760430216789\n",
      "Iteration 17154, Loss: 0.05572390928864479\n",
      "Iteration 17155, Loss: 0.055678848177194595\n",
      "Iteration 17156, Loss: 0.055719614028930664\n",
      "Iteration 17157, Loss: 0.05562233924865723\n",
      "Iteration 17158, Loss: 0.055782001465559006\n",
      "Iteration 17159, Loss: 0.05584108829498291\n",
      "Iteration 17160, Loss: 0.055792175233364105\n",
      "Iteration 17161, Loss: 0.055662237107753754\n",
      "Iteration 17162, Loss: 0.055817604064941406\n",
      "Iteration 17163, Loss: 0.0558878593146801\n",
      "Iteration 17164, Loss: 0.05577981472015381\n",
      "Iteration 17165, Loss: 0.055685799568891525\n",
      "Iteration 17166, Loss: 0.05577051639556885\n",
      "Iteration 17167, Loss: 0.055751800537109375\n",
      "Iteration 17168, Loss: 0.05564677715301514\n",
      "Iteration 17169, Loss: 0.055810414254665375\n",
      "Iteration 17170, Loss: 0.05585766211152077\n",
      "Iteration 17171, Loss: 0.05572768300771713\n",
      "Iteration 17172, Loss: 0.05573781579732895\n",
      "Iteration 17173, Loss: 0.05583596229553223\n",
      "Iteration 17174, Loss: 0.055830519646406174\n",
      "Iteration 17175, Loss: 0.05573205277323723\n",
      "Iteration 17176, Loss: 0.055693548172712326\n",
      "Iteration 17177, Loss: 0.05574425309896469\n",
      "Iteration 17178, Loss: 0.05562838166952133\n",
      "Iteration 17179, Loss: 0.055773138999938965\n",
      "Iteration 17180, Loss: 0.05583135411143303\n",
      "Iteration 17181, Loss: 0.05578482151031494\n",
      "Iteration 17182, Loss: 0.055651307106018066\n",
      "Iteration 17183, Loss: 0.055835090577602386\n",
      "Iteration 17184, Loss: 0.055914126336574554\n",
      "Iteration 17185, Loss: 0.0558164119720459\n",
      "Iteration 17186, Loss: 0.05564912408590317\n",
      "Iteration 17187, Loss: 0.05573026463389397\n",
      "Iteration 17188, Loss: 0.05570781230926514\n",
      "Iteration 17189, Loss: 0.05563708394765854\n",
      "Iteration 17190, Loss: 0.055626869201660156\n",
      "Iteration 17191, Loss: 0.05571385473012924\n",
      "Iteration 17192, Loss: 0.055717628449201584\n",
      "Iteration 17193, Loss: 0.05562949180603027\n",
      "Iteration 17194, Loss: 0.0557963065803051\n",
      "Iteration 17195, Loss: 0.055810414254665375\n",
      "Iteration 17196, Loss: 0.05565059185028076\n",
      "Iteration 17197, Loss: 0.055814068764448166\n",
      "Iteration 17198, Loss: 0.055930059403181076\n",
      "Iteration 17199, Loss: 0.05593935772776604\n",
      "Iteration 17200, Loss: 0.0558522567152977\n",
      "Iteration 17201, Loss: 0.05568122863769531\n",
      "Iteration 17202, Loss: 0.055850785225629807\n",
      "Iteration 17203, Loss: 0.055979494005441666\n",
      "Iteration 17204, Loss: 0.055920880287885666\n",
      "Iteration 17205, Loss: 0.055695537477731705\n",
      "Iteration 17206, Loss: 0.05582384392619133\n",
      "Iteration 17207, Loss: 0.055978935211896896\n",
      "Iteration 17208, Loss: 0.05602371692657471\n",
      "Iteration 17209, Loss: 0.05596832558512688\n",
      "Iteration 17210, Loss: 0.0558241605758667\n",
      "Iteration 17211, Loss: 0.05562698841094971\n",
      "Iteration 17212, Loss: 0.05577949807047844\n",
      "Iteration 17213, Loss: 0.05576201528310776\n",
      "Iteration 17214, Loss: 0.05564570799469948\n",
      "Iteration 17215, Loss: 0.05572080612182617\n",
      "Iteration 17216, Loss: 0.05570761486887932\n",
      "Iteration 17217, Loss: 0.05564705654978752\n",
      "Iteration 17218, Loss: 0.05563787743449211\n",
      "Iteration 17219, Loss: 0.05570852756500244\n",
      "Iteration 17220, Loss: 0.05568993464112282\n",
      "Iteration 17221, Loss: 0.055675946176052094\n",
      "Iteration 17222, Loss: 0.055682223290205\n",
      "Iteration 17223, Loss: 0.05566056817770004\n",
      "Iteration 17224, Loss: 0.055641017854213715\n",
      "Iteration 17225, Loss: 0.05572124570608139\n",
      "Iteration 17226, Loss: 0.05571536347270012\n",
      "Iteration 17227, Loss: 0.055646102875471115\n",
      "Iteration 17228, Loss: 0.055675309151411057\n",
      "Iteration 17229, Loss: 0.0556492805480957\n",
      "Iteration 17230, Loss: 0.05564534664154053\n",
      "Iteration 17231, Loss: 0.05565845966339111\n",
      "Iteration 17232, Loss: 0.05564987659454346\n",
      "Iteration 17233, Loss: 0.055638235062360764\n",
      "Iteration 17234, Loss: 0.055674437433481216\n",
      "Iteration 17235, Loss: 0.05563255399465561\n",
      "Iteration 17236, Loss: 0.05566132441163063\n",
      "Iteration 17237, Loss: 0.05563191697001457\n",
      "Iteration 17238, Loss: 0.05565420910716057\n",
      "Iteration 17239, Loss: 0.0556306466460228\n",
      "Iteration 17240, Loss: 0.05566426366567612\n",
      "Iteration 17241, Loss: 0.05563469976186752\n",
      "Iteration 17242, Loss: 0.055652979761362076\n",
      "Iteration 17243, Loss: 0.05566342920064926\n",
      "Iteration 17244, Loss: 0.055644672363996506\n",
      "Iteration 17245, Loss: 0.05571075528860092\n",
      "Iteration 17246, Loss: 0.055670980364084244\n",
      "Iteration 17247, Loss: 0.055719178169965744\n",
      "Iteration 17248, Loss: 0.05576284974813461\n",
      "Iteration 17249, Loss: 0.055708013474941254\n",
      "Iteration 17250, Loss: 0.05567241087555885\n",
      "Iteration 17251, Loss: 0.05567729473114014\n",
      "Iteration 17252, Loss: 0.05568472668528557\n",
      "Iteration 17253, Loss: 0.05570145696401596\n",
      "Iteration 17254, Loss: 0.05562679097056389\n",
      "Iteration 17255, Loss: 0.05579686164855957\n",
      "Iteration 17256, Loss: 0.055810295045375824\n",
      "Iteration 17257, Loss: 0.055650871247053146\n",
      "Iteration 17258, Loss: 0.055812638252973557\n",
      "Iteration 17259, Loss: 0.05592648312449455\n",
      "Iteration 17260, Loss: 0.055931054055690765\n",
      "Iteration 17261, Loss: 0.05583763122558594\n",
      "Iteration 17262, Loss: 0.055663108825683594\n",
      "Iteration 17263, Loss: 0.055876851081848145\n",
      "Iteration 17264, Loss: 0.05600504204630852\n",
      "Iteration 17265, Loss: 0.05594547837972641\n",
      "Iteration 17266, Loss: 0.05571866035461426\n",
      "Iteration 17267, Loss: 0.05580922216176987\n",
      "Iteration 17268, Loss: 0.05596574395895004\n",
      "Iteration 17269, Loss: 0.05601155757904053\n",
      "Iteration 17270, Loss: 0.055956125259399414\n",
      "Iteration 17271, Loss: 0.05581160634756088\n",
      "Iteration 17272, Loss: 0.05565015599131584\n",
      "Iteration 17273, Loss: 0.0557883195579052\n",
      "Iteration 17274, Loss: 0.055762290954589844\n",
      "Iteration 17275, Loss: 0.055657029151916504\n",
      "Iteration 17276, Loss: 0.0557200126349926\n",
      "Iteration 17277, Loss: 0.055708687752485275\n",
      "Iteration 17278, Loss: 0.05564248561859131\n",
      "Iteration 17279, Loss: 0.05567046254873276\n",
      "Iteration 17280, Loss: 0.05564673990011215\n",
      "Iteration 17281, Loss: 0.05569048970937729\n",
      "Iteration 17282, Loss: 0.05563485994935036\n",
      "Iteration 17283, Loss: 0.05574024096131325\n",
      "Iteration 17284, Loss: 0.055777668952941895\n",
      "Iteration 17285, Loss: 0.055717311799526215\n",
      "Iteration 17286, Loss: 0.05566748231649399\n",
      "Iteration 17287, Loss: 0.055676382035017014\n",
      "Iteration 17288, Loss: 0.055683813989162445\n",
      "Iteration 17289, Loss: 0.05569899082183838\n",
      "Iteration 17290, Loss: 0.05561959743499756\n",
      "Iteration 17291, Loss: 0.05582018941640854\n",
      "Iteration 17292, Loss: 0.05584518238902092\n",
      "Iteration 17293, Loss: 0.055694304406642914\n",
      "Iteration 17294, Loss: 0.055776797235012054\n",
      "Iteration 17295, Loss: 0.0558878593146801\n",
      "Iteration 17296, Loss: 0.0558931864798069\n",
      "Iteration 17297, Loss: 0.05580341815948486\n",
      "Iteration 17298, Loss: 0.05563005059957504\n",
      "Iteration 17299, Loss: 0.055922627449035645\n",
      "Iteration 17300, Loss: 0.05605260655283928\n",
      "Iteration 17301, Loss: 0.05599546432495117\n",
      "Iteration 17302, Loss: 0.0557713508605957\n",
      "Iteration 17303, Loss: 0.055768612772226334\n",
      "Iteration 17304, Loss: 0.055922869592905045\n",
      "Iteration 17305, Loss: 0.055967967957258224\n",
      "Iteration 17306, Loss: 0.0559135302901268\n",
      "Iteration 17307, Loss: 0.055770598351955414\n",
      "Iteration 17308, Loss: 0.055696409195661545\n",
      "Iteration 17309, Loss: 0.055795393884181976\n",
      "Iteration 17310, Loss: 0.055713098496198654\n",
      "Iteration 17311, Loss: 0.05571683496236801\n",
      "Iteration 17312, Loss: 0.055786292999982834\n",
      "Iteration 17313, Loss: 0.0557551383972168\n",
      "Iteration 17314, Loss: 0.055633269250392914\n",
      "Iteration 17315, Loss: 0.055856190621852875\n",
      "Iteration 17316, Loss: 0.055931925773620605\n",
      "Iteration 17317, Loss: 0.05582837387919426\n",
      "Iteration 17318, Loss: 0.05564844608306885\n",
      "Iteration 17319, Loss: 0.05573805421590805\n",
      "Iteration 17320, Loss: 0.055728912353515625\n",
      "Iteration 17321, Loss: 0.055624525994062424\n",
      "Iteration 17322, Loss: 0.055843036621809006\n",
      "Iteration 17323, Loss: 0.055901847779750824\n",
      "Iteration 17324, Loss: 0.05579233169555664\n",
      "Iteration 17325, Loss: 0.055677175521850586\n",
      "Iteration 17326, Loss: 0.055767618119716644\n",
      "Iteration 17327, Loss: 0.0557560920715332\n",
      "Iteration 17328, Loss: 0.05564260482788086\n",
      "Iteration 17329, Loss: 0.055826663970947266\n",
      "Iteration 17330, Loss: 0.05589747428894043\n",
      "Iteration 17331, Loss: 0.05580636113882065\n",
      "Iteration 17332, Loss: 0.05565913766622543\n",
      "Iteration 17333, Loss: 0.055762410163879395\n",
      "Iteration 17334, Loss: 0.05576908960938454\n",
      "Iteration 17335, Loss: 0.0556560754776001\n",
      "Iteration 17336, Loss: 0.05580258369445801\n",
      "Iteration 17337, Loss: 0.05587772652506828\n",
      "Iteration 17338, Loss: 0.055807195603847504\n",
      "Iteration 17339, Loss: 0.05565798282623291\n",
      "Iteration 17340, Loss: 0.055793724954128265\n",
      "Iteration 17341, Loss: 0.05584144592285156\n",
      "Iteration 17342, Loss: 0.055742863565683365\n",
      "Iteration 17343, Loss: 0.0556999072432518\n",
      "Iteration 17344, Loss: 0.05577107518911362\n",
      "Iteration 17345, Loss: 0.055714767426252365\n",
      "Iteration 17346, Loss: 0.05568131059408188\n",
      "Iteration 17347, Loss: 0.055712345987558365\n",
      "Iteration 17348, Loss: 0.05562285706400871\n",
      "Iteration 17349, Loss: 0.05565011501312256\n",
      "Iteration 17350, Loss: 0.05563601106405258\n",
      "Iteration 17351, Loss: 0.05566664785146713\n",
      "Iteration 17352, Loss: 0.05563784018158913\n",
      "Iteration 17353, Loss: 0.0557355098426342\n",
      "Iteration 17354, Loss: 0.0557103157043457\n",
      "Iteration 17355, Loss: 0.05568047612905502\n",
      "Iteration 17356, Loss: 0.055714886635541916\n",
      "Iteration 17357, Loss: 0.0556509904563427\n",
      "Iteration 17358, Loss: 0.055760860443115234\n",
      "Iteration 17359, Loss: 0.05577429383993149\n",
      "Iteration 17360, Loss: 0.05562416836619377\n",
      "Iteration 17361, Loss: 0.05580183118581772\n",
      "Iteration 17362, Loss: 0.055876415222883224\n",
      "Iteration 17363, Loss: 0.05583930015563965\n",
      "Iteration 17364, Loss: 0.055708885192871094\n",
      "Iteration 17365, Loss: 0.05576273053884506\n",
      "Iteration 17366, Loss: 0.05584852024912834\n",
      "Iteration 17367, Loss: 0.055767618119716644\n",
      "Iteration 17368, Loss: 0.055672965943813324\n",
      "Iteration 17369, Loss: 0.055738091468811035\n",
      "Iteration 17370, Loss: 0.055697761476039886\n",
      "Iteration 17371, Loss: 0.05567427724599838\n",
      "Iteration 17372, Loss: 0.05567670240998268\n",
      "Iteration 17373, Loss: 0.05568552017211914\n",
      "Iteration 17374, Loss: 0.05569648742675781\n",
      "Iteration 17375, Loss: 0.05563736334443092\n",
      "Iteration 17376, Loss: 0.055732373148202896\n",
      "Iteration 17377, Loss: 0.05569358915090561\n",
      "Iteration 17378, Loss: 0.05570169538259506\n",
      "Iteration 17379, Loss: 0.05574321746826172\n",
      "Iteration 17380, Loss: 0.05568528175354004\n",
      "Iteration 17381, Loss: 0.05570944398641586\n",
      "Iteration 17382, Loss: 0.055716514587402344\n",
      "Iteration 17383, Loss: 0.05565512180328369\n",
      "Iteration 17384, Loss: 0.05567241087555885\n",
      "Iteration 17385, Loss: 0.055636804550886154\n",
      "Iteration 17386, Loss: 0.05565110966563225\n",
      "Iteration 17387, Loss: 0.055621229112148285\n",
      "Iteration 17388, Loss: 0.05572112649679184\n",
      "Iteration 17389, Loss: 0.05569080635905266\n",
      "Iteration 17390, Loss: 0.055687032639980316\n",
      "Iteration 17391, Loss: 0.05570491403341293\n",
      "Iteration 17392, Loss: 0.055622898042201996\n",
      "Iteration 17393, Loss: 0.05563601106405258\n",
      "Iteration 17394, Loss: 0.055671971291303635\n",
      "Iteration 17395, Loss: 0.05562424659729004\n",
      "Iteration 17396, Loss: 0.05568695068359375\n",
      "Iteration 17397, Loss: 0.05561737343668938\n",
      "Iteration 17398, Loss: 0.0556621178984642\n",
      "Iteration 17399, Loss: 0.05566128343343735\n",
      "Iteration 17400, Loss: 0.055646538734436035\n",
      "Iteration 17401, Loss: 0.05570737645030022\n",
      "Iteration 17402, Loss: 0.05566521733999252\n",
      "Iteration 17403, Loss: 0.055726293474435806\n",
      "Iteration 17404, Loss: 0.05577163025736809\n",
      "Iteration 17405, Loss: 0.055717986077070236\n",
      "Iteration 17406, Loss: 0.05565913766622543\n",
      "Iteration 17407, Loss: 0.05566481873393059\n",
      "Iteration 17408, Loss: 0.05569283291697502\n",
      "Iteration 17409, Loss: 0.05570805445313454\n",
      "Iteration 17410, Loss: 0.055630724877119064\n",
      "Iteration 17411, Loss: 0.05580183118581772\n",
      "Iteration 17412, Loss: 0.0558244027197361\n",
      "Iteration 17413, Loss: 0.055674951523542404\n",
      "Iteration 17414, Loss: 0.0557887963950634\n",
      "Iteration 17415, Loss: 0.05589707940816879\n",
      "Iteration 17416, Loss: 0.05589946359395981\n",
      "Iteration 17417, Loss: 0.05580655857920647\n",
      "Iteration 17418, Loss: 0.05563148111104965\n",
      "Iteration 17419, Loss: 0.05591535568237305\n",
      "Iteration 17420, Loss: 0.056039970368146896\n",
      "Iteration 17421, Loss: 0.05597794055938721\n",
      "Iteration 17422, Loss: 0.05574878305196762\n",
      "Iteration 17423, Loss: 0.05578792467713356\n",
      "Iteration 17424, Loss: 0.05594559758901596\n",
      "Iteration 17425, Loss: 0.055993080139160156\n",
      "Iteration 17426, Loss: 0.05594074726104736\n",
      "Iteration 17427, Loss: 0.055799249559640884\n",
      "Iteration 17428, Loss: 0.05565512180328369\n",
      "Iteration 17429, Loss: 0.05575243756175041\n",
      "Iteration 17430, Loss: 0.05566783994436264\n",
      "Iteration 17431, Loss: 0.055751800537109375\n",
      "Iteration 17432, Loss: 0.05582265183329582\n",
      "Iteration 17433, Loss: 0.055792491883039474\n",
      "Iteration 17434, Loss: 0.05567149445414543\n",
      "Iteration 17435, Loss: 0.05580338090658188\n",
      "Iteration 17436, Loss: 0.05587788671255112\n",
      "Iteration 17437, Loss: 0.055772267282009125\n",
      "Iteration 17438, Loss: 0.05568937584757805\n",
      "Iteration 17439, Loss: 0.05577322095632553\n",
      "Iteration 17440, Loss: 0.05575522035360336\n",
      "Iteration 17441, Loss: 0.05564538761973381\n",
      "Iteration 17442, Loss: 0.05582527443766594\n",
      "Iteration 17443, Loss: 0.055887620896101\n",
      "Iteration 17444, Loss: 0.05577147379517555\n",
      "Iteration 17445, Loss: 0.05569672957062721\n",
      "Iteration 17446, Loss: 0.055786848068237305\n",
      "Iteration 17447, Loss: 0.05577453225851059\n",
      "Iteration 17448, Loss: 0.05566950887441635\n",
      "Iteration 17449, Loss: 0.05578669160604477\n",
      "Iteration 17450, Loss: 0.05584430694580078\n",
      "Iteration 17451, Loss: 0.05572490021586418\n",
      "Iteration 17452, Loss: 0.05573249235749245\n",
      "Iteration 17453, Loss: 0.055824004113674164\n",
      "Iteration 17454, Loss: 0.05581216141581535\n",
      "Iteration 17455, Loss: 0.05570685863494873\n",
      "Iteration 17456, Loss: 0.05573586747050285\n",
      "Iteration 17457, Loss: 0.05579380318522453\n",
      "Iteration 17458, Loss: 0.05567523092031479\n",
      "Iteration 17459, Loss: 0.05576912686228752\n",
      "Iteration 17460, Loss: 0.055859845131635666\n",
      "Iteration 17461, Loss: 0.05584673210978508\n",
      "Iteration 17462, Loss: 0.055740319192409515\n",
      "Iteration 17463, Loss: 0.055693112313747406\n",
      "Iteration 17464, Loss: 0.05575418472290039\n",
      "Iteration 17465, Loss: 0.05563994497060776\n",
      "Iteration 17466, Loss: 0.055791936814785004\n",
      "Iteration 17467, Loss: 0.055879395455121994\n",
      "Iteration 17468, Loss: 0.05586385726928711\n",
      "Iteration 17469, Loss: 0.05575581640005112\n",
      "Iteration 17470, Loss: 0.055674515664577484\n",
      "Iteration 17471, Loss: 0.05573582649230957\n",
      "Iteration 17472, Loss: 0.055621109902858734\n",
      "Iteration 17473, Loss: 0.05579587072134018\n",
      "Iteration 17474, Loss: 0.05587192624807358\n",
      "Iteration 17475, Loss: 0.05584251880645752\n",
      "Iteration 17476, Loss: 0.05572088807821274\n",
      "Iteration 17477, Loss: 0.05573916435241699\n",
      "Iteration 17478, Loss: 0.05581466481089592\n",
      "Iteration 17479, Loss: 0.05571218580007553\n",
      "Iteration 17480, Loss: 0.05573348328471184\n",
      "Iteration 17481, Loss: 0.05581478402018547\n",
      "Iteration 17482, Loss: 0.05579300969839096\n",
      "Iteration 17483, Loss: 0.05568119138479233\n",
      "Iteration 17484, Loss: 0.05578052997589111\n",
      "Iteration 17485, Loss: 0.05584295839071274\n",
      "Iteration 17486, Loss: 0.055726807564496994\n",
      "Iteration 17487, Loss: 0.05573141947388649\n",
      "Iteration 17488, Loss: 0.055821459740400314\n",
      "Iteration 17489, Loss: 0.05580822750926018\n",
      "Iteration 17490, Loss: 0.05570332333445549\n",
      "Iteration 17491, Loss: 0.05574246495962143\n",
      "Iteration 17492, Loss: 0.05579916760325432\n",
      "Iteration 17493, Loss: 0.055678486824035645\n",
      "Iteration 17494, Loss: 0.05576833337545395\n",
      "Iteration 17495, Loss: 0.0558602437376976\n",
      "Iteration 17496, Loss: 0.055848877876996994\n",
      "Iteration 17497, Loss: 0.05574476718902588\n",
      "Iteration 17498, Loss: 0.055685244500637054\n",
      "Iteration 17499, Loss: 0.055742066353559494\n",
      "Iteration 17500, Loss: 0.055628180503845215\n",
      "Iteration 17501, Loss: 0.0557815246284008\n",
      "Iteration 17502, Loss: 0.055846892297267914\n",
      "Iteration 17503, Loss: 0.055807631462812424\n",
      "Iteration 17504, Loss: 0.05567809194326401\n",
      "Iteration 17505, Loss: 0.055802106857299805\n",
      "Iteration 17506, Loss: 0.05588400363922119\n",
      "Iteration 17507, Loss: 0.05579042434692383\n",
      "Iteration 17508, Loss: 0.055666886270046234\n",
      "Iteration 17509, Loss: 0.055743258446455\n",
      "Iteration 17510, Loss: 0.055715642869472504\n",
      "Iteration 17511, Loss: 0.0556357316672802\n",
      "Iteration 17512, Loss: 0.05565353482961655\n",
      "Iteration 17513, Loss: 0.05567821115255356\n",
      "Iteration 17514, Loss: 0.05567046254873276\n",
      "Iteration 17515, Loss: 0.055670738220214844\n",
      "Iteration 17516, Loss: 0.05562702938914299\n",
      "Iteration 17517, Loss: 0.05575263872742653\n",
      "Iteration 17518, Loss: 0.055786095559597015\n",
      "Iteration 17519, Loss: 0.05571349710226059\n",
      "Iteration 17520, Loss: 0.055693745613098145\n",
      "Iteration 17521, Loss: 0.055726371705532074\n",
      "Iteration 17522, Loss: 0.05563199520111084\n",
      "Iteration 17523, Loss: 0.05568540096282959\n",
      "Iteration 17524, Loss: 0.055662356317043304\n",
      "Iteration 17525, Loss: 0.05570018291473389\n",
      "Iteration 17526, Loss: 0.05567348003387451\n",
      "Iteration 17527, Loss: 0.05570753663778305\n",
      "Iteration 17528, Loss: 0.05573944374918938\n",
      "Iteration 17529, Loss: 0.05567070096731186\n",
      "Iteration 17530, Loss: 0.05574099346995354\n",
      "Iteration 17531, Loss: 0.055761657655239105\n",
      "Iteration 17532, Loss: 0.055622220039367676\n",
      "Iteration 17533, Loss: 0.055788520723581314\n",
      "Iteration 17534, Loss: 0.05583997815847397\n",
      "Iteration 17535, Loss: 0.05577453225851059\n",
      "Iteration 17536, Loss: 0.0556541308760643\n",
      "Iteration 17537, Loss: 0.05578263849020004\n",
      "Iteration 17538, Loss: 0.05579356476664543\n",
      "Iteration 17539, Loss: 0.05564375966787338\n",
      "Iteration 17540, Loss: 0.05581120774149895\n",
      "Iteration 17541, Loss: 0.055915676057338715\n",
      "Iteration 17542, Loss: 0.05591475963592529\n",
      "Iteration 17543, Loss: 0.055819593369960785\n",
      "Iteration 17544, Loss: 0.05564944073557854\n",
      "Iteration 17545, Loss: 0.05588686466217041\n",
      "Iteration 17546, Loss: 0.056002140045166016\n",
      "Iteration 17547, Loss: 0.05593200773000717\n",
      "Iteration 17548, Loss: 0.05569561570882797\n",
      "Iteration 17549, Loss: 0.05583294481039047\n",
      "Iteration 17550, Loss: 0.05599502846598625\n",
      "Iteration 17551, Loss: 0.0560460090637207\n",
      "Iteration 17552, Loss: 0.055995821952819824\n",
      "Iteration 17553, Loss: 0.05585571378469467\n",
      "Iteration 17554, Loss: 0.05563696473836899\n",
      "Iteration 17555, Loss: 0.05597110837697983\n",
      "Iteration 17556, Loss: 0.056152623146772385\n",
      "Iteration 17557, Loss: 0.05614086240530014\n",
      "Iteration 17558, Loss: 0.05595553293824196\n",
      "Iteration 17559, Loss: 0.05562949180603027\n",
      "Iteration 17560, Loss: 0.05589195340871811\n",
      "Iteration 17561, Loss: 0.05605630204081535\n",
      "Iteration 17562, Loss: 0.05610748380422592\n",
      "Iteration 17563, Loss: 0.05605638027191162\n",
      "Iteration 17564, Loss: 0.05591408535838127\n",
      "Iteration 17565, Loss: 0.05569549649953842\n",
      "Iteration 17566, Loss: 0.055890124291181564\n",
      "Iteration 17567, Loss: 0.05606917664408684\n",
      "Iteration 17568, Loss: 0.05606011673808098\n",
      "Iteration 17569, Loss: 0.05588122457265854\n",
      "Iteration 17570, Loss: 0.055657386779785156\n",
      "Iteration 17571, Loss: 0.055786848068237305\n",
      "Iteration 17572, Loss: 0.0558089017868042\n",
      "Iteration 17573, Loss: 0.055732809007167816\n",
      "Iteration 17574, Loss: 0.05566803738474846\n",
      "Iteration 17575, Loss: 0.05569867417216301\n",
      "Iteration 17576, Loss: 0.055652860552072525\n",
      "Iteration 17577, Loss: 0.055654171854257584\n",
      "Iteration 17578, Loss: 0.0556771382689476\n",
      "Iteration 17579, Loss: 0.05561951920390129\n",
      "Iteration 17580, Loss: 0.05575975030660629\n",
      "Iteration 17581, Loss: 0.0558035783469677\n",
      "Iteration 17582, Loss: 0.05574778839945793\n",
      "Iteration 17583, Loss: 0.05562770739197731\n",
      "Iteration 17584, Loss: 0.05572832003235817\n",
      "Iteration 17585, Loss: 0.055679403245449066\n",
      "Iteration 17586, Loss: 0.05571448802947998\n",
      "Iteration 17587, Loss: 0.05575358867645264\n",
      "Iteration 17588, Loss: 0.055685918778181076\n",
      "Iteration 17589, Loss: 0.05572152137756348\n",
      "Iteration 17590, Loss: 0.055746160447597504\n",
      "Iteration 17591, Loss: 0.055625759065151215\n",
      "Iteration 17592, Loss: 0.05576300621032715\n",
      "Iteration 17593, Loss: 0.05577930063009262\n",
      "Iteration 17594, Loss: 0.0556715726852417\n",
      "Iteration 17595, Loss: 0.05576952546834946\n",
      "Iteration 17596, Loss: 0.0558323860168457\n",
      "Iteration 17597, Loss: 0.055750928819179535\n",
      "Iteration 17598, Loss: 0.05567685887217522\n",
      "Iteration 17599, Loss: 0.05573928356170654\n",
      "Iteration 17600, Loss: 0.05567225068807602\n",
      "Iteration 17601, Loss: 0.05573996156454086\n",
      "Iteration 17602, Loss: 0.05577850341796875\n",
      "Iteration 17603, Loss: 0.055685680359601974\n",
      "Iteration 17604, Loss: 0.055743258446455\n",
      "Iteration 17605, Loss: 0.05579964444041252\n",
      "Iteration 17606, Loss: 0.05571870133280754\n",
      "Iteration 17607, Loss: 0.05570419877767563\n",
      "Iteration 17608, Loss: 0.0557580403983593\n",
      "Iteration 17609, Loss: 0.05567757412791252\n",
      "Iteration 17610, Loss: 0.05574413388967514\n",
      "Iteration 17611, Loss: 0.05579277127981186\n",
      "Iteration 17612, Loss: 0.055706582963466644\n",
      "Iteration 17613, Loss: 0.055718980729579926\n",
      "Iteration 17614, Loss: 0.05577465146780014\n",
      "Iteration 17615, Loss: 0.05569493770599365\n",
      "Iteration 17616, Loss: 0.05572795867919922\n",
      "Iteration 17617, Loss: 0.0557786226272583\n",
      "Iteration 17618, Loss: 0.05569779872894287\n",
      "Iteration 17619, Loss: 0.055721841752529144\n",
      "Iteration 17620, Loss: 0.0557708740234375\n",
      "Iteration 17621, Loss: 0.055682264268398285\n",
      "Iteration 17622, Loss: 0.05574798956513405\n",
      "Iteration 17623, Loss: 0.05580604076385498\n",
      "Iteration 17624, Loss: 0.055732809007167816\n",
      "Iteration 17625, Loss: 0.0556844100356102\n",
      "Iteration 17626, Loss: 0.0557381734251976\n",
      "Iteration 17627, Loss: 0.055660806596279144\n",
      "Iteration 17628, Loss: 0.05576137825846672\n",
      "Iteration 17629, Loss: 0.05581542104482651\n",
      "Iteration 17630, Loss: 0.055752597749233246\n",
      "Iteration 17631, Loss: 0.05566716194152832\n",
      "Iteration 17632, Loss: 0.055748943239450455\n",
      "Iteration 17633, Loss: 0.05571766942739487\n",
      "Iteration 17634, Loss: 0.055686354637145996\n",
      "Iteration 17635, Loss: 0.055719535797834396\n",
      "Iteration 17636, Loss: 0.055673085153102875\n",
      "Iteration 17637, Loss: 0.05571170896291733\n",
      "Iteration 17638, Loss: 0.05570197477936745\n",
      "Iteration 17639, Loss: 0.05568023771047592\n",
      "Iteration 17640, Loss: 0.05570396035909653\n",
      "Iteration 17641, Loss: 0.05564805120229721\n",
      "Iteration 17642, Loss: 0.055747829377651215\n",
      "Iteration 17643, Loss: 0.05573674291372299\n",
      "Iteration 17644, Loss: 0.05565635487437248\n",
      "Iteration 17645, Loss: 0.05568882077932358\n",
      "Iteration 17646, Loss: 0.05563577264547348\n",
      "Iteration 17647, Loss: 0.055763326585292816\n",
      "Iteration 17648, Loss: 0.055761776864528656\n",
      "Iteration 17649, Loss: 0.05564034357666969\n",
      "Iteration 17650, Loss: 0.05573944374918938\n",
      "Iteration 17651, Loss: 0.055742859840393066\n",
      "Iteration 17652, Loss: 0.055624764412641525\n",
      "Iteration 17653, Loss: 0.05582690238952637\n",
      "Iteration 17654, Loss: 0.05588182061910629\n",
      "Iteration 17655, Loss: 0.05578029155731201\n",
      "Iteration 17656, Loss: 0.05567912384867668\n",
      "Iteration 17657, Loss: 0.05576729774475098\n",
      "Iteration 17658, Loss: 0.05574409291148186\n",
      "Iteration 17659, Loss: 0.05562595650553703\n",
      "Iteration 17660, Loss: 0.05568253993988037\n",
      "Iteration 17661, Loss: 0.05565265938639641\n",
      "Iteration 17662, Loss: 0.05571341514587402\n",
      "Iteration 17663, Loss: 0.055685363709926605\n",
      "Iteration 17664, Loss: 0.055702053010463715\n",
      "Iteration 17665, Loss: 0.05573694035410881\n",
      "Iteration 17666, Loss: 0.05567380040884018\n",
      "Iteration 17667, Loss: 0.055729907006025314\n",
      "Iteration 17668, Loss: 0.05574234575033188\n",
      "Iteration 17669, Loss: 0.055631957948207855\n",
      "Iteration 17670, Loss: 0.05564336106181145\n",
      "Iteration 17671, Loss: 0.055681705474853516\n",
      "Iteration 17672, Loss: 0.05561773106455803\n",
      "Iteration 17673, Loss: 0.05576372146606445\n",
      "Iteration 17674, Loss: 0.05579336732625961\n",
      "Iteration 17675, Loss: 0.055713772773742676\n",
      "Iteration 17676, Loss: 0.055703043937683105\n",
      "Iteration 17677, Loss: 0.055747151374816895\n",
      "Iteration 17678, Loss: 0.05564272776246071\n",
      "Iteration 17679, Loss: 0.055786650627851486\n",
      "Iteration 17680, Loss: 0.0558626651763916\n",
      "Iteration 17681, Loss: 0.055831313133239746\n",
      "Iteration 17682, Loss: 0.05571107193827629\n",
      "Iteration 17683, Loss: 0.055752914398908615\n",
      "Iteration 17684, Loss: 0.05582873150706291\n",
      "Iteration 17685, Loss: 0.05573602765798569\n",
      "Iteration 17686, Loss: 0.0557098388671875\n",
      "Iteration 17687, Loss: 0.05578383058309555\n",
      "Iteration 17688, Loss: 0.05575517937541008\n",
      "Iteration 17689, Loss: 0.05564836785197258\n",
      "Iteration 17690, Loss: 0.05580604448914528\n",
      "Iteration 17691, Loss: 0.05584617704153061\n",
      "Iteration 17692, Loss: 0.05571047589182854\n",
      "Iteration 17693, Loss: 0.05575581640005112\n",
      "Iteration 17694, Loss: 0.05585738271474838\n",
      "Iteration 17695, Loss: 0.05585483834147453\n",
      "Iteration 17696, Loss: 0.05575859546661377\n",
      "Iteration 17697, Loss: 0.055656712502241135\n",
      "Iteration 17698, Loss: 0.0557074174284935\n",
      "Iteration 17699, Loss: 0.05563624948263168\n",
      "Iteration 17700, Loss: 0.05565091222524643\n",
      "Iteration 17701, Loss: 0.055660247802734375\n",
      "Iteration 17702, Loss: 0.05563843250274658\n",
      "Iteration 17703, Loss: 0.05564840883016586\n",
      "Iteration 17704, Loss: 0.05566783994436264\n",
      "Iteration 17705, Loss: 0.055640898644924164\n",
      "Iteration 17706, Loss: 0.05572092533111572\n",
      "Iteration 17707, Loss: 0.05569092556834221\n",
      "Iteration 17708, Loss: 0.05569692701101303\n",
      "Iteration 17709, Loss: 0.05572982877492905\n",
      "Iteration 17710, Loss: 0.05566064640879631\n",
      "Iteration 17711, Loss: 0.05575255677103996\n",
      "Iteration 17712, Loss: 0.055772583931684494\n",
      "Iteration 17713, Loss: 0.05563032627105713\n",
      "Iteration 17714, Loss: 0.05581521987915039\n",
      "Iteration 17715, Loss: 0.05590558052062988\n",
      "Iteration 17716, Loss: 0.0558808259665966\n",
      "Iteration 17717, Loss: 0.0557587556540966\n",
      "Iteration 17718, Loss: 0.05570213124155998\n",
      "Iteration 17719, Loss: 0.05579444020986557\n",
      "Iteration 17720, Loss: 0.05573658272624016\n",
      "Iteration 17721, Loss: 0.055690132081508636\n",
      "Iteration 17722, Loss: 0.05574099346995354\n",
      "Iteration 17723, Loss: 0.055701177567243576\n",
      "Iteration 17724, Loss: 0.055677734315395355\n",
      "Iteration 17725, Loss: 0.05569084733724594\n",
      "Iteration 17726, Loss: 0.05566958710551262\n",
      "Iteration 17727, Loss: 0.05567753687500954\n",
      "Iteration 17728, Loss: 0.055655837059020996\n",
      "Iteration 17729, Loss: 0.055689457803964615\n",
      "Iteration 17730, Loss: 0.0556536540389061\n",
      "Iteration 17731, Loss: 0.055700939148664474\n",
      "Iteration 17732, Loss: 0.05569791793823242\n",
      "Iteration 17733, Loss: 0.055646542459726334\n",
      "Iteration 17734, Loss: 0.055626075714826584\n",
      "Iteration 17735, Loss: 0.05573086068034172\n",
      "Iteration 17736, Loss: 0.055715762078762054\n",
      "Iteration 17737, Loss: 0.05565520375967026\n",
      "Iteration 17738, Loss: 0.0556797981262207\n",
      "Iteration 17739, Loss: 0.055646978318691254\n",
      "Iteration 17740, Loss: 0.055646978318691254\n",
      "Iteration 17741, Loss: 0.05564916506409645\n",
      "Iteration 17742, Loss: 0.05566366761922836\n",
      "Iteration 17743, Loss: 0.055636368691921234\n",
      "Iteration 17744, Loss: 0.05570578947663307\n",
      "Iteration 17745, Loss: 0.05565285682678223\n",
      "Iteration 17746, Loss: 0.055743299424648285\n",
      "Iteration 17747, Loss: 0.05579185485839844\n",
      "Iteration 17748, Loss: 0.05573594570159912\n",
      "Iteration 17749, Loss: 0.05564948171377182\n",
      "Iteration 17750, Loss: 0.05571262165904045\n",
      "Iteration 17751, Loss: 0.05565023422241211\n",
      "Iteration 17752, Loss: 0.055723629891872406\n",
      "Iteration 17753, Loss: 0.055750612169504166\n",
      "Iteration 17754, Loss: 0.055675629526376724\n",
      "Iteration 17755, Loss: 0.055741071701049805\n",
      "Iteration 17756, Loss: 0.055770277976989746\n",
      "Iteration 17757, Loss: 0.055640142410993576\n",
      "Iteration 17758, Loss: 0.05579746142029762\n",
      "Iteration 17759, Loss: 0.055879317224025726\n",
      "Iteration 17760, Loss: 0.055845897644758224\n",
      "Iteration 17761, Loss: 0.05571580305695534\n",
      "Iteration 17762, Loss: 0.055752914398908615\n",
      "Iteration 17763, Loss: 0.055839501321315765\n",
      "Iteration 17764, Loss: 0.05576328560709953\n",
      "Iteration 17765, Loss: 0.05567193031311035\n",
      "Iteration 17766, Loss: 0.05573336407542229\n",
      "Iteration 17767, Loss: 0.055688343942165375\n",
      "Iteration 17768, Loss: 0.055690210312604904\n",
      "Iteration 17769, Loss: 0.05569279193878174\n",
      "Iteration 17770, Loss: 0.055672407150268555\n",
      "Iteration 17771, Loss: 0.05568302050232887\n",
      "Iteration 17772, Loss: 0.0556415319442749\n",
      "Iteration 17773, Loss: 0.05567769333720207\n",
      "Iteration 17774, Loss: 0.05564308166503906\n",
      "Iteration 17775, Loss: 0.05566788092255592\n",
      "Iteration 17776, Loss: 0.05562667176127434\n",
      "Iteration 17777, Loss: 0.055713098496198654\n",
      "Iteration 17778, Loss: 0.05571933835744858\n",
      "Iteration 17779, Loss: 0.055635690689086914\n",
      "Iteration 17780, Loss: 0.05579773709177971\n",
      "Iteration 17781, Loss: 0.05582154169678688\n",
      "Iteration 17782, Loss: 0.05566994473338127\n",
      "Iteration 17783, Loss: 0.05579451844096184\n",
      "Iteration 17784, Loss: 0.05590585991740227\n",
      "Iteration 17785, Loss: 0.055911384522914886\n",
      "Iteration 17786, Loss: 0.05582141876220703\n",
      "Iteration 17787, Loss: 0.055647335946559906\n",
      "Iteration 17788, Loss: 0.055901169776916504\n",
      "Iteration 17789, Loss: 0.05603409186005592\n",
      "Iteration 17790, Loss: 0.05597861856222153\n",
      "Iteration 17791, Loss: 0.05575506016612053\n",
      "Iteration 17792, Loss: 0.05577961727976799\n",
      "Iteration 17793, Loss: 0.05593375489115715\n",
      "Iteration 17794, Loss: 0.055978063493967056\n",
      "Iteration 17795, Loss: 0.05592278763651848\n",
      "Iteration 17796, Loss: 0.05577882379293442\n",
      "Iteration 17797, Loss: 0.055685244500637054\n",
      "Iteration 17798, Loss: 0.05578557774424553\n",
      "Iteration 17799, Loss: 0.05570479482412338\n",
      "Iteration 17800, Loss: 0.05572180077433586\n",
      "Iteration 17801, Loss: 0.05579034611582756\n",
      "Iteration 17802, Loss: 0.05575835704803467\n",
      "Iteration 17803, Loss: 0.05563632771372795\n",
      "Iteration 17804, Loss: 0.05585161969065666\n",
      "Iteration 17805, Loss: 0.05592759698629379\n",
      "Iteration 17806, Loss: 0.0558241605758667\n",
      "Iteration 17807, Loss: 0.05565126985311508\n",
      "Iteration 17808, Loss: 0.05574023723602295\n",
      "Iteration 17809, Loss: 0.05573121830821037\n",
      "Iteration 17810, Loss: 0.05562707036733627\n",
      "Iteration 17811, Loss: 0.055838268250226974\n",
      "Iteration 17812, Loss: 0.055896759033203125\n",
      "Iteration 17813, Loss: 0.05578768253326416\n",
      "Iteration 17814, Loss: 0.055679719895124435\n",
      "Iteration 17815, Loss: 0.05576912686228752\n",
      "Iteration 17816, Loss: 0.055756013840436935\n",
      "Iteration 17817, Loss: 0.05564109608530998\n",
      "Iteration 17818, Loss: 0.05582980439066887\n",
      "Iteration 17819, Loss: 0.055901966989040375\n",
      "Iteration 17820, Loss: 0.055812399834394455\n",
      "Iteration 17821, Loss: 0.05565623566508293\n",
      "Iteration 17822, Loss: 0.055767856538295746\n",
      "Iteration 17823, Loss: 0.055785536766052246\n",
      "Iteration 17824, Loss: 0.05568063631653786\n",
      "Iteration 17825, Loss: 0.05576658248901367\n",
      "Iteration 17826, Loss: 0.05583501234650612\n",
      "Iteration 17827, Loss: 0.05575975030660629\n",
      "Iteration 17828, Loss: 0.05566982552409172\n",
      "Iteration 17829, Loss: 0.055742982774972916\n",
      "Iteration 17830, Loss: 0.055699072778224945\n",
      "Iteration 17831, Loss: 0.05569827929139137\n",
      "Iteration 17832, Loss: 0.05572426691651344\n",
      "Iteration 17833, Loss: 0.05564979836344719\n",
      "Iteration 17834, Loss: 0.05574452877044678\n",
      "Iteration 17835, Loss: 0.05574619770050049\n",
      "Iteration 17836, Loss: 0.05562595650553703\n",
      "Iteration 17837, Loss: 0.055640578269958496\n",
      "Iteration 17838, Loss: 0.0556919202208519\n",
      "Iteration 17839, Loss: 0.055658262223005295\n",
      "Iteration 17840, Loss: 0.05571679398417473\n",
      "Iteration 17841, Loss: 0.05572768300771713\n",
      "Iteration 17842, Loss: 0.05562838166952133\n",
      "Iteration 17843, Loss: 0.055765312165021896\n",
      "Iteration 17844, Loss: 0.055757999420166016\n",
      "Iteration 17845, Loss: 0.05563012883067131\n",
      "Iteration 17846, Loss: 0.05569184198975563\n",
      "Iteration 17847, Loss: 0.05563553422689438\n",
      "Iteration 17848, Loss: 0.055766861885786057\n",
      "Iteration 17849, Loss: 0.05579587072134018\n",
      "Iteration 17850, Loss: 0.055697523057460785\n",
      "Iteration 17851, Loss: 0.05573562905192375\n",
      "Iteration 17852, Loss: 0.05579710379242897\n",
      "Iteration 17853, Loss: 0.055720411241054535\n",
      "Iteration 17854, Loss: 0.05569911003112793\n",
      "Iteration 17855, Loss: 0.055749617516994476\n",
      "Iteration 17856, Loss: 0.05567145347595215\n",
      "Iteration 17857, Loss: 0.05574309825897217\n",
      "Iteration 17858, Loss: 0.0557841882109642\n",
      "Iteration 17859, Loss: 0.05568496510386467\n",
      "Iteration 17860, Loss: 0.055750250816345215\n",
      "Iteration 17861, Loss: 0.0558164119720459\n",
      "Iteration 17862, Loss: 0.05575112625956535\n",
      "Iteration 17863, Loss: 0.05566044896841049\n",
      "Iteration 17864, Loss: 0.0557272844016552\n",
      "Iteration 17865, Loss: 0.055666208267211914\n",
      "Iteration 17866, Loss: 0.0557405948638916\n",
      "Iteration 17867, Loss: 0.055787526071071625\n",
      "Iteration 17868, Loss: 0.05572907254099846\n",
      "Iteration 17869, Loss: 0.05566974729299545\n",
      "Iteration 17870, Loss: 0.05572180077433586\n",
      "Iteration 17871, Loss: 0.055657826364040375\n",
      "Iteration 17872, Loss: 0.05572744458913803\n",
      "Iteration 17873, Loss: 0.05576439946889877\n",
      "Iteration 17874, Loss: 0.055706582963466644\n",
      "Iteration 17875, Loss: 0.055681269615888596\n",
      "Iteration 17876, Loss: 0.05569052696228027\n",
      "Iteration 17877, Loss: 0.05567244812846184\n",
      "Iteration 17878, Loss: 0.05568687245249748\n",
      "Iteration 17879, Loss: 0.05562802404165268\n",
      "Iteration 17880, Loss: 0.05572676658630371\n",
      "Iteration 17881, Loss: 0.05570332333445549\n",
      "Iteration 17882, Loss: 0.05567304417490959\n",
      "Iteration 17883, Loss: 0.055694662034511566\n",
      "Iteration 17884, Loss: 0.05563477799296379\n",
      "Iteration 17885, Loss: 0.05567852780222893\n",
      "Iteration 17886, Loss: 0.05565830320119858\n",
      "Iteration 17887, Loss: 0.055696289986371994\n",
      "Iteration 17888, Loss: 0.05565842241048813\n",
      "Iteration 17889, Loss: 0.055726729333400726\n",
      "Iteration 17890, Loss: 0.055769048631191254\n",
      "Iteration 17891, Loss: 0.05571369454264641\n",
      "Iteration 17892, Loss: 0.05566593259572983\n",
      "Iteration 17893, Loss: 0.055669426918029785\n",
      "Iteration 17894, Loss: 0.05569184198975563\n",
      "Iteration 17895, Loss: 0.055710237473249435\n",
      "Iteration 17896, Loss: 0.05563390254974365\n",
      "Iteration 17897, Loss: 0.05579972267150879\n",
      "Iteration 17898, Loss: 0.0558246374130249\n",
      "Iteration 17899, Loss: 0.055673401802778244\n",
      "Iteration 17900, Loss: 0.055791739374399185\n",
      "Iteration 17901, Loss: 0.05590284243226051\n",
      "Iteration 17902, Loss: 0.05590756982564926\n",
      "Iteration 17903, Loss: 0.055816613137722015\n",
      "Iteration 17904, Loss: 0.055642884224653244\n",
      "Iteration 17905, Loss: 0.05590534210205078\n",
      "Iteration 17906, Loss: 0.056035321205854416\n",
      "Iteration 17907, Loss: 0.05597766488790512\n",
      "Iteration 17908, Loss: 0.05575251951813698\n",
      "Iteration 17909, Loss: 0.055782679468393326\n",
      "Iteration 17910, Loss: 0.05593772977590561\n",
      "Iteration 17911, Loss: 0.05598294734954834\n",
      "Iteration 17912, Loss: 0.05592862889170647\n",
      "Iteration 17913, Loss: 0.055785421282052994\n",
      "Iteration 17914, Loss: 0.05567602440714836\n",
      "Iteration 17915, Loss: 0.05577604100108147\n",
      "Iteration 17916, Loss: 0.05569645017385483\n",
      "Iteration 17917, Loss: 0.05572676658630371\n",
      "Iteration 17918, Loss: 0.055793724954128265\n",
      "Iteration 17919, Loss: 0.055760543793439865\n",
      "Iteration 17920, Loss: 0.05563664808869362\n",
      "Iteration 17921, Loss: 0.05585328862071037\n",
      "Iteration 17922, Loss: 0.055931370705366135\n",
      "Iteration 17923, Loss: 0.05583024397492409\n",
      "Iteration 17924, Loss: 0.05564570799469948\n",
      "Iteration 17925, Loss: 0.05573849007487297\n",
      "Iteration 17926, Loss: 0.05573372170329094\n",
      "Iteration 17927, Loss: 0.05563100427389145\n",
      "Iteration 17928, Loss: 0.055831633508205414\n",
      "Iteration 17929, Loss: 0.05588961020112038\n",
      "Iteration 17930, Loss: 0.05578069016337395\n",
      "Iteration 17931, Loss: 0.05568405240774155\n",
      "Iteration 17932, Loss: 0.05577174946665764\n",
      "Iteration 17933, Loss: 0.05575573816895485\n",
      "Iteration 17934, Loss: 0.05563784018158913\n",
      "Iteration 17935, Loss: 0.05583711713552475\n",
      "Iteration 17936, Loss: 0.05591221898794174\n",
      "Iteration 17937, Loss: 0.055824004113674164\n",
      "Iteration 17938, Loss: 0.055652063339948654\n",
      "Iteration 17939, Loss: 0.05578092858195305\n",
      "Iteration 17940, Loss: 0.05581963062286377\n",
      "Iteration 17941, Loss: 0.055732809007167816\n",
      "Iteration 17942, Loss: 0.0556919202208519\n",
      "Iteration 17943, Loss: 0.05574699491262436\n",
      "Iteration 17944, Loss: 0.055659495294094086\n",
      "Iteration 17945, Loss: 0.05576428025960922\n",
      "Iteration 17946, Loss: 0.05581963062286377\n",
      "Iteration 17947, Loss: 0.05574588105082512\n",
      "Iteration 17948, Loss: 0.05567236989736557\n",
      "Iteration 17949, Loss: 0.05573586747050285\n",
      "Iteration 17950, Loss: 0.055671337991952896\n",
      "Iteration 17951, Loss: 0.05574003979563713\n",
      "Iteration 17952, Loss: 0.055787526071071625\n",
      "Iteration 17953, Loss: 0.05572700500488281\n",
      "Iteration 17954, Loss: 0.055676184594631195\n",
      "Iteration 17955, Loss: 0.055724941194057465\n",
      "Iteration 17956, Loss: 0.05565818399190903\n",
      "Iteration 17957, Loss: 0.05573229119181633\n",
      "Iteration 17958, Loss: 0.055772583931684494\n",
      "Iteration 17959, Loss: 0.05571858212351799\n",
      "Iteration 17960, Loss: 0.055664222687482834\n",
      "Iteration 17961, Loss: 0.05568870157003403\n",
      "Iteration 17962, Loss: 0.055662672966718674\n",
      "Iteration 17963, Loss: 0.055670659989118576\n",
      "Iteration 17964, Loss: 0.055651307106018066\n",
      "Iteration 17965, Loss: 0.05567566677927971\n",
      "Iteration 17966, Loss: 0.05565170571208\n",
      "Iteration 17967, Loss: 0.055692195892333984\n",
      "Iteration 17968, Loss: 0.05565512180328369\n",
      "Iteration 17969, Loss: 0.055730342864990234\n",
      "Iteration 17970, Loss: 0.055761974304914474\n",
      "Iteration 17971, Loss: 0.055690012872219086\n",
      "Iteration 17972, Loss: 0.05571822449564934\n",
      "Iteration 17973, Loss: 0.055746398866176605\n",
      "Iteration 17974, Loss: 0.055619996041059494\n",
      "Iteration 17975, Loss: 0.055794280022382736\n",
      "Iteration 17976, Loss: 0.055870018899440765\n",
      "Iteration 17977, Loss: 0.055843353271484375\n",
      "Iteration 17978, Loss: 0.05572577565908432\n",
      "Iteration 17979, Loss: 0.05572609230875969\n",
      "Iteration 17980, Loss: 0.05579821392893791\n",
      "Iteration 17981, Loss: 0.05569366738200188\n",
      "Iteration 17982, Loss: 0.055746398866176605\n",
      "Iteration 17983, Loss: 0.055829327553510666\n",
      "Iteration 17984, Loss: 0.055809974670410156\n",
      "Iteration 17985, Loss: 0.0556994304060936\n",
      "Iteration 17986, Loss: 0.05575263872742653\n",
      "Iteration 17987, Loss: 0.05581613630056381\n",
      "Iteration 17988, Loss: 0.05570109933614731\n",
      "Iteration 17989, Loss: 0.055747710168361664\n",
      "Iteration 17990, Loss: 0.0558369979262352\n",
      "Iteration 17991, Loss: 0.05582348629832268\n",
      "Iteration 17992, Loss: 0.055717431008815765\n",
      "Iteration 17993, Loss: 0.0557224377989769\n",
      "Iteration 17994, Loss: 0.055781010538339615\n",
      "Iteration 17995, Loss: 0.055661123245954514\n",
      "Iteration 17996, Loss: 0.05578009411692619\n",
      "Iteration 17997, Loss: 0.05587224289774895\n",
      "Iteration 17998, Loss: 0.055861156433820724\n",
      "Iteration 17999, Loss: 0.05575677007436752\n",
      "Iteration 18000, Loss: 0.05566660687327385\n",
      "Iteration 18001, Loss: 0.05572358891367912\n",
      "Iteration 18002, Loss: 0.055618010461330414\n",
      "Iteration 18003, Loss: 0.055658262223005295\n",
      "Iteration 18004, Loss: 0.05564483255147934\n",
      "Iteration 18005, Loss: 0.05562933534383774\n",
      "Iteration 18006, Loss: 0.05567439645528793\n",
      "Iteration 18007, Loss: 0.05563005059957504\n",
      "Iteration 18008, Loss: 0.05574313923716545\n",
      "Iteration 18009, Loss: 0.0557277612388134\n",
      "Iteration 18010, Loss: 0.055653732270002365\n",
      "Iteration 18011, Loss: 0.0556815080344677\n",
      "Iteration 18012, Loss: 0.055626433342695236\n",
      "Iteration 18013, Loss: 0.05566370487213135\n",
      "Iteration 18014, Loss: 0.055627308785915375\n",
      "Iteration 18015, Loss: 0.05575935170054436\n",
      "Iteration 18016, Loss: 0.05574214458465576\n",
      "Iteration 18017, Loss: 0.05565178394317627\n",
      "Iteration 18018, Loss: 0.05568508431315422\n",
      "Iteration 18019, Loss: 0.05562349408864975\n",
      "Iteration 18020, Loss: 0.05578744411468506\n",
      "Iteration 18021, Loss: 0.05580127611756325\n",
      "Iteration 18022, Loss: 0.05566652864217758\n",
      "Iteration 18023, Loss: 0.05578383058309555\n",
      "Iteration 18024, Loss: 0.05587160959839821\n",
      "Iteration 18025, Loss: 0.05583957955241203\n",
      "Iteration 18026, Loss: 0.05570554733276367\n",
      "Iteration 18027, Loss: 0.055766426026821136\n",
      "Iteration 18028, Loss: 0.05585603043437004\n",
      "Iteration 18029, Loss: 0.0557841882109642\n",
      "Iteration 18030, Loss: 0.0556483268737793\n",
      "Iteration 18031, Loss: 0.05571158975362778\n",
      "Iteration 18032, Loss: 0.055658500641584396\n",
      "Iteration 18033, Loss: 0.05573773756623268\n",
      "Iteration 18034, Loss: 0.0557536706328392\n",
      "Iteration 18035, Loss: 0.05563632771372795\n",
      "Iteration 18036, Loss: 0.0557815246284008\n",
      "Iteration 18037, Loss: 0.055823683738708496\n",
      "Iteration 18038, Loss: 0.055731020867824554\n",
      "Iteration 18039, Loss: 0.055701058357954025\n",
      "Iteration 18040, Loss: 0.055766068398952484\n",
      "Iteration 18041, Loss: 0.0556967668235302\n",
      "Iteration 18042, Loss: 0.055717866867780685\n",
      "Iteration 18043, Loss: 0.05576169863343239\n",
      "Iteration 18044, Loss: 0.05568063259124756\n",
      "Iteration 18045, Loss: 0.05573558807373047\n",
      "Iteration 18046, Loss: 0.0557783879339695\n",
      "Iteration 18047, Loss: 0.055679917335510254\n",
      "Iteration 18048, Loss: 0.05575660988688469\n",
      "Iteration 18049, Loss: 0.05582364648580551\n",
      "Iteration 18050, Loss: 0.055761419236660004\n",
      "Iteration 18051, Loss: 0.05565635487437248\n",
      "Iteration 18052, Loss: 0.05574842542409897\n",
      "Iteration 18053, Loss: 0.05572100728750229\n",
      "Iteration 18054, Loss: 0.05567431449890137\n",
      "Iteration 18055, Loss: 0.05570296570658684\n",
      "Iteration 18056, Loss: 0.05565170571208\n",
      "Iteration 18057, Loss: 0.05572954937815666\n",
      "Iteration 18058, Loss: 0.05570948123931885\n",
      "Iteration 18059, Loss: 0.05567952245473862\n",
      "Iteration 18060, Loss: 0.05571099370718002\n",
      "Iteration 18061, Loss: 0.05565103143453598\n",
      "Iteration 18062, Loss: 0.05575355142354965\n",
      "Iteration 18063, Loss: 0.05575665086507797\n",
      "Iteration 18064, Loss: 0.05563012883067131\n",
      "Iteration 18065, Loss: 0.055662237107753754\n",
      "Iteration 18066, Loss: 0.05562698841094971\n",
      "Iteration 18067, Loss: 0.055677931755781174\n",
      "Iteration 18068, Loss: 0.05565464496612549\n",
      "Iteration 18069, Loss: 0.055706024169921875\n",
      "Iteration 18070, Loss: 0.05567324534058571\n",
      "Iteration 18071, Loss: 0.05571349710226059\n",
      "Iteration 18072, Loss: 0.05575327202677727\n",
      "Iteration 18073, Loss: 0.05569533631205559\n",
      "Iteration 18074, Loss: 0.05569374933838844\n",
      "Iteration 18075, Loss: 0.05570010468363762\n",
      "Iteration 18076, Loss: 0.05566776171326637\n",
      "Iteration 18077, Loss: 0.055684447288513184\n",
      "Iteration 18078, Loss: 0.055620551109313965\n",
      "Iteration 18079, Loss: 0.0556996688246727\n",
      "Iteration 18080, Loss: 0.0556565560400486\n",
      "Iteration 18081, Loss: 0.05572180077433586\n",
      "Iteration 18082, Loss: 0.05573173612356186\n",
      "Iteration 18083, Loss: 0.05562512204051018\n",
      "Iteration 18084, Loss: 0.055758439004421234\n",
      "Iteration 18085, Loss: 0.055743258446455\n",
      "Iteration 18086, Loss: 0.055644355714321136\n",
      "Iteration 18087, Loss: 0.05567900463938713\n",
      "Iteration 18088, Loss: 0.055622898042201996\n",
      "Iteration 18089, Loss: 0.055676382035017014\n",
      "Iteration 18090, Loss: 0.055649083107709885\n",
      "Iteration 18091, Loss: 0.055719975382089615\n",
      "Iteration 18092, Loss: 0.055691998451948166\n",
      "Iteration 18093, Loss: 0.05569680780172348\n",
      "Iteration 18094, Loss: 0.05573431774973869\n",
      "Iteration 18095, Loss: 0.05567371845245361\n",
      "Iteration 18096, Loss: 0.05572633072733879\n",
      "Iteration 18097, Loss: 0.05573439970612526\n",
      "Iteration 18098, Loss: 0.055641770362854004\n",
      "Iteration 18099, Loss: 0.05565798282623291\n",
      "Iteration 18100, Loss: 0.05565397068858147\n",
      "Iteration 18101, Loss: 0.05563485622406006\n",
      "Iteration 18102, Loss: 0.055637162178754807\n",
      "Iteration 18103, Loss: 0.055679045617580414\n",
      "Iteration 18104, Loss: 0.05566326901316643\n",
      "Iteration 18105, Loss: 0.05568631738424301\n",
      "Iteration 18106, Loss: 0.05564650148153305\n",
      "Iteration 18107, Loss: 0.0557381734251976\n",
      "Iteration 18108, Loss: 0.055781446397304535\n",
      "Iteration 18109, Loss: 0.0557251013815403\n",
      "Iteration 18110, Loss: 0.055655043572187424\n",
      "Iteration 18111, Loss: 0.05567491427063942\n",
      "Iteration 18112, Loss: 0.05567324161529541\n",
      "Iteration 18113, Loss: 0.055677495896816254\n",
      "Iteration 18114, Loss: 0.05564514920115471\n",
      "Iteration 18115, Loss: 0.05565917491912842\n",
      "Iteration 18116, Loss: 0.05564602464437485\n",
      "Iteration 18117, Loss: 0.055655717849731445\n",
      "Iteration 18118, Loss: 0.05566227808594704\n",
      "Iteration 18119, Loss: 0.05564232915639877\n",
      "Iteration 18120, Loss: 0.055704157799482346\n",
      "Iteration 18121, Loss: 0.05565420910716057\n",
      "Iteration 18122, Loss: 0.05573948472738266\n",
      "Iteration 18123, Loss: 0.05578812211751938\n",
      "Iteration 18124, Loss: 0.055735789239406586\n",
      "Iteration 18125, Loss: 0.05564109608530998\n",
      "Iteration 18126, Loss: 0.0557071790099144\n",
      "Iteration 18127, Loss: 0.05564789101481438\n",
      "Iteration 18128, Loss: 0.055722832679748535\n",
      "Iteration 18129, Loss: 0.055741313844919205\n",
      "Iteration 18130, Loss: 0.055650513619184494\n",
      "Iteration 18131, Loss: 0.0557866096496582\n",
      "Iteration 18132, Loss: 0.055832426995038986\n",
      "Iteration 18133, Loss: 0.055720292031764984\n",
      "Iteration 18134, Loss: 0.05572863668203354\n",
      "Iteration 18135, Loss: 0.05581192299723625\n",
      "Iteration 18136, Loss: 0.05577981472015381\n",
      "Iteration 18137, Loss: 0.05564872547984123\n",
      "Iteration 18138, Loss: 0.05582929030060768\n",
      "Iteration 18139, Loss: 0.05590752884745598\n",
      "Iteration 18140, Loss: 0.05581824108958244\n",
      "Iteration 18141, Loss: 0.055644553154706955\n",
      "Iteration 18142, Loss: 0.05575152486562729\n",
      "Iteration 18143, Loss: 0.05575362965464592\n",
      "Iteration 18144, Loss: 0.05563942715525627\n",
      "Iteration 18145, Loss: 0.05582042783498764\n",
      "Iteration 18146, Loss: 0.05589060112833977\n",
      "Iteration 18147, Loss: 0.055807117372751236\n",
      "Iteration 18148, Loss: 0.05565448850393295\n",
      "Iteration 18149, Loss: 0.05577564239501953\n",
      "Iteration 18150, Loss: 0.05579765886068344\n",
      "Iteration 18151, Loss: 0.05568445101380348\n",
      "Iteration 18152, Loss: 0.05576781556010246\n",
      "Iteration 18153, Loss: 0.055846333503723145\n",
      "Iteration 18154, Loss: 0.05578871816396713\n",
      "Iteration 18155, Loss: 0.05565262213349342\n",
      "Iteration 18156, Loss: 0.05579209327697754\n",
      "Iteration 18157, Loss: 0.05582837387919426\n",
      "Iteration 18158, Loss: 0.05570662021636963\n",
      "Iteration 18159, Loss: 0.05575112625956535\n",
      "Iteration 18160, Loss: 0.055840931832790375\n",
      "Iteration 18161, Loss: 0.0558096207678318\n",
      "Iteration 18162, Loss: 0.05568158999085426\n",
      "Iteration 18163, Loss: 0.05578840151429176\n",
      "Iteration 18164, Loss: 0.055864930152893066\n",
      "Iteration 18165, Loss: 0.055772505700588226\n",
      "Iteration 18166, Loss: 0.05567582696676254\n",
      "Iteration 18167, Loss: 0.05574909970164299\n",
      "Iteration 18168, Loss: 0.05570952221751213\n",
      "Iteration 18169, Loss: 0.05566176027059555\n",
      "Iteration 18170, Loss: 0.05567093938589096\n",
      "Iteration 18171, Loss: 0.05568397417664528\n",
      "Iteration 18172, Loss: 0.05568898096680641\n",
      "Iteration 18173, Loss: 0.055642448365688324\n",
      "Iteration 18174, Loss: 0.05568460747599602\n",
      "Iteration 18175, Loss: 0.05564204975962639\n",
      "Iteration 18176, Loss: 0.055686913430690765\n",
      "Iteration 18177, Loss: 0.055654607713222504\n",
      "Iteration 18178, Loss: 0.055725615471601486\n",
      "Iteration 18179, Loss: 0.05573348328471184\n",
      "Iteration 18180, Loss: 0.055639784783124924\n",
      "Iteration 18181, Loss: 0.05574445053935051\n",
      "Iteration 18182, Loss: 0.05573388189077377\n",
      "Iteration 18183, Loss: 0.055643241852521896\n",
      "Iteration 18184, Loss: 0.05565985292196274\n",
      "Iteration 18185, Loss: 0.055665574967861176\n",
      "Iteration 18186, Loss: 0.05562349408864975\n",
      "Iteration 18187, Loss: 0.05575144663453102\n",
      "Iteration 18188, Loss: 0.05575398728251457\n",
      "Iteration 18189, Loss: 0.05564276501536369\n",
      "Iteration 18190, Loss: 0.055777788162231445\n",
      "Iteration 18191, Loss: 0.05581093207001686\n",
      "Iteration 18192, Loss: 0.05569645017385483\n",
      "Iteration 18193, Loss: 0.055751923471689224\n",
      "Iteration 18194, Loss: 0.0558321475982666\n",
      "Iteration 18195, Loss: 0.05578363314270973\n",
      "Iteration 18196, Loss: 0.055642250925302505\n",
      "Iteration 18197, Loss: 0.05581764504313469\n",
      "Iteration 18198, Loss: 0.055871568620204926\n",
      "Iteration 18199, Loss: 0.05575820058584213\n",
      "Iteration 18200, Loss: 0.05570157617330551\n",
      "Iteration 18201, Loss: 0.05578836053609848\n",
      "Iteration 18202, Loss: 0.05575994774699211\n",
      "Iteration 18203, Loss: 0.05563557520508766\n",
      "Iteration 18204, Loss: 0.05583290383219719\n",
      "Iteration 18205, Loss: 0.05589127540588379\n",
      "Iteration 18206, Loss: 0.055774055421352386\n",
      "Iteration 18207, Loss: 0.05569426342844963\n",
      "Iteration 18208, Loss: 0.05578530207276344\n",
      "Iteration 18209, Loss: 0.05576765909790993\n",
      "Iteration 18210, Loss: 0.0556536540389061\n",
      "Iteration 18211, Loss: 0.05581454560160637\n",
      "Iteration 18212, Loss: 0.05587899684906006\n",
      "Iteration 18213, Loss: 0.055766426026821136\n",
      "Iteration 18214, Loss: 0.05569728463888168\n",
      "Iteration 18215, Loss: 0.05578533932566643\n",
      "Iteration 18216, Loss: 0.055766068398952484\n",
      "Iteration 18217, Loss: 0.05565154552459717\n",
      "Iteration 18218, Loss: 0.05581852048635483\n",
      "Iteration 18219, Loss: 0.05588368698954582\n",
      "Iteration 18220, Loss: 0.05577024072408676\n",
      "Iteration 18221, Loss: 0.05569493770599365\n",
      "Iteration 18222, Loss: 0.05578351020812988\n",
      "Iteration 18223, Loss: 0.05576586723327637\n",
      "Iteration 18224, Loss: 0.05565337464213371\n",
      "Iteration 18225, Loss: 0.055814266204833984\n",
      "Iteration 18226, Loss: 0.055877648293972015\n",
      "Iteration 18227, Loss: 0.05576205253601074\n",
      "Iteration 18228, Loss: 0.05570264905691147\n",
      "Iteration 18229, Loss: 0.05579233542084694\n",
      "Iteration 18230, Loss: 0.05577639862895012\n",
      "Iteration 18231, Loss: 0.05566636845469475\n",
      "Iteration 18232, Loss: 0.055795155465602875\n",
      "Iteration 18233, Loss: 0.05585666745901108\n",
      "Iteration 18234, Loss: 0.05573916435241699\n",
      "Iteration 18235, Loss: 0.05572112649679184\n",
      "Iteration 18236, Loss: 0.05581200495362282\n",
      "Iteration 18237, Loss: 0.055797021836042404\n",
      "Iteration 18238, Loss: 0.05568854138255119\n",
      "Iteration 18239, Loss: 0.05576412007212639\n",
      "Iteration 18240, Loss: 0.055824004113674164\n",
      "Iteration 18241, Loss: 0.055705588310956955\n",
      "Iteration 18242, Loss: 0.05574711412191391\n",
      "Iteration 18243, Loss: 0.05583835020661354\n",
      "Iteration 18244, Loss: 0.055824559181928635\n",
      "Iteration 18245, Loss: 0.055717628449201584\n",
      "Iteration 18246, Loss: 0.05572390556335449\n",
      "Iteration 18247, Loss: 0.05578359216451645\n",
      "Iteration 18248, Loss: 0.055666886270046234\n",
      "Iteration 18249, Loss: 0.05577377602458\n",
      "Iteration 18250, Loss: 0.05586298555135727\n",
      "Iteration 18251, Loss: 0.05584852024912834\n",
      "Iteration 18252, Loss: 0.05574166774749756\n",
      "Iteration 18253, Loss: 0.055691998451948166\n",
      "Iteration 18254, Loss: 0.0557529553771019\n",
      "Iteration 18255, Loss: 0.05564320087432861\n",
      "Iteration 18256, Loss: 0.05577639862895012\n",
      "Iteration 18257, Loss: 0.05585193634033203\n",
      "Iteration 18258, Loss: 0.05582519620656967\n",
      "Iteration 18259, Loss: 0.05570642277598381\n",
      "Iteration 18260, Loss: 0.05575243756175041\n",
      "Iteration 18261, Loss: 0.05582519620656967\n",
      "Iteration 18262, Loss: 0.05572009086608887\n",
      "Iteration 18263, Loss: 0.05572644993662834\n",
      "Iteration 18264, Loss: 0.05580969899892807\n",
      "Iteration 18265, Loss: 0.05578986927866936\n",
      "Iteration 18266, Loss: 0.055677931755781174\n",
      "Iteration 18267, Loss: 0.055782318115234375\n",
      "Iteration 18268, Loss: 0.055847011506557465\n",
      "Iteration 18269, Loss: 0.05573451891541481\n",
      "Iteration 18270, Loss: 0.05572092533111572\n",
      "Iteration 18271, Loss: 0.05580870434641838\n",
      "Iteration 18272, Loss: 0.05579320713877678\n",
      "Iteration 18273, Loss: 0.055685125291347504\n",
      "Iteration 18274, Loss: 0.055768292397260666\n",
      "Iteration 18275, Loss: 0.055829089134931564\n",
      "Iteration 18276, Loss: 0.05571270361542702\n",
      "Iteration 18277, Loss: 0.05573968216776848\n",
      "Iteration 18278, Loss: 0.055829524993896484\n",
      "Iteration 18279, Loss: 0.055815935134887695\n",
      "Iteration 18280, Loss: 0.0557093620300293\n",
      "Iteration 18281, Loss: 0.05573344603180885\n",
      "Iteration 18282, Loss: 0.05579268932342529\n",
      "Iteration 18283, Loss: 0.05567502975463867\n",
      "Iteration 18284, Loss: 0.05576833337545395\n",
      "Iteration 18285, Loss: 0.055859051644802094\n",
      "Iteration 18286, Loss: 0.05584617704153061\n",
      "Iteration 18287, Loss: 0.05574023723602295\n",
      "Iteration 18288, Loss: 0.055691640824079514\n",
      "Iteration 18289, Loss: 0.05575168505311012\n",
      "Iteration 18290, Loss: 0.0556364469230175\n",
      "Iteration 18291, Loss: 0.05579284951090813\n",
      "Iteration 18292, Loss: 0.05588027089834213\n",
      "Iteration 18293, Loss: 0.05586453527212143\n",
      "Iteration 18294, Loss: 0.05575593560934067\n",
      "Iteration 18295, Loss: 0.055672965943813324\n",
      "Iteration 18296, Loss: 0.05573415756225586\n",
      "Iteration 18297, Loss: 0.05561622232198715\n",
      "Iteration 18298, Loss: 0.05581168457865715\n",
      "Iteration 18299, Loss: 0.055902641266584396\n",
      "Iteration 18300, Loss: 0.055890005081892014\n",
      "Iteration 18301, Loss: 0.05578434467315674\n",
      "Iteration 18302, Loss: 0.055631160736083984\n",
      "Iteration 18303, Loss: 0.05568961426615715\n",
      "Iteration 18304, Loss: 0.0556412935256958\n",
      "Iteration 18305, Loss: 0.055629175156354904\n",
      "Iteration 18306, Loss: 0.05572529882192612\n",
      "Iteration 18307, Loss: 0.055681388825178146\n",
      "Iteration 18308, Loss: 0.05571318045258522\n",
      "Iteration 18309, Loss: 0.055757563561201096\n",
      "Iteration 18310, Loss: 0.055701617151498795\n",
      "Iteration 18311, Loss: 0.05568341538310051\n",
      "Iteration 18312, Loss: 0.05569029226899147\n",
      "Iteration 18313, Loss: 0.055673401802778244\n",
      "Iteration 18314, Loss: 0.05568739026784897\n",
      "Iteration 18315, Loss: 0.055624090135097504\n",
      "Iteration 18316, Loss: 0.05570828914642334\n",
      "Iteration 18317, Loss: 0.05565766617655754\n",
      "Iteration 18318, Loss: 0.05572867393493652\n",
      "Iteration 18319, Loss: 0.05575629323720932\n",
      "Iteration 18320, Loss: 0.055666450411081314\n",
      "Iteration 18321, Loss: 0.0557636022567749\n",
      "Iteration 18322, Loss: 0.05581025406718254\n",
      "Iteration 18323, Loss: 0.05570356175303459\n",
      "Iteration 18324, Loss: 0.055738769471645355\n",
      "Iteration 18325, Loss: 0.05581510066986084\n",
      "Iteration 18326, Loss: 0.05576698109507561\n",
      "Iteration 18327, Loss: 0.05563434213399887\n",
      "Iteration 18328, Loss: 0.05579423904418945\n",
      "Iteration 18329, Loss: 0.05580810829997063\n",
      "Iteration 18330, Loss: 0.05565246194601059\n",
      "Iteration 18331, Loss: 0.05581474304199219\n",
      "Iteration 18332, Loss: 0.055927835404872894\n",
      "Iteration 18333, Loss: 0.05592691898345947\n",
      "Iteration 18334, Loss: 0.05582563206553459\n",
      "Iteration 18335, Loss: 0.055658500641584396\n",
      "Iteration 18336, Loss: 0.05586608499288559\n",
      "Iteration 18337, Loss: 0.0559663400053978\n",
      "Iteration 18338, Loss: 0.05588337033987045\n",
      "Iteration 18339, Loss: 0.05564276501536369\n",
      "Iteration 18340, Loss: 0.055861394852399826\n",
      "Iteration 18341, Loss: 0.05601295083761215\n",
      "Iteration 18342, Loss: 0.05605431646108627\n",
      "Iteration 18343, Loss: 0.05599598214030266\n",
      "Iteration 18344, Loss: 0.05584907904267311\n",
      "Iteration 18345, Loss: 0.05562734603881836\n",
      "Iteration 18346, Loss: 0.0559723787009716\n",
      "Iteration 18347, Loss: 0.05614396184682846\n",
      "Iteration 18348, Loss: 0.056126952171325684\n",
      "Iteration 18349, Loss: 0.05594205856323242\n",
      "Iteration 18350, Loss: 0.05564789101481438\n",
      "Iteration 18351, Loss: 0.05585793778300285\n",
      "Iteration 18352, Loss: 0.055989306420087814\n",
      "Iteration 18353, Loss: 0.05600631609559059\n",
      "Iteration 18354, Loss: 0.05592012405395508\n",
      "Iteration 18355, Loss: 0.05574413388967514\n",
      "Iteration 18356, Loss: 0.05576988309621811\n",
      "Iteration 18357, Loss: 0.05590411275625229\n",
      "Iteration 18358, Loss: 0.05586433410644531\n",
      "Iteration 18359, Loss: 0.05566529557108879\n",
      "Iteration 18360, Loss: 0.05583548918366432\n",
      "Iteration 18361, Loss: 0.05597563832998276\n",
      "Iteration 18362, Loss: 0.056000713258981705\n",
      "Iteration 18363, Loss: 0.05592409893870354\n",
      "Iteration 18364, Loss: 0.05576137825846672\n",
      "Iteration 18365, Loss: 0.05574055761098862\n",
      "Iteration 18366, Loss: 0.05586513131856918\n",
      "Iteration 18367, Loss: 0.05582193657755852\n",
      "Iteration 18368, Loss: 0.05563509464263916\n",
      "Iteration 18369, Loss: 0.05580031871795654\n",
      "Iteration 18370, Loss: 0.05588797852396965\n",
      "Iteration 18371, Loss: 0.05587264150381088\n",
      "Iteration 18372, Loss: 0.055764198303222656\n",
      "Iteration 18373, Loss: 0.055662237107753754\n",
      "Iteration 18374, Loss: 0.05572442337870598\n",
      "Iteration 18375, Loss: 0.055620789527893066\n",
      "Iteration 18376, Loss: 0.0557352714240551\n",
      "Iteration 18377, Loss: 0.055740080773830414\n",
      "Iteration 18378, Loss: 0.05564224720001221\n",
      "Iteration 18379, Loss: 0.055790066719055176\n",
      "Iteration 18380, Loss: 0.05582205578684807\n",
      "Iteration 18381, Loss: 0.05568762868642807\n",
      "Iteration 18382, Loss: 0.05577262490987778\n",
      "Iteration 18383, Loss: 0.055871009826660156\n",
      "Iteration 18384, Loss: 0.0558522567152977\n",
      "Iteration 18385, Loss: 0.05573292821645737\n",
      "Iteration 18386, Loss: 0.05572088807821274\n",
      "Iteration 18387, Loss: 0.055798571556806564\n",
      "Iteration 18388, Loss: 0.05570952221751213\n",
      "Iteration 18389, Loss: 0.05572700500488281\n",
      "Iteration 18390, Loss: 0.055798016488552094\n",
      "Iteration 18391, Loss: 0.05576149746775627\n",
      "Iteration 18392, Loss: 0.05565003678202629\n",
      "Iteration 18393, Loss: 0.05580366030335426\n",
      "Iteration 18394, Loss: 0.05584128946065903\n",
      "Iteration 18395, Loss: 0.055704355239868164\n",
      "Iteration 18396, Loss: 0.05576050281524658\n",
      "Iteration 18397, Loss: 0.05586230754852295\n",
      "Iteration 18398, Loss: 0.055859290063381195\n",
      "Iteration 18399, Loss: 0.05576273053884506\n",
      "Iteration 18400, Loss: 0.05565508455038071\n",
      "Iteration 18401, Loss: 0.055728040635585785\n",
      "Iteration 18402, Loss: 0.05565663427114487\n",
      "Iteration 18403, Loss: 0.055733561515808105\n",
      "Iteration 18404, Loss: 0.055773377418518066\n",
      "Iteration 18405, Loss: 0.055708885192871094\n",
      "Iteration 18406, Loss: 0.055687230080366135\n",
      "Iteration 18407, Loss: 0.05570785328745842\n",
      "Iteration 18408, Loss: 0.0556466206908226\n",
      "Iteration 18409, Loss: 0.055649083107709885\n",
      "Iteration 18410, Loss: 0.05568750947713852\n",
      "Iteration 18411, Loss: 0.0556417740881443\n",
      "Iteration 18412, Loss: 0.055742304772138596\n",
      "Iteration 18413, Loss: 0.05577632039785385\n",
      "Iteration 18414, Loss: 0.05570241063833237\n",
      "Iteration 18415, Loss: 0.05570654198527336\n",
      "Iteration 18416, Loss: 0.055740635842084885\n",
      "Iteration 18417, Loss: 0.055619917809963226\n",
      "Iteration 18418, Loss: 0.05580469220876694\n",
      "Iteration 18419, Loss: 0.05588718503713608\n",
      "Iteration 18420, Loss: 0.055864691734313965\n",
      "Iteration 18421, Loss: 0.055749934166669846\n",
      "Iteration 18422, Loss: 0.0556948184967041\n",
      "Iteration 18423, Loss: 0.055772505700588226\n",
      "Iteration 18424, Loss: 0.055685918778181076\n",
      "Iteration 18425, Loss: 0.05574055761098862\n",
      "Iteration 18426, Loss: 0.05581037327647209\n",
      "Iteration 18427, Loss: 0.05578061193227768\n",
      "Iteration 18428, Loss: 0.0556640625\n",
      "Iteration 18429, Loss: 0.05580739304423332\n",
      "Iteration 18430, Loss: 0.055874429643154144\n",
      "Iteration 18431, Loss: 0.05576130002737045\n",
      "Iteration 18432, Loss: 0.055702291429042816\n",
      "Iteration 18433, Loss: 0.05579070374369621\n",
      "Iteration 18434, Loss: 0.055776797235012054\n",
      "Iteration 18435, Loss: 0.05567074194550514\n",
      "Iteration 18436, Loss: 0.0557861328125\n",
      "Iteration 18437, Loss: 0.05584351345896721\n",
      "Iteration 18438, Loss: 0.05572187900543213\n",
      "Iteration 18439, Loss: 0.05573749542236328\n",
      "Iteration 18440, Loss: 0.055831316858530045\n",
      "Iteration 18441, Loss: 0.055821023881435394\n",
      "Iteration 18442, Loss: 0.0557178258895874\n",
      "Iteration 18443, Loss: 0.05571933835744858\n",
      "Iteration 18444, Loss: 0.055774055421352386\n",
      "Iteration 18445, Loss: 0.055650435388088226\n",
      "Iteration 18446, Loss: 0.05579086393117905\n",
      "Iteration 18447, Loss: 0.055885199457407\n",
      "Iteration 18448, Loss: 0.05587562173604965\n",
      "Iteration 18449, Loss: 0.05577262490987778\n",
      "Iteration 18450, Loss: 0.05564519017934799\n",
      "Iteration 18451, Loss: 0.05570332333445549\n",
      "Iteration 18452, Loss: 0.055633626878261566\n",
      "Iteration 18453, Loss: 0.055654846131801605\n",
      "Iteration 18454, Loss: 0.05565027520060539\n",
      "Iteration 18455, Loss: 0.05564272776246071\n",
      "Iteration 18456, Loss: 0.05563521757721901\n",
      "Iteration 18457, Loss: 0.05568408966064453\n",
      "Iteration 18458, Loss: 0.05566982552409172\n",
      "Iteration 18459, Loss: 0.05567697808146477\n",
      "Iteration 18460, Loss: 0.05563784018158913\n",
      "Iteration 18461, Loss: 0.05574345588684082\n",
      "Iteration 18462, Loss: 0.05578514188528061\n",
      "Iteration 18463, Loss: 0.055727921426296234\n",
      "Iteration 18464, Loss: 0.05565246194601059\n",
      "Iteration 18465, Loss: 0.055675309151411057\n",
      "Iteration 18466, Loss: 0.055670738220214844\n",
      "Iteration 18467, Loss: 0.055672645568847656\n",
      "Iteration 18468, Loss: 0.055653415620326996\n",
      "Iteration 18469, Loss: 0.055645983666181564\n",
      "Iteration 18470, Loss: 0.055660806596279144\n",
      "Iteration 18471, Loss: 0.05562261864542961\n",
      "Iteration 18472, Loss: 0.05574135109782219\n",
      "Iteration 18473, Loss: 0.0557527169585228\n",
      "Iteration 18474, Loss: 0.05566195771098137\n",
      "Iteration 18475, Loss: 0.05576682090759277\n",
      "Iteration 18476, Loss: 0.05580171197652817\n",
      "Iteration 18477, Loss: 0.05567304417490959\n",
      "Iteration 18478, Loss: 0.05578327178955078\n",
      "Iteration 18479, Loss: 0.055877648293972015\n",
      "Iteration 18480, Loss: 0.05585316941142082\n",
      "Iteration 18481, Loss: 0.055728476494550705\n",
      "Iteration 18482, Loss: 0.055734239518642426\n",
      "Iteration 18483, Loss: 0.05581645295023918\n",
      "Iteration 18484, Loss: 0.05573328584432602\n",
      "Iteration 18485, Loss: 0.0557052306830883\n",
      "Iteration 18486, Loss: 0.05577167123556137\n",
      "Iteration 18487, Loss: 0.055731140077114105\n",
      "Iteration 18488, Loss: 0.055651746690273285\n",
      "Iteration 18489, Loss: 0.055729590356349945\n",
      "Iteration 18490, Loss: 0.05568178743124008\n",
      "Iteration 18491, Loss: 0.05571417137980461\n",
      "Iteration 18492, Loss: 0.05575641244649887\n",
      "Iteration 18493, Loss: 0.05570375919342041\n",
      "Iteration 18494, Loss: 0.05568043515086174\n",
      "Iteration 18495, Loss: 0.055684369057416916\n",
      "Iteration 18496, Loss: 0.05567936226725578\n",
      "Iteration 18497, Loss: 0.055695097893476486\n",
      "Iteration 18498, Loss: 0.055623654276132584\n",
      "Iteration 18499, Loss: 0.05577699467539787\n",
      "Iteration 18500, Loss: 0.055784743279218674\n",
      "Iteration 18501, Loss: 0.05565810203552246\n",
      "Iteration 18502, Loss: 0.05578192323446274\n",
      "Iteration 18503, Loss: 0.05585193634033203\n",
      "Iteration 18504, Loss: 0.055793605744838715\n",
      "Iteration 18505, Loss: 0.055644355714321136\n",
      "Iteration 18506, Loss: 0.05582161992788315\n",
      "Iteration 18507, Loss: 0.05588313192129135\n",
      "Iteration 18508, Loss: 0.05578295513987541\n",
      "Iteration 18509, Loss: 0.05567161366343498\n",
      "Iteration 18510, Loss: 0.055752240121364594\n",
      "Iteration 18511, Loss: 0.055718861520290375\n",
      "Iteration 18512, Loss: 0.05564793199300766\n",
      "Iteration 18513, Loss: 0.05565560236573219\n",
      "Iteration 18514, Loss: 0.055695973336696625\n",
      "Iteration 18515, Loss: 0.055701255798339844\n",
      "Iteration 18516, Loss: 0.055634260177612305\n",
      "Iteration 18517, Loss: 0.05572768300771713\n",
      "Iteration 18518, Loss: 0.05568019673228264\n",
      "Iteration 18519, Loss: 0.05571695417165756\n",
      "Iteration 18520, Loss: 0.05576284974813461\n",
      "Iteration 18521, Loss: 0.055708132684230804\n",
      "Iteration 18522, Loss: 0.055675070732831955\n",
      "Iteration 18523, Loss: 0.05567888543009758\n",
      "Iteration 18524, Loss: 0.055685482919216156\n",
      "Iteration 18525, Loss: 0.0557025671005249\n",
      "Iteration 18526, Loss: 0.05562484636902809\n",
      "Iteration 18527, Loss: 0.05580699443817139\n",
      "Iteration 18528, Loss: 0.05582551285624504\n",
      "Iteration 18529, Loss: 0.055669110268354416\n",
      "Iteration 18530, Loss: 0.05579908937215805\n",
      "Iteration 18531, Loss: 0.0559130534529686\n",
      "Iteration 18532, Loss: 0.05592111870646477\n",
      "Iteration 18533, Loss: 0.05583366006612778\n",
      "Iteration 18534, Loss: 0.05566171929240227\n",
      "Iteration 18535, Loss: 0.05587979406118393\n",
      "Iteration 18536, Loss: 0.0560099296271801\n",
      "Iteration 18537, Loss: 0.05595290660858154\n",
      "Iteration 18538, Loss: 0.05572891607880592\n",
      "Iteration 18539, Loss: 0.05579988285899162\n",
      "Iteration 18540, Loss: 0.05595429986715317\n",
      "Iteration 18541, Loss: 0.05599868297576904\n",
      "Iteration 18542, Loss: 0.05594360828399658\n",
      "Iteration 18543, Loss: 0.055799923837184906\n",
      "Iteration 18544, Loss: 0.05565774440765381\n",
      "Iteration 18545, Loss: 0.05575820058584213\n",
      "Iteration 18546, Loss: 0.05567852780222893\n",
      "Iteration 18547, Loss: 0.0557403564453125\n",
      "Iteration 18548, Loss: 0.05580727383494377\n",
      "Iteration 18549, Loss: 0.05577322095632553\n",
      "Iteration 18550, Loss: 0.0556485690176487\n",
      "Iteration 18551, Loss: 0.05583878606557846\n",
      "Iteration 18552, Loss: 0.055917028337717056\n",
      "Iteration 18553, Loss: 0.05581554025411606\n",
      "Iteration 18554, Loss: 0.05565568059682846\n",
      "Iteration 18555, Loss: 0.05573940649628639\n",
      "Iteration 18556, Loss: 0.05572275444865227\n",
      "Iteration 18557, Loss: 0.055614035576581955\n",
      "Iteration 18558, Loss: 0.055824678391218185\n",
      "Iteration 18559, Loss: 0.05586497113108635\n",
      "Iteration 18560, Loss: 0.05575207993388176\n",
      "Iteration 18561, Loss: 0.05570630356669426\n",
      "Iteration 18562, Loss: 0.05579265207052231\n",
      "Iteration 18563, Loss: 0.05576491355895996\n",
      "Iteration 18564, Loss: 0.0556282214820385\n",
      "Iteration 18565, Loss: 0.05586107820272446\n",
      "Iteration 18566, Loss: 0.055950723588466644\n",
      "Iteration 18567, Loss: 0.05587844178080559\n",
      "Iteration 18568, Loss: 0.055680036544799805\n",
      "Iteration 18569, Loss: 0.055824797600507736\n",
      "Iteration 18570, Loss: 0.055953703820705414\n",
      "Iteration 18571, Loss: 0.05595235154032707\n",
      "Iteration 18572, Loss: 0.05583493039011955\n",
      "Iteration 18573, Loss: 0.05565575882792473\n",
      "Iteration 18574, Loss: 0.0558478869497776\n",
      "Iteration 18575, Loss: 0.055929265916347504\n",
      "Iteration 18576, Loss: 0.055844347923994064\n",
      "Iteration 18577, Loss: 0.0556185282766819\n",
      "Iteration 18578, Loss: 0.05580946058034897\n",
      "Iteration 18579, Loss: 0.05586659908294678\n",
      "Iteration 18580, Loss: 0.055797021836042404\n",
      "Iteration 18581, Loss: 0.05566000938415527\n",
      "Iteration 18582, Loss: 0.05579988285899162\n",
      "Iteration 18583, Loss: 0.055846136063337326\n",
      "Iteration 18584, Loss: 0.05573415756225586\n",
      "Iteration 18585, Loss: 0.05572231858968735\n",
      "Iteration 18586, Loss: 0.055806439369916916\n",
      "Iteration 18587, Loss: 0.05577782914042473\n",
      "Iteration 18588, Loss: 0.05566295236349106\n",
      "Iteration 18589, Loss: 0.055795591324567795\n",
      "Iteration 18590, Loss: 0.05585193634033203\n",
      "Iteration 18591, Loss: 0.055738650262355804\n",
      "Iteration 18592, Loss: 0.05571901798248291\n",
      "Iteration 18593, Loss: 0.05580596253275871\n",
      "Iteration 18594, Loss: 0.0557863712310791\n",
      "Iteration 18595, Loss: 0.05567499250173569\n",
      "Iteration 18596, Loss: 0.05578422546386719\n",
      "Iteration 18597, Loss: 0.0558466911315918\n",
      "Iteration 18598, Loss: 0.05573539063334465\n",
      "Iteration 18599, Loss: 0.05572013184428215\n",
      "Iteration 18600, Loss: 0.0558064803481102\n",
      "Iteration 18601, Loss: 0.055788200348615646\n",
      "Iteration 18602, Loss: 0.055677615106105804\n",
      "Iteration 18603, Loss: 0.05578112602233887\n",
      "Iteration 18604, Loss: 0.055843912065029144\n",
      "Iteration 18605, Loss: 0.05573117733001709\n",
      "Iteration 18606, Loss: 0.05572418496012688\n",
      "Iteration 18607, Loss: 0.05581176280975342\n",
      "Iteration 18608, Loss: 0.05579531192779541\n",
      "Iteration 18609, Loss: 0.05568619817495346\n",
      "Iteration 18610, Loss: 0.055768609046936035\n",
      "Iteration 18611, Loss: 0.055830519646406174\n",
      "Iteration 18612, Loss: 0.055715881288051605\n",
      "Iteration 18613, Loss: 0.055736660957336426\n",
      "Iteration 18614, Loss: 0.05582527443766594\n",
      "Iteration 18615, Loss: 0.05581005662679672\n",
      "Iteration 18616, Loss: 0.055702369660139084\n",
      "Iteration 18617, Loss: 0.055745963007211685\n",
      "Iteration 18618, Loss: 0.05580679699778557\n",
      "Iteration 18619, Loss: 0.055691324174404144\n",
      "Iteration 18620, Loss: 0.05575593560934067\n",
      "Iteration 18621, Loss: 0.05584554001688957\n",
      "Iteration 18622, Loss: 0.05583123490214348\n",
      "Iteration 18623, Loss: 0.05572442337870598\n",
      "Iteration 18624, Loss: 0.05571524426341057\n",
      "Iteration 18625, Loss: 0.055775683373212814\n",
      "Iteration 18626, Loss: 0.05566032975912094\n",
      "Iteration 18627, Loss: 0.055779021233320236\n",
      "Iteration 18628, Loss: 0.055868785828351974\n",
      "Iteration 18629, Loss: 0.055855076760053635\n",
      "Iteration 18630, Loss: 0.05574890226125717\n",
      "Iteration 18631, Loss: 0.0556817464530468\n",
      "Iteration 18632, Loss: 0.055742423981428146\n",
      "Iteration 18633, Loss: 0.055630724877119064\n",
      "Iteration 18634, Loss: 0.05578800290822983\n",
      "Iteration 18635, Loss: 0.055865053087472916\n",
      "Iteration 18636, Loss: 0.05583823099732399\n",
      "Iteration 18637, Loss: 0.055718861520290375\n",
      "Iteration 18638, Loss: 0.05573729798197746\n",
      "Iteration 18639, Loss: 0.05580977723002434\n",
      "Iteration 18640, Loss: 0.05570264905691147\n",
      "Iteration 18641, Loss: 0.055742304772138596\n",
      "Iteration 18642, Loss: 0.05582694336771965\n",
      "Iteration 18643, Loss: 0.055807989090681076\n",
      "Iteration 18644, Loss: 0.055697642266750336\n",
      "Iteration 18645, Loss: 0.055756013840436935\n",
      "Iteration 18646, Loss: 0.05581852048635483\n",
      "Iteration 18647, Loss: 0.05570189282298088\n",
      "Iteration 18648, Loss: 0.05574878305196762\n",
      "Iteration 18649, Loss: 0.05583878606557846\n",
      "Iteration 18650, Loss: 0.05582547187805176\n",
      "Iteration 18651, Loss: 0.05571973696351051\n",
      "Iteration 18652, Loss: 0.05572013184428215\n",
      "Iteration 18653, Loss: 0.0557781457901001\n",
      "Iteration 18654, Loss: 0.055659376084804535\n",
      "Iteration 18655, Loss: 0.05577965825796127\n",
      "Iteration 18656, Loss: 0.05586930364370346\n",
      "Iteration 18657, Loss: 0.05585598945617676\n",
      "Iteration 18658, Loss: 0.05574989318847656\n",
      "Iteration 18659, Loss: 0.055679045617580414\n",
      "Iteration 18660, Loss: 0.05573749542236328\n",
      "Iteration 18661, Loss: 0.055622659623622894\n",
      "Iteration 18662, Loss: 0.05578716844320297\n",
      "Iteration 18663, Loss: 0.05585348606109619\n",
      "Iteration 18664, Loss: 0.0558144673705101\n",
      "Iteration 18665, Loss: 0.05568588152527809\n",
      "Iteration 18666, Loss: 0.055790822952985764\n",
      "Iteration 18667, Loss: 0.05587228387594223\n",
      "Iteration 18668, Loss: 0.055779337882995605\n",
      "Iteration 18669, Loss: 0.055674199014902115\n",
      "Iteration 18670, Loss: 0.055749379098415375\n",
      "Iteration 18671, Loss: 0.055720292031764984\n",
      "Iteration 18672, Loss: 0.05563334748148918\n",
      "Iteration 18673, Loss: 0.055690012872219086\n",
      "Iteration 18674, Loss: 0.05563807487487793\n",
      "Iteration 18675, Loss: 0.05569235607981682\n",
      "Iteration 18676, Loss: 0.05565556138753891\n",
      "Iteration 18677, Loss: 0.05572962760925293\n",
      "Iteration 18678, Loss: 0.05574468895792961\n",
      "Iteration 18679, Loss: 0.05564725399017334\n",
      "Iteration 18680, Loss: 0.05576952546834946\n",
      "Iteration 18681, Loss: 0.05579908937215805\n",
      "Iteration 18682, Loss: 0.05568051338195801\n",
      "Iteration 18683, Loss: 0.05577019974589348\n",
      "Iteration 18684, Loss: 0.055853843688964844\n",
      "Iteration 18685, Loss: 0.05581267923116684\n",
      "Iteration 18686, Loss: 0.05567514896392822\n",
      "Iteration 18687, Loss: 0.05580075830221176\n",
      "Iteration 18688, Loss: 0.05588368698954582\n",
      "Iteration 18689, Loss: 0.055806320160627365\n",
      "Iteration 18690, Loss: 0.05563589185476303\n",
      "Iteration 18691, Loss: 0.05572299286723137\n",
      "Iteration 18692, Loss: 0.05568762868642807\n",
      "Iteration 18693, Loss: 0.05569259449839592\n",
      "Iteration 18694, Loss: 0.05570431798696518\n",
      "Iteration 18695, Loss: 0.05564193055033684\n",
      "Iteration 18696, Loss: 0.055653177201747894\n",
      "Iteration 18697, Loss: 0.05568492412567139\n",
      "Iteration 18698, Loss: 0.055669669061899185\n",
      "Iteration 18699, Loss: 0.05568337440490723\n",
      "Iteration 18700, Loss: 0.055664144456386566\n",
      "Iteration 18701, Loss: 0.0557100772857666\n",
      "Iteration 18702, Loss: 0.05572597309947014\n",
      "Iteration 18703, Loss: 0.05564359948039055\n",
      "Iteration 18704, Loss: 0.05576789379119873\n",
      "Iteration 18705, Loss: 0.055777233093976974\n",
      "Iteration 18706, Loss: 0.05562214180827141\n",
      "Iteration 18707, Loss: 0.05583620443940163\n",
      "Iteration 18708, Loss: 0.05593840405344963\n",
      "Iteration 18709, Loss: 0.055918656289577484\n",
      "Iteration 18710, Loss: 0.055795036256313324\n",
      "Iteration 18711, Loss: 0.05567634478211403\n",
      "Iteration 18712, Loss: 0.05580493062734604\n",
      "Iteration 18713, Loss: 0.05580004304647446\n",
      "Iteration 18714, Loss: 0.05565508455038071\n",
      "Iteration 18715, Loss: 0.05577787011861801\n",
      "Iteration 18716, Loss: 0.055854640901088715\n",
      "Iteration 18717, Loss: 0.055831193923950195\n",
      "Iteration 18718, Loss: 0.055718064308166504\n",
      "Iteration 18719, Loss: 0.05573451519012451\n",
      "Iteration 18720, Loss: 0.05580063909292221\n",
      "Iteration 18721, Loss: 0.05569291114807129\n",
      "Iteration 18722, Loss: 0.05574802681803703\n",
      "Iteration 18723, Loss: 0.05583071708679199\n",
      "Iteration 18724, Loss: 0.05581124871969223\n",
      "Iteration 18725, Loss: 0.0556996688246727\n",
      "Iteration 18726, Loss: 0.05575474351644516\n",
      "Iteration 18727, Loss: 0.0558193139731884\n",
      "Iteration 18728, Loss: 0.05570737645030022\n",
      "Iteration 18729, Loss: 0.05574091523885727\n",
      "Iteration 18730, Loss: 0.055827777832746506\n",
      "Iteration 18731, Loss: 0.05581117048859596\n",
      "Iteration 18732, Loss: 0.05570169538259506\n",
      "Iteration 18733, Loss: 0.05574822425842285\n",
      "Iteration 18734, Loss: 0.05581089109182358\n",
      "Iteration 18735, Loss: 0.055696647614240646\n",
      "Iteration 18736, Loss: 0.05575041100382805\n",
      "Iteration 18737, Loss: 0.05583878606557846\n",
      "Iteration 18738, Loss: 0.05582328885793686\n",
      "Iteration 18739, Loss: 0.05571504682302475\n",
      "Iteration 18740, Loss: 0.055729273706674576\n",
      "Iteration 18741, Loss: 0.0557909831404686\n",
      "Iteration 18742, Loss: 0.055676065385341644\n",
      "Iteration 18743, Loss: 0.055766742676496506\n",
      "Iteration 18744, Loss: 0.05585595220327377\n",
      "Iteration 18745, Loss: 0.05584144592285156\n",
      "Iteration 18746, Loss: 0.05573415756225586\n",
      "Iteration 18747, Loss: 0.0557025671005249\n",
      "Iteration 18748, Loss: 0.05576400086283684\n",
      "Iteration 18749, Loss: 0.05564979836344719\n",
      "Iteration 18750, Loss: 0.055784743279218674\n",
      "Iteration 18751, Loss: 0.0558728389441967\n",
      "Iteration 18752, Loss: 0.055857859551906586\n",
      "Iteration 18753, Loss: 0.0557502917945385\n",
      "Iteration 18754, Loss: 0.05568075180053711\n",
      "Iteration 18755, Loss: 0.055741630494594574\n",
      "Iteration 18756, Loss: 0.055625878274440765\n",
      "Iteration 18757, Loss: 0.055796269327402115\n",
      "Iteration 18758, Loss: 0.05587872117757797\n",
      "Iteration 18759, Loss: 0.05585666745901108\n",
      "Iteration 18760, Loss: 0.05574158951640129\n",
      "Iteration 18761, Loss: 0.05570086091756821\n",
      "Iteration 18762, Loss: 0.05577035993337631\n",
      "Iteration 18763, Loss: 0.05566374585032463\n",
      "Iteration 18764, Loss: 0.05576753616333008\n",
      "Iteration 18765, Loss: 0.05585014820098877\n",
      "Iteration 18766, Loss: 0.055830005556344986\n",
      "Iteration 18767, Loss: 0.05571850389242172\n",
      "Iteration 18768, Loss: 0.055727481842041016\n",
      "Iteration 18769, Loss: 0.05579165741801262\n",
      "Iteration 18770, Loss: 0.055677853524684906\n",
      "Iteration 18771, Loss: 0.05576245114207268\n",
      "Iteration 18772, Loss: 0.05585014820098877\n",
      "Iteration 18773, Loss: 0.0558350495994091\n",
      "Iteration 18774, Loss: 0.0557277612388134\n",
      "Iteration 18775, Loss: 0.055709563195705414\n",
      "Iteration 18776, Loss: 0.05576924607157707\n",
      "Iteration 18777, Loss: 0.05565158650279045\n",
      "Iteration 18778, Loss: 0.05578390881419182\n",
      "Iteration 18779, Loss: 0.05587327480316162\n",
      "Iteration 18780, Loss: 0.055859409272670746\n",
      "Iteration 18781, Loss: 0.05575263500213623\n",
      "Iteration 18782, Loss: 0.055675309151411057\n",
      "Iteration 18783, Loss: 0.0557357482612133\n",
      "Iteration 18784, Loss: 0.055619996041059494\n",
      "Iteration 18785, Loss: 0.05580075830221176\n",
      "Iteration 18786, Loss: 0.055884044617414474\n",
      "Iteration 18787, Loss: 0.055863503366708755\n",
      "Iteration 18788, Loss: 0.055750489234924316\n",
      "Iteration 18789, Loss: 0.055686116218566895\n",
      "Iteration 18790, Loss: 0.05575374886393547\n",
      "Iteration 18791, Loss: 0.055646222084760666\n",
      "Iteration 18792, Loss: 0.055777911096811295\n",
      "Iteration 18793, Loss: 0.05585825443267822\n",
      "Iteration 18794, Loss: 0.05583643913269043\n",
      "Iteration 18795, Loss: 0.055722832679748535\n",
      "Iteration 18796, Loss: 0.05572354793548584\n",
      "Iteration 18797, Loss: 0.05579042807221413\n",
      "Iteration 18798, Loss: 0.05567864701151848\n",
      "Iteration 18799, Loss: 0.05576113983988762\n",
      "Iteration 18800, Loss: 0.05584832280874252\n",
      "Iteration 18801, Loss: 0.05583258718252182\n",
      "Iteration 18802, Loss: 0.055724501609802246\n",
      "Iteration 18803, Loss: 0.055714648216962814\n",
      "Iteration 18804, Loss: 0.055775802582502365\n",
      "Iteration 18805, Loss: 0.0556587390601635\n",
      "Iteration 18806, Loss: 0.05577949807047844\n",
      "Iteration 18807, Loss: 0.0558699406683445\n",
      "Iteration 18808, Loss: 0.05585698410868645\n",
      "Iteration 18809, Loss: 0.05575120449066162\n",
      "Iteration 18810, Loss: 0.05567586421966553\n",
      "Iteration 18811, Loss: 0.0557355098426342\n",
      "Iteration 18812, Loss: 0.0556204728782177\n",
      "Iteration 18813, Loss: 0.055790822952985764\n",
      "Iteration 18814, Loss: 0.05586346238851547\n",
      "Iteration 18815, Loss: 0.05583135411143303\n",
      "Iteration 18816, Loss: 0.05570753663778305\n",
      "Iteration 18817, Loss: 0.055757008492946625\n",
      "Iteration 18818, Loss: 0.055834174156188965\n",
      "Iteration 18819, Loss: 0.05573165416717529\n",
      "Iteration 18820, Loss: 0.05571715161204338\n",
      "Iteration 18821, Loss: 0.05579861253499985\n",
      "Iteration 18822, Loss: 0.05577683821320534\n",
      "Iteration 18823, Loss: 0.05566537380218506\n",
      "Iteration 18824, Loss: 0.055798254907131195\n",
      "Iteration 18825, Loss: 0.0558595284819603\n",
      "Iteration 18826, Loss: 0.05574170872569084\n",
      "Iteration 18827, Loss: 0.05571981519460678\n",
      "Iteration 18828, Loss: 0.05581101030111313\n",
      "Iteration 18829, Loss: 0.05579892918467522\n",
      "Iteration 18830, Loss: 0.05569501966238022\n",
      "Iteration 18831, Loss: 0.055750053375959396\n",
      "Iteration 18832, Loss: 0.055805567651987076\n",
      "Iteration 18833, Loss: 0.05568321794271469\n",
      "Iteration 18834, Loss: 0.05576471611857414\n",
      "Iteration 18835, Loss: 0.055858176201581955\n",
      "Iteration 18836, Loss: 0.055847883224487305\n",
      "Iteration 18837, Loss: 0.055744849145412445\n",
      "Iteration 18838, Loss: 0.055681388825178146\n",
      "Iteration 18839, Loss: 0.055737100541591644\n",
      "Iteration 18840, Loss: 0.05562456697225571\n",
      "Iteration 18841, Loss: 0.05577012151479721\n",
      "Iteration 18842, Loss: 0.055821340531110764\n",
      "Iteration 18843, Loss: 0.05576737970113754\n",
      "Iteration 18844, Loss: 0.05563871189951897\n",
      "Iteration 18845, Loss: 0.05582042783498764\n",
      "Iteration 18846, Loss: 0.05586179345846176\n",
      "Iteration 18847, Loss: 0.05572780221700668\n",
      "Iteration 18848, Loss: 0.05574047937989235\n",
      "Iteration 18849, Loss: 0.05584121122956276\n",
      "Iteration 18850, Loss: 0.055838149040937424\n",
      "Iteration 18851, Loss: 0.05574147030711174\n",
      "Iteration 18852, Loss: 0.055678367614746094\n",
      "Iteration 18853, Loss: 0.05572907254099846\n",
      "Iteration 18854, Loss: 0.055623650550842285\n",
      "Iteration 18855, Loss: 0.05571568384766579\n",
      "Iteration 18856, Loss: 0.05570817366242409\n",
      "Iteration 18857, Loss: 0.05563541501760483\n",
      "Iteration 18858, Loss: 0.055677734315395355\n",
      "Iteration 18859, Loss: 0.05564570426940918\n",
      "Iteration 18860, Loss: 0.055649284273386\n",
      "Iteration 18861, Loss: 0.05566314980387688\n",
      "Iteration 18862, Loss: 0.055641453713178635\n",
      "Iteration 18863, Loss: 0.05564944073557854\n",
      "Iteration 18864, Loss: 0.055658500641584396\n",
      "Iteration 18865, Loss: 0.055635374039411545\n",
      "Iteration 18866, Loss: 0.055668674409389496\n",
      "Iteration 18867, Loss: 0.0556383952498436\n",
      "Iteration 18868, Loss: 0.05563712120056152\n",
      "Iteration 18869, Loss: 0.05567272752523422\n",
      "Iteration 18870, Loss: 0.05562810227274895\n",
      "Iteration 18871, Loss: 0.055703163146972656\n",
      "Iteration 18872, Loss: 0.05564955994486809\n",
      "Iteration 18873, Loss: 0.05574580281972885\n",
      "Iteration 18874, Loss: 0.05578633397817612\n",
      "Iteration 18875, Loss: 0.05571313947439194\n",
      "Iteration 18876, Loss: 0.055695097893476486\n",
      "Iteration 18877, Loss: 0.05573316663503647\n",
      "Iteration 18878, Loss: 0.05562329292297363\n",
      "Iteration 18879, Loss: 0.055774133652448654\n",
      "Iteration 18880, Loss: 0.055832307785749435\n",
      "Iteration 18881, Loss: 0.0557887963950634\n",
      "Iteration 18882, Loss: 0.05565973371267319\n",
      "Iteration 18883, Loss: 0.0558268241584301\n",
      "Iteration 18884, Loss: 0.055905383080244064\n",
      "Iteration 18885, Loss: 0.055802345275878906\n",
      "Iteration 18886, Loss: 0.05566605180501938\n",
      "Iteration 18887, Loss: 0.055749259889125824\n",
      "Iteration 18888, Loss: 0.05573189631104469\n",
      "Iteration 18889, Loss: 0.05562901869416237\n",
      "Iteration 18890, Loss: 0.0558241605758667\n",
      "Iteration 18891, Loss: 0.05586715787649155\n",
      "Iteration 18892, Loss: 0.05573856830596924\n",
      "Iteration 18893, Loss: 0.055728793144226074\n",
      "Iteration 18894, Loss: 0.05582507699728012\n",
      "Iteration 18895, Loss: 0.05581676959991455\n",
      "Iteration 18896, Loss: 0.0557129792869091\n",
      "Iteration 18897, Loss: 0.055726610124111176\n",
      "Iteration 18898, Loss: 0.055784307420253754\n",
      "Iteration 18899, Loss: 0.055669866502285004\n",
      "Iteration 18900, Loss: 0.05577024072408676\n",
      "Iteration 18901, Loss: 0.055856626480817795\n",
      "Iteration 18902, Loss: 0.05583711713552475\n",
      "Iteration 18903, Loss: 0.05572358891367912\n",
      "Iteration 18904, Loss: 0.0557246208190918\n",
      "Iteration 18905, Loss: 0.0557938814163208\n",
      "Iteration 18906, Loss: 0.05569092556834221\n",
      "Iteration 18907, Loss: 0.055748503655195236\n",
      "Iteration 18908, Loss: 0.0558297261595726\n",
      "Iteration 18909, Loss: 0.05580727383494377\n",
      "Iteration 18910, Loss: 0.05569402500987053\n",
      "Iteration 18911, Loss: 0.05576316639780998\n",
      "Iteration 18912, Loss: 0.055829524993896484\n",
      "Iteration 18913, Loss: 0.05572013184428215\n",
      "Iteration 18914, Loss: 0.055730465799570084\n",
      "Iteration 18915, Loss: 0.05581621453166008\n",
      "Iteration 18916, Loss: 0.055798452347517014\n",
      "Iteration 18917, Loss: 0.05568937584757805\n",
      "Iteration 18918, Loss: 0.05576428025960922\n",
      "Iteration 18919, Loss: 0.05582603067159653\n",
      "Iteration 18920, Loss: 0.05571071431040764\n",
      "Iteration 18921, Loss: 0.05574111267924309\n",
      "Iteration 18922, Loss: 0.05583067983388901\n",
      "Iteration 18923, Loss: 0.05581669136881828\n",
      "Iteration 18924, Loss: 0.05571059510111809\n",
      "Iteration 18925, Loss: 0.055732887238264084\n",
      "Iteration 18926, Loss: 0.055791936814785004\n",
      "Iteration 18927, Loss: 0.055673997849226\n",
      "Iteration 18928, Loss: 0.05576996132731438\n",
      "Iteration 18929, Loss: 0.05586107820272446\n",
      "Iteration 18930, Loss: 0.055848799645900726\n",
      "Iteration 18931, Loss: 0.05574369803071022\n",
      "Iteration 18932, Loss: 0.05568655580282211\n",
      "Iteration 18933, Loss: 0.055745601654052734\n",
      "Iteration 18934, Loss: 0.055629532784223557\n",
      "Iteration 18935, Loss: 0.055795591324567795\n",
      "Iteration 18936, Loss: 0.055879514664411545\n",
      "Iteration 18937, Loss: 0.0558597669005394\n",
      "Iteration 18938, Loss: 0.05574743077158928\n",
      "Iteration 18939, Loss: 0.05569080635905266\n",
      "Iteration 18940, Loss: 0.05575677007436752\n",
      "Iteration 18941, Loss: 0.055645786225795746\n",
      "Iteration 18942, Loss: 0.05578327178955078\n",
      "Iteration 18943, Loss: 0.05586763471364975\n",
      "Iteration 18944, Loss: 0.05584939569234848\n",
      "Iteration 18945, Loss: 0.05573884770274162\n",
      "Iteration 18946, Loss: 0.05569903180003166\n",
      "Iteration 18947, Loss: 0.05576217547059059\n",
      "Iteration 18948, Loss: 0.05564693734049797\n",
      "Iteration 18949, Loss: 0.055786650627851486\n",
      "Iteration 18950, Loss: 0.0558750256896019\n",
      "Iteration 18951, Loss: 0.055860161781311035\n",
      "Iteration 18952, Loss: 0.05575239658355713\n",
      "Iteration 18953, Loss: 0.055677734315395355\n",
      "Iteration 18954, Loss: 0.05573880672454834\n",
      "Iteration 18955, Loss: 0.055623214691877365\n",
      "Iteration 18956, Loss: 0.05580226704478264\n",
      "Iteration 18957, Loss: 0.05588861554861069\n",
      "Iteration 18958, Loss: 0.05587121099233627\n",
      "Iteration 18959, Loss: 0.0557609423995018\n",
      "Iteration 18960, Loss: 0.05566927045583725\n",
      "Iteration 18961, Loss: 0.05573348328471184\n",
      "Iteration 18962, Loss: 0.05562683194875717\n",
      "Iteration 18963, Loss: 0.05577528476715088\n",
      "Iteration 18964, Loss: 0.05583536997437477\n",
      "Iteration 18965, Loss: 0.055791936814785004\n",
      "Iteration 18966, Loss: 0.05566048622131348\n",
      "Iteration 18967, Loss: 0.055826108902692795\n",
      "Iteration 18968, Loss: 0.0559079647064209\n",
      "Iteration 18969, Loss: 0.05581248179078102\n",
      "Iteration 18970, Loss: 0.05565170571208\n",
      "Iteration 18971, Loss: 0.05573097988963127\n",
      "Iteration 18972, Loss: 0.05570705980062485\n",
      "Iteration 18973, Loss: 0.05564026162028313\n",
      "Iteration 18974, Loss: 0.05562504380941391\n",
      "Iteration 18975, Loss: 0.055722713470458984\n",
      "Iteration 18976, Loss: 0.05573515221476555\n",
      "Iteration 18977, Loss: 0.055650752037763596\n",
      "Iteration 18978, Loss: 0.05578351020812988\n",
      "Iteration 18979, Loss: 0.055814824998378754\n",
      "Iteration 18980, Loss: 0.055670980364084244\n",
      "Iteration 18981, Loss: 0.05579046532511711\n",
      "Iteration 18982, Loss: 0.05589668080210686\n",
      "Iteration 18983, Loss: 0.055893898010253906\n",
      "Iteration 18984, Loss: 0.05579439923167229\n",
      "Iteration 18985, Loss: 0.05563819408416748\n",
      "Iteration 18986, Loss: 0.05583171173930168\n",
      "Iteration 18987, Loss: 0.05587097257375717\n",
      "Iteration 18988, Loss: 0.05573968216776848\n",
      "Iteration 18989, Loss: 0.05573137849569321\n",
      "Iteration 18990, Loss: 0.05582917109131813\n",
      "Iteration 18991, Loss: 0.0558243989944458\n",
      "Iteration 18992, Loss: 0.055725615471601486\n",
      "Iteration 18993, Loss: 0.0557052306830883\n",
      "Iteration 18994, Loss: 0.05575692653656006\n",
      "Iteration 18995, Loss: 0.05564173310995102\n",
      "Iteration 18996, Loss: 0.05578196048736572\n",
      "Iteration 18997, Loss: 0.05585622787475586\n",
      "Iteration 18998, Loss: 0.05582233518362045\n",
      "Iteration 18999, Loss: 0.055695414543151855\n",
      "Iteration 19000, Loss: 0.055775050073862076\n",
      "Iteration 19001, Loss: 0.055856071412563324\n",
      "Iteration 19002, Loss: 0.05576547235250473\n",
      "Iteration 19003, Loss: 0.05568186566233635\n",
      "Iteration 19004, Loss: 0.05575482174754143\n",
      "Iteration 19005, Loss: 0.05572211742401123\n",
      "Iteration 19006, Loss: 0.05563732236623764\n",
      "Iteration 19007, Loss: 0.05568854138255119\n",
      "Iteration 19008, Loss: 0.05564093589782715\n",
      "Iteration 19009, Loss: 0.05567590519785881\n",
      "Iteration 19010, Loss: 0.0556316003203392\n",
      "Iteration 19011, Loss: 0.05575728416442871\n",
      "Iteration 19012, Loss: 0.05577759072184563\n",
      "Iteration 19013, Loss: 0.05568162724375725\n",
      "Iteration 19014, Loss: 0.0557483471930027\n",
      "Iteration 19015, Loss: 0.05580008029937744\n",
      "Iteration 19016, Loss: 0.0557069405913353\n",
      "Iteration 19017, Loss: 0.05572807788848877\n",
      "Iteration 19018, Loss: 0.05579380318522453\n",
      "Iteration 19019, Loss: 0.05573487654328346\n",
      "Iteration 19020, Loss: 0.055667322129011154\n",
      "Iteration 19021, Loss: 0.055718861520290375\n",
      "Iteration 19022, Loss: 0.05564562603831291\n",
      "Iteration 19023, Loss: 0.05575207993388176\n",
      "Iteration 19024, Loss: 0.055800795555114746\n",
      "Iteration 19025, Loss: 0.05574989691376686\n",
      "Iteration 19026, Loss: 0.05564491078257561\n",
      "Iteration 19027, Loss: 0.05577484890818596\n",
      "Iteration 19028, Loss: 0.0557682141661644\n",
      "Iteration 19029, Loss: 0.05564320087432861\n",
      "Iteration 19030, Loss: 0.055724501609802246\n",
      "Iteration 19031, Loss: 0.05572732537984848\n",
      "Iteration 19032, Loss: 0.05562329292297363\n",
      "Iteration 19033, Loss: 0.055838823318481445\n",
      "Iteration 19034, Loss: 0.05589946359395981\n",
      "Iteration 19035, Loss: 0.055799126625061035\n",
      "Iteration 19036, Loss: 0.05566978454589844\n",
      "Iteration 19037, Loss: 0.05576833337545395\n",
      "Iteration 19038, Loss: 0.05576582998037338\n",
      "Iteration 19039, Loss: 0.05564447492361069\n",
      "Iteration 19040, Loss: 0.05582185834646225\n",
      "Iteration 19041, Loss: 0.055903755128383636\n",
      "Iteration 19042, Loss: 0.055843476206064224\n",
      "Iteration 19043, Loss: 0.05568850412964821\n",
      "Iteration 19044, Loss: 0.05579753965139389\n",
      "Iteration 19045, Loss: 0.055891118943691254\n",
      "Iteration 19046, Loss: 0.0558394193649292\n",
      "Iteration 19047, Loss: 0.05565408989787102\n",
      "Iteration 19048, Loss: 0.05585658550262451\n",
      "Iteration 19049, Loss: 0.05598879233002663\n",
      "Iteration 19050, Loss: 0.05598040670156479\n",
      "Iteration 19051, Loss: 0.055853329598903656\n",
      "Iteration 19052, Loss: 0.05568576231598854\n",
      "Iteration 19053, Loss: 0.055836282670497894\n",
      "Iteration 19054, Loss: 0.05592465400695801\n",
      "Iteration 19055, Loss: 0.05585837364196777\n",
      "Iteration 19056, Loss: 0.055648528039455414\n",
      "Iteration 19057, Loss: 0.055866360664367676\n",
      "Iteration 19058, Loss: 0.056014180183410645\n",
      "Iteration 19059, Loss: 0.05604056641459465\n",
      "Iteration 19060, Loss: 0.05596085637807846\n",
      "Iteration 19061, Loss: 0.055796265602111816\n",
      "Iteration 19062, Loss: 0.05572303384542465\n",
      "Iteration 19063, Loss: 0.055860958993434906\n",
      "Iteration 19064, Loss: 0.055859725922346115\n",
      "Iteration 19065, Loss: 0.05570932477712631\n",
      "Iteration 19066, Loss: 0.05577389523386955\n",
      "Iteration 19067, Loss: 0.05587967485189438\n",
      "Iteration 19068, Loss: 0.05588209629058838\n",
      "Iteration 19069, Loss: 0.055793922394514084\n",
      "Iteration 19070, Loss: 0.05565973371267319\n",
      "Iteration 19071, Loss: 0.05582575127482414\n",
      "Iteration 19072, Loss: 0.05587450787425041\n",
      "Iteration 19073, Loss: 0.05575176328420639\n",
      "Iteration 19074, Loss: 0.0557200126349926\n",
      "Iteration 19075, Loss: 0.055813830345869064\n",
      "Iteration 19076, Loss: 0.05580683797597885\n",
      "Iteration 19077, Loss: 0.05570634454488754\n",
      "Iteration 19078, Loss: 0.05573475360870361\n",
      "Iteration 19079, Loss: 0.055787764489650726\n",
      "Iteration 19080, Loss: 0.055668316781520844\n",
      "Iteration 19081, Loss: 0.055772703140974045\n",
      "Iteration 19082, Loss: 0.055859845131635666\n",
      "Iteration 19083, Loss: 0.055838506668806076\n",
      "Iteration 19084, Loss: 0.05572136491537094\n",
      "Iteration 19085, Loss: 0.05573109909892082\n",
      "Iteration 19086, Loss: 0.05580230802297592\n",
      "Iteration 19087, Loss: 0.05569668859243393\n",
      "Iteration 19088, Loss: 0.05574611946940422\n",
      "Iteration 19089, Loss: 0.05582905188202858\n",
      "Iteration 19090, Loss: 0.05580584332346916\n",
      "Iteration 19091, Loss: 0.055691324174404144\n",
      "Iteration 19092, Loss: 0.05576881021261215\n",
      "Iteration 19093, Loss: 0.05583469197154045\n",
      "Iteration 19094, Loss: 0.055722713470458984\n",
      "Iteration 19095, Loss: 0.055731259286403656\n",
      "Iteration 19096, Loss: 0.055818479508161545\n",
      "Iteration 19097, Loss: 0.05580131337046623\n",
      "Iteration 19098, Loss: 0.05569283291697502\n",
      "Iteration 19099, Loss: 0.05576014891266823\n",
      "Iteration 19100, Loss: 0.0558197908103466\n",
      "Iteration 19101, Loss: 0.05570153519511223\n",
      "Iteration 19102, Loss: 0.055750373750925064\n",
      "Iteration 19103, Loss: 0.05584101006388664\n",
      "Iteration 19104, Loss: 0.055828094482421875\n",
      "Iteration 19105, Loss: 0.055722832679748535\n",
      "Iteration 19106, Loss: 0.05571611970663071\n",
      "Iteration 19107, Loss: 0.055773936212062836\n",
      "Iteration 19108, Loss: 0.05565723031759262\n",
      "Iteration 19109, Loss: 0.05577850341796875\n",
      "Iteration 19110, Loss: 0.0558655671775341\n",
      "Iteration 19111, Loss: 0.05584971234202385\n",
      "Iteration 19112, Loss: 0.05574151128530502\n",
      "Iteration 19113, Loss: 0.055692993104457855\n",
      "Iteration 19114, Loss: 0.05575374886393547\n",
      "Iteration 19115, Loss: 0.05563831329345703\n",
      "Iteration 19116, Loss: 0.05579054355621338\n",
      "Iteration 19117, Loss: 0.05587534233927727\n",
      "Iteration 19118, Loss: 0.055856429040431976\n",
      "Iteration 19119, Loss: 0.05574492737650871\n",
      "Iteration 19120, Loss: 0.05569323152303696\n",
      "Iteration 19121, Loss: 0.05576002597808838\n",
      "Iteration 19122, Loss: 0.05565150827169418\n",
      "Iteration 19123, Loss: 0.05577937886118889\n",
      "Iteration 19124, Loss: 0.05586389824748039\n",
      "Iteration 19125, Loss: 0.05584581941366196\n",
      "Iteration 19126, Loss: 0.05573562905192375\n",
      "Iteration 19127, Loss: 0.05570368096232414\n",
      "Iteration 19128, Loss: 0.05576702207326889\n",
      "Iteration 19129, Loss: 0.05565277859568596\n",
      "Iteration 19130, Loss: 0.05578279495239258\n",
      "Iteration 19131, Loss: 0.055871449410915375\n",
      "Iteration 19132, Loss: 0.05585670843720436\n",
      "Iteration 19133, Loss: 0.05574957653880119\n",
      "Iteration 19134, Loss: 0.055680833756923676\n",
      "Iteration 19135, Loss: 0.05574055761098862\n",
      "Iteration 19136, Loss: 0.0556214265525341\n",
      "Iteration 19137, Loss: 0.05580934137105942\n",
      "Iteration 19138, Loss: 0.05590121075510979\n",
      "Iteration 19139, Loss: 0.05588928982615471\n",
      "Iteration 19140, Loss: 0.05578410625457764\n",
      "Iteration 19141, Loss: 0.05563155934214592\n",
      "Iteration 19142, Loss: 0.05569028854370117\n",
      "Iteration 19143, Loss: 0.05564038082957268\n",
      "Iteration 19144, Loss: 0.05563175678253174\n",
      "Iteration 19145, Loss: 0.055715203285217285\n",
      "Iteration 19146, Loss: 0.05567038059234619\n",
      "Iteration 19147, Loss: 0.055720411241054535\n",
      "Iteration 19148, Loss: 0.05576002597808838\n",
      "Iteration 19149, Loss: 0.055695533752441406\n",
      "Iteration 19150, Loss: 0.055703919380903244\n",
      "Iteration 19151, Loss: 0.05572398751974106\n",
      "Iteration 19152, Loss: 0.05563489720225334\n",
      "Iteration 19153, Loss: 0.05563664808869362\n",
      "Iteration 19154, Loss: 0.055704474449157715\n",
      "Iteration 19155, Loss: 0.05565790459513664\n",
      "Iteration 19156, Loss: 0.05573177710175514\n",
      "Iteration 19157, Loss: 0.05577067658305168\n",
      "Iteration 19158, Loss: 0.05570352450013161\n",
      "Iteration 19159, Loss: 0.05569756031036377\n",
      "Iteration 19160, Loss: 0.05572374910116196\n",
      "Iteration 19161, Loss: 0.05563012883067131\n",
      "Iteration 19162, Loss: 0.05564451217651367\n",
      "Iteration 19163, Loss: 0.05566001310944557\n",
      "Iteration 19164, Loss: 0.05564248934388161\n",
      "Iteration 19165, Loss: 0.055627189576625824\n",
      "Iteration 19166, Loss: 0.05570574849843979\n",
      "Iteration 19167, Loss: 0.05566946789622307\n",
      "Iteration 19168, Loss: 0.05570833012461662\n",
      "Iteration 19169, Loss: 0.0557229146361351\n",
      "Iteration 19170, Loss: 0.05562218278646469\n",
      "Iteration 19171, Loss: 0.0557682141661644\n",
      "Iteration 19172, Loss: 0.05575470253825188\n",
      "Iteration 19173, Loss: 0.0556388720870018\n",
      "Iteration 19174, Loss: 0.05568091198801994\n",
      "Iteration 19175, Loss: 0.05561789125204086\n",
      "Iteration 19176, Loss: 0.05577949807047844\n",
      "Iteration 19177, Loss: 0.055808067321777344\n",
      "Iteration 19178, Loss: 0.05571572110056877\n",
      "Iteration 19179, Loss: 0.055716000497341156\n",
      "Iteration 19180, Loss: 0.05577528476715088\n",
      "Iteration 19181, Loss: 0.055696647614240646\n",
      "Iteration 19182, Loss: 0.05572943016886711\n",
      "Iteration 19183, Loss: 0.05578406900167465\n",
      "Iteration 19184, Loss: 0.055719178169965744\n",
      "Iteration 19185, Loss: 0.05568762868642807\n",
      "Iteration 19186, Loss: 0.055729907006025314\n",
      "Iteration 19187, Loss: 0.055644791573286057\n",
      "Iteration 19188, Loss: 0.05576459690928459\n",
      "Iteration 19189, Loss: 0.05582022666931152\n",
      "Iteration 19190, Loss: 0.05577218532562256\n",
      "Iteration 19191, Loss: 0.05565587803721428\n",
      "Iteration 19192, Loss: 0.05580425634980202\n",
      "Iteration 19193, Loss: 0.05584728717803955\n",
      "Iteration 19194, Loss: 0.05571699142456055\n",
      "Iteration 19195, Loss: 0.05574766919016838\n",
      "Iteration 19196, Loss: 0.05584586039185524\n",
      "Iteration 19197, Loss: 0.05584053322672844\n",
      "Iteration 19198, Loss: 0.055742423981428146\n",
      "Iteration 19199, Loss: 0.05568206310272217\n",
      "Iteration 19200, Loss: 0.05573670193552971\n",
      "Iteration 19201, Loss: 0.05563322827219963\n",
      "Iteration 19202, Loss: 0.05575498193502426\n",
      "Iteration 19203, Loss: 0.055798374116420746\n",
      "Iteration 19204, Loss: 0.05573626607656479\n",
      "Iteration 19205, Loss: 0.055651549249887466\n",
      "Iteration 19206, Loss: 0.05569112300872803\n",
      "Iteration 19207, Loss: 0.05565126985311508\n",
      "Iteration 19208, Loss: 0.055658064782619476\n",
      "Iteration 19209, Loss: 0.055654607713222504\n",
      "Iteration 19210, Loss: 0.05565985292196274\n",
      "Iteration 19211, Loss: 0.05564284324645996\n",
      "Iteration 19212, Loss: 0.055686116218566895\n",
      "Iteration 19213, Loss: 0.05562635511159897\n",
      "Iteration 19214, Loss: 0.0557355098426342\n",
      "Iteration 19215, Loss: 0.05575959011912346\n",
      "Iteration 19216, Loss: 0.055684804916381836\n",
      "Iteration 19217, Loss: 0.05572887510061264\n",
      "Iteration 19218, Loss: 0.055755697190761566\n",
      "Iteration 19219, Loss: 0.05561598390340805\n",
      "Iteration 19220, Loss: 0.055802345275878906\n",
      "Iteration 19221, Loss: 0.05586529150605202\n",
      "Iteration 19222, Loss: 0.055812202394008636\n",
      "Iteration 19223, Loss: 0.055675748735666275\n",
      "Iteration 19224, Loss: 0.055804137140512466\n",
      "Iteration 19225, Loss: 0.0558830127120018\n",
      "Iteration 19226, Loss: 0.055796705186367035\n",
      "Iteration 19227, Loss: 0.05565262213349342\n",
      "Iteration 19228, Loss: 0.05572156235575676\n",
      "Iteration 19229, Loss: 0.05568186566233635\n",
      "Iteration 19230, Loss: 0.05569327250123024\n",
      "Iteration 19231, Loss: 0.0556894950568676\n",
      "Iteration 19232, Loss: 0.05567880719900131\n",
      "Iteration 19233, Loss: 0.055693309754133224\n",
      "Iteration 19234, Loss: 0.055630091577768326\n",
      "Iteration 19235, Loss: 0.05571889877319336\n",
      "Iteration 19236, Loss: 0.05566283315420151\n",
      "Iteration 19237, Loss: 0.05573320761322975\n",
      "Iteration 19238, Loss: 0.0557810477912426\n",
      "Iteration 19239, Loss: 0.0557253360748291\n",
      "Iteration 19240, Loss: 0.05565556138753891\n",
      "Iteration 19241, Loss: 0.05567292496562004\n",
      "Iteration 19242, Loss: 0.05567876622080803\n",
      "Iteration 19243, Loss: 0.05568476766347885\n",
      "Iteration 19244, Loss: 0.05563728138804436\n",
      "Iteration 19245, Loss: 0.05568043515086174\n",
      "Iteration 19246, Loss: 0.05564109608530998\n",
      "Iteration 19247, Loss: 0.055701255798339844\n",
      "Iteration 19248, Loss: 0.0556643009185791\n",
      "Iteration 19249, Loss: 0.055722832679748535\n",
      "Iteration 19250, Loss: 0.05574909970164299\n",
      "Iteration 19251, Loss: 0.055663786828517914\n",
      "Iteration 19252, Loss: 0.055758677423000336\n",
      "Iteration 19253, Loss: 0.05579444020986557\n",
      "Iteration 19254, Loss: 0.055673759430646896\n",
      "Iteration 19255, Loss: 0.05577675625681877\n",
      "Iteration 19256, Loss: 0.05586286634206772\n",
      "Iteration 19257, Loss: 0.05582531541585922\n",
      "Iteration 19258, Loss: 0.055687546730041504\n",
      "Iteration 19259, Loss: 0.05579086393117905\n",
      "Iteration 19260, Loss: 0.05587848275899887\n",
      "Iteration 19261, Loss: 0.05579916760325432\n",
      "Iteration 19262, Loss: 0.05564439296722412\n",
      "Iteration 19263, Loss: 0.05571846291422844\n",
      "Iteration 19264, Loss: 0.0556747131049633\n",
      "Iteration 19265, Loss: 0.055712223052978516\n",
      "Iteration 19266, Loss: 0.05572247877717018\n",
      "Iteration 19267, Loss: 0.0556386336684227\n",
      "Iteration 19268, Loss: 0.055685244500637054\n",
      "Iteration 19269, Loss: 0.055622898042201996\n",
      "Iteration 19270, Loss: 0.05571949481964111\n",
      "Iteration 19271, Loss: 0.055730462074279785\n",
      "Iteration 19272, Loss: 0.05564657971262932\n",
      "Iteration 19273, Loss: 0.05579213425517082\n",
      "Iteration 19274, Loss: 0.05582531541585922\n",
      "Iteration 19275, Loss: 0.0556815080344677\n",
      "Iteration 19276, Loss: 0.05578124523162842\n",
      "Iteration 19277, Loss: 0.055887822061777115\n",
      "Iteration 19278, Loss: 0.055888574570417404\n",
      "Iteration 19279, Loss: 0.05579444020986557\n",
      "Iteration 19280, Loss: 0.055621981620788574\n",
      "Iteration 19281, Loss: 0.055896323174238205\n",
      "Iteration 19282, Loss: 0.055992286652326584\n",
      "Iteration 19283, Loss: 0.055908799171447754\n",
      "Iteration 19284, Loss: 0.05567248910665512\n",
      "Iteration 19285, Loss: 0.055845897644758224\n",
      "Iteration 19286, Loss: 0.056000832468271255\n",
      "Iteration 19287, Loss: 0.05604275315999985\n",
      "Iteration 19288, Loss: 0.0559825524687767\n",
      "Iteration 19289, Loss: 0.055831752717494965\n",
      "Iteration 19290, Loss: 0.055644989013671875\n",
      "Iteration 19291, Loss: 0.05584486573934555\n",
      "Iteration 19292, Loss: 0.05588432401418686\n",
      "Iteration 19293, Loss: 0.05574930086731911\n",
      "Iteration 19294, Loss: 0.0557279996573925\n",
      "Iteration 19295, Loss: 0.055829208344221115\n",
      "Iteration 19296, Loss: 0.05582793802022934\n",
      "Iteration 19297, Loss: 0.0557328499853611\n",
      "Iteration 19298, Loss: 0.05569025129079819\n",
      "Iteration 19299, Loss: 0.05573785677552223\n",
      "Iteration 19300, Loss: 0.05562659353017807\n",
      "Iteration 19301, Loss: 0.055743616074323654\n",
      "Iteration 19302, Loss: 0.05576292797923088\n",
      "Iteration 19303, Loss: 0.05567384138703346\n",
      "Iteration 19304, Loss: 0.055756133049726486\n",
      "Iteration 19305, Loss: 0.05579841136932373\n",
      "Iteration 19306, Loss: 0.05567825213074684\n",
      "Iteration 19307, Loss: 0.055771272629499435\n",
      "Iteration 19308, Loss: 0.055858854204416275\n",
      "Iteration 19309, Loss: 0.055827222764492035\n",
      "Iteration 19310, Loss: 0.0556967668235302\n",
      "Iteration 19311, Loss: 0.05577552318572998\n",
      "Iteration 19312, Loss: 0.05585857480764389\n",
      "Iteration 19313, Loss: 0.05577190965414047\n",
      "Iteration 19314, Loss: 0.05567292496562004\n",
      "Iteration 19315, Loss: 0.05574266240000725\n",
      "Iteration 19316, Loss: 0.055700503289699554\n",
      "Iteration 19317, Loss: 0.05567459389567375\n",
      "Iteration 19318, Loss: 0.055678170174360275\n",
      "Iteration 19319, Loss: 0.05568373575806618\n",
      "Iteration 19320, Loss: 0.05569279566407204\n",
      "Iteration 19321, Loss: 0.05563744157552719\n",
      "Iteration 19322, Loss: 0.0556999072432518\n",
      "Iteration 19323, Loss: 0.05564209073781967\n",
      "Iteration 19324, Loss: 0.05573161691427231\n",
      "Iteration 19325, Loss: 0.05575649067759514\n",
      "Iteration 19326, Loss: 0.05567511171102524\n",
      "Iteration 19327, Loss: 0.055747151374816895\n",
      "Iteration 19328, Loss: 0.055783711373806\n",
      "Iteration 19329, Loss: 0.055661045014858246\n",
      "Iteration 19330, Loss: 0.05578533932566643\n",
      "Iteration 19331, Loss: 0.05587303638458252\n",
      "Iteration 19332, Loss: 0.05584494397044182\n",
      "Iteration 19333, Loss: 0.055719535797834396\n",
      "Iteration 19334, Loss: 0.055744171142578125\n",
      "Iteration 19335, Loss: 0.05582694336771965\n",
      "Iteration 19336, Loss: 0.05574754998087883\n",
      "Iteration 19337, Loss: 0.055687904357910156\n",
      "Iteration 19338, Loss: 0.055751245468854904\n",
      "Iteration 19339, Loss: 0.05570829287171364\n",
      "Iteration 19340, Loss: 0.05566795915365219\n",
      "Iteration 19341, Loss: 0.055686913430690765\n",
      "Iteration 19342, Loss: 0.0556662492454052\n",
      "Iteration 19343, Loss: 0.05566851422190666\n",
      "Iteration 19344, Loss: 0.05565913766622543\n",
      "Iteration 19345, Loss: 0.05565750598907471\n",
      "Iteration 19346, Loss: 0.05566652864217758\n",
      "Iteration 19347, Loss: 0.05565357208251953\n",
      "Iteration 19348, Loss: 0.055694542825222015\n",
      "Iteration 19349, Loss: 0.05565079301595688\n",
      "Iteration 19350, Loss: 0.05573153495788574\n",
      "Iteration 19351, Loss: 0.05576837435364723\n",
      "Iteration 19352, Loss: 0.05570213124155998\n",
      "Iteration 19353, Loss: 0.0556972436606884\n",
      "Iteration 19354, Loss: 0.05571651831269264\n",
      "Iteration 19355, Loss: 0.05564519017934799\n",
      "Iteration 19356, Loss: 0.055650752037763596\n",
      "Iteration 19357, Loss: 0.05567701905965805\n",
      "Iteration 19358, Loss: 0.05561590567231178\n",
      "Iteration 19359, Loss: 0.05577604100108147\n",
      "Iteration 19360, Loss: 0.05583103746175766\n",
      "Iteration 19361, Loss: 0.05578339099884033\n",
      "Iteration 19362, Loss: 0.05565055459737778\n",
      "Iteration 19363, Loss: 0.05584089085459709\n",
      "Iteration 19364, Loss: 0.05591980740427971\n",
      "Iteration 19365, Loss: 0.05581669136881828\n",
      "Iteration 19366, Loss: 0.05565611645579338\n",
      "Iteration 19367, Loss: 0.05573924630880356\n",
      "Iteration 19368, Loss: 0.05572224035859108\n",
      "Iteration 19369, Loss: 0.05562357231974602\n",
      "Iteration 19370, Loss: 0.055793486535549164\n",
      "Iteration 19371, Loss: 0.055805645883083344\n",
      "Iteration 19372, Loss: 0.05566676706075668\n",
      "Iteration 19373, Loss: 0.05578494444489479\n",
      "Iteration 19374, Loss: 0.05587538331747055\n",
      "Iteration 19375, Loss: 0.05584975332021713\n",
      "Iteration 19376, Loss: 0.055722832679748535\n",
      "Iteration 19377, Loss: 0.05573992058634758\n",
      "Iteration 19378, Loss: 0.055824797600507736\n",
      "Iteration 19379, Loss: 0.055746715515851974\n",
      "Iteration 19380, Loss: 0.05568552389740944\n",
      "Iteration 19381, Loss: 0.05574774742126465\n",
      "Iteration 19382, Loss: 0.055700819939374924\n",
      "Iteration 19383, Loss: 0.05567944049835205\n",
      "Iteration 19384, Loss: 0.055689454078674316\n",
      "Iteration 19385, Loss: 0.05566927045583725\n",
      "Iteration 19386, Loss: 0.05567380040884018\n",
      "Iteration 19387, Loss: 0.05565508455038071\n",
      "Iteration 19388, Loss: 0.05564646050333977\n",
      "Iteration 19389, Loss: 0.055686354637145996\n",
      "Iteration 19390, Loss: 0.055682700127363205\n",
      "Iteration 19391, Loss: 0.05564626306295395\n",
      "Iteration 19392, Loss: 0.05563831329345703\n",
      "Iteration 19393, Loss: 0.055668435990810394\n",
      "Iteration 19394, Loss: 0.055616460740566254\n",
      "Iteration 19395, Loss: 0.055780135095119476\n",
      "Iteration 19396, Loss: 0.055789195001125336\n",
      "Iteration 19397, Loss: 0.055664025247097015\n",
      "Iteration 19398, Loss: 0.0557808093726635\n",
      "Iteration 19399, Loss: 0.055853646248579025\n",
      "Iteration 19400, Loss: 0.05579269304871559\n",
      "Iteration 19401, Loss: 0.055637918412685394\n",
      "Iteration 19402, Loss: 0.055816650390625\n",
      "Iteration 19403, Loss: 0.05586453527212143\n",
      "Iteration 19404, Loss: 0.05574942007660866\n",
      "Iteration 19405, Loss: 0.05570928379893303\n",
      "Iteration 19406, Loss: 0.05579666420817375\n",
      "Iteration 19407, Loss: 0.05576571077108383\n",
      "Iteration 19408, Loss: 0.055637042969465256\n",
      "Iteration 19409, Loss: 0.05583469197154045\n",
      "Iteration 19410, Loss: 0.055897634476423264\n",
      "Iteration 19411, Loss: 0.05578649416565895\n",
      "Iteration 19412, Loss: 0.05568047612905502\n",
      "Iteration 19413, Loss: 0.05576881021261215\n",
      "Iteration 19414, Loss: 0.05574754998087883\n",
      "Iteration 19415, Loss: 0.05562790483236313\n",
      "Iteration 19416, Loss: 0.05584971234202385\n",
      "Iteration 19417, Loss: 0.05591428652405739\n",
      "Iteration 19418, Loss: 0.055801987648010254\n",
      "Iteration 19419, Loss: 0.05567173287272453\n",
      "Iteration 19420, Loss: 0.05576257035136223\n",
      "Iteration 19421, Loss: 0.055747389793395996\n",
      "Iteration 19422, Loss: 0.0556330680847168\n",
      "Iteration 19423, Loss: 0.05584283918142319\n",
      "Iteration 19424, Loss: 0.05590975657105446\n",
      "Iteration 19425, Loss: 0.05580171197652817\n",
      "Iteration 19426, Loss: 0.0556696280837059\n",
      "Iteration 19427, Loss: 0.0557607039809227\n",
      "Iteration 19428, Loss: 0.05574679374694824\n",
      "Iteration 19429, Loss: 0.05562989041209221\n",
      "Iteration 19430, Loss: 0.05584736913442612\n",
      "Iteration 19431, Loss: 0.055919766426086426\n",
      "Iteration 19432, Loss: 0.05582376569509506\n",
      "Iteration 19433, Loss: 0.05565611645579338\n",
      "Iteration 19434, Loss: 0.05577500909566879\n",
      "Iteration 19435, Loss: 0.055800359696149826\n",
      "Iteration 19436, Loss: 0.05570252984762192\n",
      "Iteration 19437, Loss: 0.05573705956339836\n",
      "Iteration 19438, Loss: 0.05579829216003418\n",
      "Iteration 19439, Loss: 0.05571572110056877\n",
      "Iteration 19440, Loss: 0.055708449333906174\n",
      "Iteration 19441, Loss: 0.0557660274207592\n",
      "Iteration 19442, Loss: 0.0556970052421093\n",
      "Iteration 19443, Loss: 0.05571611970663071\n",
      "Iteration 19444, Loss: 0.05575498193502426\n",
      "Iteration 19445, Loss: 0.0556621178984642\n",
      "Iteration 19446, Loss: 0.055760860443115234\n",
      "Iteration 19447, Loss: 0.05581013485789299\n",
      "Iteration 19448, Loss: 0.05572199821472168\n",
      "Iteration 19449, Loss: 0.055706024169921875\n",
      "Iteration 19450, Loss: 0.0557662658393383\n",
      "Iteration 19451, Loss: 0.05569196119904518\n",
      "Iteration 19452, Loss: 0.05572342872619629\n",
      "Iteration 19453, Loss: 0.055768489837646484\n",
      "Iteration 19454, Loss: 0.05567944422364235\n",
      "Iteration 19455, Loss: 0.05574663728475571\n",
      "Iteration 19456, Loss: 0.0558011531829834\n",
      "Iteration 19457, Loss: 0.05571889877319336\n",
      "Iteration 19458, Loss: 0.055703483521938324\n",
      "Iteration 19459, Loss: 0.055758558213710785\n",
      "Iteration 19460, Loss: 0.05567896366119385\n",
      "Iteration 19461, Loss: 0.055743101984262466\n",
      "Iteration 19462, Loss: 0.05579324811697006\n",
      "Iteration 19463, Loss: 0.055711667984724045\n",
      "Iteration 19464, Loss: 0.055708885192871094\n",
      "Iteration 19465, Loss: 0.05576106160879135\n",
      "Iteration 19466, Loss: 0.055677495896816254\n",
      "Iteration 19467, Loss: 0.05574957653880119\n",
      "Iteration 19468, Loss: 0.05580504983663559\n",
      "Iteration 19469, Loss: 0.0557323694229126\n",
      "Iteration 19470, Loss: 0.05568432807922363\n",
      "Iteration 19471, Loss: 0.055738966912031174\n",
      "Iteration 19472, Loss: 0.05566617101430893\n",
      "Iteration 19473, Loss: 0.05575168505311012\n",
      "Iteration 19474, Loss: 0.05580314248800278\n",
      "Iteration 19475, Loss: 0.05574306100606918\n",
      "Iteration 19476, Loss: 0.05566895380616188\n",
      "Iteration 19477, Loss: 0.0557398796081543\n",
      "Iteration 19478, Loss: 0.05569577217102051\n",
      "Iteration 19479, Loss: 0.055708251893520355\n",
      "Iteration 19480, Loss: 0.05574822425842285\n",
      "Iteration 19481, Loss: 0.05569815635681152\n",
      "Iteration 19482, Loss: 0.05568874254822731\n",
      "Iteration 19483, Loss: 0.055699072778224945\n",
      "Iteration 19484, Loss: 0.0556691512465477\n",
      "Iteration 19485, Loss: 0.055682383477687836\n",
      "Iteration 19486, Loss: 0.055650632828474045\n",
      "Iteration 19487, Loss: 0.05570375919342041\n",
      "Iteration 19488, Loss: 0.05566394701600075\n",
      "Iteration 19489, Loss: 0.05571206659078598\n",
      "Iteration 19490, Loss: 0.05573543161153793\n",
      "Iteration 19491, Loss: 0.05564979836344719\n",
      "Iteration 19492, Loss: 0.0557856559753418\n",
      "Iteration 19493, Loss: 0.055826667696237564\n",
      "Iteration 19494, Loss: 0.055706702172756195\n",
      "Iteration 19495, Loss: 0.055746160447597504\n",
      "Iteration 19496, Loss: 0.055833183228969574\n",
      "Iteration 19497, Loss: 0.05580171197652817\n",
      "Iteration 19498, Loss: 0.05566807836294174\n",
      "Iteration 19499, Loss: 0.05581212416291237\n",
      "Iteration 19500, Loss: 0.05589676275849342\n",
      "Iteration 19501, Loss: 0.05580993741750717\n",
      "Iteration 19502, Loss: 0.05564761534333229\n",
      "Iteration 19503, Loss: 0.05574234575033188\n",
      "Iteration 19504, Loss: 0.05572756379842758\n",
      "Iteration 19505, Loss: 0.05562647432088852\n",
      "Iteration 19506, Loss: 0.05562857910990715\n",
      "Iteration 19507, Loss: 0.055716514587402344\n",
      "Iteration 19508, Loss: 0.055728714913129807\n",
      "Iteration 19509, Loss: 0.05564657971262932\n",
      "Iteration 19510, Loss: 0.055788878351449966\n",
      "Iteration 19511, Loss: 0.05581895634531975\n",
      "Iteration 19512, Loss: 0.055672645568847656\n",
      "Iteration 19513, Loss: 0.05578943341970444\n",
      "Iteration 19514, Loss: 0.055897753685712814\n",
      "Iteration 19515, Loss: 0.05589989945292473\n",
      "Iteration 19516, Loss: 0.055807195603847504\n",
      "Iteration 19517, Loss: 0.05563303083181381\n",
      "Iteration 19518, Loss: 0.05591539666056633\n",
      "Iteration 19519, Loss: 0.05604195594787598\n",
      "Iteration 19520, Loss: 0.05598195642232895\n",
      "Iteration 19521, Loss: 0.05575625225901604\n",
      "Iteration 19522, Loss: 0.05578065291047096\n",
      "Iteration 19523, Loss: 0.0559358224272728\n",
      "Iteration 19524, Loss: 0.05598139762878418\n",
      "Iteration 19525, Loss: 0.05592747777700424\n",
      "Iteration 19526, Loss: 0.05578470230102539\n",
      "Iteration 19527, Loss: 0.055676620453596115\n",
      "Iteration 19528, Loss: 0.05577540397644043\n",
      "Iteration 19529, Loss: 0.05569374933838844\n",
      "Iteration 19530, Loss: 0.0557304248213768\n",
      "Iteration 19531, Loss: 0.055799130350351334\n",
      "Iteration 19532, Loss: 0.055766861885786057\n",
      "Iteration 19533, Loss: 0.055643998086452484\n",
      "Iteration 19534, Loss: 0.055842481553554535\n",
      "Iteration 19535, Loss: 0.0559188537299633\n",
      "Iteration 19536, Loss: 0.05581585690379143\n",
      "Iteration 19537, Loss: 0.05565599724650383\n",
      "Iteration 19538, Loss: 0.05574067682027817\n",
      "Iteration 19539, Loss: 0.05572490021586418\n",
      "Iteration 19540, Loss: 0.05561625957489014\n",
      "Iteration 19541, Loss: 0.05585356801748276\n",
      "Iteration 19542, Loss: 0.05591309070587158\n",
      "Iteration 19543, Loss: 0.055804215371608734\n",
      "Iteration 19544, Loss: 0.05566895008087158\n",
      "Iteration 19545, Loss: 0.055763088166713715\n",
      "Iteration 19546, Loss: 0.05575740337371826\n",
      "Iteration 19547, Loss: 0.055647335946559906\n",
      "Iteration 19548, Loss: 0.05581601709127426\n",
      "Iteration 19549, Loss: 0.05588432401418686\n",
      "Iteration 19550, Loss: 0.0557941235601902\n",
      "Iteration 19551, Loss: 0.055664658546447754\n",
      "Iteration 19552, Loss: 0.05575895681977272\n",
      "Iteration 19553, Loss: 0.0557534322142601\n",
      "Iteration 19554, Loss: 0.05562857910990715\n",
      "Iteration 19555, Loss: 0.05583830922842026\n",
      "Iteration 19556, Loss: 0.05592390149831772\n",
      "Iteration 19557, Loss: 0.05586807057261467\n",
      "Iteration 19558, Loss: 0.0557052306830883\n",
      "Iteration 19559, Loss: 0.05579046532511711\n",
      "Iteration 19560, Loss: 0.055900733917951584\n",
      "Iteration 19561, Loss: 0.055863939225673676\n",
      "Iteration 19562, Loss: 0.05569326877593994\n",
      "Iteration 19563, Loss: 0.05580512806773186\n",
      "Iteration 19564, Loss: 0.05592735856771469\n",
      "Iteration 19565, Loss: 0.05590740963816643\n",
      "Iteration 19566, Loss: 0.0557657890021801\n",
      "Iteration 19567, Loss: 0.05571373552083969\n",
      "Iteration 19568, Loss: 0.05582078546285629\n",
      "Iteration 19569, Loss: 0.055786967277526855\n",
      "Iteration 19570, Loss: 0.05561455339193344\n",
      "Iteration 19571, Loss: 0.05587303638458252\n",
      "Iteration 19572, Loss: 0.05599558353424072\n",
      "Iteration 19573, Loss: 0.05598648637533188\n",
      "Iteration 19574, Loss: 0.055864255875349045\n",
      "Iteration 19575, Loss: 0.055685680359601974\n",
      "Iteration 19576, Loss: 0.05585050582885742\n",
      "Iteration 19577, Loss: 0.055961571633815765\n",
      "Iteration 19578, Loss: 0.05590057373046875\n",
      "Iteration 19579, Loss: 0.055682223290205\n",
      "Iteration 19580, Loss: 0.055841170251369476\n",
      "Iteration 19581, Loss: 0.05599542707204819\n",
      "Iteration 19582, Loss: 0.05602677911520004\n",
      "Iteration 19583, Loss: 0.05594801902770996\n",
      "Iteration 19584, Loss: 0.0557812862098217\n",
      "Iteration 19585, Loss: 0.05573034659028053\n",
      "Iteration 19586, Loss: 0.05586278438568115\n",
      "Iteration 19587, Loss: 0.05583592504262924\n",
      "Iteration 19588, Loss: 0.05566596984863281\n",
      "Iteration 19589, Loss: 0.05579960346221924\n",
      "Iteration 19590, Loss: 0.055902205407619476\n",
      "Iteration 19591, Loss: 0.0559006966650486\n",
      "Iteration 19592, Loss: 0.055806517601013184\n",
      "Iteration 19593, Loss: 0.0556488037109375\n",
      "Iteration 19594, Loss: 0.05586473271250725\n",
      "Iteration 19595, Loss: 0.055952273309230804\n",
      "Iteration 19596, Loss: 0.05586358159780502\n",
      "Iteration 19597, Loss: 0.055653929710388184\n",
      "Iteration 19598, Loss: 0.055812202394008636\n",
      "Iteration 19599, Loss: 0.055909160524606705\n",
      "Iteration 19600, Loss: 0.055894218385219574\n",
      "Iteration 19601, Loss: 0.05577775090932846\n",
      "Iteration 19602, Loss: 0.05566374585032463\n",
      "Iteration 19603, Loss: 0.05575593560934067\n",
      "Iteration 19604, Loss: 0.055686354637145996\n",
      "Iteration 19605, Loss: 0.05572954937815666\n",
      "Iteration 19606, Loss: 0.05578923597931862\n",
      "Iteration 19607, Loss: 0.0557483434677124\n",
      "Iteration 19608, Loss: 0.05563867464661598\n",
      "Iteration 19609, Loss: 0.05580047890543938\n",
      "Iteration 19610, Loss: 0.05581935495138168\n",
      "Iteration 19611, Loss: 0.05566895008087158\n",
      "Iteration 19612, Loss: 0.055792927742004395\n",
      "Iteration 19613, Loss: 0.05589902400970459\n",
      "Iteration 19614, Loss: 0.05589628592133522\n",
      "Iteration 19615, Loss: 0.05579610913991928\n",
      "Iteration 19616, Loss: 0.05562925711274147\n",
      "Iteration 19617, Loss: 0.05583957955241203\n",
      "Iteration 19618, Loss: 0.05588213726878166\n",
      "Iteration 19619, Loss: 0.05575152486562729\n",
      "Iteration 19620, Loss: 0.0557224377989769\n",
      "Iteration 19621, Loss: 0.055820148438215256\n",
      "Iteration 19622, Loss: 0.05581545829772949\n",
      "Iteration 19623, Loss: 0.055716633796691895\n",
      "Iteration 19624, Loss: 0.05571770668029785\n",
      "Iteration 19625, Loss: 0.05576924607157707\n",
      "Iteration 19626, Loss: 0.05565023794770241\n",
      "Iteration 19627, Loss: 0.05578478425741196\n",
      "Iteration 19628, Loss: 0.05586954206228256\n",
      "Iteration 19629, Loss: 0.0558478869497776\n",
      "Iteration 19630, Loss: 0.05573229119181633\n",
      "Iteration 19631, Loss: 0.055717192590236664\n",
      "Iteration 19632, Loss: 0.05578979104757309\n",
      "Iteration 19633, Loss: 0.05569116398692131\n",
      "Iteration 19634, Loss: 0.05574647709727287\n",
      "Iteration 19635, Loss: 0.05582495778799057\n",
      "Iteration 19636, Loss: 0.05580016225576401\n",
      "Iteration 19637, Loss: 0.05568560212850571\n",
      "Iteration 19638, Loss: 0.05577683448791504\n",
      "Iteration 19639, Loss: 0.05584363266825676\n",
      "Iteration 19640, Loss: 0.055734001100063324\n",
      "Iteration 19641, Loss: 0.05572076886892319\n",
      "Iteration 19642, Loss: 0.055806636810302734\n",
      "Iteration 19643, Loss: 0.055789392441511154\n",
      "Iteration 19644, Loss: 0.05568067356944084\n",
      "Iteration 19645, Loss: 0.05577651783823967\n",
      "Iteration 19646, Loss: 0.055837392807006836\n",
      "Iteration 19647, Loss: 0.05572100728750229\n",
      "Iteration 19648, Loss: 0.055734675377607346\n",
      "Iteration 19649, Loss: 0.05582507699728012\n",
      "Iteration 19650, Loss: 0.05581172555685043\n",
      "Iteration 19651, Loss: 0.05570618435740471\n",
      "Iteration 19652, Loss: 0.05573872849345207\n",
      "Iteration 19653, Loss: 0.05579710379242897\n",
      "Iteration 19654, Loss: 0.05567789077758789\n",
      "Iteration 19655, Loss: 0.05576857179403305\n",
      "Iteration 19656, Loss: 0.055860403925180435\n",
      "Iteration 19657, Loss: 0.05584871768951416\n",
      "Iteration 19658, Loss: 0.055744051933288574\n",
      "Iteration 19659, Loss: 0.05568647384643555\n",
      "Iteration 19660, Loss: 0.05574452877044678\n",
      "Iteration 19661, Loss: 0.05562746524810791\n",
      "Iteration 19662, Loss: 0.05579876899719238\n",
      "Iteration 19663, Loss: 0.05588396638631821\n",
      "Iteration 19664, Loss: 0.055865172296762466\n",
      "Iteration 19665, Loss: 0.05575370788574219\n",
      "Iteration 19666, Loss: 0.055681824684143066\n",
      "Iteration 19667, Loss: 0.05574731156229973\n",
      "Iteration 19668, Loss: 0.055637478828430176\n",
      "Iteration 19669, Loss: 0.05578573793172836\n",
      "Iteration 19670, Loss: 0.05586564913392067\n",
      "Iteration 19671, Loss: 0.05584311485290527\n",
      "Iteration 19672, Loss: 0.05572883412241936\n",
      "Iteration 19673, Loss: 0.05571877956390381\n",
      "Iteration 19674, Loss: 0.05578681081533432\n",
      "Iteration 19675, Loss: 0.05567622557282448\n",
      "Iteration 19676, Loss: 0.05576388165354729\n",
      "Iteration 19677, Loss: 0.05585046857595444\n",
      "Iteration 19678, Loss: 0.05583401769399643\n",
      "Iteration 19679, Loss: 0.05572529882192612\n",
      "Iteration 19680, Loss: 0.055716753005981445\n",
      "Iteration 19681, Loss: 0.05577870458364487\n",
      "Iteration 19682, Loss: 0.055662792176008224\n",
      "Iteration 19683, Loss: 0.055777471512556076\n",
      "Iteration 19684, Loss: 0.0558675155043602\n",
      "Iteration 19685, Loss: 0.05585404485464096\n",
      "Iteration 19686, Loss: 0.055748067796230316\n",
      "Iteration 19687, Loss: 0.05568266287446022\n",
      "Iteration 19688, Loss: 0.05574222654104233\n",
      "Iteration 19689, Loss: 0.05562559887766838\n",
      "Iteration 19690, Loss: 0.05579916760325432\n",
      "Iteration 19691, Loss: 0.05588313192129135\n",
      "Iteration 19692, Loss: 0.05586298555135727\n",
      "Iteration 19693, Loss: 0.05575001239776611\n",
      "Iteration 19694, Loss: 0.05568905919790268\n",
      "Iteration 19695, Loss: 0.05575668811798096\n",
      "Iteration 19696, Loss: 0.05564948171377182\n",
      "Iteration 19697, Loss: 0.05577775090932846\n",
      "Iteration 19698, Loss: 0.05585825443267822\n",
      "Iteration 19699, Loss: 0.05583691596984863\n",
      "Iteration 19700, Loss: 0.05572358891367912\n",
      "Iteration 19701, Loss: 0.05572470277547836\n",
      "Iteration 19702, Loss: 0.05579086393117905\n",
      "Iteration 19703, Loss: 0.055678170174360275\n",
      "Iteration 19704, Loss: 0.055763326585292816\n",
      "Iteration 19705, Loss: 0.05585090443491936\n",
      "Iteration 19706, Loss: 0.05583512783050537\n",
      "Iteration 19707, Loss: 0.05572712421417236\n",
      "Iteration 19708, Loss: 0.0557129792869091\n",
      "Iteration 19709, Loss: 0.05577385798096657\n",
      "Iteration 19710, Loss: 0.05565647408366203\n",
      "Iteration 19711, Loss: 0.055782876908779144\n",
      "Iteration 19712, Loss: 0.05587335675954819\n",
      "Iteration 19713, Loss: 0.05586044117808342\n",
      "Iteration 19714, Loss: 0.05575474351644516\n",
      "Iteration 19715, Loss: 0.055673323571681976\n",
      "Iteration 19716, Loss: 0.0557333268225193\n",
      "Iteration 19717, Loss: 0.05561975762248039\n",
      "Iteration 19718, Loss: 0.05578073114156723\n",
      "Iteration 19719, Loss: 0.055839698761701584\n",
      "Iteration 19720, Loss: 0.0557934045791626\n",
      "Iteration 19721, Loss: 0.05566021054983139\n",
      "Iteration 19722, Loss: 0.05582992359995842\n",
      "Iteration 19723, Loss: 0.05591066926717758\n",
      "Iteration 19724, Loss: 0.055809855461120605\n",
      "Iteration 19725, Loss: 0.055659256875514984\n",
      "Iteration 19726, Loss: 0.055740199983119965\n",
      "Iteration 19727, Loss: 0.05571826547384262\n",
      "Iteration 19728, Loss: 0.05562818422913551\n",
      "Iteration 19729, Loss: 0.055719416588544846\n",
      "Iteration 19730, Loss: 0.055672530084848404\n",
      "Iteration 19731, Loss: 0.05571611970663071\n",
      "Iteration 19732, Loss: 0.05574886128306389\n",
      "Iteration 19733, Loss: 0.05567070096731186\n",
      "Iteration 19734, Loss: 0.05575060844421387\n",
      "Iteration 19735, Loss: 0.05578681081533432\n",
      "Iteration 19736, Loss: 0.055669430643320084\n",
      "Iteration 19737, Loss: 0.055775050073862076\n",
      "Iteration 19738, Loss: 0.05585638806223869\n",
      "Iteration 19739, Loss: 0.05581784248352051\n",
      "Iteration 19740, Loss: 0.05568230524659157\n",
      "Iteration 19741, Loss: 0.05579507723450661\n",
      "Iteration 19742, Loss: 0.05588090419769287\n",
      "Iteration 19743, Loss: 0.05580326169729233\n",
      "Iteration 19744, Loss: 0.055640023201704025\n",
      "Iteration 19745, Loss: 0.05571818724274635\n",
      "Iteration 19746, Loss: 0.05567868798971176\n",
      "Iteration 19747, Loss: 0.055704474449157715\n",
      "Iteration 19748, Loss: 0.05571361631155014\n",
      "Iteration 19749, Loss: 0.05564209073781967\n",
      "Iteration 19750, Loss: 0.05566692724823952\n",
      "Iteration 19751, Loss: 0.05565830320119858\n",
      "Iteration 19752, Loss: 0.05563811585307121\n",
      "Iteration 19753, Loss: 0.05569645017385483\n",
      "Iteration 19754, Loss: 0.0556383952498436\n",
      "Iteration 19755, Loss: 0.05575994774699211\n",
      "Iteration 19756, Loss: 0.05581005662679672\n",
      "Iteration 19757, Loss: 0.05575351044535637\n",
      "Iteration 19758, Loss: 0.05564519017934799\n",
      "Iteration 19759, Loss: 0.05576479434967041\n",
      "Iteration 19760, Loss: 0.05574425309896469\n",
      "Iteration 19761, Loss: 0.05566120520234108\n",
      "Iteration 19762, Loss: 0.05570288747549057\n",
      "Iteration 19763, Loss: 0.05566728487610817\n",
      "Iteration 19764, Loss: 0.055711232125759125\n",
      "Iteration 19765, Loss: 0.055695176124572754\n",
      "Iteration 19766, Loss: 0.055684011429548264\n",
      "Iteration 19767, Loss: 0.055709999054670334\n",
      "Iteration 19768, Loss: 0.055640339851379395\n",
      "Iteration 19769, Loss: 0.05578259751200676\n",
      "Iteration 19770, Loss: 0.05580612272024155\n",
      "Iteration 19771, Loss: 0.055673759430646896\n",
      "Iteration 19772, Loss: 0.05577787011861801\n",
      "Iteration 19773, Loss: 0.05586842820048332\n",
      "Iteration 19774, Loss: 0.055844347923994064\n",
      "Iteration 19775, Loss: 0.05572037026286125\n",
      "Iteration 19776, Loss: 0.05574103444814682\n",
      "Iteration 19777, Loss: 0.05582241341471672\n",
      "Iteration 19778, Loss: 0.05573924630880356\n",
      "Iteration 19779, Loss: 0.05569680780172348\n",
      "Iteration 19780, Loss: 0.05576280876994133\n",
      "Iteration 19781, Loss: 0.05572017282247543\n",
      "Iteration 19782, Loss: 0.055655401200056076\n",
      "Iteration 19783, Loss: 0.05568826198577881\n",
      "Iteration 19784, Loss: 0.05565512180328369\n",
      "Iteration 19785, Loss: 0.055661045014858246\n",
      "Iteration 19786, Loss: 0.05565301701426506\n",
      "Iteration 19787, Loss: 0.05567046254873276\n",
      "Iteration 19788, Loss: 0.05564669892191887\n",
      "Iteration 19789, Loss: 0.0556926354765892\n",
      "Iteration 19790, Loss: 0.05564606562256813\n",
      "Iteration 19791, Loss: 0.055745482444763184\n",
      "Iteration 19792, Loss: 0.0557863712310791\n",
      "Iteration 19793, Loss: 0.055723391473293304\n",
      "Iteration 19794, Loss: 0.05567415803670883\n",
      "Iteration 19795, Loss: 0.05571206659078598\n",
      "Iteration 19796, Loss: 0.05564570426940918\n",
      "Iteration 19797, Loss: 0.05569259449839592\n",
      "Iteration 19798, Loss: 0.05568289756774902\n",
      "Iteration 19799, Loss: 0.05566056817770004\n",
      "Iteration 19800, Loss: 0.05563831329345703\n",
      "Iteration 19801, Loss: 0.055700819939374924\n",
      "Iteration 19802, Loss: 0.055685125291347504\n",
      "Iteration 19803, Loss: 0.05567336454987526\n",
      "Iteration 19804, Loss: 0.05566231533885002\n",
      "Iteration 19805, Loss: 0.05570296570658684\n",
      "Iteration 19806, Loss: 0.05570896714925766\n",
      "Iteration 19807, Loss: 0.055637042969465256\n",
      "Iteration 19808, Loss: 0.05570455640554428\n",
      "Iteration 19809, Loss: 0.05563763901591301\n",
      "Iteration 19810, Loss: 0.055751681327819824\n",
      "Iteration 19811, Loss: 0.05580063909292221\n",
      "Iteration 19812, Loss: 0.055751167237758636\n",
      "Iteration 19813, Loss: 0.055613916367292404\n",
      "Iteration 19814, Loss: 0.05587780475616455\n",
      "Iteration 19815, Loss: 0.055952828377485275\n",
      "Iteration 19816, Loss: 0.055854521691799164\n",
      "Iteration 19817, Loss: 0.05564296245574951\n",
      "Iteration 19818, Loss: 0.055799804627895355\n",
      "Iteration 19819, Loss: 0.055868588387966156\n",
      "Iteration 19820, Loss: 0.055820148438215256\n",
      "Iteration 19821, Loss: 0.05567248910665512\n",
      "Iteration 19822, Loss: 0.05582007020711899\n",
      "Iteration 19823, Loss: 0.055917978286743164\n",
      "Iteration 19824, Loss: 0.05585102364420891\n",
      "Iteration 19825, Loss: 0.05564693734049797\n",
      "Iteration 19826, Loss: 0.055847566574811935\n",
      "Iteration 19827, Loss: 0.05597277730703354\n",
      "Iteration 19828, Loss: 0.05596844479441643\n",
      "Iteration 19829, Loss: 0.055851977318525314\n",
      "Iteration 19830, Loss: 0.05567511171102524\n",
      "Iteration 19831, Loss: 0.05584820359945297\n",
      "Iteration 19832, Loss: 0.055953145027160645\n",
      "Iteration 19833, Loss: 0.05589187145233154\n",
      "Iteration 19834, Loss: 0.055678170174360275\n",
      "Iteration 19835, Loss: 0.0558398999273777\n",
      "Iteration 19836, Loss: 0.055990856140851974\n",
      "Iteration 19837, Loss: 0.05602220818400383\n",
      "Iteration 19838, Loss: 0.05594845861196518\n",
      "Iteration 19839, Loss: 0.05578772351145744\n",
      "Iteration 19840, Loss: 0.055717114359140396\n",
      "Iteration 19841, Loss: 0.05584947392344475\n",
      "Iteration 19842, Loss: 0.05583202838897705\n",
      "Iteration 19843, Loss: 0.05566895380616188\n",
      "Iteration 19844, Loss: 0.05579785630106926\n",
      "Iteration 19845, Loss: 0.0559031181037426\n",
      "Iteration 19846, Loss: 0.055904947221279144\n",
      "Iteration 19847, Loss: 0.05581367015838623\n",
      "Iteration 19848, Loss: 0.05565003678202629\n",
      "Iteration 19849, Loss: 0.05587812513113022\n",
      "Iteration 19850, Loss: 0.05598462000489235\n",
      "Iteration 19851, Loss: 0.055906496942043304\n",
      "Iteration 19852, Loss: 0.05566660687327385\n",
      "Iteration 19853, Loss: 0.05585269257426262\n",
      "Iteration 19854, Loss: 0.05601322650909424\n",
      "Iteration 19855, Loss: 0.05605948343873024\n",
      "Iteration 19856, Loss: 0.05600174516439438\n",
      "Iteration 19857, Loss: 0.05585257336497307\n",
      "Iteration 19858, Loss: 0.05563974753022194\n",
      "Iteration 19859, Loss: 0.05593264102935791\n",
      "Iteration 19860, Loss: 0.05607327073812485\n",
      "Iteration 19861, Loss: 0.056025706231594086\n",
      "Iteration 19862, Loss: 0.05581112951040268\n",
      "Iteration 19863, Loss: 0.05573447793722153\n",
      "Iteration 19864, Loss: 0.055882733315229416\n",
      "Iteration 19865, Loss: 0.05592421814799309\n",
      "Iteration 19866, Loss: 0.055867552757263184\n",
      "Iteration 19867, Loss: 0.05572330951690674\n",
      "Iteration 19868, Loss: 0.05576261132955551\n",
      "Iteration 19869, Loss: 0.0558629035949707\n",
      "Iteration 19870, Loss: 0.05578255653381348\n",
      "Iteration 19871, Loss: 0.05566664785146713\n",
      "Iteration 19872, Loss: 0.055738769471645355\n",
      "Iteration 19873, Loss: 0.05571560189127922\n",
      "Iteration 19874, Loss: 0.05563219636678696\n",
      "Iteration 19875, Loss: 0.05568031594157219\n",
      "Iteration 19876, Loss: 0.055643439292907715\n",
      "Iteration 19877, Loss: 0.055709682404994965\n",
      "Iteration 19878, Loss: 0.05567403882741928\n",
      "Iteration 19879, Loss: 0.05571448802947998\n",
      "Iteration 19880, Loss: 0.05574766919016838\n",
      "Iteration 19881, Loss: 0.05567527189850807\n",
      "Iteration 19882, Loss: 0.0557355098426342\n",
      "Iteration 19883, Loss: 0.055759552866220474\n",
      "Iteration 19884, Loss: 0.05562027543783188\n",
      "Iteration 19885, Loss: 0.0558314323425293\n",
      "Iteration 19886, Loss: 0.05592688173055649\n",
      "Iteration 19887, Loss: 0.05590025708079338\n",
      "Iteration 19888, Loss: 0.05577099695801735\n",
      "Iteration 19889, Loss: 0.055698953568935394\n",
      "Iteration 19890, Loss: 0.05580254644155502\n",
      "Iteration 19891, Loss: 0.055759791284799576\n",
      "Iteration 19892, Loss: 0.05566704273223877\n",
      "Iteration 19893, Loss: 0.05571655556559563\n",
      "Iteration 19894, Loss: 0.05569366738200188\n",
      "Iteration 19895, Loss: 0.05567058175802231\n",
      "Iteration 19896, Loss: 0.055683135986328125\n",
      "Iteration 19897, Loss: 0.055667996406555176\n",
      "Iteration 19898, Loss: 0.05567324534058571\n",
      "Iteration 19899, Loss: 0.05565730854868889\n",
      "Iteration 19900, Loss: 0.055687349289655685\n",
      "Iteration 19901, Loss: 0.05566283315420151\n",
      "Iteration 19902, Loss: 0.05569223687052727\n",
      "Iteration 19903, Loss: 0.05568162724375725\n",
      "Iteration 19904, Loss: 0.05568162724375725\n",
      "Iteration 19905, Loss: 0.055678967386484146\n",
      "Iteration 19906, Loss: 0.05566970631480217\n",
      "Iteration 19907, Loss: 0.05566084757447243\n",
      "Iteration 19908, Loss: 0.055699270218610764\n",
      "Iteration 19909, Loss: 0.05569406598806381\n",
      "Iteration 19910, Loss: 0.055660050362348557\n",
      "Iteration 19911, Loss: 0.055665574967861176\n",
      "Iteration 19912, Loss: 0.05568186566233635\n",
      "Iteration 19913, Loss: 0.05566764250397682\n",
      "Iteration 19914, Loss: 0.0556846484541893\n",
      "Iteration 19915, Loss: 0.055671095848083496\n",
      "Iteration 19916, Loss: 0.05569569393992424\n",
      "Iteration 19917, Loss: 0.05569982901215553\n",
      "Iteration 19918, Loss: 0.05564657971262932\n",
      "Iteration 19919, Loss: 0.055673997849226\n",
      "Iteration 19920, Loss: 0.05565480515360832\n",
      "Iteration 19921, Loss: 0.055648963898420334\n",
      "Iteration 19922, Loss: 0.055667996406555176\n",
      "Iteration 19923, Loss: 0.05563827604055405\n",
      "Iteration 19924, Loss: 0.05566485971212387\n",
      "Iteration 19925, Loss: 0.055623412132263184\n",
      "Iteration 19926, Loss: 0.05576678365468979\n",
      "Iteration 19927, Loss: 0.05577484890818596\n",
      "Iteration 19928, Loss: 0.05565524101257324\n",
      "Iteration 19929, Loss: 0.05578120797872543\n",
      "Iteration 19930, Loss: 0.05584323778748512\n",
      "Iteration 19931, Loss: 0.05576884746551514\n",
      "Iteration 19932, Loss: 0.05565448850393295\n",
      "Iteration 19933, Loss: 0.05573801323771477\n",
      "Iteration 19934, Loss: 0.05569279193878174\n",
      "Iteration 19935, Loss: 0.055706821382045746\n",
      "Iteration 19936, Loss: 0.05574210733175278\n",
      "Iteration 19937, Loss: 0.05567411705851555\n",
      "Iteration 19938, Loss: 0.05572962760925293\n",
      "Iteration 19939, Loss: 0.05574933812022209\n",
      "Iteration 19940, Loss: 0.05561423674225807\n",
      "Iteration 19941, Loss: 0.05580528825521469\n",
      "Iteration 19942, Loss: 0.05588678643107414\n",
      "Iteration 19943, Loss: 0.05586453527212143\n",
      "Iteration 19944, Loss: 0.055750250816345215\n",
      "Iteration 19945, Loss: 0.05569072812795639\n",
      "Iteration 19946, Loss: 0.05576404184103012\n",
      "Iteration 19947, Loss: 0.055667757987976074\n",
      "Iteration 19948, Loss: 0.05575847998261452\n",
      "Iteration 19949, Loss: 0.05583421513438225\n",
      "Iteration 19950, Loss: 0.05580870434641838\n",
      "Iteration 19951, Loss: 0.055692873895168304\n",
      "Iteration 19952, Loss: 0.055768292397260666\n",
      "Iteration 19953, Loss: 0.0558372363448143\n",
      "Iteration 19954, Loss: 0.05572621151804924\n",
      "Iteration 19955, Loss: 0.05572621151804924\n",
      "Iteration 19956, Loss: 0.055813394486904144\n",
      "Iteration 19957, Loss: 0.05579797551035881\n",
      "Iteration 19958, Loss: 0.05569041147828102\n",
      "Iteration 19959, Loss: 0.055760979652404785\n",
      "Iteration 19960, Loss: 0.05582070350646973\n",
      "Iteration 19961, Loss: 0.05570109933614731\n",
      "Iteration 19962, Loss: 0.05575033277273178\n",
      "Iteration 19963, Loss: 0.05584267899394035\n",
      "Iteration 19964, Loss: 0.05583127588033676\n",
      "Iteration 19965, Loss: 0.055727165192365646\n",
      "Iteration 19966, Loss: 0.05570630356669426\n",
      "Iteration 19967, Loss: 0.055762212723493576\n",
      "Iteration 19968, Loss: 0.05563998222351074\n",
      "Iteration 19969, Loss: 0.05579618737101555\n",
      "Iteration 19970, Loss: 0.05588921159505844\n",
      "Iteration 19971, Loss: 0.05587844178080559\n",
      "Iteration 19972, Loss: 0.05577465146780014\n",
      "Iteration 19973, Loss: 0.055642250925302505\n",
      "Iteration 19974, Loss: 0.05569903180003166\n",
      "Iteration 19975, Loss: 0.05563529580831528\n",
      "Iteration 19976, Loss: 0.05562460422515869\n",
      "Iteration 19977, Loss: 0.055729154497385025\n",
      "Iteration 19978, Loss: 0.0556817464530468\n",
      "Iteration 19979, Loss: 0.055716078728437424\n",
      "Iteration 19980, Loss: 0.05576300621032715\n",
      "Iteration 19981, Loss: 0.05570817366242409\n",
      "Iteration 19982, Loss: 0.055674515664577484\n",
      "Iteration 19983, Loss: 0.055681269615888596\n",
      "Iteration 19984, Loss: 0.05568119138479233\n",
      "Iteration 19985, Loss: 0.05569545552134514\n",
      "Iteration 19986, Loss: 0.05562615767121315\n",
      "Iteration 19987, Loss: 0.05576050654053688\n",
      "Iteration 19988, Loss: 0.055742502212524414\n",
      "Iteration 19989, Loss: 0.05565289780497551\n",
      "Iteration 19990, Loss: 0.055693548172712326\n",
      "Iteration 19991, Loss: 0.055642370134592056\n",
      "Iteration 19992, Loss: 0.05575629323720932\n",
      "Iteration 19993, Loss: 0.05576745793223381\n",
      "Iteration 19994, Loss: 0.055650196969509125\n",
      "Iteration 19995, Loss: 0.05577417463064194\n",
      "Iteration 19996, Loss: 0.05582607164978981\n",
      "Iteration 19997, Loss: 0.05574647709727287\n",
      "Iteration 19998, Loss: 0.05567368119955063\n",
      "Iteration 19999, Loss: 0.05573336407542229\n",
      "Iteration 20000, Loss: 0.05565599724650383\n",
      "Iteration 20001, Loss: 0.05576372519135475\n",
      "Iteration 20002, Loss: 0.05581856146454811\n",
      "Iteration 20003, Loss: 0.05575549975037575\n",
      "Iteration 20004, Loss: 0.05566330999135971\n",
      "Iteration 20005, Loss: 0.05575132369995117\n",
      "Iteration 20006, Loss: 0.055724143981933594\n",
      "Iteration 20007, Loss: 0.055676739662885666\n",
      "Iteration 20008, Loss: 0.05570896714925766\n",
      "Iteration 20009, Loss: 0.05566553398966789\n",
      "Iteration 20010, Loss: 0.05571397393941879\n",
      "Iteration 20011, Loss: 0.05569617077708244\n",
      "Iteration 20012, Loss: 0.05568770691752434\n",
      "Iteration 20013, Loss: 0.05571679398417473\n",
      "Iteration 20014, Loss: 0.055657029151916504\n",
      "Iteration 20015, Loss: 0.05574623867869377\n",
      "Iteration 20016, Loss: 0.05574914067983627\n",
      "Iteration 20017, Loss: 0.055636607110500336\n",
      "Iteration 20018, Loss: 0.055670976638793945\n",
      "Iteration 20019, Loss: 0.055623650550842285\n",
      "Iteration 20020, Loss: 0.05574604123830795\n",
      "Iteration 20021, Loss: 0.055741868913173676\n",
      "Iteration 20022, Loss: 0.05564705654978752\n",
      "Iteration 20023, Loss: 0.05574445053935051\n",
      "Iteration 20024, Loss: 0.055739402770996094\n",
      "Iteration 20025, Loss: 0.055633071810007095\n",
      "Iteration 20026, Loss: 0.05564320087432861\n",
      "Iteration 20027, Loss: 0.05568361282348633\n",
      "Iteration 20028, Loss: 0.05563021078705788\n",
      "Iteration 20029, Loss: 0.05576050654053688\n",
      "Iteration 20030, Loss: 0.055797379463911057\n",
      "Iteration 20031, Loss: 0.055722713470458984\n",
      "Iteration 20032, Loss: 0.05568842217326164\n",
      "Iteration 20033, Loss: 0.055734001100063324\n",
      "Iteration 20034, Loss: 0.055640701204538345\n",
      "Iteration 20035, Loss: 0.0557682141661644\n",
      "Iteration 20036, Loss: 0.055829886347055435\n",
      "Iteration 20037, Loss: 0.05578983202576637\n",
      "Iteration 20038, Loss: 0.05566684529185295\n",
      "Iteration 20039, Loss: 0.0558091439306736\n",
      "Iteration 20040, Loss: 0.05587999150156975\n",
      "Iteration 20041, Loss: 0.05577341839671135\n",
      "Iteration 20042, Loss: 0.05568898096680641\n",
      "Iteration 20043, Loss: 0.0557732991874218\n",
      "Iteration 20044, Loss: 0.055755339562892914\n",
      "Iteration 20045, Loss: 0.05564789101481438\n",
      "Iteration 20046, Loss: 0.05581676959991455\n",
      "Iteration 20047, Loss: 0.05587383359670639\n",
      "Iteration 20048, Loss: 0.05575180426239967\n",
      "Iteration 20049, Loss: 0.05571504682302475\n",
      "Iteration 20050, Loss: 0.05580862611532211\n",
      "Iteration 20051, Loss: 0.05579916760325432\n",
      "Iteration 20052, Loss: 0.05569692701101303\n",
      "Iteration 20053, Loss: 0.055745285004377365\n",
      "Iteration 20054, Loss: 0.055799126625061035\n",
      "Iteration 20055, Loss: 0.055674754083156586\n",
      "Iteration 20056, Loss: 0.05577298253774643\n",
      "Iteration 20057, Loss: 0.055867914110422134\n",
      "Iteration 20058, Loss: 0.05585924908518791\n",
      "Iteration 20059, Loss: 0.05575736612081528\n",
      "Iteration 20060, Loss: 0.05566319078207016\n",
      "Iteration 20061, Loss: 0.05571715161204338\n",
      "Iteration 20062, Loss: 0.055624209344387054\n",
      "Iteration 20063, Loss: 0.0556231364607811\n",
      "Iteration 20064, Loss: 0.055717866867780685\n",
      "Iteration 20065, Loss: 0.05568770691752434\n",
      "Iteration 20066, Loss: 0.05568882077932358\n",
      "Iteration 20067, Loss: 0.05570630356669426\n",
      "Iteration 20068, Loss: 0.055622220039367676\n",
      "Iteration 20069, Loss: 0.05566100403666496\n",
      "Iteration 20070, Loss: 0.0556621178984642\n",
      "Iteration 20071, Loss: 0.05564817041158676\n",
      "Iteration 20072, Loss: 0.05570431798696518\n",
      "Iteration 20073, Loss: 0.05566207692027092\n",
      "Iteration 20074, Loss: 0.05572796240448952\n",
      "Iteration 20075, Loss: 0.055772822350263596\n",
      "Iteration 20076, Loss: 0.055717866867780685\n",
      "Iteration 20077, Loss: 0.05566096305847168\n",
      "Iteration 20078, Loss: 0.05566851422190666\n",
      "Iteration 20079, Loss: 0.05568913742899895\n",
      "Iteration 20080, Loss: 0.05570324510335922\n",
      "Iteration 20081, Loss: 0.05562766641378403\n",
      "Iteration 20082, Loss: 0.05579603090882301\n",
      "Iteration 20083, Loss: 0.05581001564860344\n",
      "Iteration 20084, Loss: 0.05565675348043442\n",
      "Iteration 20085, Loss: 0.055802226066589355\n",
      "Iteration 20086, Loss: 0.055908918380737305\n",
      "Iteration 20087, Loss: 0.0559082068502903\n",
      "Iteration 20088, Loss: 0.05581104755401611\n",
      "Iteration 20089, Loss: 0.05563831329345703\n",
      "Iteration 20090, Loss: 0.05589143559336662\n",
      "Iteration 20091, Loss: 0.0559992790222168\n",
      "Iteration 20092, Loss: 0.05592282861471176\n",
      "Iteration 20093, Loss: 0.055681269615888596\n",
      "Iteration 20094, Loss: 0.055845897644758224\n",
      "Iteration 20095, Loss: 0.05601147934794426\n",
      "Iteration 20096, Loss: 0.056065481156110764\n",
      "Iteration 20097, Loss: 0.05601867288351059\n",
      "Iteration 20098, Loss: 0.05588150396943092\n",
      "Iteration 20099, Loss: 0.05566469952464104\n",
      "Iteration 20100, Loss: 0.05592906475067139\n",
      "Iteration 20101, Loss: 0.05610918998718262\n",
      "Iteration 20102, Loss: 0.05609647557139397\n",
      "Iteration 20103, Loss: 0.0559108667075634\n",
      "Iteration 20104, Loss: 0.05564193055033684\n",
      "Iteration 20105, Loss: 0.05578148365020752\n",
      "Iteration 20106, Loss: 0.055817246437072754\n",
      "Iteration 20107, Loss: 0.05575462430715561\n",
      "Iteration 20108, Loss: 0.055621348321437836\n",
      "Iteration 20109, Loss: 0.05564463138580322\n",
      "Iteration 20110, Loss: 0.055695176124572754\n",
      "Iteration 20111, Loss: 0.055698953568935394\n",
      "Iteration 20112, Loss: 0.055617332458496094\n",
      "Iteration 20113, Loss: 0.055715642869472504\n",
      "Iteration 20114, Loss: 0.05567149445414543\n",
      "Iteration 20115, Loss: 0.05571381375193596\n",
      "Iteration 20116, Loss: 0.05573996156454086\n",
      "Iteration 20117, Loss: 0.05564824864268303\n",
      "Iteration 20118, Loss: 0.055784109979867935\n",
      "Iteration 20119, Loss: 0.05583072081208229\n",
      "Iteration 20120, Loss: 0.05572656914591789\n",
      "Iteration 20121, Loss: 0.055717311799526215\n",
      "Iteration 20122, Loss: 0.055794160813093185\n",
      "Iteration 20123, Loss: 0.05575041100382805\n",
      "Iteration 20124, Loss: 0.055632393807172775\n",
      "Iteration 20125, Loss: 0.055726055055856705\n",
      "Iteration 20126, Loss: 0.055666130036115646\n",
      "Iteration 20127, Loss: 0.05573714151978493\n",
      "Iteration 20128, Loss: 0.05579209700226784\n",
      "Iteration 20129, Loss: 0.0557478703558445\n",
      "Iteration 20130, Loss: 0.055628061294555664\n",
      "Iteration 20131, Loss: 0.05581164360046387\n",
      "Iteration 20132, Loss: 0.05583401769399643\n",
      "Iteration 20133, Loss: 0.055686794221401215\n",
      "Iteration 20134, Loss: 0.055778782814741135\n",
      "Iteration 20135, Loss: 0.05588475987315178\n",
      "Iteration 20136, Loss: 0.0558801107108593\n",
      "Iteration 20137, Loss: 0.05577639862895012\n",
      "Iteration 20138, Loss: 0.055648647248744965\n",
      "Iteration 20139, Loss: 0.055744051933288574\n",
      "Iteration 20140, Loss: 0.05568158999085426\n",
      "Iteration 20141, Loss: 0.055724501609802246\n",
      "Iteration 20142, Loss: 0.05577826872467995\n",
      "Iteration 20143, Loss: 0.05573419854044914\n",
      "Iteration 20144, Loss: 0.05562937259674072\n",
      "Iteration 20145, Loss: 0.05569680780172348\n",
      "Iteration 20146, Loss: 0.055648528039455414\n",
      "Iteration 20147, Loss: 0.05572235956788063\n",
      "Iteration 20148, Loss: 0.055723708122968674\n",
      "Iteration 20149, Loss: 0.055631402879953384\n",
      "Iteration 20150, Loss: 0.055676739662885666\n",
      "Iteration 20151, Loss: 0.05564018338918686\n",
      "Iteration 20152, Loss: 0.05563700199127197\n",
      "Iteration 20153, Loss: 0.05567546933889389\n",
      "Iteration 20154, Loss: 0.05562937259674072\n",
      "Iteration 20155, Loss: 0.05567403882741928\n",
      "Iteration 20156, Loss: 0.055620551109313965\n",
      "Iteration 20157, Loss: 0.05566215515136719\n",
      "Iteration 20158, Loss: 0.05565250292420387\n",
      "Iteration 20159, Loss: 0.05562790483236313\n",
      "Iteration 20160, Loss: 0.055745046585798264\n",
      "Iteration 20161, Loss: 0.05572323128581047\n",
      "Iteration 20162, Loss: 0.05566505715250969\n",
      "Iteration 20163, Loss: 0.055696725845336914\n",
      "Iteration 20164, Loss: 0.055626869201660156\n",
      "Iteration 20165, Loss: 0.05579479783773422\n",
      "Iteration 20166, Loss: 0.05582376569509506\n",
      "Iteration 20167, Loss: 0.05570455640554428\n",
      "Iteration 20168, Loss: 0.05574607849121094\n",
      "Iteration 20169, Loss: 0.05583024024963379\n",
      "Iteration 20170, Loss: 0.05579455941915512\n",
      "Iteration 20171, Loss: 0.055654846131801605\n",
      "Iteration 20172, Loss: 0.055827897042036057\n",
      "Iteration 20173, Loss: 0.05591630935668945\n",
      "Iteration 20174, Loss: 0.05584132671356201\n",
      "Iteration 20175, Loss: 0.055644236505031586\n",
      "Iteration 20176, Loss: 0.0558299645781517\n",
      "Iteration 20177, Loss: 0.05592791363596916\n",
      "Iteration 20178, Loss: 0.0558931827545166\n",
      "Iteration 20179, Loss: 0.055746160447597504\n",
      "Iteration 20180, Loss: 0.05573618784546852\n",
      "Iteration 20181, Loss: 0.055842719972133636\n",
      "Iteration 20182, Loss: 0.05580143257975578\n",
      "Iteration 20183, Loss: 0.0556204728782177\n",
      "Iteration 20184, Loss: 0.05587979406118393\n",
      "Iteration 20185, Loss: 0.056008219718933105\n",
      "Iteration 20186, Loss: 0.05600305646657944\n",
      "Iteration 20187, Loss: 0.05588464066386223\n",
      "Iteration 20188, Loss: 0.05570554733276367\n",
      "Iteration 20189, Loss: 0.0558323860168457\n",
      "Iteration 20190, Loss: 0.05595354363322258\n",
      "Iteration 20191, Loss: 0.05591662973165512\n",
      "Iteration 20192, Loss: 0.05573093891143799\n",
      "Iteration 20193, Loss: 0.055781446397304535\n",
      "Iteration 20194, Loss: 0.05591475963592529\n",
      "Iteration 20195, Loss: 0.055927399545907974\n",
      "Iteration 20196, Loss: 0.05583680048584938\n",
      "Iteration 20197, Loss: 0.055683813989162445\n",
      "Iteration 20198, Loss: 0.05582614988088608\n",
      "Iteration 20199, Loss: 0.05592028424143791\n",
      "Iteration 20200, Loss: 0.0558445081114769\n",
      "Iteration 20201, Loss: 0.05562504380941391\n",
      "Iteration 20202, Loss: 0.055805087089538574\n",
      "Iteration 20203, Loss: 0.055891238152980804\n",
      "Iteration 20204, Loss: 0.05587458610534668\n",
      "Iteration 20205, Loss: 0.05576499551534653\n",
      "Iteration 20206, Loss: 0.05566231533885002\n",
      "Iteration 20207, Loss: 0.05572458356618881\n",
      "Iteration 20208, Loss: 0.05561662092804909\n",
      "Iteration 20209, Loss: 0.05572887510061264\n",
      "Iteration 20210, Loss: 0.05572330951690674\n",
      "Iteration 20211, Loss: 0.0556364469230175\n",
      "Iteration 20212, Loss: 0.05574067682027817\n",
      "Iteration 20213, Loss: 0.055705588310956955\n",
      "Iteration 20214, Loss: 0.055691998451948166\n",
      "Iteration 20215, Loss: 0.05573209375143051\n",
      "Iteration 20216, Loss: 0.05567539110779762\n",
      "Iteration 20217, Loss: 0.055718980729579926\n",
      "Iteration 20218, Loss: 0.055722516030073166\n",
      "Iteration 20219, Loss: 0.05565369501709938\n",
      "Iteration 20220, Loss: 0.05567292496562004\n",
      "Iteration 20221, Loss: 0.05563414469361305\n",
      "Iteration 20222, Loss: 0.05568043515086174\n",
      "Iteration 20223, Loss: 0.05564197152853012\n",
      "Iteration 20224, Loss: 0.05571862310171127\n",
      "Iteration 20225, Loss: 0.05569728463888168\n",
      "Iteration 20226, Loss: 0.05568075552582741\n",
      "Iteration 20227, Loss: 0.05570018291473389\n",
      "Iteration 20228, Loss: 0.05562547966837883\n",
      "Iteration 20229, Loss: 0.0557248592376709\n",
      "Iteration 20230, Loss: 0.05566772073507309\n",
      "Iteration 20231, Loss: 0.05573320761322975\n",
      "Iteration 20232, Loss: 0.05578728765249252\n",
      "Iteration 20233, Loss: 0.05574246495962143\n",
      "Iteration 20234, Loss: 0.05561594292521477\n",
      "Iteration 20235, Loss: 0.05573483556509018\n",
      "Iteration 20236, Loss: 0.05570225045084953\n",
      "Iteration 20237, Loss: 0.05568234249949455\n",
      "Iteration 20238, Loss: 0.055709801614284515\n",
      "Iteration 20239, Loss: 0.055622540414333344\n",
      "Iteration 20240, Loss: 0.055814385414123535\n",
      "Iteration 20241, Loss: 0.05586552992463112\n",
      "Iteration 20242, Loss: 0.05577262490987778\n",
      "Iteration 20243, Loss: 0.0556764230132103\n",
      "Iteration 20244, Loss: 0.055762093514204025\n",
      "Iteration 20245, Loss: 0.05573546886444092\n",
      "Iteration 20246, Loss: 0.05564650148153305\n",
      "Iteration 20247, Loss: 0.055668674409389496\n",
      "Iteration 20248, Loss: 0.05565027520060539\n",
      "Iteration 20249, Loss: 0.05566120147705078\n",
      "Iteration 20250, Loss: 0.05566243454813957\n",
      "Iteration 20251, Loss: 0.055649202316999435\n",
      "Iteration 20252, Loss: 0.05568953603506088\n",
      "Iteration 20253, Loss: 0.05563608929514885\n",
      "Iteration 20254, Loss: 0.05574631690979004\n",
      "Iteration 20255, Loss: 0.055790942162275314\n",
      "Iteration 20256, Loss: 0.055737417191267014\n",
      "Iteration 20257, Loss: 0.05563124269247055\n",
      "Iteration 20258, Loss: 0.0556359700858593\n",
      "Iteration 20259, Loss: 0.05571210756897926\n",
      "Iteration 20260, Loss: 0.05572343245148659\n",
      "Iteration 20261, Loss: 0.05563954636454582\n",
      "Iteration 20262, Loss: 0.055795472115278244\n",
      "Iteration 20263, Loss: 0.055824875831604004\n",
      "Iteration 20264, Loss: 0.05568135157227516\n",
      "Iteration 20265, Loss: 0.055780891329050064\n",
      "Iteration 20266, Loss: 0.055886827409267426\n",
      "Iteration 20267, Loss: 0.05588706582784653\n",
      "Iteration 20268, Loss: 0.05579201504588127\n",
      "Iteration 20269, Loss: 0.055626872926950455\n",
      "Iteration 20270, Loss: 0.05585237592458725\n",
      "Iteration 20271, Loss: 0.05590745061635971\n",
      "Iteration 20272, Loss: 0.05578402802348137\n",
      "Iteration 20273, Loss: 0.05569307133555412\n",
      "Iteration 20274, Loss: 0.0557885579764843\n",
      "Iteration 20275, Loss: 0.05578061193227768\n",
      "Iteration 20276, Loss: 0.055677615106105804\n",
      "Iteration 20277, Loss: 0.05577302351593971\n",
      "Iteration 20278, Loss: 0.05582817643880844\n",
      "Iteration 20279, Loss: 0.05570673942565918\n",
      "Iteration 20280, Loss: 0.05574747174978256\n",
      "Iteration 20281, Loss: 0.05584017559885979\n",
      "Iteration 20282, Loss: 0.05582575127482414\n",
      "Iteration 20283, Loss: 0.05571616068482399\n",
      "Iteration 20284, Loss: 0.05572883412241936\n",
      "Iteration 20285, Loss: 0.05579153820872307\n",
      "Iteration 20286, Loss: 0.055677298456430435\n",
      "Iteration 20287, Loss: 0.05576610565185547\n",
      "Iteration 20288, Loss: 0.055854681879282\n",
      "Iteration 20289, Loss: 0.05583735555410385\n",
      "Iteration 20290, Loss: 0.0557272844016552\n",
      "Iteration 20291, Loss: 0.05571584030985832\n",
      "Iteration 20292, Loss: 0.055780015885829926\n",
      "Iteration 20293, Loss: 0.055670421570539474\n",
      "Iteration 20294, Loss: 0.05576590821146965\n",
      "Iteration 20295, Loss: 0.055849991738796234\n",
      "Iteration 20296, Loss: 0.05583107843995094\n",
      "Iteration 20297, Loss: 0.05572088807821274\n",
      "Iteration 20298, Loss: 0.05572414770722389\n",
      "Iteration 20299, Loss: 0.055786967277526855\n",
      "Iteration 20300, Loss: 0.055673759430646896\n",
      "Iteration 20301, Loss: 0.05576527118682861\n",
      "Iteration 20302, Loss: 0.05585157871246338\n",
      "Iteration 20303, Loss: 0.05583544820547104\n",
      "Iteration 20304, Loss: 0.0557272844016552\n",
      "Iteration 20305, Loss: 0.055711787194013596\n",
      "Iteration 20306, Loss: 0.05577266588807106\n",
      "Iteration 20307, Loss: 0.05565647408366203\n",
      "Iteration 20308, Loss: 0.05577930063009262\n",
      "Iteration 20309, Loss: 0.055867355316877365\n",
      "Iteration 20310, Loss: 0.055852215737104416\n",
      "Iteration 20311, Loss: 0.055744290351867676\n",
      "Iteration 20312, Loss: 0.05568830296397209\n",
      "Iteration 20313, Loss: 0.055749617516994476\n",
      "Iteration 20314, Loss: 0.05563315004110336\n",
      "Iteration 20315, Loss: 0.05579841136932373\n",
      "Iteration 20316, Loss: 0.055888574570417404\n",
      "Iteration 20317, Loss: 0.055875103920698166\n",
      "Iteration 20318, Loss: 0.05576888844370842\n",
      "Iteration 20319, Loss: 0.05565309524536133\n",
      "Iteration 20320, Loss: 0.05571683496236801\n",
      "Iteration 20321, Loss: 0.055624447762966156\n",
      "Iteration 20322, Loss: 0.055706582963466644\n",
      "Iteration 20323, Loss: 0.05568528175354004\n",
      "Iteration 20324, Loss: 0.055676303803920746\n",
      "Iteration 20325, Loss: 0.055667441338300705\n",
      "Iteration 20326, Loss: 0.05569756403565407\n",
      "Iteration 20327, Loss: 0.055704955011606216\n",
      "Iteration 20328, Loss: 0.055636804550886154\n",
      "Iteration 20329, Loss: 0.055703483521938324\n",
      "Iteration 20330, Loss: 0.055637482553720474\n",
      "Iteration 20331, Loss: 0.05574723333120346\n",
      "Iteration 20332, Loss: 0.055792056024074554\n",
      "Iteration 20333, Loss: 0.055737853050231934\n",
      "Iteration 20334, Loss: 0.05563442036509514\n",
      "Iteration 20335, Loss: 0.05565444752573967\n",
      "Iteration 20336, Loss: 0.05568655580282211\n",
      "Iteration 20337, Loss: 0.05568882077932358\n",
      "Iteration 20338, Loss: 0.05563132092356682\n",
      "Iteration 20339, Loss: 0.055640578269958496\n",
      "Iteration 20340, Loss: 0.05564487352967262\n",
      "Iteration 20341, Loss: 0.055665019899606705\n",
      "Iteration 20342, Loss: 0.05564228817820549\n",
      "Iteration 20343, Loss: 0.05572374910116196\n",
      "Iteration 20344, Loss: 0.0556897334754467\n",
      "Iteration 20345, Loss: 0.05570296570658684\n",
      "Iteration 20346, Loss: 0.055744051933288574\n",
      "Iteration 20347, Loss: 0.0556873083114624\n",
      "Iteration 20348, Loss: 0.055704277008771896\n",
      "Iteration 20349, Loss: 0.05570805072784424\n",
      "Iteration 20350, Loss: 0.05566462129354477\n",
      "Iteration 20351, Loss: 0.05568349361419678\n",
      "Iteration 20352, Loss: 0.05561864748597145\n",
      "Iteration 20353, Loss: 0.05569295212626457\n",
      "Iteration 20354, Loss: 0.0556541308760643\n",
      "Iteration 20355, Loss: 0.055722277611494064\n",
      "Iteration 20356, Loss: 0.055719535797834396\n",
      "Iteration 20357, Loss: 0.05564693734049797\n",
      "Iteration 20358, Loss: 0.055661678314208984\n",
      "Iteration 20359, Loss: 0.055671416223049164\n",
      "Iteration 20360, Loss: 0.05564042180776596\n",
      "Iteration 20361, Loss: 0.05572390556335449\n",
      "Iteration 20362, Loss: 0.05571679398417473\n",
      "Iteration 20363, Loss: 0.055647097527980804\n",
      "Iteration 20364, Loss: 0.055668752640485764\n",
      "Iteration 20365, Loss: 0.05566283315420151\n",
      "Iteration 20366, Loss: 0.05564149469137192\n",
      "Iteration 20367, Loss: 0.05570511147379875\n",
      "Iteration 20368, Loss: 0.05566660687327385\n",
      "Iteration 20369, Loss: 0.05571989342570305\n",
      "Iteration 20370, Loss: 0.05575120449066162\n",
      "Iteration 20371, Loss: 0.05567288398742676\n",
      "Iteration 20372, Loss: 0.055743854492902756\n",
      "Iteration 20373, Loss: 0.05577552691102028\n",
      "Iteration 20374, Loss: 0.05564737692475319\n",
      "Iteration 20375, Loss: 0.05580262467265129\n",
      "Iteration 20376, Loss: 0.05589338392019272\n",
      "Iteration 20377, Loss: 0.05586135387420654\n",
      "Iteration 20378, Loss: 0.05572807788848877\n",
      "Iteration 20379, Loss: 0.05574290081858635\n",
      "Iteration 20380, Loss: 0.05583377927541733\n",
      "Iteration 20381, Loss: 0.05576201528310776\n",
      "Iteration 20382, Loss: 0.05567117780447006\n",
      "Iteration 20383, Loss: 0.05572911351919174\n",
      "Iteration 20384, Loss: 0.05568154901266098\n",
      "Iteration 20385, Loss: 0.055700939148664474\n",
      "Iteration 20386, Loss: 0.055703047662973404\n",
      "Iteration 20387, Loss: 0.05566549673676491\n",
      "Iteration 20388, Loss: 0.055677175521850586\n",
      "Iteration 20389, Loss: 0.055645428597927094\n",
      "Iteration 20390, Loss: 0.055663030594587326\n",
      "Iteration 20391, Loss: 0.05565552040934563\n",
      "Iteration 20392, Loss: 0.05564519017934799\n",
      "Iteration 20393, Loss: 0.0556948222219944\n",
      "Iteration 20394, Loss: 0.055648088455200195\n",
      "Iteration 20395, Loss: 0.055731020867824554\n",
      "Iteration 20396, Loss: 0.05576189607381821\n",
      "Iteration 20397, Loss: 0.055686116218566895\n",
      "Iteration 20398, Loss: 0.05572625249624252\n",
      "Iteration 20399, Loss: 0.05575815960764885\n",
      "Iteration 20400, Loss: 0.05563120171427727\n",
      "Iteration 20401, Loss: 0.055812202394008636\n",
      "Iteration 20402, Loss: 0.055899105966091156\n",
      "Iteration 20403, Loss: 0.05586811155080795\n",
      "Iteration 20404, Loss: 0.05573992058634758\n",
      "Iteration 20405, Loss: 0.05572696775197983\n",
      "Iteration 20406, Loss: 0.05581728741526604\n",
      "Iteration 20407, Loss: 0.05575386807322502\n",
      "Iteration 20408, Loss: 0.05567483231425285\n",
      "Iteration 20409, Loss: 0.055727723985910416\n",
      "Iteration 20410, Loss: 0.05568687245249748\n",
      "Iteration 20411, Loss: 0.05568882077932358\n",
      "Iteration 20412, Loss: 0.05568870157003403\n",
      "Iteration 20413, Loss: 0.055679600685834885\n",
      "Iteration 20414, Loss: 0.05569180101156235\n",
      "Iteration 20415, Loss: 0.055645428597927094\n",
      "Iteration 20416, Loss: 0.05571548268198967\n",
      "Iteration 20417, Loss: 0.05567129701375961\n",
      "Iteration 20418, Loss: 0.05571715161204338\n",
      "Iteration 20419, Loss: 0.05575716868042946\n",
      "Iteration 20420, Loss: 0.055696647614240646\n",
      "Iteration 20421, Loss: 0.05569613352417946\n",
      "Iteration 20422, Loss: 0.055706899613142014\n",
      "Iteration 20423, Loss: 0.05565822497010231\n",
      "Iteration 20424, Loss: 0.05567121505737305\n",
      "Iteration 20425, Loss: 0.05564181134104729\n",
      "Iteration 20426, Loss: 0.055638592690229416\n",
      "Iteration 20427, Loss: 0.05563795566558838\n",
      "Iteration 20428, Loss: 0.05567359924316406\n",
      "Iteration 20429, Loss: 0.055654607713222504\n",
      "Iteration 20430, Loss: 0.055701058357954025\n",
      "Iteration 20431, Loss: 0.05566398426890373\n",
      "Iteration 20432, Loss: 0.055722832679748535\n",
      "Iteration 20433, Loss: 0.05576499551534653\n",
      "Iteration 20434, Loss: 0.05570809170603752\n",
      "Iteration 20435, Loss: 0.055674951523542404\n",
      "Iteration 20436, Loss: 0.05568147078156471\n",
      "Iteration 20437, Loss: 0.055680952966213226\n",
      "Iteration 20438, Loss: 0.055696647614240646\n",
      "Iteration 20439, Loss: 0.055624764412641525\n",
      "Iteration 20440, Loss: 0.05578438565135002\n",
      "Iteration 20441, Loss: 0.05578693002462387\n",
      "Iteration 20442, Loss: 0.05563577264547348\n",
      "Iteration 20443, Loss: 0.05579495429992676\n",
      "Iteration 20444, Loss: 0.055871687829494476\n",
      "Iteration 20445, Loss: 0.05583461374044418\n",
      "Iteration 20446, Loss: 0.05570213124155998\n",
      "Iteration 20447, Loss: 0.05577024072408676\n",
      "Iteration 20448, Loss: 0.055857621133327484\n",
      "Iteration 20449, Loss: 0.055778663605451584\n",
      "Iteration 20450, Loss: 0.05566016957163811\n",
      "Iteration 20451, Loss: 0.0557248592376709\n",
      "Iteration 20452, Loss: 0.05568039417266846\n",
      "Iteration 20453, Loss: 0.05569879338145256\n",
      "Iteration 20454, Loss: 0.05569954961538315\n",
      "Iteration 20455, Loss: 0.055665574967861176\n",
      "Iteration 20456, Loss: 0.055676937103271484\n",
      "Iteration 20457, Loss: 0.055643003433942795\n",
      "Iteration 20458, Loss: 0.055637042969465256\n",
      "Iteration 20459, Loss: 0.055677734315395355\n",
      "Iteration 20460, Loss: 0.055652182549238205\n",
      "Iteration 20461, Loss: 0.05571500584483147\n",
      "Iteration 20462, Loss: 0.05569668859243393\n",
      "Iteration 20463, Loss: 0.05568110942840576\n",
      "Iteration 20464, Loss: 0.055704355239868164\n",
      "Iteration 20465, Loss: 0.05562270060181618\n",
      "Iteration 20466, Loss: 0.05580655857920647\n",
      "Iteration 20467, Loss: 0.05582793802022934\n",
      "Iteration 20468, Loss: 0.05567976087331772\n",
      "Iteration 20469, Loss: 0.055785536766052246\n",
      "Iteration 20470, Loss: 0.055892590433359146\n",
      "Iteration 20471, Loss: 0.05588611215353012\n",
      "Iteration 20472, Loss: 0.055779341608285904\n",
      "Iteration 20473, Loss: 0.05565524101257324\n",
      "Iteration 20474, Loss: 0.05576710030436516\n",
      "Iteration 20475, Loss: 0.055727243423461914\n",
      "Iteration 20476, Loss: 0.055683258920907974\n",
      "Iteration 20477, Loss: 0.055727921426296234\n",
      "Iteration 20478, Loss: 0.05568695068359375\n",
      "Iteration 20479, Loss: 0.05568993091583252\n",
      "Iteration 20480, Loss: 0.055682383477687836\n",
      "Iteration 20481, Loss: 0.05568643659353256\n",
      "Iteration 20482, Loss: 0.05570654198527336\n",
      "Iteration 20483, Loss: 0.055640023201704025\n",
      "Iteration 20484, Loss: 0.055773619562387466\n",
      "Iteration 20485, Loss: 0.055785536766052246\n",
      "Iteration 20486, Loss: 0.055646419525146484\n",
      "Iteration 20487, Loss: 0.0557885579764843\n",
      "Iteration 20488, Loss: 0.05586683750152588\n",
      "Iteration 20489, Loss: 0.055830441415309906\n",
      "Iteration 20490, Loss: 0.05569648742675781\n",
      "Iteration 20491, Loss: 0.055777352303266525\n",
      "Iteration 20492, Loss: 0.0558655671775341\n",
      "Iteration 20493, Loss: 0.055787764489650726\n",
      "Iteration 20494, Loss: 0.05565142631530762\n",
      "Iteration 20495, Loss: 0.05571834370493889\n",
      "Iteration 20496, Loss: 0.05567324161529541\n",
      "Iteration 20497, Loss: 0.05571059510111809\n",
      "Iteration 20498, Loss: 0.055715881288051605\n",
      "Iteration 20499, Loss: 0.05564673990011215\n",
      "Iteration 20500, Loss: 0.05566168203949928\n",
      "Iteration 20501, Loss: 0.05566410347819328\n",
      "Iteration 20502, Loss: 0.05563024803996086\n",
      "Iteration 20503, Loss: 0.05571611970663071\n",
      "Iteration 20504, Loss: 0.055685680359601974\n",
      "Iteration 20505, Loss: 0.055694423615932465\n",
      "Iteration 20506, Loss: 0.05571615695953369\n",
      "Iteration 20507, Loss: 0.055626314133405685\n",
      "Iteration 20508, Loss: 0.05579225346446037\n",
      "Iteration 20509, Loss: 0.055808864533901215\n",
      "Iteration 20510, Loss: 0.05566314980387688\n",
      "Iteration 20511, Loss: 0.05579742044210434\n",
      "Iteration 20512, Loss: 0.05589870736002922\n",
      "Iteration 20513, Loss: 0.0558801107108593\n",
      "Iteration 20514, Loss: 0.05575820058584213\n",
      "Iteration 20515, Loss: 0.055696211755275726\n",
      "Iteration 20516, Loss: 0.05578545853495598\n",
      "Iteration 20517, Loss: 0.05571639910340309\n",
      "Iteration 20518, Loss: 0.055709563195705414\n",
      "Iteration 20519, Loss: 0.055767178535461426\n",
      "Iteration 20520, Loss: 0.05572402477264404\n",
      "Iteration 20521, Loss: 0.05565885826945305\n",
      "Iteration 20522, Loss: 0.055716633796691895\n",
      "Iteration 20523, Loss: 0.0556645430624485\n",
      "Iteration 20524, Loss: 0.05571480840444565\n",
      "Iteration 20525, Loss: 0.05574440956115723\n",
      "Iteration 20526, Loss: 0.05567725747823715\n",
      "Iteration 20527, Loss: 0.055731773376464844\n",
      "Iteration 20528, Loss: 0.05575184151530266\n",
      "Iteration 20529, Loss: 0.055632393807172775\n",
      "Iteration 20530, Loss: 0.05574357882142067\n",
      "Iteration 20531, Loss: 0.05575708672404289\n",
      "Iteration 20532, Loss: 0.05565448850393295\n",
      "Iteration 20533, Loss: 0.05578569695353508\n",
      "Iteration 20534, Loss: 0.055841248482465744\n",
      "Iteration 20535, Loss: 0.055744051933288574\n",
      "Iteration 20536, Loss: 0.055696845054626465\n",
      "Iteration 20537, Loss: 0.05577079579234123\n",
      "Iteration 20538, Loss: 0.05572501942515373\n",
      "Iteration 20539, Loss: 0.055656515061855316\n",
      "Iteration 20540, Loss: 0.05567526817321777\n",
      "Iteration 20541, Loss: 0.05566748231649399\n",
      "Iteration 20542, Loss: 0.05565818399190903\n",
      "Iteration 20543, Loss: 0.05568838119506836\n",
      "Iteration 20544, Loss: 0.05566028878092766\n",
      "Iteration 20545, Loss: 0.05571707338094711\n",
      "Iteration 20546, Loss: 0.05574027821421623\n",
      "Iteration 20547, Loss: 0.05565766617655754\n",
      "Iteration 20548, Loss: 0.055762410163879395\n",
      "Iteration 20549, Loss: 0.05579352378845215\n",
      "Iteration 20550, Loss: 0.05566636845469475\n",
      "Iteration 20551, Loss: 0.055784545838832855\n",
      "Iteration 20552, Loss: 0.055876296013593674\n",
      "Iteration 20553, Loss: 0.055851858109235764\n",
      "Iteration 20554, Loss: 0.055729590356349945\n",
      "Iteration 20555, Loss: 0.055729273706674576\n",
      "Iteration 20556, Loss: 0.055811285972595215\n",
      "Iteration 20557, Loss: 0.05573391914367676\n",
      "Iteration 20558, Loss: 0.05569815635681152\n",
      "Iteration 20559, Loss: 0.05576006695628166\n",
      "Iteration 20560, Loss: 0.055717747658491135\n",
      "Iteration 20561, Loss: 0.05566044896841049\n",
      "Iteration 20562, Loss: 0.05570173263549805\n",
      "Iteration 20563, Loss: 0.05565059185028076\n",
      "Iteration 20564, Loss: 0.05569374933838844\n",
      "Iteration 20565, Loss: 0.055688660591840744\n",
      "Iteration 20566, Loss: 0.05564860627055168\n",
      "Iteration 20567, Loss: 0.05564296618103981\n",
      "Iteration 20568, Loss: 0.05566879361867905\n",
      "Iteration 20569, Loss: 0.05562019348144531\n",
      "Iteration 20570, Loss: 0.055769287049770355\n",
      "Iteration 20571, Loss: 0.05579901114106178\n",
      "Iteration 20572, Loss: 0.055713456124067307\n",
      "Iteration 20573, Loss: 0.05570944398641586\n",
      "Iteration 20574, Loss: 0.05576130002737045\n",
      "Iteration 20575, Loss: 0.05567038059234619\n",
      "Iteration 20576, Loss: 0.05576106160879135\n",
      "Iteration 20577, Loss: 0.055827539414167404\n",
      "Iteration 20578, Loss: 0.055778346955776215\n",
      "Iteration 20579, Loss: 0.055661361664533615\n",
      "Iteration 20580, Loss: 0.055790070444345474\n",
      "Iteration 20581, Loss: 0.05582980439066887\n",
      "Iteration 20582, Loss: 0.05570455640554428\n",
      "Iteration 20583, Loss: 0.055754028260707855\n",
      "Iteration 20584, Loss: 0.055847883224487305\n",
      "Iteration 20585, Loss: 0.05583469197154045\n",
      "Iteration 20586, Loss: 0.05572931095957756\n",
      "Iteration 20587, Loss: 0.055710673332214355\n",
      "Iteration 20588, Loss: 0.05577560514211655\n",
      "Iteration 20589, Loss: 0.055677615106105804\n",
      "Iteration 20590, Loss: 0.05575358867645264\n",
      "Iteration 20591, Loss: 0.055829763412475586\n",
      "Iteration 20592, Loss: 0.055804651230573654\n",
      "Iteration 20593, Loss: 0.055691324174404144\n",
      "Iteration 20594, Loss: 0.05576745793223381\n",
      "Iteration 20595, Loss: 0.05583310127258301\n",
      "Iteration 20596, Loss: 0.05572160333395004\n",
      "Iteration 20597, Loss: 0.05573006719350815\n",
      "Iteration 20598, Loss: 0.05581720918416977\n",
      "Iteration 20599, Loss: 0.05580206960439682\n",
      "Iteration 20600, Loss: 0.05569513887166977\n",
      "Iteration 20601, Loss: 0.05575275793671608\n",
      "Iteration 20602, Loss: 0.0558118037879467\n",
      "Iteration 20603, Loss: 0.05569259449839592\n",
      "Iteration 20604, Loss: 0.055756013840436935\n",
      "Iteration 20605, Loss: 0.05584792420268059\n",
      "Iteration 20606, Loss: 0.055836718529462814\n",
      "Iteration 20607, Loss: 0.05573252961039543\n",
      "Iteration 20608, Loss: 0.05569843575358391\n",
      "Iteration 20609, Loss: 0.05575462430715561\n",
      "Iteration 20610, Loss: 0.055632829666137695\n",
      "Iteration 20611, Loss: 0.055801473557949066\n",
      "Iteration 20612, Loss: 0.055894892662763596\n",
      "Iteration 20613, Loss: 0.05588444322347641\n",
      "Iteration 20614, Loss: 0.055780570954084396\n",
      "Iteration 20615, Loss: 0.055633544921875\n",
      "Iteration 20616, Loss: 0.055690448731184006\n",
      "Iteration 20617, Loss: 0.05564085766673088\n",
      "Iteration 20618, Loss: 0.0556316003203392\n",
      "Iteration 20619, Loss: 0.05571667477488518\n",
      "Iteration 20620, Loss: 0.055670540779829025\n",
      "Iteration 20621, Loss: 0.055721402168273926\n",
      "Iteration 20622, Loss: 0.05576511472463608\n",
      "Iteration 20623, Loss: 0.055706899613142014\n",
      "Iteration 20624, Loss: 0.05567944049835205\n",
      "Iteration 20625, Loss: 0.055692158639431\n",
      "Iteration 20626, Loss: 0.05566680431365967\n",
      "Iteration 20627, Loss: 0.055675629526376724\n",
      "Iteration 20628, Loss: 0.055642805993556976\n",
      "Iteration 20629, Loss: 0.05565083026885986\n",
      "Iteration 20630, Loss: 0.055657945573329926\n",
      "Iteration 20631, Loss: 0.05562877655029297\n",
      "Iteration 20632, Loss: 0.05574393644928932\n",
      "Iteration 20633, Loss: 0.05573582649230957\n",
      "Iteration 20634, Loss: 0.05564634129405022\n",
      "Iteration 20635, Loss: 0.055712223052978516\n",
      "Iteration 20636, Loss: 0.055671773850917816\n",
      "Iteration 20637, Loss: 0.05571945756673813\n",
      "Iteration 20638, Loss: 0.05574774742126465\n",
      "Iteration 20639, Loss: 0.05567058175802231\n",
      "Iteration 20640, Loss: 0.05574071407318115\n",
      "Iteration 20641, Loss: 0.05576900765299797\n",
      "Iteration 20642, Loss: 0.055643003433942795\n",
      "Iteration 20643, Loss: 0.055807434022426605\n",
      "Iteration 20644, Loss: 0.055898986756801605\n",
      "Iteration 20645, Loss: 0.055872123688459396\n",
      "Iteration 20646, Loss: 0.05574750900268555\n",
      "Iteration 20647, Loss: 0.05571659654378891\n",
      "Iteration 20648, Loss: 0.05580679699778557\n",
      "Iteration 20649, Loss: 0.055748067796230316\n",
      "Iteration 20650, Loss: 0.05567900463938713\n",
      "Iteration 20651, Loss: 0.05572951212525368\n",
      "Iteration 20652, Loss: 0.05569116398692131\n",
      "Iteration 20653, Loss: 0.05568420886993408\n",
      "Iteration 20654, Loss: 0.05568774789571762\n",
      "Iteration 20655, Loss: 0.05567757412791252\n",
      "Iteration 20656, Loss: 0.05568675324320793\n",
      "Iteration 20657, Loss: 0.05565115064382553\n",
      "Iteration 20658, Loss: 0.05570109933614731\n",
      "Iteration 20659, Loss: 0.05565647408366203\n",
      "Iteration 20660, Loss: 0.055716753005981445\n",
      "Iteration 20661, Loss: 0.05574111267924309\n",
      "Iteration 20662, Loss: 0.05566016957163811\n",
      "Iteration 20663, Loss: 0.05576741695404053\n",
      "Iteration 20664, Loss: 0.055802226066589355\n",
      "Iteration 20665, Loss: 0.05567316338419914\n",
      "Iteration 20666, Loss: 0.05577775090932846\n",
      "Iteration 20667, Loss: 0.05586914345622063\n",
      "Iteration 20668, Loss: 0.05584276095032692\n",
      "Iteration 20669, Loss: 0.05571556091308594\n",
      "Iteration 20670, Loss: 0.05574878305196762\n",
      "Iteration 20671, Loss: 0.0558316707611084\n",
      "Iteration 20672, Loss: 0.055744610726833344\n",
      "Iteration 20673, Loss: 0.05569493770599365\n",
      "Iteration 20674, Loss: 0.05576423928141594\n",
      "Iteration 20675, Loss: 0.055723391473293304\n",
      "Iteration 20676, Loss: 0.055649399757385254\n",
      "Iteration 20677, Loss: 0.055696211755275726\n",
      "Iteration 20678, Loss: 0.05564491078257561\n",
      "Iteration 20679, Loss: 0.05568588152527809\n",
      "Iteration 20680, Loss: 0.05566748231649399\n",
      "Iteration 20681, Loss: 0.05569148063659668\n",
      "Iteration 20682, Loss: 0.05567129701375961\n",
      "Iteration 20683, Loss: 0.055698275566101074\n",
      "Iteration 20684, Loss: 0.05571671575307846\n",
      "Iteration 20685, Loss: 0.05562921613454819\n",
      "Iteration 20686, Loss: 0.05580350011587143\n",
      "Iteration 20687, Loss: 0.05583556741476059\n",
      "Iteration 20688, Loss: 0.05570400133728981\n",
      "Iteration 20689, Loss: 0.05575482174754143\n",
      "Iteration 20690, Loss: 0.05585090443491936\n",
      "Iteration 20691, Loss: 0.05583620071411133\n",
      "Iteration 20692, Loss: 0.05572287365794182\n",
      "Iteration 20693, Loss: 0.05572374910116196\n",
      "Iteration 20694, Loss: 0.05579444020986557\n",
      "Iteration 20695, Loss: 0.05569732189178467\n",
      "Iteration 20696, Loss: 0.05573860928416252\n",
      "Iteration 20697, Loss: 0.0558147057890892\n",
      "Iteration 20698, Loss: 0.055783871561288834\n",
      "Iteration 20699, Loss: 0.05566509813070297\n",
      "Iteration 20700, Loss: 0.055801909416913986\n",
      "Iteration 20701, Loss: 0.055868469178676605\n",
      "Iteration 20702, Loss: 0.05576090142130852\n",
      "Iteration 20703, Loss: 0.05569672957062721\n",
      "Iteration 20704, Loss: 0.05578085035085678\n",
      "Iteration 20705, Loss: 0.05576058477163315\n",
      "Iteration 20706, Loss: 0.05564936250448227\n",
      "Iteration 20707, Loss: 0.055816374719142914\n",
      "Iteration 20708, Loss: 0.05587713047862053\n",
      "Iteration 20709, Loss: 0.0557607039809227\n",
      "Iteration 20710, Loss: 0.05570328235626221\n",
      "Iteration 20711, Loss: 0.05579324811697006\n",
      "Iteration 20712, Loss: 0.055779777467250824\n",
      "Iteration 20713, Loss: 0.055673401802778244\n",
      "Iteration 20714, Loss: 0.05578096956014633\n",
      "Iteration 20715, Loss: 0.05584001913666725\n",
      "Iteration 20716, Loss: 0.05572224035859108\n",
      "Iteration 20717, Loss: 0.055732689797878265\n",
      "Iteration 20718, Loss: 0.055823326110839844\n",
      "Iteration 20719, Loss: 0.05581025406718254\n",
      "Iteration 20720, Loss: 0.05570423603057861\n",
      "Iteration 20721, Loss: 0.055739402770996094\n",
      "Iteration 20722, Loss: 0.055798374116420746\n",
      "Iteration 20723, Loss: 0.05568079277873039\n",
      "Iteration 20724, Loss: 0.05576344579458237\n",
      "Iteration 20725, Loss: 0.05585404485464096\n",
      "Iteration 20726, Loss: 0.05584065243601799\n",
      "Iteration 20727, Loss: 0.055734358727931976\n",
      "Iteration 20728, Loss: 0.0556994304060936\n",
      "Iteration 20729, Loss: 0.05575990676879883\n",
      "Iteration 20730, Loss: 0.055644553154706955\n",
      "Iteration 20731, Loss: 0.05578812211751938\n",
      "Iteration 20732, Loss: 0.05587681382894516\n",
      "Iteration 20733, Loss: 0.05586246773600578\n",
      "Iteration 20734, Loss: 0.0557556189596653\n",
      "Iteration 20735, Loss: 0.0556713342666626\n",
      "Iteration 20736, Loss: 0.05573264881968498\n",
      "Iteration 20737, Loss: 0.05562214180827141\n",
      "Iteration 20738, Loss: 0.055778346955776215\n",
      "Iteration 20739, Loss: 0.055838268250226974\n",
      "Iteration 20740, Loss: 0.05579221621155739\n",
      "Iteration 20741, Loss: 0.055658817291259766\n",
      "Iteration 20742, Loss: 0.055829208344221115\n",
      "Iteration 20743, Loss: 0.05591022968292236\n",
      "Iteration 20744, Loss: 0.05580965802073479\n",
      "Iteration 20745, Loss: 0.05565746873617172\n",
      "Iteration 20746, Loss: 0.055737972259521484\n",
      "Iteration 20747, Loss: 0.05571548268198967\n",
      "Iteration 20748, Loss: 0.05562905594706535\n",
      "Iteration 20749, Loss: 0.055696647614240646\n",
      "Iteration 20750, Loss: 0.05564240738749504\n",
      "Iteration 20751, Loss: 0.05572708696126938\n",
      "Iteration 20752, Loss: 0.05573447793722153\n",
      "Iteration 20753, Loss: 0.055629532784223557\n",
      "Iteration 20754, Loss: 0.0557887963950634\n",
      "Iteration 20755, Loss: 0.05581136792898178\n",
      "Iteration 20756, Loss: 0.05567833036184311\n",
      "Iteration 20757, Loss: 0.05577751249074936\n",
      "Iteration 20758, Loss: 0.055871766060590744\n",
      "Iteration 20759, Loss: 0.05584832280874252\n",
      "Iteration 20760, Loss: 0.05572422593832016\n",
      "Iteration 20761, Loss: 0.05573539063334465\n",
      "Iteration 20762, Loss: 0.055818162858486176\n",
      "Iteration 20763, Loss: 0.055740438401699066\n",
      "Iteration 20764, Loss: 0.05569072812795639\n",
      "Iteration 20765, Loss: 0.05575239658355713\n",
      "Iteration 20766, Loss: 0.05570630356669426\n",
      "Iteration 20767, Loss: 0.05567260831594467\n",
      "Iteration 20768, Loss: 0.0556894950568676\n",
      "Iteration 20769, Loss: 0.05566462129354477\n",
      "Iteration 20770, Loss: 0.055667243897914886\n",
      "Iteration 20771, Loss: 0.05565941706299782\n",
      "Iteration 20772, Loss: 0.055650632828474045\n",
      "Iteration 20773, Loss: 0.05567685887217522\n",
      "Iteration 20774, Loss: 0.055667679756879807\n",
      "Iteration 20775, Loss: 0.05567189306020737\n",
      "Iteration 20776, Loss: 0.055631838738918304\n",
      "Iteration 20777, Loss: 0.055731140077114105\n",
      "Iteration 20778, Loss: 0.05575315281748772\n",
      "Iteration 20779, Loss: 0.05567189306020737\n",
      "Iteration 20780, Loss: 0.05575132369995117\n",
      "Iteration 20781, Loss: 0.055783115327358246\n",
      "Iteration 20782, Loss: 0.0556461438536644\n",
      "Iteration 20783, Loss: 0.05580727383494377\n",
      "Iteration 20784, Loss: 0.055906735360622406\n",
      "Iteration 20785, Loss: 0.055890560150146484\n",
      "Iteration 20786, Loss: 0.05577477067708969\n",
      "Iteration 20787, Loss: 0.055675946176052094\n",
      "Iteration 20788, Loss: 0.055778663605451584\n",
      "Iteration 20789, Loss: 0.055733244866132736\n",
      "Iteration 20790, Loss: 0.05568719282746315\n",
      "Iteration 20791, Loss: 0.05573376268148422\n",
      "Iteration 20792, Loss: 0.055698078125715256\n",
      "Iteration 20793, Loss: 0.055676382035017014\n",
      "Iteration 20794, Loss: 0.05568377301096916\n",
      "Iteration 20795, Loss: 0.055674076080322266\n",
      "Iteration 20796, Loss: 0.05568122863769531\n",
      "Iteration 20797, Loss: 0.055652063339948654\n",
      "Iteration 20798, Loss: 0.055696092545986176\n",
      "Iteration 20799, Loss: 0.0556667260825634\n",
      "Iteration 20800, Loss: 0.05569716542959213\n",
      "Iteration 20801, Loss: 0.05569915100932121\n",
      "Iteration 20802, Loss: 0.05564701929688454\n",
      "Iteration 20803, Loss: 0.05563291162252426\n",
      "Iteration 20804, Loss: 0.05571313947439194\n",
      "Iteration 20805, Loss: 0.05568111315369606\n",
      "Iteration 20806, Loss: 0.05570046231150627\n",
      "Iteration 20807, Loss: 0.055723272264003754\n",
      "Iteration 20808, Loss: 0.0556333102285862\n",
      "Iteration 20809, Loss: 0.05579543486237526\n",
      "Iteration 20810, Loss: 0.05582582950592041\n",
      "Iteration 20811, Loss: 0.05569565296173096\n",
      "Iteration 20812, Loss: 0.055762652307748795\n",
      "Iteration 20813, Loss: 0.05585654824972153\n",
      "Iteration 20814, Loss: 0.055830083787441254\n",
      "Iteration 20815, Loss: 0.05570065975189209\n",
      "Iteration 20816, Loss: 0.05576876923441887\n",
      "Iteration 20817, Loss: 0.05585237592458725\n",
      "Iteration 20818, Loss: 0.05576710030436516\n",
      "Iteration 20819, Loss: 0.055674951523542404\n",
      "Iteration 20820, Loss: 0.05574421212077141\n",
      "Iteration 20821, Loss: 0.05569998547434807\n",
      "Iteration 20822, Loss: 0.055678728967905045\n",
      "Iteration 20823, Loss: 0.055683575570583344\n",
      "Iteration 20824, Loss: 0.055676501244306564\n",
      "Iteration 20825, Loss: 0.055683135986328125\n",
      "Iteration 20826, Loss: 0.05564538761973381\n",
      "Iteration 20827, Loss: 0.05564550682902336\n",
      "Iteration 20828, Loss: 0.05568190664052963\n",
      "Iteration 20829, Loss: 0.05567415803670883\n",
      "Iteration 20830, Loss: 0.055662594735622406\n",
      "Iteration 20831, Loss: 0.05562202259898186\n",
      "Iteration 20832, Loss: 0.05573276802897453\n",
      "Iteration 20833, Loss: 0.055736981332302094\n",
      "Iteration 20834, Loss: 0.05564340204000473\n",
      "Iteration 20835, Loss: 0.05577941983938217\n",
      "Iteration 20836, Loss: 0.055802904069423676\n",
      "Iteration 20837, Loss: 0.055663589388132095\n",
      "Iteration 20838, Loss: 0.05579531192779541\n",
      "Iteration 20839, Loss: 0.05589640513062477\n",
      "Iteration 20840, Loss: 0.05588483810424805\n",
      "Iteration 20841, Loss: 0.05577552318572998\n",
      "Iteration 20842, Loss: 0.05566994473338127\n",
      "Iteration 20843, Loss: 0.05577544495463371\n",
      "Iteration 20844, Loss: 0.0557376965880394\n",
      "Iteration 20845, Loss: 0.05567797273397446\n",
      "Iteration 20846, Loss: 0.05572211742401123\n",
      "Iteration 20847, Loss: 0.05568671226501465\n",
      "Iteration 20848, Loss: 0.055686358362436295\n",
      "Iteration 20849, Loss: 0.05567777156829834\n",
      "Iteration 20850, Loss: 0.05568953603506088\n",
      "Iteration 20851, Loss: 0.05570733919739723\n",
      "Iteration 20852, Loss: 0.055641770362854004\n",
      "Iteration 20853, Loss: 0.05576944351196289\n",
      "Iteration 20854, Loss: 0.05577488988637924\n",
      "Iteration 20855, Loss: 0.05563632771372795\n",
      "Iteration 20856, Loss: 0.05576876923441887\n",
      "Iteration 20857, Loss: 0.05581299588084221\n",
      "Iteration 20858, Loss: 0.055738888680934906\n",
      "Iteration 20859, Loss: 0.05567006394267082\n",
      "Iteration 20860, Loss: 0.05571926012635231\n",
      "Iteration 20861, Loss: 0.05562639236450195\n",
      "Iteration 20862, Loss: 0.05575120449066162\n",
      "Iteration 20863, Loss: 0.05579177662730217\n",
      "Iteration 20864, Loss: 0.05573451519012451\n",
      "Iteration 20865, Loss: 0.055642884224653244\n",
      "Iteration 20866, Loss: 0.05567669868469238\n",
      "Iteration 20867, Loss: 0.055658262223005295\n",
      "Iteration 20868, Loss: 0.055655598640441895\n",
      "Iteration 20869, Loss: 0.055678606033325195\n",
      "Iteration 20870, Loss: 0.055635690689086914\n",
      "Iteration 20871, Loss: 0.05571993440389633\n",
      "Iteration 20872, Loss: 0.05572386831045151\n",
      "Iteration 20873, Loss: 0.05562996864318848\n",
      "Iteration 20874, Loss: 0.05577488988637924\n",
      "Iteration 20875, Loss: 0.05577310174703598\n",
      "Iteration 20876, Loss: 0.055618766695261\n",
      "Iteration 20877, Loss: 0.05571635812520981\n",
      "Iteration 20878, Loss: 0.05569060891866684\n",
      "Iteration 20879, Loss: 0.05567765608429909\n",
      "Iteration 20880, Loss: 0.0556819848716259\n",
      "Iteration 20881, Loss: 0.05566927045583725\n",
      "Iteration 20882, Loss: 0.05566021054983139\n",
      "Iteration 20883, Loss: 0.055690567940473557\n",
      "Iteration 20884, Loss: 0.05567304417490959\n",
      "Iteration 20885, Loss: 0.05569545552134514\n",
      "Iteration 20886, Loss: 0.05570312589406967\n",
      "Iteration 20887, Loss: 0.055638790130615234\n",
      "Iteration 20888, Loss: 0.055678170174360275\n",
      "Iteration 20889, Loss: 0.055643562227487564\n",
      "Iteration 20890, Loss: 0.05565067380666733\n",
      "Iteration 20891, Loss: 0.05564793199300766\n",
      "Iteration 20892, Loss: 0.05566330999135971\n",
      "Iteration 20893, Loss: 0.055642127990722656\n",
      "Iteration 20894, Loss: 0.05571707338094711\n",
      "Iteration 20895, Loss: 0.05567697808146477\n",
      "Iteration 20896, Loss: 0.055716078728437424\n",
      "Iteration 20897, Loss: 0.05576078221201897\n",
      "Iteration 20898, Loss: 0.055707138031721115\n",
      "Iteration 20899, Loss: 0.05567256733775139\n",
      "Iteration 20900, Loss: 0.055673085153102875\n",
      "Iteration 20901, Loss: 0.055692158639431\n",
      "Iteration 20902, Loss: 0.05571269989013672\n",
      "Iteration 20903, Loss: 0.05563795939087868\n",
      "Iteration 20904, Loss: 0.05579257383942604\n",
      "Iteration 20905, Loss: 0.05581581965088844\n",
      "Iteration 20906, Loss: 0.05566350743174553\n",
      "Iteration 20907, Loss: 0.05580016225576401\n",
      "Iteration 20908, Loss: 0.05591205880045891\n",
      "Iteration 20909, Loss: 0.055917978286743164\n",
      "Iteration 20910, Loss: 0.05582833290100098\n",
      "Iteration 20911, Loss: 0.05565468594431877\n",
      "Iteration 20912, Loss: 0.055890683084726334\n",
      "Iteration 20913, Loss: 0.056022725999355316\n",
      "Iteration 20914, Loss: 0.055966816842556\n",
      "Iteration 20915, Loss: 0.055743616074323654\n",
      "Iteration 20916, Loss: 0.055788200348615646\n",
      "Iteration 20917, Loss: 0.05594245716929436\n",
      "Iteration 20918, Loss: 0.05598648637533188\n",
      "Iteration 20919, Loss: 0.05593132972717285\n",
      "Iteration 20920, Loss: 0.05578736588358879\n",
      "Iteration 20921, Loss: 0.055674515664577484\n",
      "Iteration 20922, Loss: 0.05577500909566879\n",
      "Iteration 20923, Loss: 0.055694859474897385\n",
      "Iteration 20924, Loss: 0.05572867393493652\n",
      "Iteration 20925, Loss: 0.055796150118112564\n",
      "Iteration 20926, Loss: 0.05576316639780998\n",
      "Iteration 20927, Loss: 0.05563982576131821\n",
      "Iteration 20928, Loss: 0.055849116295576096\n",
      "Iteration 20929, Loss: 0.05592676252126694\n",
      "Iteration 20930, Loss: 0.05582460016012192\n",
      "Iteration 20931, Loss: 0.05564967915415764\n",
      "Iteration 20932, Loss: 0.05573761463165283\n",
      "Iteration 20933, Loss: 0.05572640895843506\n",
      "Iteration 20934, Loss: 0.05562051385641098\n",
      "Iteration 20935, Loss: 0.055847249925136566\n",
      "Iteration 20936, Loss: 0.05590657517313957\n",
      "Iteration 20937, Loss: 0.0557989701628685\n",
      "Iteration 20938, Loss: 0.055671852082014084\n",
      "Iteration 20939, Loss: 0.05576435849070549\n",
      "Iteration 20940, Loss: 0.055756133049726486\n",
      "Iteration 20941, Loss: 0.05564316362142563\n",
      "Iteration 20942, Loss: 0.05582376569509506\n",
      "Iteration 20943, Loss: 0.05589541047811508\n",
      "Iteration 20944, Loss: 0.0558091439306736\n",
      "Iteration 20945, Loss: 0.05565766617655754\n",
      "Iteration 20946, Loss: 0.05577186867594719\n",
      "Iteration 20947, Loss: 0.055792611092329025\n",
      "Iteration 20948, Loss: 0.05568532273173332\n",
      "Iteration 20949, Loss: 0.05576388165354729\n",
      "Iteration 20950, Loss: 0.05583604425191879\n",
      "Iteration 20951, Loss: 0.05576833337545395\n",
      "Iteration 20952, Loss: 0.055659811943769455\n",
      "Iteration 20953, Loss: 0.055747948586940765\n",
      "Iteration 20954, Loss: 0.05572144314646721\n",
      "Iteration 20955, Loss: 0.055669426918029785\n",
      "Iteration 20956, Loss: 0.055691443383693695\n",
      "Iteration 20957, Loss: 0.05564618483185768\n",
      "Iteration 20958, Loss: 0.055703919380903244\n",
      "Iteration 20959, Loss: 0.055650435388088226\n",
      "Iteration 20960, Loss: 0.05573646351695061\n",
      "Iteration 20961, Loss: 0.05578037351369858\n",
      "Iteration 20962, Loss: 0.05572688579559326\n",
      "Iteration 20963, Loss: 0.05564749240875244\n",
      "Iteration 20964, Loss: 0.05566084384918213\n",
      "Iteration 20965, Loss: 0.05568365380167961\n",
      "Iteration 20966, Loss: 0.055687546730041504\n",
      "Iteration 20967, Loss: 0.05563211441040039\n",
      "Iteration 20968, Loss: 0.055643320083618164\n",
      "Iteration 20969, Loss: 0.05563966557383537\n",
      "Iteration 20970, Loss: 0.05567014589905739\n",
      "Iteration 20971, Loss: 0.055644553154706955\n",
      "Iteration 20972, Loss: 0.05572080612182617\n",
      "Iteration 20973, Loss: 0.05569112300872803\n",
      "Iteration 20974, Loss: 0.05569803714752197\n",
      "Iteration 20975, Loss: 0.05573376268148422\n",
      "Iteration 20976, Loss: 0.05566903203725815\n",
      "Iteration 20977, Loss: 0.05573789402842522\n",
      "Iteration 20978, Loss: 0.05575152486562729\n",
      "Iteration 20979, Loss: 0.05562381073832512\n",
      "Iteration 20980, Loss: 0.055646978318691254\n",
      "Iteration 20981, Loss: 0.0556696280837059\n",
      "Iteration 20982, Loss: 0.0556236132979393\n",
      "Iteration 20983, Loss: 0.05572736635804176\n",
      "Iteration 20984, Loss: 0.055693745613098145\n",
      "Iteration 20985, Loss: 0.055692195892333984\n",
      "Iteration 20986, Loss: 0.055721085518598557\n",
      "Iteration 20987, Loss: 0.055639468133449554\n",
      "Iteration 20988, Loss: 0.05578792467713356\n",
      "Iteration 20989, Loss: 0.05582110211253166\n",
      "Iteration 20990, Loss: 0.0556974820792675\n",
      "Iteration 20991, Loss: 0.055755775421857834\n",
      "Iteration 20992, Loss: 0.05584514141082764\n",
      "Iteration 20993, Loss: 0.055818598717451096\n",
      "Iteration 20994, Loss: 0.05569219961762428\n",
      "Iteration 20995, Loss: 0.05577508732676506\n",
      "Iteration 20996, Loss: 0.05585603043437004\n",
      "Iteration 20997, Loss: 0.05577194690704346\n",
      "Iteration 20998, Loss: 0.05566903203725815\n",
      "Iteration 20999, Loss: 0.05573801323771477\n",
      "Iteration 21000, Loss: 0.05569549649953842\n",
      "Iteration 21001, Loss: 0.055682145059108734\n",
      "Iteration 21002, Loss: 0.05568563938140869\n",
      "Iteration 21003, Loss: 0.055674079805612564\n",
      "Iteration 21004, Loss: 0.05568079277873039\n",
      "Iteration 21005, Loss: 0.0556466206908226\n",
      "Iteration 21006, Loss: 0.055633626878261566\n",
      "Iteration 21007, Loss: 0.05569692701101303\n",
      "Iteration 21008, Loss: 0.05569358915090561\n",
      "Iteration 21009, Loss: 0.0556333065032959\n",
      "Iteration 21010, Loss: 0.05563843250274658\n",
      "Iteration 21011, Loss: 0.05566728487610817\n",
      "Iteration 21012, Loss: 0.055618010461330414\n",
      "Iteration 21013, Loss: 0.05573618412017822\n",
      "Iteration 21014, Loss: 0.05568957328796387\n",
      "Iteration 21015, Loss: 0.05570928379893303\n",
      "Iteration 21016, Loss: 0.055754780769348145\n",
      "Iteration 21017, Loss: 0.05569911003112793\n",
      "Iteration 21018, Loss: 0.05568766966462135\n",
      "Iteration 21019, Loss: 0.055695533752441406\n",
      "Iteration 21020, Loss: 0.055668555200099945\n",
      "Iteration 21021, Loss: 0.05568099021911621\n",
      "Iteration 21022, Loss: 0.055633388459682465\n",
      "Iteration 21023, Loss: 0.05566680431365967\n",
      "Iteration 21024, Loss: 0.05564030259847641\n",
      "Iteration 21025, Loss: 0.05566176027059555\n",
      "Iteration 21026, Loss: 0.0556514672935009\n",
      "Iteration 21027, Loss: 0.05563640594482422\n",
      "Iteration 21028, Loss: 0.055689696222543716\n",
      "Iteration 21029, Loss: 0.055623214691877365\n",
      "Iteration 21030, Loss: 0.05574687570333481\n",
      "Iteration 21031, Loss: 0.055773019790649414\n",
      "Iteration 21032, Loss: 0.05569378659129143\n",
      "Iteration 21033, Loss: 0.0557221993803978\n",
      "Iteration 21034, Loss: 0.05575549602508545\n",
      "Iteration 21035, Loss: 0.05562329664826393\n",
      "Iteration 21036, Loss: 0.055824439972639084\n",
      "Iteration 21037, Loss: 0.055922508239746094\n",
      "Iteration 21038, Loss: 0.05590590089559555\n",
      "Iteration 21039, Loss: 0.055790625512599945\n",
      "Iteration 21040, Loss: 0.055666886270046234\n",
      "Iteration 21041, Loss: 0.055798016488552094\n",
      "Iteration 21042, Loss: 0.055790942162275314\n",
      "Iteration 21043, Loss: 0.05565452575683594\n",
      "Iteration 21044, Loss: 0.05575895309448242\n",
      "Iteration 21045, Loss: 0.055814228951931\n",
      "Iteration 21046, Loss: 0.05576952546834946\n",
      "Iteration 21047, Loss: 0.05563386529684067\n",
      "Iteration 21048, Loss: 0.05586783215403557\n",
      "Iteration 21049, Loss: 0.05595918744802475\n",
      "Iteration 21050, Loss: 0.055876970291137695\n",
      "Iteration 21051, Loss: 0.05565822124481201\n",
      "Iteration 21052, Loss: 0.055838584899902344\n",
      "Iteration 21053, Loss: 0.05596884340047836\n",
      "Iteration 21054, Loss: 0.0559798888862133\n",
      "Iteration 21055, Loss: 0.05588448420166969\n",
      "Iteration 21056, Loss: 0.05570336431264877\n",
      "Iteration 21057, Loss: 0.055825792253017426\n",
      "Iteration 21058, Loss: 0.05596005916595459\n",
      "Iteration 21059, Loss: 0.05592215061187744\n",
      "Iteration 21060, Loss: 0.0557275228202343\n",
      "Iteration 21061, Loss: 0.05578434467315674\n",
      "Iteration 21062, Loss: 0.05592247098684311\n",
      "Iteration 21063, Loss: 0.055944085121154785\n",
      "Iteration 21064, Loss: 0.05586231127381325\n",
      "Iteration 21065, Loss: 0.055698078125715256\n",
      "Iteration 21066, Loss: 0.055817365646362305\n",
      "Iteration 21067, Loss: 0.05593506619334221\n",
      "Iteration 21068, Loss: 0.05587824434041977\n",
      "Iteration 21069, Loss: 0.05566251277923584\n",
      "Iteration 21070, Loss: 0.05584891885519028\n",
      "Iteration 21071, Loss: 0.055999755859375\n",
      "Iteration 21072, Loss: 0.05603647604584694\n",
      "Iteration 21073, Loss: 0.05597126856446266\n",
      "Iteration 21074, Loss: 0.05581752583384514\n",
      "Iteration 21075, Loss: 0.055673401802778244\n",
      "Iteration 21076, Loss: 0.05583314225077629\n",
      "Iteration 21077, Loss: 0.0558469332754612\n",
      "Iteration 21078, Loss: 0.055699191987514496\n",
      "Iteration 21079, Loss: 0.05577190965414047\n",
      "Iteration 21080, Loss: 0.055876217782497406\n",
      "Iteration 21081, Loss: 0.055877409875392914\n",
      "Iteration 21082, Loss: 0.05578462406992912\n",
      "Iteration 21083, Loss: 0.05562857910990715\n",
      "Iteration 21084, Loss: 0.05580131337046623\n",
      "Iteration 21085, Loss: 0.055814389139413834\n",
      "Iteration 21086, Loss: 0.05567769333720207\n",
      "Iteration 21087, Loss: 0.0557759627699852\n",
      "Iteration 21088, Loss: 0.05586497113108635\n",
      "Iteration 21089, Loss: 0.055832862854003906\n",
      "Iteration 21090, Loss: 0.05569557473063469\n",
      "Iteration 21091, Loss: 0.05578315630555153\n",
      "Iteration 21092, Loss: 0.055874865502119064\n",
      "Iteration 21093, Loss: 0.055798374116420746\n",
      "Iteration 21094, Loss: 0.05564657971262932\n",
      "Iteration 21095, Loss: 0.05573328584432602\n",
      "Iteration 21096, Loss: 0.05570705980062485\n",
      "Iteration 21097, Loss: 0.055663470178842545\n",
      "Iteration 21098, Loss: 0.05566466227173805\n",
      "Iteration 21099, Loss: 0.05568572133779526\n",
      "Iteration 21100, Loss: 0.055678725242614746\n",
      "Iteration 21101, Loss: 0.05567292496562004\n",
      "Iteration 21102, Loss: 0.05565750598907471\n",
      "Iteration 21103, Loss: 0.055708449333906174\n",
      "Iteration 21104, Loss: 0.055713217705488205\n",
      "Iteration 21105, Loss: 0.05563712120056152\n",
      "Iteration 21106, Loss: 0.05570673942565918\n",
      "Iteration 21107, Loss: 0.05564181134104729\n",
      "Iteration 21108, Loss: 0.055754028260707855\n",
      "Iteration 21109, Loss: 0.05580767244100571\n",
      "Iteration 21110, Loss: 0.05576146021485329\n",
      "Iteration 21111, Loss: 0.05563298985362053\n",
      "Iteration 21112, Loss: 0.055849555879831314\n",
      "Iteration 21113, Loss: 0.055914562195539474\n",
      "Iteration 21114, Loss: 0.05580171197652817\n",
      "Iteration 21115, Loss: 0.0556742362678051\n",
      "Iteration 21116, Loss: 0.05576439946889877\n",
      "Iteration 21117, Loss: 0.055755697190761566\n",
      "Iteration 21118, Loss: 0.05565452575683594\n",
      "Iteration 21119, Loss: 0.05580250546336174\n",
      "Iteration 21120, Loss: 0.055857103317976\n",
      "Iteration 21121, Loss: 0.05574003979563713\n",
      "Iteration 21122, Loss: 0.05571949481964111\n",
      "Iteration 21123, Loss: 0.055808745324611664\n",
      "Iteration 21124, Loss: 0.055793605744838715\n",
      "Iteration 21125, Loss: 0.05568274110555649\n",
      "Iteration 21126, Loss: 0.055774688720703125\n",
      "Iteration 21127, Loss: 0.05583957955241203\n",
      "Iteration 21128, Loss: 0.055732570588588715\n",
      "Iteration 21129, Loss: 0.055717747658491135\n",
      "Iteration 21130, Loss: 0.05580095574259758\n",
      "Iteration 21131, Loss: 0.05577870458364487\n",
      "Iteration 21132, Loss: 0.055661559104919434\n",
      "Iteration 21133, Loss: 0.05580810829997063\n",
      "Iteration 21134, Loss: 0.05587856099009514\n",
      "Iteration 21135, Loss: 0.055775247514247894\n",
      "Iteration 21136, Loss: 0.055684011429548264\n",
      "Iteration 21137, Loss: 0.05576678365468979\n",
      "Iteration 21138, Loss: 0.055745285004377365\n",
      "Iteration 21139, Loss: 0.055628418922424316\n",
      "Iteration 21140, Loss: 0.055851541459560394\n",
      "Iteration 21141, Loss: 0.05592068284749985\n",
      "Iteration 21142, Loss: 0.05581613630056381\n",
      "Iteration 21143, Loss: 0.055657707154750824\n",
      "Iteration 21144, Loss: 0.055750809609889984\n",
      "Iteration 21145, Loss: 0.05574464797973633\n",
      "Iteration 21146, Loss: 0.05563700571656227\n",
      "Iteration 21147, Loss: 0.05582781881093979\n",
      "Iteration 21148, Loss: 0.05589231103658676\n",
      "Iteration 21149, Loss: 0.0557941198348999\n",
      "Iteration 21150, Loss: 0.05566879361867905\n",
      "Iteration 21151, Loss: 0.05575970932841301\n",
      "Iteration 21152, Loss: 0.055749498307704926\n",
      "Iteration 21153, Loss: 0.055627863854169846\n",
      "Iteration 21154, Loss: 0.05584283918142319\n",
      "Iteration 21155, Loss: 0.055924296379089355\n",
      "Iteration 21156, Loss: 0.05585690587759018\n",
      "Iteration 21157, Loss: 0.05568234249949455\n",
      "Iteration 21158, Loss: 0.05581267923116684\n",
      "Iteration 21159, Loss: 0.05592211335897446\n",
      "Iteration 21160, Loss: 0.05588940903544426\n",
      "Iteration 21161, Loss: 0.055730026215314865\n",
      "Iteration 21162, Loss: 0.0557604655623436\n",
      "Iteration 21163, Loss: 0.05587534233927727\n",
      "Iteration 21164, Loss: 0.055844906717538834\n",
      "Iteration 21165, Loss: 0.05568874254822731\n",
      "Iteration 21166, Loss: 0.05579742044210434\n",
      "Iteration 21167, Loss: 0.05590566247701645\n",
      "Iteration 21168, Loss: 0.05586818978190422\n",
      "Iteration 21169, Loss: 0.055710237473249435\n",
      "Iteration 21170, Loss: 0.05577826872467995\n",
      "Iteration 21171, Loss: 0.05588841810822487\n",
      "Iteration 21172, Loss: 0.055853962898254395\n",
      "Iteration 21173, Loss: 0.05569052696228027\n",
      "Iteration 21174, Loss: 0.05580242723226547\n",
      "Iteration 21175, Loss: 0.055917661637067795\n",
      "Iteration 21176, Loss: 0.055888812988996506\n",
      "Iteration 21177, Loss: 0.05573968216776848\n",
      "Iteration 21178, Loss: 0.05574445053935051\n",
      "Iteration 21179, Loss: 0.05585130304098129\n",
      "Iteration 21180, Loss: 0.05581708997488022\n",
      "Iteration 21181, Loss: 0.055650752037763596\n",
      "Iteration 21182, Loss: 0.05584677308797836\n",
      "Iteration 21183, Loss: 0.05596518516540527\n",
      "Iteration 21184, Loss: 0.05594217777252197\n",
      "Iteration 21185, Loss: 0.05580254644155502\n",
      "Iteration 21186, Loss: 0.05569779872894287\n",
      "Iteration 21187, Loss: 0.05581967160105705\n",
      "Iteration 21188, Loss: 0.05583067983388901\n",
      "Iteration 21189, Loss: 0.0557025671005249\n",
      "Iteration 21190, Loss: 0.055768053978681564\n",
      "Iteration 21191, Loss: 0.0558600053191185\n",
      "Iteration 21192, Loss: 0.05583592504262924\n",
      "Iteration 21193, Loss: 0.055723272264003754\n",
      "Iteration 21194, Loss: 0.055736903101205826\n",
      "Iteration 21195, Loss: 0.055809419602155685\n",
      "Iteration 21196, Loss: 0.055733323097229004\n",
      "Iteration 21197, Loss: 0.05570463463664055\n",
      "Iteration 21198, Loss: 0.05576491355895996\n",
      "Iteration 21199, Loss: 0.0557272844016552\n",
      "Iteration 21200, Loss: 0.05566263571381569\n",
      "Iteration 21201, Loss: 0.0557299479842186\n",
      "Iteration 21202, Loss: 0.05568929761648178\n",
      "Iteration 21203, Loss: 0.05570630356669426\n",
      "Iteration 21204, Loss: 0.055744290351867676\n",
      "Iteration 21205, Loss: 0.055691204965114594\n",
      "Iteration 21206, Loss: 0.055699191987514496\n",
      "Iteration 21207, Loss: 0.05570197105407715\n",
      "Iteration 21208, Loss: 0.05567026510834694\n",
      "Iteration 21209, Loss: 0.055690210312604904\n",
      "Iteration 21210, Loss: 0.055630724877119064\n",
      "Iteration 21211, Loss: 0.05575593560934067\n",
      "Iteration 21212, Loss: 0.05574985593557358\n",
      "Iteration 21213, Loss: 0.05564681813120842\n",
      "Iteration 21214, Loss: 0.055730465799570084\n",
      "Iteration 21215, Loss: 0.05571870133280754\n",
      "Iteration 21216, Loss: 0.055647533386945724\n",
      "Iteration 21217, Loss: 0.05564960092306137\n",
      "Iteration 21218, Loss: 0.05568957328796387\n",
      "Iteration 21219, Loss: 0.055660050362348557\n",
      "Iteration 21220, Loss: 0.05571707338094711\n",
      "Iteration 21221, Loss: 0.05573193356394768\n",
      "Iteration 21222, Loss: 0.05563918873667717\n",
      "Iteration 21223, Loss: 0.05577508732676506\n",
      "Iteration 21224, Loss: 0.05579455941915512\n",
      "Iteration 21225, Loss: 0.05565913766622543\n",
      "Iteration 21226, Loss: 0.05579722300171852\n",
      "Iteration 21227, Loss: 0.055891912430524826\n",
      "Iteration 21228, Loss: 0.055867355316877365\n",
      "Iteration 21229, Loss: 0.05574226379394531\n",
      "Iteration 21230, Loss: 0.05572032928466797\n",
      "Iteration 21231, Loss: 0.05580822750926018\n",
      "Iteration 21232, Loss: 0.055742066353559494\n",
      "Iteration 21233, Loss: 0.05568563938140869\n",
      "Iteration 21234, Loss: 0.05573948472738266\n",
      "Iteration 21235, Loss: 0.05569350719451904\n",
      "Iteration 21236, Loss: 0.055688224732875824\n",
      "Iteration 21237, Loss: 0.05569601058959961\n",
      "Iteration 21238, Loss: 0.05566931143403053\n",
      "Iteration 21239, Loss: 0.055677853524684906\n",
      "Iteration 21240, Loss: 0.0556519441306591\n",
      "Iteration 21241, Loss: 0.055678367614746094\n",
      "Iteration 21242, Loss: 0.05564868450164795\n",
      "Iteration 21243, Loss: 0.055671535432338715\n",
      "Iteration 21244, Loss: 0.05563608929514885\n",
      "Iteration 21245, Loss: 0.05573737993836403\n",
      "Iteration 21246, Loss: 0.0557476282119751\n",
      "Iteration 21247, Loss: 0.05565774440765381\n",
      "Iteration 21248, Loss: 0.05576082319021225\n",
      "Iteration 21249, Loss: 0.055788516998291016\n",
      "Iteration 21250, Loss: 0.055661361664533615\n",
      "Iteration 21251, Loss: 0.05579300969839096\n",
      "Iteration 21252, Loss: 0.05588523671030998\n",
      "Iteration 21253, Loss: 0.05585877224802971\n",
      "Iteration 21254, Loss: 0.05573475360870361\n",
      "Iteration 21255, Loss: 0.05572879686951637\n",
      "Iteration 21256, Loss: 0.055814266204833984\n",
      "Iteration 21257, Loss: 0.05574492737650871\n",
      "Iteration 21258, Loss: 0.055685997009277344\n",
      "Iteration 21259, Loss: 0.05574194714426994\n",
      "Iteration 21260, Loss: 0.05569815635681152\n",
      "Iteration 21261, Loss: 0.05568178743124008\n",
      "Iteration 21262, Loss: 0.05569323152303696\n",
      "Iteration 21263, Loss: 0.05566927045583725\n",
      "Iteration 21264, Loss: 0.05567582696676254\n",
      "Iteration 21265, Loss: 0.05565520375967026\n",
      "Iteration 21266, Loss: 0.055675629526376724\n",
      "Iteration 21267, Loss: 0.05565182492136955\n",
      "Iteration 21268, Loss: 0.0556664876639843\n",
      "Iteration 21269, Loss: 0.0556388720870018\n",
      "Iteration 21270, Loss: 0.05571071431040764\n",
      "Iteration 21271, Loss: 0.05570995807647705\n",
      "Iteration 21272, Loss: 0.05564387887716293\n",
      "Iteration 21273, Loss: 0.055724821984767914\n",
      "Iteration 21274, Loss: 0.05568051338195801\n",
      "Iteration 21275, Loss: 0.05571568012237549\n",
      "Iteration 21276, Loss: 0.05576050281524658\n",
      "Iteration 21277, Loss: 0.055708806961774826\n",
      "Iteration 21278, Loss: 0.055669866502285004\n",
      "Iteration 21279, Loss: 0.05567324161529541\n",
      "Iteration 21280, Loss: 0.05568838119506836\n",
      "Iteration 21281, Loss: 0.0557049922645092\n",
      "Iteration 21282, Loss: 0.05562925711274147\n",
      "Iteration 21283, Loss: 0.055801987648010254\n",
      "Iteration 21284, Loss: 0.05582340806722641\n",
      "Iteration 21285, Loss: 0.055672768503427505\n",
      "Iteration 21286, Loss: 0.055792491883039474\n",
      "Iteration 21287, Loss: 0.055901091545820236\n",
      "Iteration 21288, Loss: 0.055899184197187424\n",
      "Iteration 21289, Loss: 0.0557987317442894\n",
      "Iteration 21290, Loss: 0.05563664436340332\n",
      "Iteration 21291, Loss: 0.05584554001688957\n",
      "Iteration 21292, Loss: 0.05589811131358147\n",
      "Iteration 21293, Loss: 0.05577500909566879\n",
      "Iteration 21294, Loss: 0.05570133775472641\n",
      "Iteration 21295, Loss: 0.055795591324567795\n",
      "Iteration 21296, Loss: 0.05578998848795891\n",
      "Iteration 21297, Loss: 0.05569247528910637\n",
      "Iteration 21298, Loss: 0.05574862286448479\n",
      "Iteration 21299, Loss: 0.05579817295074463\n",
      "Iteration 21300, Loss: 0.055676501244306564\n",
      "Iteration 21301, Loss: 0.055767934769392014\n",
      "Iteration 21302, Loss: 0.0558573417365551\n",
      "Iteration 21303, Loss: 0.05584180727601051\n",
      "Iteration 21304, Loss: 0.05573209375143051\n",
      "Iteration 21305, Loss: 0.05570805072784424\n",
      "Iteration 21306, Loss: 0.055772267282009125\n",
      "Iteration 21307, Loss: 0.055661920458078384\n",
      "Iteration 21308, Loss: 0.05577465146780014\n",
      "Iteration 21309, Loss: 0.05586040019989014\n",
      "Iteration 21310, Loss: 0.05584220215678215\n",
      "Iteration 21311, Loss: 0.05573161691427231\n",
      "Iteration 21312, Loss: 0.05571059510111809\n",
      "Iteration 21313, Loss: 0.05577707290649414\n",
      "Iteration 21314, Loss: 0.055669426918029785\n",
      "Iteration 21315, Loss: 0.055766504257917404\n",
      "Iteration 21316, Loss: 0.055850546807050705\n",
      "Iteration 21317, Loss: 0.05583211034536362\n",
      "Iteration 21318, Loss: 0.055721960961818695\n",
      "Iteration 21319, Loss: 0.05572231858968735\n",
      "Iteration 21320, Loss: 0.05578605458140373\n",
      "Iteration 21321, Loss: 0.05567288398742676\n",
      "Iteration 21322, Loss: 0.05576737970113754\n",
      "Iteration 21323, Loss: 0.05585543438792229\n",
      "Iteration 21324, Loss: 0.05584069341421127\n",
      "Iteration 21325, Loss: 0.055733680725097656\n",
      "Iteration 21326, Loss: 0.055702727288007736\n",
      "Iteration 21327, Loss: 0.05576261132955551\n",
      "Iteration 21328, Loss: 0.05564471334218979\n",
      "Iteration 21329, Loss: 0.05579086393117905\n",
      "Iteration 21330, Loss: 0.05588158220052719\n",
      "Iteration 21331, Loss: 0.05586870759725571\n",
      "Iteration 21332, Loss: 0.05576249212026596\n",
      "Iteration 21333, Loss: 0.055661559104919434\n",
      "Iteration 21334, Loss: 0.05571989342570305\n",
      "Iteration 21335, Loss: 0.05562027543783188\n",
      "Iteration 21336, Loss: 0.05565508455038071\n",
      "Iteration 21337, Loss: 0.055652420967817307\n",
      "Iteration 21338, Loss: 0.055621229112148285\n",
      "Iteration 21339, Loss: 0.05569382756948471\n",
      "Iteration 21340, Loss: 0.055624525994062424\n",
      "Iteration 21341, Loss: 0.05574611946940422\n",
      "Iteration 21342, Loss: 0.05575672909617424\n",
      "Iteration 21343, Loss: 0.055656515061855316\n",
      "Iteration 21344, Loss: 0.05577671527862549\n",
      "Iteration 21345, Loss: 0.055822890251874924\n",
      "Iteration 21346, Loss: 0.05571611970663071\n",
      "Iteration 21347, Loss: 0.05572951212525368\n",
      "Iteration 21348, Loss: 0.05580782890319824\n",
      "Iteration 21349, Loss: 0.05576614663004875\n",
      "Iteration 21350, Loss: 0.05563966557383537\n",
      "Iteration 21351, Loss: 0.05580496788024902\n",
      "Iteration 21352, Loss: 0.05583953857421875\n",
      "Iteration 21353, Loss: 0.05570626258850098\n",
      "Iteration 21354, Loss: 0.055757008492946625\n",
      "Iteration 21355, Loss: 0.05585626885294914\n",
      "Iteration 21356, Loss: 0.05584685131907463\n",
      "Iteration 21357, Loss: 0.05574190616607666\n",
      "Iteration 21358, Loss: 0.05569338798522949\n",
      "Iteration 21359, Loss: 0.055760107934474945\n",
      "Iteration 21360, Loss: 0.05566354840993881\n",
      "Iteration 21361, Loss: 0.055762410163879395\n",
      "Iteration 21362, Loss: 0.05583703890442848\n",
      "Iteration 21363, Loss: 0.05581037327647209\n",
      "Iteration 21364, Loss: 0.055694580078125\n",
      "Iteration 21365, Loss: 0.05576698109507561\n",
      "Iteration 21366, Loss: 0.055835843086242676\n",
      "Iteration 21367, Loss: 0.05572656914591789\n",
      "Iteration 21368, Loss: 0.05572589486837387\n",
      "Iteration 21369, Loss: 0.05581172555685043\n",
      "Iteration 21370, Loss: 0.055795393884181976\n",
      "Iteration 21371, Loss: 0.05568739026784897\n",
      "Iteration 21372, Loss: 0.05576590821146965\n",
      "Iteration 21373, Loss: 0.055825989693403244\n",
      "Iteration 21374, Loss: 0.05570749565958977\n",
      "Iteration 21375, Loss: 0.0557454451918602\n",
      "Iteration 21376, Loss: 0.05583691969513893\n",
      "Iteration 21377, Loss: 0.05582531541585922\n",
      "Iteration 21378, Loss: 0.05572112649679184\n",
      "Iteration 21379, Loss: 0.055715642869472504\n",
      "Iteration 21380, Loss: 0.05577210709452629\n",
      "Iteration 21381, Loss: 0.05565055459737778\n",
      "Iteration 21382, Loss: 0.055789630860090256\n",
      "Iteration 21383, Loss: 0.0558830127120018\n",
      "Iteration 21384, Loss: 0.0558728389441967\n",
      "Iteration 21385, Loss: 0.05576956644654274\n",
      "Iteration 21386, Loss: 0.055649083107709885\n",
      "Iteration 21387, Loss: 0.05570518970489502\n",
      "Iteration 21388, Loss: 0.05563175678253174\n",
      "Iteration 21389, Loss: 0.055624525994062424\n",
      "Iteration 21390, Loss: 0.05572303384542465\n",
      "Iteration 21391, Loss: 0.05567646026611328\n",
      "Iteration 21392, Loss: 0.055717986077070236\n",
      "Iteration 21393, Loss: 0.055758994072675705\n",
      "Iteration 21394, Loss: 0.05569267272949219\n",
      "Iteration 21395, Loss: 0.05570976063609123\n",
      "Iteration 21396, Loss: 0.05573105812072754\n",
      "Iteration 21397, Loss: 0.055629532784223557\n",
      "Iteration 21398, Loss: 0.05563553422689438\n",
      "Iteration 21399, Loss: 0.055703919380903244\n",
      "Iteration 21400, Loss: 0.05566593259572983\n",
      "Iteration 21401, Loss: 0.05571592226624489\n",
      "Iteration 21402, Loss: 0.05573912709951401\n",
      "Iteration 21403, Loss: 0.05564701557159424\n",
      "Iteration 21404, Loss: 0.055786412209272385\n",
      "Iteration 21405, Loss: 0.05582881346344948\n",
      "Iteration 21406, Loss: 0.05571480840444565\n",
      "Iteration 21407, Loss: 0.05573546886444092\n",
      "Iteration 21408, Loss: 0.055818479508161545\n",
      "Iteration 21409, Loss: 0.05577949807047844\n",
      "Iteration 21410, Loss: 0.05564066022634506\n",
      "Iteration 21411, Loss: 0.05583930015563965\n",
      "Iteration 21412, Loss: 0.05591360852122307\n",
      "Iteration 21413, Loss: 0.0558164156973362\n",
      "Iteration 21414, Loss: 0.05565091222524643\n",
      "Iteration 21415, Loss: 0.05574973672628403\n",
      "Iteration 21416, Loss: 0.0557405948638916\n",
      "Iteration 21417, Loss: 0.05561832711100578\n",
      "Iteration 21418, Loss: 0.0558551587164402\n",
      "Iteration 21419, Loss: 0.055925410240888596\n",
      "Iteration 21420, Loss: 0.05583127588033676\n",
      "Iteration 21421, Loss: 0.055656157433986664\n",
      "Iteration 21422, Loss: 0.05579253286123276\n",
      "Iteration 21423, Loss: 0.055841486901044846\n",
      "Iteration 21424, Loss: 0.05575975030660629\n",
      "Iteration 21425, Loss: 0.05566199868917465\n",
      "Iteration 21426, Loss: 0.05571659654378891\n",
      "Iteration 21427, Loss: 0.0556308850646019\n",
      "Iteration 21428, Loss: 0.055787406861782074\n",
      "Iteration 21429, Loss: 0.05583024024963379\n",
      "Iteration 21430, Loss: 0.0557379350066185\n",
      "Iteration 21431, Loss: 0.05569680780172348\n",
      "Iteration 21432, Loss: 0.05576483532786369\n",
      "Iteration 21433, Loss: 0.05570145696401596\n",
      "Iteration 21434, Loss: 0.05570964142680168\n",
      "Iteration 21435, Loss: 0.05574933812022209\n",
      "Iteration 21436, Loss: 0.0556664876639843\n",
      "Iteration 21437, Loss: 0.05574997514486313\n",
      "Iteration 21438, Loss: 0.055788516998291016\n",
      "Iteration 21439, Loss: 0.05568253993988037\n",
      "Iteration 21440, Loss: 0.05575895681977272\n",
      "Iteration 21441, Loss: 0.05583147332072258\n",
      "Iteration 21442, Loss: 0.055772822350263596\n",
      "Iteration 21443, Loss: 0.05564530938863754\n",
      "Iteration 21444, Loss: 0.05577131360769272\n",
      "Iteration 21445, Loss: 0.0557682141661644\n",
      "Iteration 21446, Loss: 0.05561892315745354\n",
      "Iteration 21447, Loss: 0.05568540096282959\n",
      "Iteration 21448, Loss: 0.05565861985087395\n",
      "Iteration 21449, Loss: 0.05570674315094948\n",
      "Iteration 21450, Loss: 0.055679600685834885\n",
      "Iteration 21451, Loss: 0.055704157799482346\n",
      "Iteration 21452, Loss: 0.05573960393667221\n",
      "Iteration 21453, Loss: 0.05567701905965805\n",
      "Iteration 21454, Loss: 0.055724501609802246\n",
      "Iteration 21455, Loss: 0.055736105889081955\n",
      "Iteration 21456, Loss: 0.05563700199127197\n",
      "Iteration 21457, Loss: 0.05565258115530014\n",
      "Iteration 21458, Loss: 0.055664144456386566\n",
      "Iteration 21459, Loss: 0.055626314133405685\n",
      "Iteration 21460, Loss: 0.05566505715250969\n",
      "Iteration 21461, Loss: 0.055646978318691254\n",
      "Iteration 21462, Loss: 0.0556204728782177\n",
      "Iteration 21463, Loss: 0.05574719235301018\n",
      "Iteration 21464, Loss: 0.055708132684230804\n",
      "Iteration 21465, Loss: 0.0556923970580101\n",
      "Iteration 21466, Loss: 0.055736660957336426\n",
      "Iteration 21467, Loss: 0.05568329617381096\n",
      "Iteration 21468, Loss: 0.05570507422089577\n",
      "Iteration 21469, Loss: 0.05570542812347412\n",
      "Iteration 21470, Loss: 0.05566851422190666\n",
      "Iteration 21471, Loss: 0.05568937584757805\n",
      "Iteration 21472, Loss: 0.0556163415312767\n",
      "Iteration 21473, Loss: 0.055808067321777344\n",
      "Iteration 21474, Loss: 0.05582316964864731\n",
      "Iteration 21475, Loss: 0.05567288398742676\n",
      "Iteration 21476, Loss: 0.05579070374369621\n",
      "Iteration 21477, Loss: 0.05589699745178223\n",
      "Iteration 21478, Loss: 0.055894337594509125\n",
      "Iteration 21479, Loss: 0.05579420179128647\n",
      "Iteration 21480, Loss: 0.055637799203395844\n",
      "Iteration 21481, Loss: 0.055822812020778656\n",
      "Iteration 21482, Loss: 0.05585182085633278\n",
      "Iteration 21483, Loss: 0.055707018822431564\n",
      "Iteration 21484, Loss: 0.055763404816389084\n",
      "Iteration 21485, Loss: 0.05586985871195793\n",
      "Iteration 21486, Loss: 0.055871449410915375\n",
      "Iteration 21487, Loss: 0.055777907371520996\n",
      "Iteration 21488, Loss: 0.055625997483730316\n",
      "Iteration 21489, Loss: 0.05567217245697975\n",
      "Iteration 21490, Loss: 0.05566160008311272\n",
      "Iteration 21491, Loss: 0.055654287338256836\n",
      "Iteration 21492, Loss: 0.05568854138255119\n",
      "Iteration 21493, Loss: 0.05563918873667717\n",
      "Iteration 21494, Loss: 0.05574973672628403\n",
      "Iteration 21495, Loss: 0.055798374116420746\n",
      "Iteration 21496, Loss: 0.055746279656887054\n",
      "Iteration 21497, Loss: 0.055628299713134766\n",
      "Iteration 21498, Loss: 0.05575021356344223\n",
      "Iteration 21499, Loss: 0.05571838468313217\n",
      "Iteration 21500, Loss: 0.0556773766875267\n",
      "Iteration 21501, Loss: 0.055715326219797134\n",
      "Iteration 21502, Loss: 0.055655837059020996\n",
      "Iteration 21503, Loss: 0.055751681327819824\n",
      "Iteration 21504, Loss: 0.055767856538295746\n",
      "Iteration 21505, Loss: 0.05564439669251442\n",
      "Iteration 21506, Loss: 0.05577727407217026\n",
      "Iteration 21507, Loss: 0.055833302438259125\n",
      "Iteration 21508, Loss: 0.05576590821146965\n",
      "Iteration 21509, Loss: 0.055648647248744965\n",
      "Iteration 21510, Loss: 0.05575069040060043\n",
      "Iteration 21511, Loss: 0.05571889877319336\n",
      "Iteration 21512, Loss: 0.0556793250143528\n",
      "Iteration 21513, Loss: 0.05571385473012924\n",
      "Iteration 21514, Loss: 0.05565778538584709\n",
      "Iteration 21515, Loss: 0.055736660957336426\n",
      "Iteration 21516, Loss: 0.05573372170329094\n",
      "Iteration 21517, Loss: 0.05564900487661362\n",
      "Iteration 21518, Loss: 0.055671337991952896\n",
      "Iteration 21519, Loss: 0.05563298985362053\n",
      "Iteration 21520, Loss: 0.05568309873342514\n",
      "Iteration 21521, Loss: 0.05564292520284653\n",
      "Iteration 21522, Loss: 0.0557204894721508\n",
      "Iteration 21523, Loss: 0.05570578575134277\n",
      "Iteration 21524, Loss: 0.05566565319895744\n",
      "Iteration 21525, Loss: 0.055678606033325195\n",
      "Iteration 21526, Loss: 0.05565023794770241\n",
      "Iteration 21527, Loss: 0.05561475083231926\n",
      "Iteration 21528, Loss: 0.05575815960764885\n",
      "Iteration 21529, Loss: 0.05578847974538803\n",
      "Iteration 21530, Loss: 0.05571560189127922\n",
      "Iteration 21531, Loss: 0.055691204965114594\n",
      "Iteration 21532, Loss: 0.05572756379842758\n",
      "Iteration 21533, Loss: 0.05563267320394516\n",
      "Iteration 21534, Loss: 0.055717986077070236\n",
      "Iteration 21535, Loss: 0.055729273706674576\n",
      "Iteration 21536, Loss: 0.055644433945417404\n",
      "Iteration 21537, Loss: 0.055796068161726\n",
      "Iteration 21538, Loss: 0.05583127588033676\n",
      "Iteration 21539, Loss: 0.055692195892333984\n",
      "Iteration 21540, Loss: 0.05577047914266586\n",
      "Iteration 21541, Loss: 0.05587335675954819\n",
      "Iteration 21542, Loss: 0.05586759373545647\n",
      "Iteration 21543, Loss: 0.055764518678188324\n",
      "Iteration 21544, Loss: 0.055660806596279144\n",
      "Iteration 21545, Loss: 0.05573495477437973\n",
      "Iteration 21546, Loss: 0.055652420967817307\n",
      "Iteration 21547, Loss: 0.05574905872344971\n",
      "Iteration 21548, Loss: 0.05580496788024902\n",
      "Iteration 21549, Loss: 0.0557609423995018\n",
      "Iteration 21550, Loss: 0.05562691017985344\n",
      "Iteration 21551, Loss: 0.055878762155771255\n",
      "Iteration 21552, Loss: 0.05596812814474106\n",
      "Iteration 21553, Loss: 0.055877767503261566\n",
      "Iteration 21554, Loss: 0.05563811585307121\n",
      "Iteration 21555, Loss: 0.055855993181467056\n",
      "Iteration 21556, Loss: 0.05599343776702881\n",
      "Iteration 21557, Loss: 0.05601680278778076\n",
      "Iteration 21558, Loss: 0.05593840405344963\n",
      "Iteration 21559, Loss: 0.055772267282009125\n",
      "Iteration 21560, Loss: 0.0557282380759716\n",
      "Iteration 21561, Loss: 0.055857103317976\n",
      "Iteration 21562, Loss: 0.05581609532237053\n",
      "Iteration 21563, Loss: 0.05562969297170639\n",
      "Iteration 21564, Loss: 0.055789750069379807\n",
      "Iteration 21565, Loss: 0.055864494293928146\n",
      "Iteration 21566, Loss: 0.055836718529462814\n",
      "Iteration 21567, Loss: 0.05571715161204338\n",
      "Iteration 21568, Loss: 0.05573980137705803\n",
      "Iteration 21569, Loss: 0.055812638252973557\n",
      "Iteration 21570, Loss: 0.05570487305521965\n",
      "Iteration 21571, Loss: 0.05574015900492668\n",
      "Iteration 21572, Loss: 0.05582543462514877\n",
      "Iteration 21573, Loss: 0.055807195603847504\n",
      "Iteration 21574, Loss: 0.05569660663604736\n",
      "Iteration 21575, Loss: 0.05575637146830559\n",
      "Iteration 21576, Loss: 0.05581891909241676\n",
      "Iteration 21577, Loss: 0.05570201203227043\n",
      "Iteration 21578, Loss: 0.05574854463338852\n",
      "Iteration 21579, Loss: 0.05583902448415756\n",
      "Iteration 21580, Loss: 0.055825673043727875\n",
      "Iteration 21581, Loss: 0.05572005361318588\n",
      "Iteration 21582, Loss: 0.055719178169965744\n",
      "Iteration 21583, Loss: 0.05577683448791504\n",
      "Iteration 21584, Loss: 0.05565734952688217\n",
      "Iteration 21585, Loss: 0.055782001465559006\n",
      "Iteration 21586, Loss: 0.0558728389441967\n",
      "Iteration 21587, Loss: 0.05586044117808342\n",
      "Iteration 21588, Loss: 0.05575517937541008\n",
      "Iteration 21589, Loss: 0.055670738220214844\n",
      "Iteration 21590, Loss: 0.05572867393493652\n",
      "Iteration 21591, Loss: 0.0556209497153759\n",
      "Iteration 21592, Loss: 0.05574202910065651\n",
      "Iteration 21593, Loss: 0.05575625225901604\n",
      "Iteration 21594, Loss: 0.05566553398966789\n",
      "Iteration 21595, Loss: 0.055764637887477875\n",
      "Iteration 21596, Loss: 0.055806200951337814\n",
      "Iteration 21597, Loss: 0.05568782612681389\n",
      "Iteration 21598, Loss: 0.05576149746775627\n",
      "Iteration 21599, Loss: 0.05584871768951416\n",
      "Iteration 21600, Loss: 0.055821340531110764\n",
      "Iteration 21601, Loss: 0.05569815635681152\n",
      "Iteration 21602, Loss: 0.05576594918966293\n",
      "Iteration 21603, Loss: 0.05584355443716049\n",
      "Iteration 21604, Loss: 0.05575549975037575\n",
      "Iteration 21605, Loss: 0.05568639561533928\n",
      "Iteration 21606, Loss: 0.05575637146830559\n",
      "Iteration 21607, Loss: 0.055717550218105316\n",
      "Iteration 21608, Loss: 0.055652182549238205\n",
      "Iteration 21609, Loss: 0.05568190664052963\n",
      "Iteration 21610, Loss: 0.05565889924764633\n",
      "Iteration 21611, Loss: 0.05565866082906723\n",
      "Iteration 21612, Loss: 0.05566481873393059\n",
      "Iteration 21613, Loss: 0.055643681436777115\n",
      "Iteration 21614, Loss: 0.055669549852609634\n",
      "Iteration 21615, Loss: 0.05563938990235329\n",
      "Iteration 21616, Loss: 0.05573860928416252\n",
      "Iteration 21617, Loss: 0.05572911351919174\n",
      "Iteration 21618, Loss: 0.05565182492136955\n",
      "Iteration 21619, Loss: 0.05569029226899147\n",
      "Iteration 21620, Loss: 0.05563144013285637\n",
      "Iteration 21621, Loss: 0.05575033277273178\n",
      "Iteration 21622, Loss: 0.05577743053436279\n",
      "Iteration 21623, Loss: 0.05570010468363762\n",
      "Iteration 21624, Loss: 0.05571440979838371\n",
      "Iteration 21625, Loss: 0.05575152486562729\n",
      "Iteration 21626, Loss: 0.05563827604055405\n",
      "Iteration 21627, Loss: 0.05579324811697006\n",
      "Iteration 21628, Loss: 0.05587426945567131\n",
      "Iteration 21629, Loss: 0.05584863945841789\n",
      "Iteration 21630, Loss: 0.05573185533285141\n",
      "Iteration 21631, Loss: 0.05572235584259033\n",
      "Iteration 21632, Loss: 0.05579829588532448\n",
      "Iteration 21633, Loss: 0.055709801614284515\n",
      "Iteration 21634, Loss: 0.05572684854269028\n",
      "Iteration 21635, Loss: 0.05579821392893791\n",
      "Iteration 21636, Loss: 0.055769048631191254\n",
      "Iteration 21637, Loss: 0.055658817291259766\n",
      "Iteration 21638, Loss: 0.055803898721933365\n",
      "Iteration 21639, Loss: 0.0558575801551342\n",
      "Iteration 21640, Loss: 0.055734239518642426\n",
      "Iteration 21641, Loss: 0.05572931095957756\n",
      "Iteration 21642, Loss: 0.05582388490438461\n",
      "Iteration 21643, Loss: 0.05581542104482651\n",
      "Iteration 21644, Loss: 0.05571417137980461\n",
      "Iteration 21645, Loss: 0.05572148412466049\n",
      "Iteration 21646, Loss: 0.055774807929992676\n",
      "Iteration 21647, Loss: 0.055650513619184494\n",
      "Iteration 21648, Loss: 0.05579022690653801\n",
      "Iteration 21649, Loss: 0.05588444322347641\n",
      "Iteration 21650, Loss: 0.05587458983063698\n",
      "Iteration 21651, Loss: 0.055770955979824066\n",
      "Iteration 21652, Loss: 0.055647097527980804\n",
      "Iteration 21653, Loss: 0.05570419877767563\n",
      "Iteration 21654, Loss: 0.055632513016462326\n",
      "Iteration 21655, Loss: 0.05564232915639877\n",
      "Iteration 21656, Loss: 0.05567602440714836\n",
      "Iteration 21657, Loss: 0.05562933534383774\n",
      "Iteration 21658, Loss: 0.05572923272848129\n",
      "Iteration 21659, Loss: 0.05571262165904045\n",
      "Iteration 21660, Loss: 0.05565929412841797\n",
      "Iteration 21661, Loss: 0.05567781254649162\n",
      "Iteration 21662, Loss: 0.05565277859568596\n",
      "Iteration 21663, Loss: 0.05563477799296379\n",
      "Iteration 21664, Loss: 0.055691562592983246\n",
      "Iteration 21665, Loss: 0.05563176050782204\n",
      "Iteration 21666, Loss: 0.05576543137431145\n",
      "Iteration 21667, Loss: 0.05581311509013176\n",
      "Iteration 21668, Loss: 0.055752240121364594\n",
      "Iteration 21669, Loss: 0.05565262213349342\n",
      "Iteration 21670, Loss: 0.055751245468854904\n",
      "Iteration 21671, Loss: 0.05571826547384262\n",
      "Iteration 21672, Loss: 0.0556844100356102\n",
      "Iteration 21673, Loss: 0.055723946541547775\n",
      "Iteration 21674, Loss: 0.05567602440714836\n",
      "Iteration 21675, Loss: 0.055709999054670334\n",
      "Iteration 21676, Loss: 0.05570356175303459\n",
      "Iteration 21677, Loss: 0.055675189942121506\n",
      "Iteration 21678, Loss: 0.055699706077575684\n",
      "Iteration 21679, Loss: 0.055638592690229416\n",
      "Iteration 21680, Loss: 0.05576813220977783\n",
      "Iteration 21681, Loss: 0.055768851190805435\n",
      "Iteration 21682, Loss: 0.055632155388593674\n",
      "Iteration 21683, Loss: 0.055735670030117035\n",
      "Iteration 21684, Loss: 0.05574103444814682\n",
      "Iteration 21685, Loss: 0.055635057389736176\n",
      "Iteration 21686, Loss: 0.055813275277614594\n",
      "Iteration 21687, Loss: 0.055862944573163986\n",
      "Iteration 21688, Loss: 0.055749259889125824\n",
      "Iteration 21689, Loss: 0.05570864677429199\n",
      "Iteration 21690, Loss: 0.05579499527812004\n",
      "Iteration 21691, Loss: 0.055765550583601\n",
      "Iteration 21692, Loss: 0.05563191697001457\n",
      "Iteration 21693, Loss: 0.05585344880819321\n",
      "Iteration 21694, Loss: 0.05593208596110344\n",
      "Iteration 21695, Loss: 0.055838070809841156\n",
      "Iteration 21696, Loss: 0.055645108222961426\n",
      "Iteration 21697, Loss: 0.05578526109457016\n",
      "Iteration 21698, Loss: 0.05582837387919426\n",
      "Iteration 21699, Loss: 0.05574647709727287\n",
      "Iteration 21700, Loss: 0.05567137524485588\n",
      "Iteration 21701, Loss: 0.05572442337870598\n",
      "Iteration 21702, Loss: 0.05562818422913551\n",
      "Iteration 21703, Loss: 0.055802345275878906\n",
      "Iteration 21704, Loss: 0.05586854740977287\n",
      "Iteration 21705, Loss: 0.05580791085958481\n",
      "Iteration 21706, Loss: 0.055664777755737305\n",
      "Iteration 21707, Loss: 0.055812399834394455\n",
      "Iteration 21708, Loss: 0.05588388815522194\n",
      "Iteration 21709, Loss: 0.0557892732322216\n",
      "Iteration 21710, Loss: 0.05566314980387688\n",
      "Iteration 21711, Loss: 0.05573789402842522\n",
      "Iteration 21712, Loss: 0.05569720268249512\n",
      "Iteration 21713, Loss: 0.055679164826869965\n",
      "Iteration 21714, Loss: 0.05568302050232887\n",
      "Iteration 21715, Loss: 0.05567721650004387\n",
      "Iteration 21716, Loss: 0.055683016777038574\n",
      "Iteration 21717, Loss: 0.05564801022410393\n",
      "Iteration 21718, Loss: 0.05564812943339348\n",
      "Iteration 21719, Loss: 0.05568420886993408\n",
      "Iteration 21720, Loss: 0.05567988008260727\n",
      "Iteration 21721, Loss: 0.055650316178798676\n",
      "Iteration 21722, Loss: 0.05563784018158913\n",
      "Iteration 21723, Loss: 0.05567602440714836\n",
      "Iteration 21724, Loss: 0.055637601763010025\n",
      "Iteration 21725, Loss: 0.055746834725141525\n",
      "Iteration 21726, Loss: 0.055751923471689224\n",
      "Iteration 21727, Loss: 0.05563851445913315\n",
      "Iteration 21728, Loss: 0.055767498910427094\n",
      "Iteration 21729, Loss: 0.05578958988189697\n",
      "Iteration 21730, Loss: 0.055671971291303635\n",
      "Iteration 21731, Loss: 0.05577731132507324\n",
      "Iteration 21732, Loss: 0.05585523694753647\n",
      "Iteration 21733, Loss: 0.055798135697841644\n",
      "Iteration 21734, Loss: 0.055647969245910645\n",
      "Iteration 21735, Loss: 0.055816929787397385\n",
      "Iteration 21736, Loss: 0.055879198014736176\n",
      "Iteration 21737, Loss: 0.05578009411692619\n",
      "Iteration 21738, Loss: 0.0556718148291111\n",
      "Iteration 21739, Loss: 0.055751919746398926\n",
      "Iteration 21740, Loss: 0.05571294203400612\n",
      "Iteration 21741, Loss: 0.055663786828517914\n",
      "Iteration 21742, Loss: 0.05567169189453125\n",
      "Iteration 21743, Loss: 0.05567988008260727\n",
      "Iteration 21744, Loss: 0.055677734315395355\n",
      "Iteration 21745, Loss: 0.05566330999135971\n",
      "Iteration 21746, Loss: 0.05563756078481674\n",
      "Iteration 21747, Loss: 0.05573638528585434\n",
      "Iteration 21748, Loss: 0.055758994072675705\n",
      "Iteration 21749, Loss: 0.05567578598856926\n",
      "Iteration 21750, Loss: 0.05574723333120346\n",
      "Iteration 21751, Loss: 0.05578180402517319\n",
      "Iteration 21752, Loss: 0.055653851479291916\n",
      "Iteration 21753, Loss: 0.05579976364970207\n",
      "Iteration 21754, Loss: 0.055893342941999435\n",
      "Iteration 21755, Loss: 0.055867791175842285\n",
      "Iteration 21756, Loss: 0.055742185562849045\n",
      "Iteration 21757, Loss: 0.055722158402204514\n",
      "Iteration 21758, Loss: 0.0558091402053833\n",
      "Iteration 21759, Loss: 0.05573515221476555\n",
      "Iteration 21760, Loss: 0.055700067430734634\n",
      "Iteration 21761, Loss: 0.055760424584150314\n",
      "Iteration 21762, Loss: 0.055718980729579926\n",
      "Iteration 21763, Loss: 0.0556638240814209\n",
      "Iteration 21764, Loss: 0.055708251893520355\n",
      "Iteration 21765, Loss: 0.05565730854868889\n",
      "Iteration 21766, Loss: 0.055704038590192795\n",
      "Iteration 21767, Loss: 0.05571401119232178\n",
      "Iteration 21768, Loss: 0.055630527436733246\n",
      "Iteration 21769, Loss: 0.0558011531829834\n",
      "Iteration 21770, Loss: 0.05583667755126953\n",
      "Iteration 21771, Loss: 0.055722676217556\n",
      "Iteration 21772, Loss: 0.05572931095957756\n",
      "Iteration 21773, Loss: 0.0558120422065258\n",
      "Iteration 21774, Loss: 0.055777352303266525\n",
      "Iteration 21775, Loss: 0.05563656613230705\n",
      "Iteration 21776, Loss: 0.055851660668849945\n",
      "Iteration 21777, Loss: 0.0559392012655735\n",
      "Iteration 21778, Loss: 0.05586298555135727\n",
      "Iteration 21779, Loss: 0.05565977096557617\n",
      "Iteration 21780, Loss: 0.05583624169230461\n",
      "Iteration 21781, Loss: 0.05595779791474342\n",
      "Iteration 21782, Loss: 0.055949412286281586\n",
      "Iteration 21783, Loss: 0.055827025324106216\n",
      "Iteration 21784, Loss: 0.05566128343343735\n",
      "Iteration 21785, Loss: 0.05583226680755615\n",
      "Iteration 21786, Loss: 0.05589032173156738\n",
      "Iteration 21787, Loss: 0.055784307420253754\n",
      "Iteration 21788, Loss: 0.055679045617580414\n",
      "Iteration 21789, Loss: 0.055760227143764496\n",
      "Iteration 21790, Loss: 0.055734436959028244\n",
      "Iteration 21791, Loss: 0.055641770362854004\n",
      "Iteration 21792, Loss: 0.05576781556010246\n",
      "Iteration 21793, Loss: 0.05575871467590332\n",
      "Iteration 21794, Loss: 0.055642805993556976\n",
      "Iteration 21795, Loss: 0.055692993104457855\n",
      "Iteration 21796, Loss: 0.05566016957163811\n",
      "Iteration 21797, Loss: 0.055717431008815765\n",
      "Iteration 21798, Loss: 0.05570375919342041\n",
      "Iteration 21799, Loss: 0.055675309151411057\n",
      "Iteration 21800, Loss: 0.055699508637189865\n",
      "Iteration 21801, Loss: 0.05562186613678932\n",
      "Iteration 21802, Loss: 0.05580707639455795\n",
      "Iteration 21803, Loss: 0.05584061145782471\n",
      "Iteration 21804, Loss: 0.05572470277547836\n",
      "Iteration 21805, Loss: 0.055729709565639496\n",
      "Iteration 21806, Loss: 0.05581319332122803\n",
      "Iteration 21807, Loss: 0.055774252861738205\n",
      "Iteration 21808, Loss: 0.05562591552734375\n",
      "Iteration 21809, Loss: 0.055865924805402756\n",
      "Iteration 21810, Loss: 0.055951278656721115\n",
      "Iteration 21811, Loss: 0.05586759373545647\n",
      "Iteration 21812, Loss: 0.055661123245954514\n",
      "Iteration 21813, Loss: 0.05583512783050537\n",
      "Iteration 21814, Loss: 0.05595302954316139\n",
      "Iteration 21815, Loss: 0.05593685433268547\n",
      "Iteration 21816, Loss: 0.05580218881368637\n",
      "Iteration 21817, Loss: 0.05567070096731186\n",
      "Iteration 21818, Loss: 0.05579332634806633\n",
      "Iteration 21819, Loss: 0.05577151104807854\n",
      "Iteration 21820, Loss: 0.05563155934214592\n",
      "Iteration 21821, Loss: 0.05569335073232651\n",
      "Iteration 21822, Loss: 0.05567336082458496\n",
      "Iteration 21823, Loss: 0.055679045617580414\n",
      "Iteration 21824, Loss: 0.0556466206908226\n",
      "Iteration 21825, Loss: 0.05572641268372536\n",
      "Iteration 21826, Loss: 0.05575915426015854\n",
      "Iteration 21827, Loss: 0.05569446086883545\n",
      "Iteration 21828, Loss: 0.055704515427351\n",
      "Iteration 21829, Loss: 0.05571802705526352\n",
      "Iteration 21830, Loss: 0.05565039440989494\n",
      "Iteration 21831, Loss: 0.055663466453552246\n",
      "Iteration 21832, Loss: 0.05565237998962402\n",
      "Iteration 21833, Loss: 0.05563489720225334\n",
      "Iteration 21834, Loss: 0.055643919855356216\n",
      "Iteration 21835, Loss: 0.05567268654704094\n",
      "Iteration 21836, Loss: 0.05565408989787102\n",
      "Iteration 21837, Loss: 0.05570300668478012\n",
      "Iteration 21838, Loss: 0.055666327476501465\n",
      "Iteration 21839, Loss: 0.05572223663330078\n",
      "Iteration 21840, Loss: 0.055763207376003265\n",
      "Iteration 21841, Loss: 0.05570388212800026\n",
      "Iteration 21842, Loss: 0.05568651482462883\n",
      "Iteration 21843, Loss: 0.055696647614240646\n",
      "Iteration 21844, Loss: 0.05566903203725815\n",
      "Iteration 21845, Loss: 0.05568190664052963\n",
      "Iteration 21846, Loss: 0.05563477799296379\n",
      "Iteration 21847, Loss: 0.05568818375468254\n",
      "Iteration 21848, Loss: 0.05564364045858383\n",
      "Iteration 21849, Loss: 0.05572069063782692\n",
      "Iteration 21850, Loss: 0.05571286007761955\n",
      "Iteration 21851, Loss: 0.05564995855093002\n",
      "Iteration 21852, Loss: 0.055658262223005295\n",
      "Iteration 21853, Loss: 0.055685799568891525\n",
      "Iteration 21854, Loss: 0.0556672029197216\n",
      "Iteration 21855, Loss: 0.055693309754133224\n",
      "Iteration 21856, Loss: 0.05568718910217285\n",
      "Iteration 21857, Loss: 0.05567161366343498\n",
      "Iteration 21858, Loss: 0.05566776171326637\n",
      "Iteration 21859, Loss: 0.055682700127363205\n",
      "Iteration 21860, Loss: 0.055667560547590256\n",
      "Iteration 21861, Loss: 0.05569744110107422\n",
      "Iteration 21862, Loss: 0.055698834359645844\n",
      "Iteration 21863, Loss: 0.05564960092306137\n",
      "Iteration 21864, Loss: 0.055655162781476974\n",
      "Iteration 21865, Loss: 0.05569016933441162\n",
      "Iteration 21866, Loss: 0.05568134784698486\n",
      "Iteration 21867, Loss: 0.05566629022359848\n",
      "Iteration 21868, Loss: 0.05565480515360832\n",
      "Iteration 21869, Loss: 0.05570721998810768\n",
      "Iteration 21870, Loss: 0.05571627616882324\n",
      "Iteration 21871, Loss: 0.055643998086452484\n",
      "Iteration 21872, Loss: 0.055753909051418304\n",
      "Iteration 21873, Loss: 0.05574333667755127\n",
      "Iteration 21874, Loss: 0.05564836785197258\n",
      "Iteration 21875, Loss: 0.05567701905965805\n",
      "Iteration 21876, Loss: 0.05562850087881088\n",
      "Iteration 21877, Loss: 0.0557352714240551\n",
      "Iteration 21878, Loss: 0.055705904960632324\n",
      "Iteration 21879, Loss: 0.05568202584981918\n",
      "Iteration 21880, Loss: 0.05571242421865463\n",
      "Iteration 21881, Loss: 0.05563418194651604\n",
      "Iteration 21882, Loss: 0.05579686537384987\n",
      "Iteration 21883, Loss: 0.055834852159023285\n",
      "Iteration 21884, Loss: 0.05572343245148659\n",
      "Iteration 21885, Loss: 0.055727165192365646\n",
      "Iteration 21886, Loss: 0.055807434022426605\n",
      "Iteration 21887, Loss: 0.05576543137431145\n",
      "Iteration 21888, Loss: 0.05562003701925278\n",
      "Iteration 21889, Loss: 0.0558096207678318\n",
      "Iteration 21890, Loss: 0.055825911462306976\n",
      "Iteration 21891, Loss: 0.05567272752523422\n",
      "Iteration 21892, Loss: 0.055796705186367035\n",
      "Iteration 21893, Loss: 0.05590677633881569\n",
      "Iteration 21894, Loss: 0.05590268224477768\n",
      "Iteration 21895, Loss: 0.055797938257455826\n",
      "Iteration 21896, Loss: 0.055647414177656174\n",
      "Iteration 21897, Loss: 0.055820029228925705\n",
      "Iteration 21898, Loss: 0.05584708973765373\n",
      "Iteration 21899, Loss: 0.05570439621806145\n",
      "Iteration 21900, Loss: 0.055764637887477875\n",
      "Iteration 21901, Loss: 0.05586763471364975\n",
      "Iteration 21902, Loss: 0.0558672770857811\n",
      "Iteration 21903, Loss: 0.05577349662780762\n",
      "Iteration 21904, Loss: 0.05563819780945778\n",
      "Iteration 21905, Loss: 0.055726051330566406\n",
      "Iteration 21906, Loss: 0.05567002296447754\n",
      "Iteration 21907, Loss: 0.05572211742401123\n",
      "Iteration 21908, Loss: 0.05575835704803467\n",
      "Iteration 21909, Loss: 0.0556846484541893\n",
      "Iteration 21910, Loss: 0.05572974681854248\n",
      "Iteration 21911, Loss: 0.055762290954589844\n",
      "Iteration 21912, Loss: 0.05564359948039055\n",
      "Iteration 21913, Loss: 0.055793408304452896\n",
      "Iteration 21914, Loss: 0.05586664006114006\n",
      "Iteration 21915, Loss: 0.055816810578107834\n",
      "Iteration 21916, Loss: 0.05567467585206032\n",
      "Iteration 21917, Loss: 0.05580679699778557\n",
      "Iteration 21918, Loss: 0.05589227005839348\n",
      "Iteration 21919, Loss: 0.055815935134887695\n",
      "Iteration 21920, Loss: 0.055629096925258636\n",
      "Iteration 21921, Loss: 0.05573276802897453\n",
      "Iteration 21922, Loss: 0.05571071431040764\n",
      "Iteration 21923, Loss: 0.05565758794546127\n",
      "Iteration 21924, Loss: 0.05566871166229248\n",
      "Iteration 21925, Loss: 0.05567431449890137\n",
      "Iteration 21926, Loss: 0.05565846338868141\n",
      "Iteration 21927, Loss: 0.055695973336696625\n",
      "Iteration 21928, Loss: 0.0556817464530468\n",
      "Iteration 21929, Loss: 0.055684130638837814\n",
      "Iteration 21930, Loss: 0.055690132081508636\n",
      "Iteration 21931, Loss: 0.05564848706126213\n",
      "Iteration 21932, Loss: 0.05564276501536369\n",
      "Iteration 21933, Loss: 0.05570610612630844\n",
      "Iteration 21934, Loss: 0.0557098425924778\n",
      "Iteration 21935, Loss: 0.055637042969465256\n",
      "Iteration 21936, Loss: 0.055746834725141525\n",
      "Iteration 21937, Loss: 0.055719733238220215\n",
      "Iteration 21938, Loss: 0.05567646026611328\n",
      "Iteration 21939, Loss: 0.05571385473012924\n",
      "Iteration 21940, Loss: 0.055657509714365005\n",
      "Iteration 21941, Loss: 0.05574306100606918\n",
      "Iteration 21942, Loss: 0.055745404213666916\n",
      "Iteration 21943, Loss: 0.05563875287771225\n",
      "Iteration 21944, Loss: 0.05566692352294922\n",
      "Iteration 21945, Loss: 0.055627983063459396\n",
      "Iteration 21946, Loss: 0.05568107217550278\n",
      "Iteration 21947, Loss: 0.055658500641584396\n",
      "Iteration 21948, Loss: 0.05570042133331299\n",
      "Iteration 21949, Loss: 0.055670421570539474\n",
      "Iteration 21950, Loss: 0.055713776499032974\n",
      "Iteration 21951, Loss: 0.05574921891093254\n",
      "Iteration 21952, Loss: 0.055685680359601974\n",
      "Iteration 21953, Loss: 0.05571448802947998\n",
      "Iteration 21954, Loss: 0.05572911351919174\n",
      "Iteration 21955, Loss: 0.05563895031809807\n",
      "Iteration 21956, Loss: 0.055648647248744965\n",
      "Iteration 21957, Loss: 0.05567435547709465\n",
      "Iteration 21958, Loss: 0.055614471435546875\n",
      "Iteration 21959, Loss: 0.05567602440714836\n",
      "Iteration 21960, Loss: 0.05563784018158913\n",
      "Iteration 21961, Loss: 0.05563008785247803\n",
      "Iteration 21962, Loss: 0.055705152451992035\n",
      "Iteration 21963, Loss: 0.05569346994161606\n",
      "Iteration 21964, Loss: 0.05565675348043442\n",
      "Iteration 21965, Loss: 0.05565909668803215\n",
      "Iteration 21966, Loss: 0.05569037050008774\n",
      "Iteration 21967, Loss: 0.055691324174404144\n",
      "Iteration 21968, Loss: 0.05564522743225098\n",
      "Iteration 21969, Loss: 0.055687032639980316\n",
      "Iteration 21970, Loss: 0.05564439669251442\n",
      "Iteration 21971, Loss: 0.05569108575582504\n",
      "Iteration 21972, Loss: 0.05566684529185295\n",
      "Iteration 21973, Loss: 0.05570455640554428\n",
      "Iteration 21974, Loss: 0.0557025708258152\n",
      "Iteration 21975, Loss: 0.05565734952688217\n",
      "Iteration 21976, Loss: 0.05566922947764397\n",
      "Iteration 21977, Loss: 0.05566776171326637\n",
      "Iteration 21978, Loss: 0.055643562227487564\n",
      "Iteration 21979, Loss: 0.05570685863494873\n",
      "Iteration 21980, Loss: 0.05568329617381096\n",
      "Iteration 21981, Loss: 0.055689696222543716\n",
      "Iteration 21982, Loss: 0.055702608078718185\n",
      "Iteration 21983, Loss: 0.055632513016462326\n",
      "Iteration 21984, Loss: 0.05568043515086174\n",
      "Iteration 21985, Loss: 0.05563771724700928\n",
      "Iteration 21986, Loss: 0.055652063339948654\n",
      "Iteration 21987, Loss: 0.055643003433942795\n",
      "Iteration 21988, Loss: 0.05565834045410156\n",
      "Iteration 21989, Loss: 0.055628541857004166\n",
      "Iteration 21990, Loss: 0.05574846267700195\n",
      "Iteration 21991, Loss: 0.0557279996573925\n",
      "Iteration 21992, Loss: 0.055661678314208984\n",
      "Iteration 21993, Loss: 0.055694580078125\n",
      "Iteration 21994, Loss: 0.05562881752848625\n",
      "Iteration 21995, Loss: 0.055788278579711914\n",
      "Iteration 21996, Loss: 0.055810969322919846\n",
      "Iteration 21997, Loss: 0.05568361654877663\n",
      "Iteration 21998, Loss: 0.05576658248901367\n",
      "Iteration 21999, Loss: 0.05585400387644768\n",
      "Iteration 22000, Loss: 0.05582209676504135\n",
      "Iteration 22001, Loss: 0.055687665939331055\n",
      "Iteration 22002, Loss: 0.05578601360321045\n",
      "Iteration 22003, Loss: 0.05587410926818848\n",
      "Iteration 22004, Loss: 0.055799566209316254\n",
      "Iteration 22005, Loss: 0.05564077943563461\n",
      "Iteration 22006, Loss: 0.05572911351919174\n",
      "Iteration 22007, Loss: 0.055701058357954025\n",
      "Iteration 22008, Loss: 0.05567049980163574\n",
      "Iteration 22009, Loss: 0.055675867944955826\n",
      "Iteration 22010, Loss: 0.055670976638793945\n",
      "Iteration 22011, Loss: 0.05566128343343735\n",
      "Iteration 22012, Loss: 0.055693112313747406\n",
      "Iteration 22013, Loss: 0.0556800402700901\n",
      "Iteration 22014, Loss: 0.055682264268398285\n",
      "Iteration 22015, Loss: 0.05568647384643555\n",
      "Iteration 22016, Loss: 0.05565397068858147\n",
      "Iteration 22017, Loss: 0.055632952600717545\n",
      "Iteration 22018, Loss: 0.055735986679792404\n",
      "Iteration 22019, Loss: 0.05574655532836914\n",
      "Iteration 22020, Loss: 0.055653177201747894\n",
      "Iteration 22021, Loss: 0.0557689294219017\n",
      "Iteration 22022, Loss: 0.05580131337046623\n",
      "Iteration 22023, Loss: 0.05567809194326401\n",
      "Iteration 22024, Loss: 0.055773817002773285\n",
      "Iteration 22025, Loss: 0.055863022804260254\n",
      "Iteration 22026, Loss: 0.05583401769399643\n",
      "Iteration 22027, Loss: 0.055707454681396484\n",
      "Iteration 22028, Loss: 0.0557578019797802\n",
      "Iteration 22029, Loss: 0.055839620530605316\n",
      "Iteration 22030, Loss: 0.055760227143764496\n",
      "Iteration 22031, Loss: 0.055675070732831955\n",
      "Iteration 22032, Loss: 0.05573761463165283\n",
      "Iteration 22033, Loss: 0.055691443383693695\n",
      "Iteration 22034, Loss: 0.05568798631429672\n",
      "Iteration 22035, Loss: 0.05569426342844963\n",
      "Iteration 22036, Loss: 0.05566708371043205\n",
      "Iteration 22037, Loss: 0.05567380040884018\n",
      "Iteration 22038, Loss: 0.05565214157104492\n",
      "Iteration 22039, Loss: 0.05564646050333977\n",
      "Iteration 22040, Loss: 0.05568242073059082\n",
      "Iteration 22041, Loss: 0.05567578598856926\n",
      "Iteration 22042, Loss: 0.05565810203552246\n",
      "Iteration 22043, Loss: 0.05562583729624748\n",
      "Iteration 22044, Loss: 0.05570606514811516\n",
      "Iteration 22045, Loss: 0.05568365380167961\n",
      "Iteration 22046, Loss: 0.055680155754089355\n",
      "Iteration 22047, Loss: 0.05567821115255356\n",
      "Iteration 22048, Loss: 0.05567809194326401\n",
      "Iteration 22049, Loss: 0.055675189942121506\n",
      "Iteration 22050, Loss: 0.055670224130153656\n",
      "Iteration 22051, Loss: 0.05565190315246582\n",
      "Iteration 22052, Loss: 0.05571671575307846\n",
      "Iteration 22053, Loss: 0.055724821984767914\n",
      "Iteration 22054, Loss: 0.05563827604055405\n",
      "Iteration 22055, Loss: 0.05575243756175041\n",
      "Iteration 22056, Loss: 0.055740002542734146\n",
      "Iteration 22057, Loss: 0.055646222084760666\n",
      "Iteration 22058, Loss: 0.055669188499450684\n",
      "Iteration 22059, Loss: 0.05563855171203613\n",
      "Iteration 22060, Loss: 0.0556492805480957\n",
      "Iteration 22061, Loss: 0.05564789101481438\n",
      "Iteration 22062, Loss: 0.055628739297389984\n",
      "Iteration 22063, Loss: 0.055684804916381836\n",
      "Iteration 22064, Loss: 0.05566950887441635\n",
      "Iteration 22065, Loss: 0.055676382035017014\n",
      "Iteration 22066, Loss: 0.05563656613230705\n",
      "Iteration 22067, Loss: 0.05574452877044678\n",
      "Iteration 22068, Loss: 0.055787961930036545\n",
      "Iteration 22069, Loss: 0.055732570588588715\n",
      "Iteration 22070, Loss: 0.05564232915639877\n",
      "Iteration 22071, Loss: 0.05567248910665512\n",
      "Iteration 22072, Loss: 0.055664580315351486\n",
      "Iteration 22073, Loss: 0.055661641061306\n",
      "Iteration 22074, Loss: 0.05567193031311035\n",
      "Iteration 22075, Loss: 0.05562834069132805\n",
      "Iteration 22076, Loss: 0.0557229146361351\n",
      "Iteration 22077, Loss: 0.055727601051330566\n",
      "Iteration 22078, Loss: 0.05563398450613022\n",
      "Iteration 22079, Loss: 0.05579058453440666\n",
      "Iteration 22080, Loss: 0.05580870434641838\n",
      "Iteration 22081, Loss: 0.055656593292951584\n",
      "Iteration 22082, Loss: 0.05580751225352287\n",
      "Iteration 22083, Loss: 0.055917900055646896\n",
      "Iteration 22084, Loss: 0.055914442986249924\n",
      "Iteration 22085, Loss: 0.05581117048859596\n",
      "Iteration 22086, Loss: 0.05564948171377182\n",
      "Iteration 22087, Loss: 0.055855195969343185\n",
      "Iteration 22088, Loss: 0.05592958256602287\n",
      "Iteration 22089, Loss: 0.05582396313548088\n",
      "Iteration 22090, Loss: 0.055657267570495605\n",
      "Iteration 22091, Loss: 0.05574886128306389\n",
      "Iteration 22092, Loss: 0.055749498307704926\n",
      "Iteration 22093, Loss: 0.05566120147705078\n",
      "Iteration 22094, Loss: 0.055778149515390396\n",
      "Iteration 22095, Loss: 0.0558166541159153\n",
      "Iteration 22096, Loss: 0.05568826198577881\n",
      "Iteration 22097, Loss: 0.05576304718852043\n",
      "Iteration 22098, Loss: 0.055855873972177505\n",
      "Iteration 22099, Loss: 0.05584239959716797\n",
      "Iteration 22100, Loss: 0.055733125656843185\n",
      "Iteration 22101, Loss: 0.055705152451992035\n",
      "Iteration 22102, Loss: 0.05577008053660393\n",
      "Iteration 22103, Loss: 0.055662594735622406\n",
      "Iteration 22104, Loss: 0.05577179044485092\n",
      "Iteration 22105, Loss: 0.0558553971350193\n",
      "Iteration 22106, Loss: 0.05583421513438225\n",
      "Iteration 22107, Loss: 0.055720530450344086\n",
      "Iteration 22108, Loss: 0.05572811886668205\n",
      "Iteration 22109, Loss: 0.05579733848571777\n",
      "Iteration 22110, Loss: 0.05569358915090561\n",
      "Iteration 22111, Loss: 0.05574619770050049\n",
      "Iteration 22112, Loss: 0.055827897042036057\n",
      "Iteration 22113, Loss: 0.055806636810302734\n",
      "Iteration 22114, Loss: 0.055694978684186935\n",
      "Iteration 22115, Loss: 0.05575939267873764\n",
      "Iteration 22116, Loss: 0.05582408234477043\n",
      "Iteration 22117, Loss: 0.055712223052978516\n",
      "Iteration 22118, Loss: 0.05573725700378418\n",
      "Iteration 22119, Loss: 0.05582460016012192\n",
      "Iteration 22120, Loss: 0.055808983743190765\n",
      "Iteration 22121, Loss: 0.055701375007629395\n",
      "Iteration 22122, Loss: 0.05574560537934303\n",
      "Iteration 22123, Loss: 0.055805765092372894\n",
      "Iteration 22124, Loss: 0.055688660591840744\n",
      "Iteration 22125, Loss: 0.05575760453939438\n",
      "Iteration 22126, Loss: 0.05584820359945297\n",
      "Iteration 22127, Loss: 0.05583544820547104\n",
      "Iteration 22128, Loss: 0.05573010817170143\n",
      "Iteration 22129, Loss: 0.055703721940517426\n",
      "Iteration 22130, Loss: 0.05576213449239731\n",
      "Iteration 22131, Loss: 0.05564344301819801\n",
      "Iteration 22132, Loss: 0.05579110234975815\n",
      "Iteration 22133, Loss: 0.05588209629058838\n",
      "Iteration 22134, Loss: 0.05586973950266838\n",
      "Iteration 22135, Loss: 0.05576443672180176\n",
      "Iteration 22136, Loss: 0.05565730854868889\n",
      "Iteration 22137, Loss: 0.05571528524160385\n",
      "Iteration 22138, Loss: 0.05562257766723633\n",
      "Iteration 22139, Loss: 0.05563442036509514\n",
      "Iteration 22140, Loss: 0.05569128319621086\n",
      "Iteration 22141, Loss: 0.055656515061855316\n",
      "Iteration 22142, Loss: 0.055716872215270996\n",
      "Iteration 22143, Loss: 0.05572299286723137\n",
      "Iteration 22144, Loss: 0.05563068389892578\n",
      "Iteration 22145, Loss: 0.055701497942209244\n",
      "Iteration 22146, Loss: 0.05563005059957504\n",
      "Iteration 22147, Loss: 0.05577167123556137\n",
      "Iteration 22148, Loss: 0.05583071708679199\n",
      "Iteration 22149, Loss: 0.055784665048122406\n",
      "Iteration 22150, Loss: 0.05565524473786354\n",
      "Iteration 22151, Loss: 0.05582738295197487\n",
      "Iteration 22152, Loss: 0.055899620056152344\n",
      "Iteration 22153, Loss: 0.055791258811950684\n",
      "Iteration 22154, Loss: 0.05567733570933342\n",
      "Iteration 22155, Loss: 0.05576268956065178\n",
      "Iteration 22156, Loss: 0.05574588105082512\n",
      "Iteration 22157, Loss: 0.05564165115356445\n",
      "Iteration 22158, Loss: 0.05581796169281006\n",
      "Iteration 22159, Loss: 0.055867116898298264\n",
      "Iteration 22160, Loss: 0.05573936551809311\n",
      "Iteration 22161, Loss: 0.05572764202952385\n",
      "Iteration 22162, Loss: 0.055824439972639084\n",
      "Iteration 22163, Loss: 0.055818162858486176\n",
      "Iteration 22164, Loss: 0.05571901798248291\n",
      "Iteration 22165, Loss: 0.055711906403303146\n",
      "Iteration 22166, Loss: 0.05576292797923088\n",
      "Iteration 22167, Loss: 0.05563966557383537\n",
      "Iteration 22168, Loss: 0.05579312890768051\n",
      "Iteration 22169, Loss: 0.05588158220052719\n",
      "Iteration 22170, Loss: 0.05586572736501694\n",
      "Iteration 22171, Loss: 0.05575653165578842\n",
      "Iteration 22172, Loss: 0.05567483231425285\n",
      "Iteration 22173, Loss: 0.05574309825897217\n",
      "Iteration 22174, Loss: 0.05564181134104729\n",
      "Iteration 22175, Loss: 0.05577520653605461\n",
      "Iteration 22176, Loss: 0.055849794298410416\n",
      "Iteration 22177, Loss: 0.055822256952524185\n",
      "Iteration 22178, Loss: 0.05570324510335922\n",
      "Iteration 22179, Loss: 0.055756449699401855\n",
      "Iteration 22180, Loss: 0.05582849308848381\n",
      "Iteration 22181, Loss: 0.05571957677602768\n",
      "Iteration 22182, Loss: 0.05572938919067383\n",
      "Iteration 22183, Loss: 0.055815182626247406\n",
      "Iteration 22184, Loss: 0.05579809471964836\n",
      "Iteration 22185, Loss: 0.05568905919790268\n",
      "Iteration 22186, Loss: 0.055764079093933105\n",
      "Iteration 22187, Loss: 0.05582527443766594\n",
      "Iteration 22188, Loss: 0.055706899613142014\n",
      "Iteration 22189, Loss: 0.05574492737650871\n",
      "Iteration 22190, Loss: 0.05583620071411133\n",
      "Iteration 22191, Loss: 0.05582427978515625\n",
      "Iteration 22192, Loss: 0.05571965500712395\n",
      "Iteration 22193, Loss: 0.05571722984313965\n",
      "Iteration 22194, Loss: 0.05577389523386955\n",
      "Iteration 22195, Loss: 0.055652741342782974\n",
      "Iteration 22196, Loss: 0.05578557774424553\n",
      "Iteration 22197, Loss: 0.0558776892721653\n",
      "Iteration 22198, Loss: 0.05586608499288559\n",
      "Iteration 22199, Loss: 0.05576173588633537\n",
      "Iteration 22200, Loss: 0.055659811943769455\n",
      "Iteration 22201, Loss: 0.05571651831269264\n",
      "Iteration 22202, Loss: 0.05562257766723633\n",
      "Iteration 22203, Loss: 0.05564387887716293\n",
      "Iteration 22204, Loss: 0.055673640221357346\n",
      "Iteration 22205, Loss: 0.05563553422689438\n",
      "Iteration 22206, Loss: 0.05573781579732895\n",
      "Iteration 22207, Loss: 0.05573217198252678\n",
      "Iteration 22208, Loss: 0.05564077943563461\n",
      "Iteration 22209, Loss: 0.05569259449839592\n",
      "Iteration 22210, Loss: 0.055628061294555664\n",
      "Iteration 22211, Loss: 0.05576245114207268\n",
      "Iteration 22212, Loss: 0.055806875228881836\n",
      "Iteration 22213, Loss: 0.05574711412191391\n",
      "Iteration 22214, Loss: 0.05565265938639641\n",
      "Iteration 22215, Loss: 0.05574647709727287\n",
      "Iteration 22216, Loss: 0.05570797249674797\n",
      "Iteration 22217, Loss: 0.05569406598806381\n",
      "Iteration 22218, Loss: 0.055735986679792404\n",
      "Iteration 22219, Loss: 0.055687546730041504\n",
      "Iteration 22220, Loss: 0.05569589138031006\n",
      "Iteration 22221, Loss: 0.05569338798522949\n",
      "Iteration 22222, Loss: 0.05567849054932594\n",
      "Iteration 22223, Loss: 0.05569978803396225\n",
      "Iteration 22224, Loss: 0.05563465878367424\n",
      "Iteration 22225, Loss: 0.055775921791791916\n",
      "Iteration 22226, Loss: 0.05578180402517319\n",
      "Iteration 22227, Loss: 0.055637042969465256\n",
      "Iteration 22228, Loss: 0.05578700825572014\n",
      "Iteration 22229, Loss: 0.055852215737104416\n",
      "Iteration 22230, Loss: 0.05579777806997299\n",
      "Iteration 22231, Loss: 0.05565222352743149\n",
      "Iteration 22232, Loss: 0.05583357810974121\n",
      "Iteration 22233, Loss: 0.055913567543029785\n",
      "Iteration 22234, Loss: 0.05581967160105705\n",
      "Iteration 22235, Loss: 0.055642884224653244\n",
      "Iteration 22236, Loss: 0.05573249235749245\n",
      "Iteration 22237, Loss: 0.0557103157043457\n",
      "Iteration 22238, Loss: 0.05564646050333977\n",
      "Iteration 22239, Loss: 0.05563906952738762\n",
      "Iteration 22240, Loss: 0.05572021007537842\n",
      "Iteration 22241, Loss: 0.055734358727931976\n",
      "Iteration 22242, Loss: 0.05565126985311508\n",
      "Iteration 22243, Loss: 0.05577345937490463\n",
      "Iteration 22244, Loss: 0.05579833313822746\n",
      "Iteration 22245, Loss: 0.05565246194601059\n",
      "Iteration 22246, Loss: 0.05580727383494377\n",
      "Iteration 22247, Loss: 0.05591408535838127\n",
      "Iteration 22248, Loss: 0.05590752884745598\n",
      "Iteration 22249, Loss: 0.055801551789045334\n",
      "Iteration 22250, Loss: 0.055650513619184494\n",
      "Iteration 22251, Loss: 0.05583024397492409\n",
      "Iteration 22252, Loss: 0.0558723621070385\n",
      "Iteration 22253, Loss: 0.05574166774749756\n",
      "Iteration 22254, Loss: 0.05573034659028053\n",
      "Iteration 22255, Loss: 0.055827777832746506\n",
      "Iteration 22256, Loss: 0.05582384392619133\n",
      "Iteration 22257, Loss: 0.05572879686951637\n",
      "Iteration 22258, Loss: 0.05569736286997795\n",
      "Iteration 22259, Loss: 0.05574647709727287\n",
      "Iteration 22260, Loss: 0.05564304441213608\n",
      "Iteration 22261, Loss: 0.0557536706328392\n",
      "Iteration 22262, Loss: 0.05580159276723862\n",
      "Iteration 22263, Loss: 0.0557434968650341\n",
      "Iteration 22264, Loss: 0.055639784783124924\n",
      "Iteration 22265, Loss: 0.055700939148664474\n",
      "Iteration 22266, Loss: 0.05563418194651604\n",
      "Iteration 22267, Loss: 0.05570487305521965\n",
      "Iteration 22268, Loss: 0.05569581314921379\n",
      "Iteration 22269, Loss: 0.055647335946559906\n",
      "Iteration 22270, Loss: 0.05562679097056389\n",
      "Iteration 22271, Loss: 0.05573403835296631\n",
      "Iteration 22272, Loss: 0.05575808137655258\n",
      "Iteration 22273, Loss: 0.055684011429548264\n",
      "Iteration 22274, Loss: 0.055727921426296234\n",
      "Iteration 22275, Loss: 0.05575140565633774\n",
      "Iteration 22276, Loss: 0.055618446320295334\n",
      "Iteration 22277, Loss: 0.05568190664052963\n",
      "Iteration 22278, Loss: 0.05564570799469948\n",
      "Iteration 22279, Loss: 0.05573272705078125\n",
      "Iteration 22280, Loss: 0.055721960961818695\n",
      "Iteration 22281, Loss: 0.05565742775797844\n",
      "Iteration 22282, Loss: 0.05568202584981918\n",
      "Iteration 22283, Loss: 0.055621229112148285\n",
      "Iteration 22284, Loss: 0.05566549673676491\n",
      "Iteration 22285, Loss: 0.05562826246023178\n",
      "Iteration 22286, Loss: 0.05575883388519287\n",
      "Iteration 22287, Loss: 0.05574151128530502\n",
      "Iteration 22288, Loss: 0.05565258115530014\n",
      "Iteration 22289, Loss: 0.05568460747599602\n",
      "Iteration 22290, Loss: 0.055621467530727386\n",
      "Iteration 22291, Loss: 0.055795591324567795\n",
      "Iteration 22292, Loss: 0.05580782890319824\n",
      "Iteration 22293, Loss: 0.05565842241048813\n",
      "Iteration 22294, Loss: 0.05579829216003418\n",
      "Iteration 22295, Loss: 0.05590017884969711\n",
      "Iteration 22296, Loss: 0.05589048191905022\n",
      "Iteration 22297, Loss: 0.055782441049814224\n",
      "Iteration 22298, Loss: 0.055655598640441895\n",
      "Iteration 22299, Loss: 0.05577433481812477\n",
      "Iteration 22300, Loss: 0.055744051933288574\n",
      "Iteration 22301, Loss: 0.055666565895080566\n",
      "Iteration 22302, Loss: 0.05570995807647705\n",
      "Iteration 22303, Loss: 0.05567435547709465\n",
      "Iteration 22304, Loss: 0.05569934844970703\n",
      "Iteration 22305, Loss: 0.055681824684143066\n",
      "Iteration 22306, Loss: 0.05569414421916008\n",
      "Iteration 22307, Loss: 0.05572168156504631\n",
      "Iteration 22308, Loss: 0.05565444752573967\n",
      "Iteration 22309, Loss: 0.05576082319021225\n",
      "Iteration 22310, Loss: 0.055776678025722504\n",
      "Iteration 22311, Loss: 0.05562945455312729\n",
      "Iteration 22312, Loss: 0.05579821392893791\n",
      "Iteration 22313, Loss: 0.055874429643154144\n",
      "Iteration 22314, Loss: 0.055835288017988205\n",
      "Iteration 22315, Loss: 0.05570030212402344\n",
      "Iteration 22316, Loss: 0.05577632039785385\n",
      "Iteration 22317, Loss: 0.05586386099457741\n",
      "Iteration 22318, Loss: 0.05577830597758293\n",
      "Iteration 22319, Loss: 0.05566795915365219\n",
      "Iteration 22320, Loss: 0.05573773384094238\n",
      "Iteration 22321, Loss: 0.055698834359645844\n",
      "Iteration 22322, Loss: 0.05567105859518051\n",
      "Iteration 22323, Loss: 0.05567034333944321\n",
      "Iteration 22324, Loss: 0.05569251626729965\n",
      "Iteration 22325, Loss: 0.05570626258850098\n",
      "Iteration 22326, Loss: 0.055635929107666016\n",
      "Iteration 22327, Loss: 0.05576292797923088\n",
      "Iteration 22328, Loss: 0.055752597749233246\n",
      "Iteration 22329, Loss: 0.05564260855317116\n",
      "Iteration 22330, Loss: 0.05568087473511696\n",
      "Iteration 22331, Loss: 0.055634140968322754\n",
      "Iteration 22332, Loss: 0.05575653165578842\n",
      "Iteration 22333, Loss: 0.05575430393218994\n",
      "Iteration 22334, Loss: 0.055640142410993576\n",
      "Iteration 22335, Loss: 0.05573364347219467\n",
      "Iteration 22336, Loss: 0.05572883412241936\n",
      "Iteration 22337, Loss: 0.0556265152990818\n",
      "Iteration 22338, Loss: 0.055668119341135025\n",
      "Iteration 22339, Loss: 0.055654529482126236\n",
      "Iteration 22340, Loss: 0.05563787743449211\n",
      "Iteration 22341, Loss: 0.055714529007673264\n",
      "Iteration 22342, Loss: 0.05567018315196037\n",
      "Iteration 22343, Loss: 0.05572287365794182\n",
      "Iteration 22344, Loss: 0.05576912686228752\n",
      "Iteration 22345, Loss: 0.05571647733449936\n",
      "Iteration 22346, Loss: 0.05565913766622543\n",
      "Iteration 22347, Loss: 0.05566629022359848\n",
      "Iteration 22348, Loss: 0.05568965524435043\n",
      "Iteration 22349, Loss: 0.05570364370942116\n",
      "Iteration 22350, Loss: 0.05562726780772209\n",
      "Iteration 22351, Loss: 0.05579833313822746\n",
      "Iteration 22352, Loss: 0.05581367015838623\n",
      "Iteration 22353, Loss: 0.055657271295785904\n",
      "Iteration 22354, Loss: 0.05580552667379379\n",
      "Iteration 22355, Loss: 0.055916789919137955\n",
      "Iteration 22356, Loss: 0.055917780846357346\n",
      "Iteration 22357, Loss: 0.05582011118531227\n",
      "Iteration 22358, Loss: 0.055646538734436035\n",
      "Iteration 22359, Loss: 0.05588948726654053\n",
      "Iteration 22360, Loss: 0.05600492283701897\n",
      "Iteration 22361, Loss: 0.055934034287929535\n",
      "Iteration 22362, Loss: 0.05569728463888168\n",
      "Iteration 22363, Loss: 0.055830519646406174\n",
      "Iteration 22364, Loss: 0.05599216744303703\n",
      "Iteration 22365, Loss: 0.056042395532131195\n",
      "Iteration 22366, Loss: 0.05599157139658928\n",
      "Iteration 22367, Loss: 0.05585114285349846\n",
      "Iteration 22368, Loss: 0.0556388720870018\n",
      "Iteration 22369, Loss: 0.055948656052351\n",
      "Iteration 22370, Loss: 0.05610990524291992\n",
      "Iteration 22371, Loss: 0.056082647293806076\n",
      "Iteration 22372, Loss: 0.05588698387145996\n",
      "Iteration 22373, Loss: 0.05567117780447006\n",
      "Iteration 22374, Loss: 0.05581601709127426\n",
      "Iteration 22375, Loss: 0.05586358159780502\n",
      "Iteration 22376, Loss: 0.0558113269507885\n",
      "Iteration 22377, Loss: 0.05566760152578354\n",
      "Iteration 22378, Loss: 0.055833421647548676\n",
      "Iteration 22379, Loss: 0.05593355745077133\n",
      "Iteration 22380, Loss: 0.055858734995126724\n",
      "Iteration 22381, Loss: 0.05564892292022705\n",
      "Iteration 22382, Loss: 0.0558290109038353\n",
      "Iteration 22383, Loss: 0.05594225972890854\n",
      "Iteration 22384, Loss: 0.05593717098236084\n",
      "Iteration 22385, Loss: 0.05582698434591293\n",
      "Iteration 22386, Loss: 0.05565353482961655\n",
      "Iteration 22387, Loss: 0.05585833638906479\n",
      "Iteration 22388, Loss: 0.05594822019338608\n",
      "Iteration 22389, Loss: 0.05586361885070801\n",
      "Iteration 22390, Loss: 0.05562102794647217\n",
      "Iteration 22391, Loss: 0.05589660257101059\n",
      "Iteration 22392, Loss: 0.05606210231781006\n",
      "Iteration 22393, Loss: 0.05611201375722885\n",
      "Iteration 22394, Loss: 0.05605856701731682\n",
      "Iteration 22395, Loss: 0.05591408535838127\n",
      "Iteration 22396, Loss: 0.05570177361369133\n",
      "Iteration 22397, Loss: 0.05587788671255112\n",
      "Iteration 22398, Loss: 0.056047677993774414\n",
      "Iteration 22399, Loss: 0.0560334138572216\n",
      "Iteration 22400, Loss: 0.05585082620382309\n",
      "Iteration 22401, Loss: 0.05568134784698486\n",
      "Iteration 22402, Loss: 0.05581120774149895\n",
      "Iteration 22403, Loss: 0.055833421647548676\n",
      "Iteration 22404, Loss: 0.05575907230377197\n",
      "Iteration 22405, Loss: 0.05563855171203613\n",
      "Iteration 22406, Loss: 0.05574862286448479\n",
      "Iteration 22407, Loss: 0.05570849031209946\n",
      "Iteration 22408, Loss: 0.05569148063659668\n",
      "Iteration 22409, Loss: 0.05573364347219467\n",
      "Iteration 22410, Loss: 0.055677931755781174\n",
      "Iteration 22411, Loss: 0.05571838468313217\n",
      "Iteration 22412, Loss: 0.0557255744934082\n",
      "Iteration 22413, Loss: 0.05564836785197258\n",
      "Iteration 22414, Loss: 0.055676501244306564\n",
      "Iteration 22415, Loss: 0.05562857910990715\n",
      "Iteration 22416, Loss: 0.05571015924215317\n",
      "Iteration 22417, Loss: 0.055705390870571136\n",
      "Iteration 22418, Loss: 0.05564193055033684\n",
      "Iteration 22419, Loss: 0.05571095272898674\n",
      "Iteration 22420, Loss: 0.05565456673502922\n",
      "Iteration 22421, Loss: 0.05573336407542229\n",
      "Iteration 22422, Loss: 0.055776797235012054\n",
      "Iteration 22423, Loss: 0.05571945756673813\n",
      "Iteration 22424, Loss: 0.05566152185201645\n",
      "Iteration 22425, Loss: 0.055669307708740234\n",
      "Iteration 22426, Loss: 0.055688779801130295\n",
      "Iteration 22427, Loss: 0.055703483521938324\n",
      "Iteration 22428, Loss: 0.055625997483730316\n",
      "Iteration 22429, Loss: 0.055800002068281174\n",
      "Iteration 22430, Loss: 0.05581486225128174\n",
      "Iteration 22431, Loss: 0.05565707013010979\n",
      "Iteration 22432, Loss: 0.055806320160627365\n",
      "Iteration 22433, Loss: 0.05591984838247299\n",
      "Iteration 22434, Loss: 0.05592731758952141\n",
      "Iteration 22435, Loss: 0.055839262902736664\n",
      "Iteration 22436, Loss: 0.055666688829660416\n",
      "Iteration 22437, Loss: 0.05587152764201164\n",
      "Iteration 22438, Loss: 0.0560028962790966\n",
      "Iteration 22439, Loss: 0.055948060005903244\n",
      "Iteration 22440, Loss: 0.05572644993662834\n",
      "Iteration 22441, Loss: 0.05579876899719238\n",
      "Iteration 22442, Loss: 0.055951595306396484\n",
      "Iteration 22443, Loss: 0.05599479004740715\n",
      "Iteration 22444, Loss: 0.0559384860098362\n",
      "Iteration 22445, Loss: 0.055793289095163345\n",
      "Iteration 22446, Loss: 0.05566684529185295\n",
      "Iteration 22447, Loss: 0.055769167840480804\n",
      "Iteration 22448, Loss: 0.05568961426615715\n",
      "Iteration 22449, Loss: 0.055732011795043945\n",
      "Iteration 22450, Loss: 0.05580040067434311\n",
      "Iteration 22451, Loss: 0.05576781556010246\n",
      "Iteration 22452, Loss: 0.05564550682902336\n",
      "Iteration 22453, Loss: 0.05583890527486801\n",
      "Iteration 22454, Loss: 0.05591440573334694\n",
      "Iteration 22455, Loss: 0.05580858513712883\n",
      "Iteration 22456, Loss: 0.0556618794798851\n",
      "Iteration 22457, Loss: 0.05574604123830795\n",
      "Iteration 22458, Loss: 0.055728714913129807\n",
      "Iteration 22459, Loss: 0.05561995506286621\n",
      "Iteration 22460, Loss: 0.05585682764649391\n",
      "Iteration 22461, Loss: 0.05591794103384018\n",
      "Iteration 22462, Loss: 0.05580047890543938\n",
      "Iteration 22463, Loss: 0.05567558854818344\n",
      "Iteration 22464, Loss: 0.055767178535461426\n",
      "Iteration 22465, Loss: 0.05575728416442871\n",
      "Iteration 22466, Loss: 0.055654048919677734\n",
      "Iteration 22467, Loss: 0.05580409616231918\n",
      "Iteration 22468, Loss: 0.05586008355021477\n",
      "Iteration 22469, Loss: 0.05574115365743637\n",
      "Iteration 22470, Loss: 0.05571937933564186\n",
      "Iteration 22471, Loss: 0.05581049248576164\n",
      "Iteration 22472, Loss: 0.05579797551035881\n",
      "Iteration 22473, Loss: 0.05569136142730713\n",
      "Iteration 22474, Loss: 0.055757444351911545\n",
      "Iteration 22475, Loss: 0.0558173693716526\n",
      "Iteration 22476, Loss: 0.055702608078718185\n",
      "Iteration 22477, Loss: 0.055744968354701996\n",
      "Iteration 22478, Loss: 0.05583298206329346\n",
      "Iteration 22479, Loss: 0.05581653118133545\n",
      "Iteration 22480, Loss: 0.055706463754177094\n",
      "Iteration 22481, Loss: 0.055741071701049805\n",
      "Iteration 22482, Loss: 0.05580469220876694\n",
      "Iteration 22483, Loss: 0.05569314956665039\n",
      "Iteration 22484, Loss: 0.05575041100382805\n",
      "Iteration 22485, Loss: 0.05583711713552475\n",
      "Iteration 22486, Loss: 0.055819593369960785\n",
      "Iteration 22487, Loss: 0.05570944398641586\n",
      "Iteration 22488, Loss: 0.05573729798197746\n",
      "Iteration 22489, Loss: 0.05580103397369385\n",
      "Iteration 22490, Loss: 0.05568941682577133\n",
      "Iteration 22491, Loss: 0.055753789842128754\n",
      "Iteration 22492, Loss: 0.05584096908569336\n",
      "Iteration 22493, Loss: 0.05582448095083237\n",
      "Iteration 22494, Loss: 0.05571591854095459\n",
      "Iteration 22495, Loss: 0.0557272844016552\n",
      "Iteration 22496, Loss: 0.055789750069379807\n",
      "Iteration 22497, Loss: 0.05567598342895508\n",
      "Iteration 22498, Loss: 0.055764954537153244\n",
      "Iteration 22499, Loss: 0.055853407829999924\n",
      "Iteration 22500, Loss: 0.05583854764699936\n",
      "Iteration 22501, Loss: 0.05573161691427231\n",
      "Iteration 22502, Loss: 0.05570431798696518\n",
      "Iteration 22503, Loss: 0.055765192955732346\n",
      "Iteration 22504, Loss: 0.055649757385253906\n",
      "Iteration 22505, Loss: 0.055783987045288086\n",
      "Iteration 22506, Loss: 0.05587248131632805\n",
      "Iteration 22507, Loss: 0.055857859551906586\n",
      "Iteration 22508, Loss: 0.05575064942240715\n",
      "Iteration 22509, Loss: 0.05567781254649162\n",
      "Iteration 22510, Loss: 0.05573761835694313\n",
      "Iteration 22511, Loss: 0.05561983957886696\n",
      "Iteration 22512, Loss: 0.05580441281199455\n",
      "Iteration 22513, Loss: 0.055890679359436035\n",
      "Iteration 22514, Loss: 0.05587224289774895\n",
      "Iteration 22515, Loss: 0.05576034635305405\n",
      "Iteration 22516, Loss: 0.05567268654704094\n",
      "Iteration 22517, Loss: 0.05574365705251694\n",
      "Iteration 22518, Loss: 0.055648088455200195\n",
      "Iteration 22519, Loss: 0.05576241388916969\n",
      "Iteration 22520, Loss: 0.05582845211029053\n",
      "Iteration 22521, Loss: 0.05579324811697006\n",
      "Iteration 22522, Loss: 0.05566728487610817\n",
      "Iteration 22523, Loss: 0.05581303685903549\n",
      "Iteration 22524, Loss: 0.05589354410767555\n",
      "Iteration 22525, Loss: 0.05579499527812004\n",
      "Iteration 22526, Loss: 0.05566664785146713\n",
      "Iteration 22527, Loss: 0.05574759095907211\n",
      "Iteration 22528, Loss: 0.0557272844016552\n",
      "Iteration 22529, Loss: 0.05561431497335434\n",
      "Iteration 22530, Loss: 0.05586211010813713\n",
      "Iteration 22531, Loss: 0.05592767521739006\n",
      "Iteration 22532, Loss: 0.05582348629832268\n",
      "Iteration 22533, Loss: 0.055654726922512054\n",
      "Iteration 22534, Loss: 0.05576157942414284\n",
      "Iteration 22535, Loss: 0.05577322095632553\n",
      "Iteration 22536, Loss: 0.05567467212677002\n",
      "Iteration 22537, Loss: 0.05576809495687485\n",
      "Iteration 22538, Loss: 0.05582650750875473\n",
      "Iteration 22539, Loss: 0.05573177710175514\n",
      "Iteration 22540, Loss: 0.05570578575134277\n",
      "Iteration 22541, Loss: 0.05577770993113518\n",
      "Iteration 22542, Loss: 0.05573249235749245\n",
      "Iteration 22543, Loss: 0.05564634129405022\n",
      "Iteration 22544, Loss: 0.05566469952464104\n",
      "Iteration 22545, Loss: 0.05567856878042221\n",
      "Iteration 22546, Loss: 0.05567328259348869\n",
      "Iteration 22547, Loss: 0.05566596984863281\n",
      "Iteration 22548, Loss: 0.05563751980662346\n",
      "Iteration 22549, Loss: 0.05573471635580063\n",
      "Iteration 22550, Loss: 0.05576225370168686\n",
      "Iteration 22551, Loss: 0.05568957328796387\n",
      "Iteration 22552, Loss: 0.05571850389242172\n",
      "Iteration 22553, Loss: 0.0557452067732811\n",
      "Iteration 22554, Loss: 0.055617693811655045\n",
      "Iteration 22555, Loss: 0.05573054403066635\n",
      "Iteration 22556, Loss: 0.055745720863342285\n",
      "Iteration 22557, Loss: 0.055663906037807465\n",
      "Iteration 22558, Loss: 0.05576396360993385\n",
      "Iteration 22559, Loss: 0.05579487606883049\n",
      "Iteration 22560, Loss: 0.05565091222524643\n",
      "Iteration 22561, Loss: 0.05580437555909157\n",
      "Iteration 22562, Loss: 0.055910587310791016\n",
      "Iteration 22563, Loss: 0.055907849222421646\n",
      "Iteration 22564, Loss: 0.05580850690603256\n",
      "Iteration 22565, Loss: 0.05563986301422119\n",
      "Iteration 22566, Loss: 0.05587903782725334\n",
      "Iteration 22567, Loss: 0.05597333237528801\n",
      "Iteration 22568, Loss: 0.055885277688503265\n",
      "Iteration 22569, Loss: 0.055645428597927094\n",
      "Iteration 22570, Loss: 0.055852413177490234\n",
      "Iteration 22571, Loss: 0.05599534511566162\n",
      "Iteration 22572, Loss: 0.056028131395578384\n",
      "Iteration 22573, Loss: 0.05596121400594711\n",
      "Iteration 22574, Loss: 0.05580552667379379\n",
      "Iteration 22575, Loss: 0.055666688829660416\n",
      "Iteration 22576, Loss: 0.055791378021240234\n",
      "Iteration 22577, Loss: 0.055740635842084885\n",
      "Iteration 22578, Loss: 0.05567769333720207\n",
      "Iteration 22579, Loss: 0.05572911351919174\n",
      "Iteration 22580, Loss: 0.05568639561533928\n",
      "Iteration 22581, Loss: 0.05568826198577881\n",
      "Iteration 22582, Loss: 0.0556793212890625\n",
      "Iteration 22583, Loss: 0.05569183826446533\n",
      "Iteration 22584, Loss: 0.05571635812520981\n",
      "Iteration 22585, Loss: 0.055647414177656174\n",
      "Iteration 22586, Loss: 0.055770836770534515\n",
      "Iteration 22587, Loss: 0.05578657239675522\n",
      "Iteration 22588, Loss: 0.05563020706176758\n",
      "Iteration 22589, Loss: 0.05582106113433838\n",
      "Iteration 22590, Loss: 0.05592767521739006\n",
      "Iteration 22591, Loss: 0.05592461675405502\n",
      "Iteration 22592, Loss: 0.05582420155405998\n",
      "Iteration 22593, Loss: 0.05564924329519272\n",
      "Iteration 22594, Loss: 0.05588917061686516\n",
      "Iteration 22595, Loss: 0.056006792932748795\n",
      "Iteration 22596, Loss: 0.055938128381967545\n",
      "Iteration 22597, Loss: 0.055703919380903244\n",
      "Iteration 22598, Loss: 0.05582372471690178\n",
      "Iteration 22599, Loss: 0.05598382279276848\n",
      "Iteration 22600, Loss: 0.05603273957967758\n",
      "Iteration 22601, Loss: 0.055981121957302094\n",
      "Iteration 22602, Loss: 0.055840373039245605\n",
      "Iteration 22603, Loss: 0.05563036724925041\n",
      "Iteration 22604, Loss: 0.0559365376830101\n",
      "Iteration 22605, Loss: 0.056076209992170334\n",
      "Iteration 22606, Loss: 0.05603218451142311\n",
      "Iteration 22607, Loss: 0.055826108902692795\n",
      "Iteration 22608, Loss: 0.055718742311000824\n",
      "Iteration 22609, Loss: 0.055863380432128906\n",
      "Iteration 22610, Loss: 0.05590331554412842\n",
      "Iteration 22611, Loss: 0.055843036621809006\n",
      "Iteration 22612, Loss: 0.05569160357117653\n",
      "Iteration 22613, Loss: 0.05581057444214821\n",
      "Iteration 22614, Loss: 0.055919013917446136\n",
      "Iteration 22615, Loss: 0.05585070699453354\n",
      "Iteration 22616, Loss: 0.05564447492361069\n",
      "Iteration 22617, Loss: 0.05582742020487785\n",
      "Iteration 22618, Loss: 0.05593721196055412\n",
      "Iteration 22619, Loss: 0.055928509682416916\n",
      "Iteration 22620, Loss: 0.05581558123230934\n",
      "Iteration 22621, Loss: 0.05565345659852028\n",
      "Iteration 22622, Loss: 0.05583767220377922\n",
      "Iteration 22623, Loss: 0.05589672178030014\n",
      "Iteration 22624, Loss: 0.05578339099884033\n",
      "Iteration 22625, Loss: 0.05568595975637436\n",
      "Iteration 22626, Loss: 0.055773377418518066\n",
      "Iteration 22627, Loss: 0.0557582788169384\n",
      "Iteration 22628, Loss: 0.0556560754776001\n",
      "Iteration 22629, Loss: 0.055796507745981216\n",
      "Iteration 22630, Loss: 0.05584470555186272\n",
      "Iteration 22631, Loss: 0.055716078728437424\n",
      "Iteration 22632, Loss: 0.05574512481689453\n",
      "Iteration 22633, Loss: 0.05584295839071274\n",
      "Iteration 22634, Loss: 0.05583664029836655\n",
      "Iteration 22635, Loss: 0.05573725700378418\n",
      "Iteration 22636, Loss: 0.05568651482462883\n",
      "Iteration 22637, Loss: 0.05573856830596924\n",
      "Iteration 22638, Loss: 0.055618565529584885\n",
      "Iteration 22639, Loss: 0.055777471512556076\n",
      "Iteration 22640, Loss: 0.05583282560110092\n",
      "Iteration 22641, Loss: 0.05578172206878662\n",
      "Iteration 22642, Loss: 0.055647850036621094\n",
      "Iteration 22643, Loss: 0.055836956948041916\n",
      "Iteration 22644, Loss: 0.055908285081386566\n",
      "Iteration 22645, Loss: 0.055799126625061035\n",
      "Iteration 22646, Loss: 0.05567149445414543\n",
      "Iteration 22647, Loss: 0.05575748533010483\n",
      "Iteration 22648, Loss: 0.0557403564453125\n",
      "Iteration 22649, Loss: 0.05563608929514885\n",
      "Iteration 22650, Loss: 0.05582285299897194\n",
      "Iteration 22651, Loss: 0.055870138108730316\n",
      "Iteration 22652, Loss: 0.055740319192409515\n",
      "Iteration 22653, Loss: 0.05572807788848877\n",
      "Iteration 22654, Loss: 0.05582594871520996\n",
      "Iteration 22655, Loss: 0.05582066625356674\n",
      "Iteration 22656, Loss: 0.055722158402204514\n",
      "Iteration 22657, Loss: 0.055706582963466644\n",
      "Iteration 22658, Loss: 0.055756811052560806\n",
      "Iteration 22659, Loss: 0.05563334748148918\n",
      "Iteration 22660, Loss: 0.055795472115278244\n",
      "Iteration 22661, Loss: 0.05588134378194809\n",
      "Iteration 22662, Loss: 0.055862270295619965\n",
      "Iteration 22663, Loss: 0.05575017258524895\n",
      "Iteration 22664, Loss: 0.05568718910217285\n",
      "Iteration 22665, Loss: 0.055758439004421234\n",
      "Iteration 22666, Loss: 0.05565933510661125\n",
      "Iteration 22667, Loss: 0.055765748023986816\n",
      "Iteration 22668, Loss: 0.05584323778748512\n",
      "Iteration 22669, Loss: 0.055819034576416016\n",
      "Iteration 22670, Loss: 0.055704157799482346\n",
      "Iteration 22671, Loss: 0.05575057119131088\n",
      "Iteration 22672, Loss: 0.05581875890493393\n",
      "Iteration 22673, Loss: 0.05570729821920395\n",
      "Iteration 22674, Loss: 0.05573972314596176\n",
      "Iteration 22675, Loss: 0.0558270625770092\n",
      "Iteration 22676, Loss: 0.05581144616007805\n",
      "Iteration 22677, Loss: 0.05570396035909653\n",
      "Iteration 22678, Loss: 0.05574170872569084\n",
      "Iteration 22679, Loss: 0.055801473557949066\n",
      "Iteration 22680, Loss: 0.0556824617087841\n",
      "Iteration 22681, Loss: 0.055763129144907\n",
      "Iteration 22682, Loss: 0.0558551587164402\n",
      "Iteration 22683, Loss: 0.05584363266825676\n",
      "Iteration 22684, Loss: 0.05573924630880356\n",
      "Iteration 22685, Loss: 0.055689774453639984\n",
      "Iteration 22686, Loss: 0.05574623867869377\n",
      "Iteration 22687, Loss: 0.05562448501586914\n",
      "Iteration 22688, Loss: 0.05580707639455795\n",
      "Iteration 22689, Loss: 0.05589989945292473\n",
      "Iteration 22690, Loss: 0.05588917061686516\n",
      "Iteration 22691, Loss: 0.055784981697797775\n",
      "Iteration 22692, Loss: 0.055628299713134766\n",
      "Iteration 22693, Loss: 0.055700741708278656\n",
      "Iteration 22694, Loss: 0.05562679097056389\n",
      "Iteration 22695, Loss: 0.055693864822387695\n",
      "Iteration 22696, Loss: 0.05565337464213371\n",
      "Iteration 22697, Loss: 0.055730901658535004\n",
      "Iteration 22698, Loss: 0.055744051933288574\n",
      "Iteration 22699, Loss: 0.05563998594880104\n",
      "Iteration 22700, Loss: 0.05577341839671135\n",
      "Iteration 22701, Loss: 0.055801670998334885\n",
      "Iteration 22702, Loss: 0.05568420886993408\n",
      "Iteration 22703, Loss: 0.05576435849070549\n",
      "Iteration 22704, Loss: 0.05584625527262688\n",
      "Iteration 22705, Loss: 0.055801473557949066\n",
      "Iteration 22706, Loss: 0.0556614026427269\n",
      "Iteration 22707, Loss: 0.05581200495362282\n",
      "Iteration 22708, Loss: 0.0558902844786644\n",
      "Iteration 22709, Loss: 0.055808186531066895\n",
      "Iteration 22710, Loss: 0.055638156831264496\n",
      "Iteration 22711, Loss: 0.05573388189077377\n",
      "Iteration 22712, Loss: 0.055709801614284515\n",
      "Iteration 22713, Loss: 0.055658381432294846\n",
      "Iteration 22714, Loss: 0.05566374585032463\n",
      "Iteration 22715, Loss: 0.055683378130197525\n",
      "Iteration 22716, Loss: 0.05567256733775139\n",
      "Iteration 22717, Loss: 0.055680036544799805\n",
      "Iteration 22718, Loss: 0.05566827580332756\n",
      "Iteration 22719, Loss: 0.055694304406642914\n",
      "Iteration 22720, Loss: 0.055696528404951096\n",
      "Iteration 22721, Loss: 0.05564828962087631\n",
      "Iteration 22722, Loss: 0.05566052719950676\n",
      "Iteration 22723, Loss: 0.0556766614317894\n",
      "Iteration 22724, Loss: 0.05566509813070297\n",
      "Iteration 22725, Loss: 0.05567840859293938\n",
      "Iteration 22726, Loss: 0.055648885667324066\n",
      "Iteration 22727, Loss: 0.05572740361094475\n",
      "Iteration 22728, Loss: 0.05575525760650635\n",
      "Iteration 22729, Loss: 0.05568309873342514\n",
      "Iteration 22730, Loss: 0.05572553724050522\n",
      "Iteration 22731, Loss: 0.055750492960214615\n",
      "Iteration 22732, Loss: 0.05561598390340805\n",
      "Iteration 22733, Loss: 0.05578402802348137\n",
      "Iteration 22734, Loss: 0.05584987252950668\n",
      "Iteration 22735, Loss: 0.05581490322947502\n",
      "Iteration 22736, Loss: 0.05568961426615715\n",
      "Iteration 22737, Loss: 0.05578259751200676\n",
      "Iteration 22738, Loss: 0.05586203187704086\n",
      "Iteration 22739, Loss: 0.05575951188802719\n",
      "Iteration 22740, Loss: 0.05569545552134514\n",
      "Iteration 22741, Loss: 0.05577763170003891\n",
      "Iteration 22742, Loss: 0.0557578019797802\n",
      "Iteration 22743, Loss: 0.05564677715301514\n",
      "Iteration 22744, Loss: 0.05582372471690178\n",
      "Iteration 22745, Loss: 0.055887263268232346\n",
      "Iteration 22746, Loss: 0.055770955979824066\n",
      "Iteration 22747, Loss: 0.05569656938314438\n",
      "Iteration 22748, Loss: 0.0557868517935276\n",
      "Iteration 22749, Loss: 0.05577465146780014\n",
      "Iteration 22750, Loss: 0.055670302361249924\n",
      "Iteration 22751, Loss: 0.05578351393342018\n",
      "Iteration 22752, Loss: 0.05583985894918442\n",
      "Iteration 22753, Loss: 0.05571810528635979\n",
      "Iteration 22754, Loss: 0.05573856830596924\n",
      "Iteration 22755, Loss: 0.05583183094859123\n",
      "Iteration 22756, Loss: 0.0558217391371727\n",
      "Iteration 22757, Loss: 0.05571870133280754\n",
      "Iteration 22758, Loss: 0.05571611970663071\n",
      "Iteration 22759, Loss: 0.05577119439840317\n",
      "Iteration 22760, Loss: 0.05564916133880615\n",
      "Iteration 22761, Loss: 0.0557890348136425\n",
      "Iteration 22762, Loss: 0.05588154122233391\n",
      "Iteration 22763, Loss: 0.055870335549116135\n",
      "Iteration 22764, Loss: 0.05576598644256592\n",
      "Iteration 22765, Loss: 0.05565480515360832\n",
      "Iteration 22766, Loss: 0.05571778863668442\n",
      "Iteration 22767, Loss: 0.05562448501586914\n",
      "Iteration 22768, Loss: 0.055719416588544846\n",
      "Iteration 22769, Loss: 0.05571592226624489\n",
      "Iteration 22770, Loss: 0.05562921613454819\n",
      "Iteration 22771, Loss: 0.05572998896241188\n",
      "Iteration 22772, Loss: 0.05567840859293938\n",
      "Iteration 22773, Loss: 0.05572116747498512\n",
      "Iteration 22774, Loss: 0.05577174946665764\n",
      "Iteration 22775, Loss: 0.05572390556335449\n",
      "Iteration 22776, Loss: 0.05564272403717041\n",
      "Iteration 22777, Loss: 0.05565265938639641\n",
      "Iteration 22778, Loss: 0.05568806454539299\n",
      "Iteration 22779, Loss: 0.05568993091583252\n",
      "Iteration 22780, Loss: 0.05563163757324219\n",
      "Iteration 22781, Loss: 0.0556362085044384\n",
      "Iteration 22782, Loss: 0.0556618794798851\n",
      "Iteration 22783, Loss: 0.05563036724925041\n",
      "Iteration 22784, Loss: 0.05564844608306885\n",
      "Iteration 22785, Loss: 0.05566462129354477\n",
      "Iteration 22786, Loss: 0.05564252659678459\n",
      "Iteration 22787, Loss: 0.055720966309309006\n",
      "Iteration 22788, Loss: 0.05569116398692131\n",
      "Iteration 22789, Loss: 0.05569561570882797\n",
      "Iteration 22790, Loss: 0.05573121830821037\n",
      "Iteration 22791, Loss: 0.05566704273223877\n",
      "Iteration 22792, Loss: 0.05573813244700432\n",
      "Iteration 22793, Loss: 0.05575311556458473\n",
      "Iteration 22794, Loss: 0.05562273785471916\n",
      "Iteration 22795, Loss: 0.05569903180003166\n",
      "Iteration 22796, Loss: 0.05565818399190903\n",
      "Iteration 22797, Loss: 0.05572470277547836\n",
      "Iteration 22798, Loss: 0.055736541748046875\n",
      "Iteration 22799, Loss: 0.05563219636678696\n",
      "Iteration 22800, Loss: 0.05576237291097641\n",
      "Iteration 22801, Loss: 0.05576622486114502\n",
      "Iteration 22802, Loss: 0.05563068389892578\n",
      "Iteration 22803, Loss: 0.05580202862620354\n",
      "Iteration 22804, Loss: 0.05586064234375954\n",
      "Iteration 22805, Loss: 0.05578279495239258\n",
      "Iteration 22806, Loss: 0.05565381422638893\n",
      "Iteration 22807, Loss: 0.05576431751251221\n",
      "Iteration 22808, Loss: 0.05575470253825188\n",
      "Iteration 22809, Loss: 0.05562834069132805\n",
      "Iteration 22810, Loss: 0.05567046254873276\n",
      "Iteration 22811, Loss: 0.05563461780548096\n",
      "Iteration 22812, Loss: 0.055738769471645355\n",
      "Iteration 22813, Loss: 0.055711787194013596\n",
      "Iteration 22814, Loss: 0.05567944049835205\n",
      "Iteration 22815, Loss: 0.05571524426341057\n",
      "Iteration 22816, Loss: 0.05565401166677475\n",
      "Iteration 22817, Loss: 0.055753789842128754\n",
      "Iteration 22818, Loss: 0.05576590821146965\n",
      "Iteration 22819, Loss: 0.05562758445739746\n",
      "Iteration 22820, Loss: 0.05576535314321518\n",
      "Iteration 22821, Loss: 0.05580143257975578\n",
      "Iteration 22822, Loss: 0.05572279542684555\n",
      "Iteration 22823, Loss: 0.05569025129079819\n",
      "Iteration 22824, Loss: 0.05573737621307373\n",
      "Iteration 22825, Loss: 0.05563676357269287\n",
      "Iteration 22826, Loss: 0.055788759142160416\n",
      "Iteration 22827, Loss: 0.055862586945295334\n",
      "Iteration 22828, Loss: 0.05582750216126442\n",
      "Iteration 22829, Loss: 0.055703721940517426\n",
      "Iteration 22830, Loss: 0.055762652307748795\n",
      "Iteration 22831, Loss: 0.05584061145782471\n",
      "Iteration 22832, Loss: 0.055750928819179535\n",
      "Iteration 22833, Loss: 0.05569358915090561\n",
      "Iteration 22834, Loss: 0.05576543137431145\n",
      "Iteration 22835, Loss: 0.05573348328471184\n",
      "Iteration 22836, Loss: 0.055640459060668945\n",
      "Iteration 22837, Loss: 0.05576205253601074\n",
      "Iteration 22838, Loss: 0.05574576184153557\n",
      "Iteration 22839, Loss: 0.05565444752573967\n",
      "Iteration 22840, Loss: 0.05569712445139885\n",
      "Iteration 22841, Loss: 0.05565699189901352\n",
      "Iteration 22842, Loss: 0.05572736635804176\n",
      "Iteration 22843, Loss: 0.055719099938869476\n",
      "Iteration 22844, Loss: 0.05566120147705078\n",
      "Iteration 22845, Loss: 0.05568993464112282\n",
      "Iteration 22846, Loss: 0.05562969297170639\n",
      "Iteration 22847, Loss: 0.05575625225901604\n",
      "Iteration 22848, Loss: 0.055771272629499435\n",
      "Iteration 22849, Loss: 0.055673837661743164\n",
      "Iteration 22850, Loss: 0.05575382709503174\n",
      "Iteration 22851, Loss: 0.05580183118581772\n",
      "Iteration 22852, Loss: 0.05570463463664055\n",
      "Iteration 22853, Loss: 0.055732373148202896\n",
      "Iteration 22854, Loss: 0.0558011569082737\n",
      "Iteration 22855, Loss: 0.055745482444763184\n",
      "Iteration 22856, Loss: 0.05565723031759262\n",
      "Iteration 22857, Loss: 0.055729154497385025\n",
      "Iteration 22858, Loss: 0.05567634105682373\n",
      "Iteration 22859, Loss: 0.055725060403347015\n",
      "Iteration 22860, Loss: 0.05576968193054199\n",
      "Iteration 22861, Loss: 0.05571603775024414\n",
      "Iteration 22862, Loss: 0.05567149445414543\n",
      "Iteration 22863, Loss: 0.055703483521938324\n",
      "Iteration 22864, Loss: 0.05565440654754639\n",
      "Iteration 22865, Loss: 0.05568067356944084\n",
      "Iteration 22866, Loss: 0.05566263198852539\n",
      "Iteration 22867, Loss: 0.05569521710276604\n",
      "Iteration 22868, Loss: 0.05566593259572983\n",
      "Iteration 22869, Loss: 0.055706821382045746\n",
      "Iteration 22870, Loss: 0.055729787796735764\n",
      "Iteration 22871, Loss: 0.05564642325043678\n",
      "Iteration 22872, Loss: 0.05578657239675522\n",
      "Iteration 22873, Loss: 0.055823247879743576\n",
      "Iteration 22874, Loss: 0.05569565296173096\n",
      "Iteration 22875, Loss: 0.05575935170054436\n",
      "Iteration 22876, Loss: 0.05585205927491188\n",
      "Iteration 22877, Loss: 0.0558273009955883\n",
      "Iteration 22878, Loss: 0.055701375007629395\n",
      "Iteration 22879, Loss: 0.0557638444006443\n",
      "Iteration 22880, Loss: 0.05584407225251198\n",
      "Iteration 22881, Loss: 0.055753111839294434\n",
      "Iteration 22882, Loss: 0.055689893662929535\n",
      "Iteration 22883, Loss: 0.055762410163879395\n",
      "Iteration 22884, Loss: 0.05572330951690674\n",
      "Iteration 22885, Loss: 0.05564491078257561\n",
      "Iteration 22886, Loss: 0.05567995831370354\n",
      "Iteration 22887, Loss: 0.05565552040934563\n",
      "Iteration 22888, Loss: 0.05565401166677475\n",
      "Iteration 22889, Loss: 0.05566927045583725\n",
      "Iteration 22890, Loss: 0.05563632771372795\n",
      "Iteration 22891, Loss: 0.055677734315395355\n",
      "Iteration 22892, Loss: 0.055644869804382324\n",
      "Iteration 22893, Loss: 0.05573348328471184\n",
      "Iteration 22894, Loss: 0.05573121830821037\n",
      "Iteration 22895, Loss: 0.05564216896891594\n",
      "Iteration 22896, Loss: 0.05569394677877426\n",
      "Iteration 22897, Loss: 0.05563442036509514\n",
      "Iteration 22898, Loss: 0.055763524025678635\n",
      "Iteration 22899, Loss: 0.05580413341522217\n",
      "Iteration 22900, Loss: 0.05573121830821037\n",
      "Iteration 22901, Loss: 0.055681150406599045\n",
      "Iteration 22902, Loss: 0.05573336407542229\n",
      "Iteration 22903, Loss: 0.05565281957387924\n",
      "Iteration 22904, Loss: 0.055752675980329514\n",
      "Iteration 22905, Loss: 0.05580652132630348\n",
      "Iteration 22906, Loss: 0.05575907230377197\n",
      "Iteration 22907, Loss: 0.055646900087594986\n",
      "Iteration 22908, Loss: 0.05580226704478264\n",
      "Iteration 22909, Loss: 0.055830877274274826\n",
      "Iteration 22910, Loss: 0.055690567940473557\n",
      "Iteration 22911, Loss: 0.05576936528086662\n",
      "Iteration 22912, Loss: 0.0558701753616333\n",
      "Iteration 22913, Loss: 0.05586683750152588\n",
      "Iteration 22914, Loss: 0.05576936528086662\n",
      "Iteration 22915, Loss: 0.05564161390066147\n",
      "Iteration 22916, Loss: 0.0556923970580101\n",
      "Iteration 22917, Loss: 0.05564232915639877\n",
      "Iteration 22918, Loss: 0.055640220642089844\n",
      "Iteration 22919, Loss: 0.05569645017385483\n",
      "Iteration 22920, Loss: 0.05564912408590317\n",
      "Iteration 22921, Loss: 0.05573388189077377\n",
      "Iteration 22922, Loss: 0.055766940116882324\n",
      "Iteration 22923, Loss: 0.05569136142730713\n",
      "Iteration 22924, Loss: 0.05571961775422096\n",
      "Iteration 22925, Loss: 0.05575351044535637\n",
      "Iteration 22926, Loss: 0.05562957376241684\n",
      "Iteration 22927, Loss: 0.055814746767282486\n",
      "Iteration 22928, Loss: 0.05590256303548813\n",
      "Iteration 22929, Loss: 0.055872559547424316\n",
      "Iteration 22930, Loss: 0.05574552342295647\n",
      "Iteration 22931, Loss: 0.05572100728750229\n",
      "Iteration 22932, Loss: 0.05581235885620117\n",
      "Iteration 22933, Loss: 0.05575283616781235\n",
      "Iteration 22934, Loss: 0.05567511171102524\n",
      "Iteration 22935, Loss: 0.05572641268372536\n",
      "Iteration 22936, Loss: 0.055688463151454926\n",
      "Iteration 22937, Loss: 0.05568544194102287\n",
      "Iteration 22938, Loss: 0.055685363709926605\n",
      "Iteration 22939, Loss: 0.05568099394440651\n",
      "Iteration 22940, Loss: 0.055692076683044434\n",
      "Iteration 22941, Loss: 0.055647850036621094\n",
      "Iteration 22942, Loss: 0.05571528524160385\n",
      "Iteration 22943, Loss: 0.055673759430646896\n",
      "Iteration 22944, Loss: 0.05571286007761955\n",
      "Iteration 22945, Loss: 0.055750809609889984\n",
      "Iteration 22946, Loss: 0.055687908083200455\n",
      "Iteration 22947, Loss: 0.05571095272898674\n",
      "Iteration 22948, Loss: 0.05572458356618881\n",
      "Iteration 22949, Loss: 0.05564352124929428\n",
      "Iteration 22950, Loss: 0.05566096305847168\n",
      "Iteration 22951, Loss: 0.055652182549238205\n",
      "Iteration 22952, Loss: 0.05563767999410629\n",
      "Iteration 22953, Loss: 0.05564352124929428\n",
      "Iteration 22954, Loss: 0.05567006394267082\n",
      "Iteration 22955, Loss: 0.055644355714321136\n",
      "Iteration 22956, Loss: 0.05571683496236801\n",
      "Iteration 22957, Loss: 0.05568981170654297\n",
      "Iteration 22958, Loss: 0.05569370836019516\n",
      "Iteration 22959, Loss: 0.05572474002838135\n",
      "Iteration 22960, Loss: 0.055654171854257584\n",
      "Iteration 22961, Loss: 0.055759988725185394\n",
      "Iteration 22962, Loss: 0.0557808093726635\n",
      "Iteration 22963, Loss: 0.05563759803771973\n",
      "Iteration 22964, Loss: 0.0558115653693676\n",
      "Iteration 22965, Loss: 0.055908720940351486\n",
      "Iteration 22966, Loss: 0.05589306354522705\n",
      "Iteration 22967, Loss: 0.055779580026865005\n",
      "Iteration 22968, Loss: 0.05567034333944321\n",
      "Iteration 22969, Loss: 0.055782120674848557\n",
      "Iteration 22970, Loss: 0.05575240030884743\n",
      "Iteration 22971, Loss: 0.055663906037807465\n",
      "Iteration 22972, Loss: 0.05571119114756584\n",
      "Iteration 22973, Loss: 0.05568842217326164\n",
      "Iteration 22974, Loss: 0.055671337991952896\n",
      "Iteration 22975, Loss: 0.05566795915365219\n",
      "Iteration 22976, Loss: 0.05567844957113266\n",
      "Iteration 22977, Loss: 0.05567976087331772\n",
      "Iteration 22978, Loss: 0.055650196969509125\n",
      "Iteration 22979, Loss: 0.055659376084804535\n",
      "Iteration 22980, Loss: 0.055649083107709885\n",
      "Iteration 22981, Loss: 0.055660128593444824\n",
      "Iteration 22982, Loss: 0.055655959993600845\n",
      "Iteration 22983, Loss: 0.05564260482788086\n",
      "Iteration 22984, Loss: 0.055683694779872894\n",
      "Iteration 22985, Loss: 0.055624645203351974\n",
      "Iteration 22986, Loss: 0.05572851747274399\n",
      "Iteration 22987, Loss: 0.055741190910339355\n",
      "Iteration 22988, Loss: 0.055653177201747894\n",
      "Iteration 22989, Loss: 0.05578029155731201\n",
      "Iteration 22990, Loss: 0.055815380066633224\n",
      "Iteration 22991, Loss: 0.0556795597076416\n",
      "Iteration 22992, Loss: 0.05577854439616203\n",
      "Iteration 22993, Loss: 0.055878639221191406\n",
      "Iteration 22994, Loss: 0.05586544796824455\n",
      "Iteration 22995, Loss: 0.055753789842128754\n",
      "Iteration 22996, Loss: 0.05568762868642807\n",
      "Iteration 22997, Loss: 0.05576559156179428\n",
      "Iteration 22998, Loss: 0.05568305775523186\n",
      "Iteration 22999, Loss: 0.055739760398864746\n",
      "Iteration 23000, Loss: 0.05580548569560051\n",
      "Iteration 23001, Loss: 0.055770717561244965\n",
      "Iteration 23002, Loss: 0.05565548315644264\n",
      "Iteration 23003, Loss: 0.055813830345869064\n",
      "Iteration 23004, Loss: 0.055872559547424316\n",
      "Iteration 23005, Loss: 0.055753789842128754\n",
      "Iteration 23006, Loss: 0.05571194738149643\n",
      "Iteration 23007, Loss: 0.05580330267548561\n",
      "Iteration 23008, Loss: 0.0557934045791626\n",
      "Iteration 23009, Loss: 0.055691998451948166\n",
      "Iteration 23010, Loss: 0.055751923471689224\n",
      "Iteration 23011, Loss: 0.055804651230573654\n",
      "Iteration 23012, Loss: 0.05568297952413559\n",
      "Iteration 23013, Loss: 0.05576348677277565\n",
      "Iteration 23014, Loss: 0.05585511773824692\n",
      "Iteration 23015, Loss: 0.05584319680929184\n",
      "Iteration 23016, Loss: 0.055738214403390884\n",
      "Iteration 23017, Loss: 0.055692434310913086\n",
      "Iteration 23018, Loss: 0.05575060844421387\n",
      "Iteration 23019, Loss: 0.055632077157497406\n",
      "Iteration 23020, Loss: 0.05579960346221924\n",
      "Iteration 23021, Loss: 0.05588948726654053\n",
      "Iteration 23022, Loss: 0.05587538331747055\n",
      "Iteration 23023, Loss: 0.0557679757475853\n",
      "Iteration 23024, Loss: 0.055658500641584396\n",
      "Iteration 23025, Loss: 0.05573495477437973\n",
      "Iteration 23026, Loss: 0.055649518966674805\n",
      "Iteration 23027, Loss: 0.055754780769348145\n",
      "Iteration 23028, Loss: 0.05581442639231682\n",
      "Iteration 23029, Loss: 0.05577167123556137\n",
      "Iteration 23030, Loss: 0.055638473480939865\n",
      "Iteration 23031, Loss: 0.055859725922346115\n",
      "Iteration 23032, Loss: 0.05594515800476074\n",
      "Iteration 23033, Loss: 0.05584748834371567\n",
      "Iteration 23034, Loss: 0.05562862008810043\n",
      "Iteration 23035, Loss: 0.055714886635541916\n",
      "Iteration 23036, Loss: 0.055697761476039886\n",
      "Iteration 23037, Loss: 0.055644433945417404\n",
      "Iteration 23038, Loss: 0.05562388896942139\n",
      "Iteration 23039, Loss: 0.0557200126349926\n",
      "Iteration 23040, Loss: 0.05572919175028801\n",
      "Iteration 23041, Loss: 0.05564320087432861\n",
      "Iteration 23042, Loss: 0.05579102411866188\n",
      "Iteration 23043, Loss: 0.055822812020778656\n",
      "Iteration 23044, Loss: 0.05568234249949455\n",
      "Iteration 23045, Loss: 0.055777549743652344\n",
      "Iteration 23046, Loss: 0.05588134378194809\n",
      "Iteration 23047, Loss: 0.055878639221191406\n",
      "Iteration 23048, Loss: 0.05578120797872543\n",
      "Iteration 23049, Loss: 0.055636607110500336\n",
      "Iteration 23050, Loss: 0.05576956272125244\n",
      "Iteration 23051, Loss: 0.05574214830994606\n",
      "Iteration 23052, Loss: 0.05566314980387688\n",
      "Iteration 23053, Loss: 0.0557069405913353\n",
      "Iteration 23054, Loss: 0.055663708597421646\n",
      "Iteration 23055, Loss: 0.055723510682582855\n",
      "Iteration 23056, Loss: 0.055718064308166504\n",
      "Iteration 23057, Loss: 0.055660925805568695\n",
      "Iteration 23058, Loss: 0.055688463151454926\n",
      "Iteration 23059, Loss: 0.05562913790345192\n",
      "Iteration 23060, Loss: 0.05575382709503174\n",
      "Iteration 23061, Loss: 0.05576428025960922\n",
      "Iteration 23062, Loss: 0.05566521733999252\n",
      "Iteration 23063, Loss: 0.055760227143764496\n",
      "Iteration 23064, Loss: 0.05580354109406471\n",
      "Iteration 23065, Loss: 0.05570077896118164\n",
      "Iteration 23066, Loss: 0.055740080773830414\n",
      "Iteration 23067, Loss: 0.055813275277614594\n",
      "Iteration 23068, Loss: 0.05576149746775627\n",
      "Iteration 23069, Loss: 0.05564912408590317\n",
      "Iteration 23070, Loss: 0.05576368421316147\n",
      "Iteration 23071, Loss: 0.05575704947113991\n",
      "Iteration 23072, Loss: 0.05563259497284889\n",
      "Iteration 23073, Loss: 0.0556797981262207\n",
      "Iteration 23074, Loss: 0.05564769357442856\n",
      "Iteration 23075, Loss: 0.055725496262311935\n",
      "Iteration 23076, Loss: 0.05570173263549805\n",
      "Iteration 23077, Loss: 0.05568520352244377\n",
      "Iteration 23078, Loss: 0.05571878328919411\n",
      "Iteration 23079, Loss: 0.05565377324819565\n",
      "Iteration 23080, Loss: 0.055758796632289886\n",
      "Iteration 23081, Loss: 0.05577317997813225\n",
      "Iteration 23082, Loss: 0.05562484636902809\n",
      "Iteration 23083, Loss: 0.05579773709177971\n",
      "Iteration 23084, Loss: 0.05586715787649155\n",
      "Iteration 23085, Loss: 0.05581919476389885\n",
      "Iteration 23086, Loss: 0.055679045617580414\n",
      "Iteration 23087, Loss: 0.05580548569560051\n",
      "Iteration 23088, Loss: 0.05589306354522705\n",
      "Iteration 23089, Loss: 0.05580727383494377\n",
      "Iteration 23090, Loss: 0.05564483255147934\n",
      "Iteration 23091, Loss: 0.05571802705526352\n",
      "Iteration 23092, Loss: 0.05567936226725578\n",
      "Iteration 23093, Loss: 0.05569612979888916\n",
      "Iteration 23094, Loss: 0.055692557245492935\n",
      "Iteration 23095, Loss: 0.05567359924316406\n",
      "Iteration 23096, Loss: 0.055686794221401215\n",
      "Iteration 23097, Loss: 0.05563298985362053\n",
      "Iteration 23098, Loss: 0.055655281990766525\n",
      "Iteration 23099, Loss: 0.05565909668803215\n",
      "Iteration 23100, Loss: 0.05563688278198242\n",
      "Iteration 23101, Loss: 0.05572923272848129\n",
      "Iteration 23102, Loss: 0.05570578575134277\n",
      "Iteration 23103, Loss: 0.05567733570933342\n",
      "Iteration 23104, Loss: 0.05570634454488754\n",
      "Iteration 23105, Loss: 0.05563080310821533\n",
      "Iteration 23106, Loss: 0.05579789727926254\n",
      "Iteration 23107, Loss: 0.055830519646406174\n",
      "Iteration 23108, Loss: 0.05570650473237038\n",
      "Iteration 23109, Loss: 0.05574715510010719\n",
      "Iteration 23110, Loss: 0.055836599320173264\n",
      "Iteration 23111, Loss: 0.05581120774149895\n",
      "Iteration 23112, Loss: 0.05568433180451393\n",
      "Iteration 23113, Loss: 0.055783990770578384\n",
      "Iteration 23114, Loss: 0.05586552619934082\n",
      "Iteration 23115, Loss: 0.055781882256269455\n",
      "Iteration 23116, Loss: 0.055661045014858246\n",
      "Iteration 23117, Loss: 0.05573507398366928\n",
      "Iteration 23118, Loss: 0.055698197335004807\n",
      "Iteration 23119, Loss: 0.05567654222249985\n",
      "Iteration 23120, Loss: 0.055677853524684906\n",
      "Iteration 23121, Loss: 0.05567880719900131\n",
      "Iteration 23122, Loss: 0.05568317696452141\n",
      "Iteration 23123, Loss: 0.055649518966674805\n",
      "Iteration 23124, Loss: 0.05562150478363037\n",
      "Iteration 23125, Loss: 0.05573181435465813\n",
      "Iteration 23126, Loss: 0.0557536706328392\n",
      "Iteration 23127, Loss: 0.05567999929189682\n",
      "Iteration 23128, Loss: 0.055734120309352875\n",
      "Iteration 23129, Loss: 0.05575677007436752\n",
      "Iteration 23130, Loss: 0.055615585297346115\n",
      "Iteration 23131, Loss: 0.05562349408864975\n",
      "Iteration 23132, Loss: 0.05571182817220688\n",
      "Iteration 23133, Loss: 0.05564817041158676\n",
      "Iteration 23134, Loss: 0.05575176328420639\n",
      "Iteration 23135, Loss: 0.055807553231716156\n",
      "Iteration 23136, Loss: 0.05576161667704582\n",
      "Iteration 23137, Loss: 0.055632393807172775\n",
      "Iteration 23138, Loss: 0.05584804341197014\n",
      "Iteration 23139, Loss: 0.0559111051261425\n",
      "Iteration 23140, Loss: 0.05579400062561035\n",
      "Iteration 23141, Loss: 0.05568110942840576\n",
      "Iteration 23142, Loss: 0.055771950632333755\n",
      "Iteration 23143, Loss: 0.055760107934474945\n",
      "Iteration 23144, Loss: 0.05565623566508293\n",
      "Iteration 23145, Loss: 0.05580341815948486\n",
      "Iteration 23146, Loss: 0.05585896968841553\n",
      "Iteration 23147, Loss: 0.055735114961862564\n",
      "Iteration 23148, Loss: 0.05572839826345444\n",
      "Iteration 23149, Loss: 0.05582320690155029\n",
      "Iteration 23150, Loss: 0.05581434816122055\n",
      "Iteration 23151, Loss: 0.055712584406137466\n",
      "Iteration 23152, Loss: 0.05572422593832016\n",
      "Iteration 23153, Loss: 0.055777668952941895\n",
      "Iteration 23154, Loss: 0.05565293878316879\n",
      "Iteration 23155, Loss: 0.055789314210414886\n",
      "Iteration 23156, Loss: 0.05588420480489731\n",
      "Iteration 23157, Loss: 0.055875264108181\n",
      "Iteration 23158, Loss: 0.05577290430665016\n",
      "Iteration 23159, Loss: 0.05564292520284653\n",
      "Iteration 23160, Loss: 0.05569732189178467\n",
      "Iteration 23161, Loss: 0.0556383952498436\n",
      "Iteration 23162, Loss: 0.05562989041209221\n",
      "Iteration 23163, Loss: 0.0557200126349926\n",
      "Iteration 23164, Loss: 0.055673323571681976\n",
      "Iteration 23165, Loss: 0.05572104454040527\n",
      "Iteration 23166, Loss: 0.055765870958566666\n",
      "Iteration 23167, Loss: 0.05570940300822258\n",
      "Iteration 23168, Loss: 0.055675309151411057\n",
      "Iteration 23169, Loss: 0.05568631738424301\n",
      "Iteration 23170, Loss: 0.055673837661743164\n",
      "Iteration 23171, Loss: 0.0556846484541893\n",
      "Iteration 23172, Loss: 0.055632948875427246\n",
      "Iteration 23173, Loss: 0.05568520352244377\n",
      "Iteration 23174, Loss: 0.05563811585307121\n",
      "Iteration 23175, Loss: 0.055711232125759125\n",
      "Iteration 23176, Loss: 0.05568794533610344\n",
      "Iteration 23177, Loss: 0.05568770691752434\n",
      "Iteration 23178, Loss: 0.055700622498989105\n",
      "Iteration 23179, Loss: 0.05563684552907944\n",
      "Iteration 23180, Loss: 0.05567256733775139\n",
      "Iteration 23181, Loss: 0.055650632828474045\n",
      "Iteration 23182, Loss: 0.055642127990722656\n",
      "Iteration 23183, Loss: 0.05568826571106911\n",
      "Iteration 23184, Loss: 0.05562726780772209\n",
      "Iteration 23185, Loss: 0.05574500933289528\n",
      "Iteration 23186, Loss: 0.055771589279174805\n",
      "Iteration 23187, Loss: 0.05569307133555412\n",
      "Iteration 23188, Loss: 0.05572374910116196\n",
      "Iteration 23189, Loss: 0.05575641244649887\n",
      "Iteration 23190, Loss: 0.05562357231974602\n",
      "Iteration 23191, Loss: 0.05582614988088608\n",
      "Iteration 23192, Loss: 0.055924177169799805\n",
      "Iteration 23193, Loss: 0.055905781686306\n",
      "Iteration 23194, Loss: 0.05578768253326416\n",
      "Iteration 23195, Loss: 0.055673204362392426\n",
      "Iteration 23196, Loss: 0.055795155465602875\n",
      "Iteration 23197, Loss: 0.0557783842086792\n",
      "Iteration 23198, Loss: 0.055655598640441895\n",
      "Iteration 23199, Loss: 0.05573427677154541\n",
      "Iteration 23200, Loss: 0.055757444351911545\n",
      "Iteration 23201, Loss: 0.05568774789571762\n",
      "Iteration 23202, Loss: 0.05572589486837387\n",
      "Iteration 23203, Loss: 0.055747948586940765\n",
      "Iteration 23204, Loss: 0.05564018338918686\n",
      "Iteration 23205, Loss: 0.05573463439941406\n",
      "Iteration 23206, Loss: 0.05574445053935051\n",
      "Iteration 23207, Loss: 0.05563895031809807\n",
      "Iteration 23208, Loss: 0.05581033229827881\n",
      "Iteration 23209, Loss: 0.05586743354797363\n",
      "Iteration 23210, Loss: 0.055769722908735275\n",
      "Iteration 23211, Loss: 0.05568119138479233\n",
      "Iteration 23212, Loss: 0.05576225370168686\n",
      "Iteration 23213, Loss: 0.05572975054383278\n",
      "Iteration 23214, Loss: 0.05564336106181145\n",
      "Iteration 23215, Loss: 0.055647533386945724\n",
      "Iteration 23216, Loss: 0.05569823831319809\n",
      "Iteration 23217, Loss: 0.05568611994385719\n",
      "Iteration 23218, Loss: 0.05567006394267082\n",
      "Iteration 23219, Loss: 0.05566227808594704\n",
      "Iteration 23220, Loss: 0.05569978803396225\n",
      "Iteration 23221, Loss: 0.05570022389292717\n",
      "Iteration 23222, Loss: 0.05564892664551735\n",
      "Iteration 23223, Loss: 0.05567260831594467\n",
      "Iteration 23224, Loss: 0.055659692734479904\n",
      "Iteration 23225, Loss: 0.055650316178798676\n",
      "Iteration 23226, Loss: 0.05567797273397446\n",
      "Iteration 23227, Loss: 0.055628061294555664\n",
      "Iteration 23228, Loss: 0.055724743753671646\n",
      "Iteration 23229, Loss: 0.05574154853820801\n",
      "Iteration 23230, Loss: 0.05566156283020973\n",
      "Iteration 23231, Loss: 0.05576769635081291\n",
      "Iteration 23232, Loss: 0.05579721927642822\n",
      "Iteration 23233, Loss: 0.05565190315246582\n",
      "Iteration 23234, Loss: 0.0558062419295311\n",
      "Iteration 23235, Loss: 0.05591293424367905\n",
      "Iteration 23236, Loss: 0.05591090768575668\n",
      "Iteration 23237, Loss: 0.0558118037879467\n",
      "Iteration 23238, Loss: 0.05564185231924057\n",
      "Iteration 23239, Loss: 0.05588563531637192\n",
      "Iteration 23240, Loss: 0.05598723888397217\n",
      "Iteration 23241, Loss: 0.055905066430568695\n",
      "Iteration 23242, Loss: 0.055664025247097015\n",
      "Iteration 23243, Loss: 0.0558546781539917\n",
      "Iteration 23244, Loss: 0.056013189256191254\n",
      "Iteration 23245, Loss: 0.0560610294342041\n",
      "Iteration 23246, Loss: 0.05600861832499504\n",
      "Iteration 23247, Loss: 0.05586652085185051\n",
      "Iteration 23248, Loss: 0.05564514920115471\n",
      "Iteration 23249, Loss: 0.055961888283491135\n",
      "Iteration 23250, Loss: 0.05614614486694336\n",
      "Iteration 23251, Loss: 0.056138914078474045\n",
      "Iteration 23252, Loss: 0.055959224700927734\n",
      "Iteration 23253, Loss: 0.0556386336684227\n",
      "Iteration 23254, Loss: 0.05590498819947243\n",
      "Iteration 23255, Loss: 0.056087374687194824\n",
      "Iteration 23256, Loss: 0.05615409463644028\n",
      "Iteration 23257, Loss: 0.05611622333526611\n",
      "Iteration 23258, Loss: 0.0559849739074707\n",
      "Iteration 23259, Loss: 0.0557732991874218\n",
      "Iteration 23260, Loss: 0.05577957630157471\n",
      "Iteration 23261, Loss: 0.05595378205180168\n",
      "Iteration 23262, Loss: 0.055945754051208496\n",
      "Iteration 23263, Loss: 0.055769722908735275\n",
      "Iteration 23264, Loss: 0.055739402770996094\n",
      "Iteration 23265, Loss: 0.055865488946437836\n",
      "Iteration 23266, Loss: 0.055883411318063736\n",
      "Iteration 23267, Loss: 0.05580461397767067\n",
      "Iteration 23268, Loss: 0.05564868822693825\n",
      "Iteration 23269, Loss: 0.05587053671479225\n",
      "Iteration 23270, Loss: 0.055970750749111176\n",
      "Iteration 23271, Loss: 0.055887702852487564\n",
      "Iteration 23272, Loss: 0.05564042180776596\n",
      "Iteration 23273, Loss: 0.05587971210479736\n",
      "Iteration 23274, Loss: 0.05604732409119606\n",
      "Iteration 23275, Loss: 0.05610267445445061\n",
      "Iteration 23276, Loss: 0.056056223809719086\n",
      "Iteration 23277, Loss: 0.05591881647706032\n",
      "Iteration 23278, Loss: 0.05570169538259506\n",
      "Iteration 23279, Loss: 0.055880866944789886\n",
      "Iteration 23280, Loss: 0.05606083199381828\n",
      "Iteration 23281, Loss: 0.05604787915945053\n",
      "Iteration 23282, Loss: 0.05586199089884758\n",
      "Iteration 23283, Loss: 0.05567852780222893\n",
      "Iteration 23284, Loss: 0.05581080913543701\n",
      "Iteration 23285, Loss: 0.055837951600551605\n",
      "Iteration 23286, Loss: 0.055768851190805435\n",
      "Iteration 23287, Loss: 0.0556214265525341\n",
      "Iteration 23288, Loss: 0.055838070809841156\n",
      "Iteration 23289, Loss: 0.055887382477521896\n",
      "Iteration 23290, Loss: 0.055775128304958344\n",
      "Iteration 23291, Loss: 0.055692076683044434\n",
      "Iteration 23292, Loss: 0.055781763046979904\n",
      "Iteration 23293, Loss: 0.055766742676496506\n",
      "Iteration 23294, Loss: 0.055646978318691254\n",
      "Iteration 23295, Loss: 0.05582761764526367\n",
      "Iteration 23296, Loss: 0.055905818939208984\n",
      "Iteration 23297, Loss: 0.05582503601908684\n",
      "Iteration 23298, Loss: 0.055655043572187424\n",
      "Iteration 23299, Loss: 0.055794280022382736\n",
      "Iteration 23300, Loss: 0.05584995076060295\n",
      "Iteration 23301, Loss: 0.05577322095632553\n",
      "Iteration 23302, Loss: 0.055647533386945724\n",
      "Iteration 23303, Loss: 0.05572033300995827\n",
      "Iteration 23304, Loss: 0.0556509904563427\n",
      "Iteration 23305, Loss: 0.0557604655623436\n",
      "Iteration 23306, Loss: 0.055813394486904144\n",
      "Iteration 23307, Loss: 0.05575470253825188\n",
      "Iteration 23308, Loss: 0.05565635487437248\n",
      "Iteration 23309, Loss: 0.05575677007436752\n",
      "Iteration 23310, Loss: 0.055733684450387955\n",
      "Iteration 23311, Loss: 0.05566927045583725\n",
      "Iteration 23312, Loss: 0.055704955011606216\n",
      "Iteration 23313, Loss: 0.05566386505961418\n",
      "Iteration 23314, Loss: 0.05571723356842995\n",
      "Iteration 23315, Loss: 0.05569839850068092\n",
      "Iteration 23316, Loss: 0.05568766966462135\n",
      "Iteration 23317, Loss: 0.05571889877319336\n",
      "Iteration 23318, Loss: 0.05565742775797844\n",
      "Iteration 23319, Loss: 0.05575156211853027\n",
      "Iteration 23320, Loss: 0.05575959011912346\n",
      "Iteration 23321, Loss: 0.05562949180603027\n",
      "Iteration 23322, Loss: 0.055698953568935394\n",
      "Iteration 23323, Loss: 0.055668000131845474\n",
      "Iteration 23324, Loss: 0.055706702172756195\n",
      "Iteration 23325, Loss: 0.05570173263549805\n",
      "Iteration 23326, Loss: 0.05566529557108879\n",
      "Iteration 23327, Loss: 0.0556773766875267\n",
      "Iteration 23328, Loss: 0.05565237998962402\n",
      "Iteration 23329, Loss: 0.05562623590230942\n",
      "Iteration 23330, Loss: 0.055702488869428635\n",
      "Iteration 23331, Loss: 0.05564073845744133\n",
      "Iteration 23332, Loss: 0.05575820058584213\n",
      "Iteration 23333, Loss: 0.05580615997314453\n",
      "Iteration 23334, Loss: 0.05574600026011467\n",
      "Iteration 23335, Loss: 0.05565313622355461\n",
      "Iteration 23336, Loss: 0.055734675377607346\n",
      "Iteration 23337, Loss: 0.05568162724375725\n",
      "Iteration 23338, Loss: 0.05572080612182617\n",
      "Iteration 23339, Loss: 0.05576888844370842\n",
      "Iteration 23340, Loss: 0.05572100728750229\n",
      "Iteration 23341, Loss: 0.05565381050109863\n",
      "Iteration 23342, Loss: 0.055679045617580414\n",
      "Iteration 23343, Loss: 0.05566342920064926\n",
      "Iteration 23344, Loss: 0.05566426366567612\n",
      "Iteration 23345, Loss: 0.05566609278321266\n",
      "Iteration 23346, Loss: 0.05564292520284653\n",
      "Iteration 23347, Loss: 0.05567427724599838\n",
      "Iteration 23348, Loss: 0.05563616752624512\n",
      "Iteration 23349, Loss: 0.055751483887434006\n",
      "Iteration 23350, Loss: 0.055761221796274185\n",
      "Iteration 23351, Loss: 0.05565301701426506\n",
      "Iteration 23352, Loss: 0.05577286332845688\n",
      "Iteration 23353, Loss: 0.055816493928432465\n",
      "Iteration 23354, Loss: 0.05571770668029785\n",
      "Iteration 23355, Loss: 0.055721282958984375\n",
      "Iteration 23356, Loss: 0.055790625512599945\n",
      "Iteration 23357, Loss: 0.055730026215314865\n",
      "Iteration 23358, Loss: 0.0556722916662693\n",
      "Iteration 23359, Loss: 0.055713653564453125\n",
      "Iteration 23360, Loss: 0.05562206357717514\n",
      "Iteration 23361, Loss: 0.05576268956065178\n",
      "Iteration 23362, Loss: 0.05580759048461914\n",
      "Iteration 23363, Loss: 0.05575239658355713\n",
      "Iteration 23364, Loss: 0.055638156831264496\n",
      "Iteration 23365, Loss: 0.05577663704752922\n",
      "Iteration 23366, Loss: 0.05576714128255844\n",
      "Iteration 23367, Loss: 0.05564292520284653\n",
      "Iteration 23368, Loss: 0.055718183517456055\n",
      "Iteration 23369, Loss: 0.0557098425924778\n",
      "Iteration 23370, Loss: 0.05563410371541977\n",
      "Iteration 23371, Loss: 0.055630963295698166\n",
      "Iteration 23372, Loss: 0.055686354637145996\n",
      "Iteration 23373, Loss: 0.05561789125204086\n",
      "Iteration 23374, Loss: 0.05575959011912346\n",
      "Iteration 23375, Loss: 0.05577043816447258\n",
      "Iteration 23376, Loss: 0.05566684529185295\n",
      "Iteration 23377, Loss: 0.05576876923441887\n",
      "Iteration 23378, Loss: 0.05582153797149658\n",
      "Iteration 23379, Loss: 0.05572625249624252\n",
      "Iteration 23380, Loss: 0.055711787194013596\n",
      "Iteration 23381, Loss: 0.0557808093726635\n",
      "Iteration 23382, Loss: 0.05572541803121567\n",
      "Iteration 23383, Loss: 0.0556696280837059\n",
      "Iteration 23384, Loss: 0.055704593658447266\n",
      "Iteration 23385, Loss: 0.05563231557607651\n",
      "Iteration 23386, Loss: 0.055669788271188736\n",
      "Iteration 23387, Loss: 0.05563664436340332\n",
      "Iteration 23388, Loss: 0.05574437230825424\n",
      "Iteration 23389, Loss: 0.05572589486837387\n",
      "Iteration 23390, Loss: 0.05566469952464104\n",
      "Iteration 23391, Loss: 0.05569633096456528\n",
      "Iteration 23392, Loss: 0.05562838166952133\n",
      "Iteration 23393, Loss: 0.05579376593232155\n",
      "Iteration 23394, Loss: 0.05581685155630112\n",
      "Iteration 23395, Loss: 0.05568750947713852\n",
      "Iteration 23396, Loss: 0.0557679757475853\n",
      "Iteration 23397, Loss: 0.055856745690107346\n",
      "Iteration 23398, Loss: 0.055822890251874924\n",
      "Iteration 23399, Loss: 0.05568321794271469\n",
      "Iteration 23400, Loss: 0.055799685418605804\n",
      "Iteration 23401, Loss: 0.05589151754975319\n",
      "Iteration 23402, Loss: 0.05581601709127426\n",
      "Iteration 23403, Loss: 0.05563918873667717\n",
      "Iteration 23404, Loss: 0.0557679757475853\n",
      "Iteration 23405, Loss: 0.055784665048122406\n",
      "Iteration 23406, Loss: 0.05567213147878647\n",
      "Iteration 23407, Loss: 0.05577941983938217\n",
      "Iteration 23408, Loss: 0.05585261434316635\n",
      "Iteration 23409, Loss: 0.055782876908779144\n",
      "Iteration 23410, Loss: 0.05565579980611801\n",
      "Iteration 23411, Loss: 0.05576344579458237\n",
      "Iteration 23412, Loss: 0.05576161667704582\n",
      "Iteration 23413, Loss: 0.05562043562531471\n",
      "Iteration 23414, Loss: 0.0557916983962059\n",
      "Iteration 23415, Loss: 0.055860284715890884\n",
      "Iteration 23416, Loss: 0.05582571029663086\n",
      "Iteration 23417, Loss: 0.05570225045084953\n",
      "Iteration 23418, Loss: 0.05576646700501442\n",
      "Iteration 23419, Loss: 0.05584383010864258\n",
      "Iteration 23420, Loss: 0.05574635788798332\n",
      "Iteration 23421, Loss: 0.05570399761199951\n",
      "Iteration 23422, Loss: 0.05578204244375229\n",
      "Iteration 23423, Loss: 0.055757682770490646\n",
      "Iteration 23424, Loss: 0.055646419525146484\n",
      "Iteration 23425, Loss: 0.055821262300014496\n",
      "Iteration 23426, Loss: 0.055877685546875\n",
      "Iteration 23427, Loss: 0.05575593560934067\n",
      "Iteration 23428, Loss: 0.0557129792869091\n",
      "Iteration 23429, Loss: 0.05580655857920647\n",
      "Iteration 23430, Loss: 0.055797021836042404\n",
      "Iteration 23431, Loss: 0.05569501966238022\n",
      "Iteration 23432, Loss: 0.05574989691376686\n",
      "Iteration 23433, Loss: 0.055803894996643066\n",
      "Iteration 23434, Loss: 0.0556795634329319\n",
      "Iteration 23435, Loss: 0.05577051639556885\n",
      "Iteration 23436, Loss: 0.05586544796824455\n",
      "Iteration 23437, Loss: 0.05585678666830063\n",
      "Iteration 23438, Loss: 0.05575494095683098\n",
      "Iteration 23439, Loss: 0.05566815659403801\n",
      "Iteration 23440, Loss: 0.05572231858968735\n",
      "Iteration 23441, Loss: 0.055621348321437836\n",
      "Iteration 23442, Loss: 0.05561693757772446\n",
      "Iteration 23443, Loss: 0.0557275228202343\n",
      "Iteration 23444, Loss: 0.05571603775024414\n",
      "Iteration 23445, Loss: 0.055647335946559906\n",
      "Iteration 23446, Loss: 0.055699389427900314\n",
      "Iteration 23447, Loss: 0.05563883110880852\n",
      "Iteration 23448, Loss: 0.055731140077114105\n",
      "Iteration 23449, Loss: 0.0557609423995018\n",
      "Iteration 23450, Loss: 0.05569370836019516\n",
      "Iteration 23451, Loss: 0.05570920556783676\n",
      "Iteration 23452, Loss: 0.055724821984767914\n",
      "Iteration 23453, Loss: 0.05564495176076889\n",
      "Iteration 23454, Loss: 0.05565878003835678\n",
      "Iteration 23455, Loss: 0.05565766617655754\n",
      "Iteration 23456, Loss: 0.05563342571258545\n",
      "Iteration 23457, Loss: 0.055647850036621094\n",
      "Iteration 23458, Loss: 0.055667005479335785\n",
      "Iteration 23459, Loss: 0.055642370134592056\n",
      "Iteration 23460, Loss: 0.055722836405038834\n",
      "Iteration 23461, Loss: 0.05569422245025635\n",
      "Iteration 23462, Loss: 0.05569418519735336\n",
      "Iteration 23463, Loss: 0.05572811886668205\n",
      "Iteration 23464, Loss: 0.0556611642241478\n",
      "Iteration 23465, Loss: 0.05575041100382805\n",
      "Iteration 23466, Loss: 0.05576781556010246\n",
      "Iteration 23467, Loss: 0.05562388896942139\n",
      "Iteration 23468, Loss: 0.05580329895019531\n",
      "Iteration 23469, Loss: 0.055874232202768326\n",
      "Iteration 23470, Loss: 0.055829089134931564\n",
      "Iteration 23471, Loss: 0.05569267272949219\n",
      "Iteration 23472, Loss: 0.05578760430216789\n",
      "Iteration 23473, Loss: 0.0558747872710228\n",
      "Iteration 23474, Loss: 0.055797819048166275\n",
      "Iteration 23475, Loss: 0.055644553154706955\n",
      "Iteration 23476, Loss: 0.05570797249674797\n",
      "Iteration 23477, Loss: 0.055660925805568695\n",
      "Iteration 23478, Loss: 0.05572700500488281\n",
      "Iteration 23479, Loss: 0.05572867393493652\n",
      "Iteration 23480, Loss: 0.055643480271101\n",
      "Iteration 23481, Loss: 0.05566326901316643\n",
      "Iteration 23482, Loss: 0.055656395852565765\n",
      "Iteration 23483, Loss: 0.05562921613454819\n",
      "Iteration 23484, Loss: 0.055680952966213226\n",
      "Iteration 23485, Loss: 0.05562392994761467\n",
      "Iteration 23486, Loss: 0.05564646050333977\n",
      "Iteration 23487, Loss: 0.05567566677927971\n",
      "Iteration 23488, Loss: 0.055657390505075455\n",
      "Iteration 23489, Loss: 0.0556999072432518\n",
      "Iteration 23490, Loss: 0.05566883459687233\n",
      "Iteration 23491, Loss: 0.05571603775024414\n",
      "Iteration 23492, Loss: 0.05574985593557358\n",
      "Iteration 23493, Loss: 0.05567999929189682\n",
      "Iteration 23494, Loss: 0.05572975054383278\n",
      "Iteration 23495, Loss: 0.05575088784098625\n",
      "Iteration 23496, Loss: 0.055617570877075195\n",
      "Iteration 23497, Loss: 0.05565377324819565\n",
      "Iteration 23498, Loss: 0.0556640625\n",
      "Iteration 23499, Loss: 0.05563032627105713\n",
      "Iteration 23500, Loss: 0.05574556440114975\n",
      "Iteration 23501, Loss: 0.05573348328471184\n",
      "Iteration 23502, Loss: 0.05564860627055168\n",
      "Iteration 23503, Loss: 0.055684808641672134\n",
      "Iteration 23504, Loss: 0.05562460795044899\n",
      "Iteration 23505, Loss: 0.05571778863668442\n",
      "Iteration 23506, Loss: 0.055722836405038834\n",
      "Iteration 23507, Loss: 0.05563918873667717\n",
      "Iteration 23508, Loss: 0.05579046532511711\n",
      "Iteration 23509, Loss: 0.055809300392866135\n",
      "Iteration 23510, Loss: 0.05565397068858147\n",
      "Iteration 23511, Loss: 0.05580878630280495\n",
      "Iteration 23512, Loss: 0.055920880287885666\n",
      "Iteration 23513, Loss: 0.055927079170942307\n",
      "Iteration 23514, Loss: 0.055838070809841156\n",
      "Iteration 23515, Loss: 0.055665019899606705\n",
      "Iteration 23516, Loss: 0.055877648293972015\n",
      "Iteration 23517, Loss: 0.056009214371442795\n",
      "Iteration 23518, Loss: 0.05595342442393303\n",
      "Iteration 23519, Loss: 0.05573078244924545\n",
      "Iteration 23520, Loss: 0.05579805746674538\n",
      "Iteration 23521, Loss: 0.05595151707530022\n",
      "Iteration 23522, Loss: 0.05599526688456535\n",
      "Iteration 23523, Loss: 0.055939555168151855\n",
      "Iteration 23524, Loss: 0.055795155465602875\n",
      "Iteration 23525, Loss: 0.05566553398966789\n",
      "Iteration 23526, Loss: 0.05576622858643532\n",
      "Iteration 23527, Loss: 0.05568603798747063\n",
      "Iteration 23528, Loss: 0.05573559179902077\n",
      "Iteration 23529, Loss: 0.05580318346619606\n",
      "Iteration 23530, Loss: 0.05576988309621811\n",
      "Iteration 23531, Loss: 0.055646102875471115\n",
      "Iteration 23532, Loss: 0.055841606110334396\n",
      "Iteration 23533, Loss: 0.055919330567121506\n",
      "Iteration 23534, Loss: 0.05581708997488022\n",
      "Iteration 23535, Loss: 0.055655479431152344\n",
      "Iteration 23536, Loss: 0.055739764124155045\n",
      "Iteration 23537, Loss: 0.05572402477264404\n",
      "Iteration 23538, Loss: 0.05561582371592522\n",
      "Iteration 23539, Loss: 0.05584597587585449\n",
      "Iteration 23540, Loss: 0.05589982122182846\n",
      "Iteration 23541, Loss: 0.05579026788473129\n",
      "Iteration 23542, Loss: 0.05567936226725578\n",
      "Iteration 23543, Loss: 0.055770717561244965\n",
      "Iteration 23544, Loss: 0.055758994072675705\n",
      "Iteration 23545, Loss: 0.055640898644924164\n",
      "Iteration 23546, Loss: 0.05583171173930168\n",
      "Iteration 23547, Loss: 0.055908363312482834\n",
      "Iteration 23548, Loss: 0.05582809820771217\n",
      "Iteration 23549, Loss: 0.055656276643276215\n",
      "Iteration 23550, Loss: 0.05580071732401848\n",
      "Iteration 23551, Loss: 0.0558646135032177\n",
      "Iteration 23552, Loss: 0.055793724954128265\n",
      "Iteration 23553, Loss: 0.05563303083181381\n",
      "Iteration 23554, Loss: 0.0557858943939209\n",
      "Iteration 23555, Loss: 0.05579618737101555\n",
      "Iteration 23556, Loss: 0.05565170571208\n",
      "Iteration 23557, Loss: 0.05581144616007805\n",
      "Iteration 23558, Loss: 0.055912379175424576\n",
      "Iteration 23559, Loss: 0.05589350312948227\n",
      "Iteration 23560, Loss: 0.055772941559553146\n",
      "Iteration 23561, Loss: 0.05569005385041237\n",
      "Iteration 23562, Loss: 0.05579201504588127\n",
      "Iteration 23563, Loss: 0.05575549975037575\n",
      "Iteration 23564, Loss: 0.05566561594605446\n",
      "Iteration 23565, Loss: 0.0557120256125927\n",
      "Iteration 23566, Loss: 0.055689018219709396\n",
      "Iteration 23567, Loss: 0.05567403882741928\n",
      "Iteration 23568, Loss: 0.05567511171102524\n",
      "Iteration 23569, Loss: 0.05567701905965805\n",
      "Iteration 23570, Loss: 0.05567992106080055\n",
      "Iteration 23571, Loss: 0.05565226078033447\n",
      "Iteration 23572, Loss: 0.05568142980337143\n",
      "Iteration 23573, Loss: 0.05565321817994118\n",
      "Iteration 23574, Loss: 0.055691324174404144\n",
      "Iteration 23575, Loss: 0.05566108226776123\n",
      "Iteration 23576, Loss: 0.05572120472788811\n",
      "Iteration 23577, Loss: 0.0557405985891819\n",
      "Iteration 23578, Loss: 0.05565750598907471\n",
      "Iteration 23579, Loss: 0.055757999420166016\n",
      "Iteration 23580, Loss: 0.05578251928091049\n",
      "Iteration 23581, Loss: 0.05565091222524643\n",
      "Iteration 23582, Loss: 0.05580461025238037\n",
      "Iteration 23583, Loss: 0.05589934438467026\n",
      "Iteration 23584, Loss: 0.055875860154628754\n",
      "Iteration 23585, Loss: 0.0557536706328392\n",
      "Iteration 23586, Loss: 0.05570896714925766\n",
      "Iteration 23587, Loss: 0.05579928681254387\n",
      "Iteration 23588, Loss: 0.05574154853820801\n",
      "Iteration 23589, Loss: 0.05568540096282959\n",
      "Iteration 23590, Loss: 0.055735230445861816\n",
      "Iteration 23591, Loss: 0.055695097893476486\n",
      "Iteration 23592, Loss: 0.05568333715200424\n",
      "Iteration 23593, Loss: 0.055690210312604904\n",
      "Iteration 23594, Loss: 0.05567439645528793\n",
      "Iteration 23595, Loss: 0.05568297952413559\n",
      "Iteration 23596, Loss: 0.05565353482961655\n",
      "Iteration 23597, Loss: 0.05569422245025635\n",
      "Iteration 23598, Loss: 0.05565237998962402\n",
      "Iteration 23599, Loss: 0.05570884793996811\n",
      "Iteration 23600, Loss: 0.05571794509887695\n",
      "Iteration 23601, Loss: 0.05561908334493637\n",
      "Iteration 23602, Loss: 0.055832188576459885\n",
      "Iteration 23603, Loss: 0.0558757409453392\n",
      "Iteration 23604, Loss: 0.0557510070502758\n",
      "Iteration 23605, Loss: 0.05571595951914787\n",
      "Iteration 23606, Loss: 0.05581025406718254\n",
      "Iteration 23607, Loss: 0.05579225346446037\n",
      "Iteration 23608, Loss: 0.05567173287272453\n",
      "Iteration 23609, Loss: 0.05579642578959465\n",
      "Iteration 23610, Loss: 0.055870573967695236\n",
      "Iteration 23611, Loss: 0.055772583931684494\n",
      "Iteration 23612, Loss: 0.05568119138479233\n",
      "Iteration 23613, Loss: 0.05576125904917717\n",
      "Iteration 23614, Loss: 0.05573161691427231\n",
      "Iteration 23615, Loss: 0.05562460422515869\n",
      "Iteration 23616, Loss: 0.0556485652923584\n",
      "Iteration 23617, Loss: 0.055684447288513184\n",
      "Iteration 23618, Loss: 0.05568218231201172\n",
      "Iteration 23619, Loss: 0.0556439571082592\n",
      "Iteration 23620, Loss: 0.05562655255198479\n",
      "Iteration 23621, Loss: 0.05566708371043205\n",
      "Iteration 23622, Loss: 0.055644869804382324\n",
      "Iteration 23623, Loss: 0.055619917809963226\n",
      "Iteration 23624, Loss: 0.055754583328962326\n",
      "Iteration 23625, Loss: 0.05572346970438957\n",
      "Iteration 23626, Loss: 0.05567491427063942\n",
      "Iteration 23627, Loss: 0.055714093148708344\n",
      "Iteration 23628, Loss: 0.05565611645579338\n",
      "Iteration 23629, Loss: 0.055746834725141525\n",
      "Iteration 23630, Loss: 0.055753193795681\n",
      "Iteration 23631, Loss: 0.05562949180603027\n",
      "Iteration 23632, Loss: 0.055669430643320084\n",
      "Iteration 23633, Loss: 0.05561479181051254\n",
      "Iteration 23634, Loss: 0.055697761476039886\n",
      "Iteration 23635, Loss: 0.055684927850961685\n",
      "Iteration 23636, Loss: 0.055652860552072525\n",
      "Iteration 23637, Loss: 0.05562039464712143\n",
      "Iteration 23638, Loss: 0.05571989342570305\n",
      "Iteration 23639, Loss: 0.055713098496198654\n",
      "Iteration 23640, Loss: 0.05563914775848389\n",
      "Iteration 23641, Loss: 0.0557096004486084\n",
      "Iteration 23642, Loss: 0.05564745515584946\n",
      "Iteration 23643, Loss: 0.05574445053935051\n",
      "Iteration 23644, Loss: 0.05579451844096184\n",
      "Iteration 23645, Loss: 0.0557454451918602\n",
      "Iteration 23646, Loss: 0.055615466088056564\n",
      "Iteration 23647, Loss: 0.05566128343343735\n",
      "Iteration 23648, Loss: 0.05565091222524643\n",
      "Iteration 23649, Loss: 0.05562055483460426\n",
      "Iteration 23650, Loss: 0.05574365705251694\n",
      "Iteration 23651, Loss: 0.05575108528137207\n",
      "Iteration 23652, Loss: 0.055656712502241135\n",
      "Iteration 23653, Loss: 0.055767934769392014\n",
      "Iteration 23654, Loss: 0.055802226066589355\n",
      "Iteration 23655, Loss: 0.05568031594157219\n",
      "Iteration 23656, Loss: 0.05577155202627182\n",
      "Iteration 23657, Loss: 0.055860601365566254\n",
      "Iteration 23658, Loss: 0.05583234876394272\n",
      "Iteration 23659, Loss: 0.05570749565958977\n",
      "Iteration 23660, Loss: 0.055756449699401855\n",
      "Iteration 23661, Loss: 0.05583691596984863\n",
      "Iteration 23662, Loss: 0.055755339562892914\n",
      "Iteration 23663, Loss: 0.05568154901266098\n",
      "Iteration 23664, Loss: 0.05574611946940422\n",
      "Iteration 23665, Loss: 0.05570244789123535\n",
      "Iteration 23666, Loss: 0.05567371845245361\n",
      "Iteration 23667, Loss: 0.0556848868727684\n",
      "Iteration 23668, Loss: 0.05567225068807602\n",
      "Iteration 23669, Loss: 0.05567622184753418\n",
      "Iteration 23670, Loss: 0.05565353482961655\n",
      "Iteration 23671, Loss: 0.055659811943769455\n",
      "Iteration 23672, Loss: 0.055667124688625336\n",
      "Iteration 23673, Loss: 0.055657267570495605\n",
      "Iteration 23674, Loss: 0.05568405240774155\n",
      "Iteration 23675, Loss: 0.05563771724700928\n",
      "Iteration 23676, Loss: 0.05573701858520508\n",
      "Iteration 23677, Loss: 0.05577099323272705\n",
      "Iteration 23678, Loss: 0.05570375919342041\n",
      "Iteration 23679, Loss: 0.0556950569152832\n",
      "Iteration 23680, Loss: 0.05571429058909416\n",
      "Iteration 23681, Loss: 0.055647533386945724\n",
      "Iteration 23682, Loss: 0.055654965341091156\n",
      "Iteration 23683, Loss: 0.05566783994436264\n",
      "Iteration 23684, Loss: 0.05561959743499756\n",
      "Iteration 23685, Loss: 0.055690210312604904\n",
      "Iteration 23686, Loss: 0.05564626306295395\n",
      "Iteration 23687, Loss: 0.05573594570159912\n",
      "Iteration 23688, Loss: 0.055742740631103516\n",
      "Iteration 23689, Loss: 0.055631399154663086\n",
      "Iteration 23690, Loss: 0.055758118629455566\n",
      "Iteration 23691, Loss: 0.05575815960764885\n",
      "Iteration 23692, Loss: 0.05562838166952133\n",
      "Iteration 23693, Loss: 0.0557786263525486\n",
      "Iteration 23694, Loss: 0.05580373853445053\n",
      "Iteration 23695, Loss: 0.055690448731184006\n",
      "Iteration 23696, Loss: 0.055755339562892914\n",
      "Iteration 23697, Loss: 0.05583195015788078\n",
      "Iteration 23698, Loss: 0.055775683373212814\n",
      "Iteration 23699, Loss: 0.055641334503889084\n",
      "Iteration 23700, Loss: 0.0557781457901001\n",
      "Iteration 23701, Loss: 0.05578716844320297\n",
      "Iteration 23702, Loss: 0.05563775822520256\n",
      "Iteration 23703, Loss: 0.055825911462306976\n",
      "Iteration 23704, Loss: 0.05593375489115715\n",
      "Iteration 23705, Loss: 0.05592568963766098\n",
      "Iteration 23706, Loss: 0.05581764504313469\n",
      "Iteration 23707, Loss: 0.05566537380218506\n",
      "Iteration 23708, Loss: 0.05583715811371803\n",
      "Iteration 23709, Loss: 0.05590470880270004\n",
      "Iteration 23710, Loss: 0.05580020323395729\n",
      "Iteration 23711, Loss: 0.055670976638793945\n",
      "Iteration 23712, Loss: 0.05575454235076904\n",
      "Iteration 23713, Loss: 0.05574194714426994\n",
      "Iteration 23714, Loss: 0.05564924329519272\n",
      "Iteration 23715, Loss: 0.05579233542084694\n",
      "Iteration 23716, Loss: 0.05582376569509506\n",
      "Iteration 23717, Loss: 0.05568234249949455\n",
      "Iteration 23718, Loss: 0.05577775090932846\n",
      "Iteration 23719, Loss: 0.055880945175886154\n",
      "Iteration 23720, Loss: 0.0558774471282959\n",
      "Iteration 23721, Loss: 0.05577770993113518\n",
      "Iteration 23722, Loss: 0.055636290460824966\n",
      "Iteration 23723, Loss: 0.055718742311000824\n",
      "Iteration 23724, Loss: 0.05564308166503906\n",
      "Iteration 23725, Loss: 0.05574842542409897\n",
      "Iteration 23726, Loss: 0.05579400062561035\n",
      "Iteration 23727, Loss: 0.05573539063334465\n",
      "Iteration 23728, Loss: 0.05564892664551735\n",
      "Iteration 23729, Loss: 0.05569227784872055\n",
      "Iteration 23730, Loss: 0.05564626306295395\n",
      "Iteration 23731, Loss: 0.05566088482737541\n",
      "Iteration 23732, Loss: 0.05563942715525627\n",
      "Iteration 23733, Loss: 0.055691126734018326\n",
      "Iteration 23734, Loss: 0.05567391961812973\n",
      "Iteration 23735, Loss: 0.05567797273397446\n",
      "Iteration 23736, Loss: 0.05566048622131348\n",
      "Iteration 23737, Loss: 0.05571027845144272\n",
      "Iteration 23738, Loss: 0.055727243423461914\n",
      "Iteration 23739, Loss: 0.055650949478149414\n",
      "Iteration 23740, Loss: 0.055760107934474945\n",
      "Iteration 23741, Loss: 0.055773261934518814\n",
      "Iteration 23742, Loss: 0.0556209497153759\n",
      "Iteration 23743, Loss: 0.05583620443940163\n",
      "Iteration 23744, Loss: 0.055945876985788345\n",
      "Iteration 23745, Loss: 0.055944204330444336\n",
      "Iteration 23746, Loss: 0.05584510415792465\n",
      "Iteration 23747, Loss: 0.05567443370819092\n",
      "Iteration 23748, Loss: 0.05585579201579094\n",
      "Iteration 23749, Loss: 0.055973730981349945\n",
      "Iteration 23750, Loss: 0.05591193959116936\n",
      "Iteration 23751, Loss: 0.0556870736181736\n",
      "Iteration 23752, Loss: 0.05583254620432854\n",
      "Iteration 23753, Loss: 0.055988114327192307\n",
      "Iteration 23754, Loss: 0.05603273957967758\n",
      "Iteration 23755, Loss: 0.0559769906103611\n",
      "Iteration 23756, Loss: 0.05583246797323227\n",
      "Iteration 23757, Loss: 0.05564109608530998\n",
      "Iteration 23758, Loss: 0.055870018899440765\n",
      "Iteration 23759, Loss: 0.05594007298350334\n",
      "Iteration 23760, Loss: 0.05583060160279274\n",
      "Iteration 23761, Loss: 0.0556565523147583\n",
      "Iteration 23762, Loss: 0.055756568908691406\n",
      "Iteration 23763, Loss: 0.055764518678188324\n",
      "Iteration 23764, Loss: 0.05567439645528793\n",
      "Iteration 23765, Loss: 0.05576356500387192\n",
      "Iteration 23766, Loss: 0.05580707639455795\n",
      "Iteration 23767, Loss: 0.05568365380167961\n",
      "Iteration 23768, Loss: 0.05576439946889877\n",
      "Iteration 23769, Loss: 0.05585348978638649\n",
      "Iteration 23770, Loss: 0.05582869425415993\n",
      "Iteration 23771, Loss: 0.055704593658447266\n",
      "Iteration 23772, Loss: 0.05575895309448242\n",
      "Iteration 23773, Loss: 0.05583707615733147\n",
      "Iteration 23774, Loss: 0.05574151128530502\n",
      "Iteration 23775, Loss: 0.05570364370942116\n",
      "Iteration 23776, Loss: 0.05577957630157471\n",
      "Iteration 23777, Loss: 0.05574600026011467\n",
      "Iteration 23778, Loss: 0.05563211813569069\n",
      "Iteration 23779, Loss: 0.05580417439341545\n",
      "Iteration 23780, Loss: 0.05582352727651596\n",
      "Iteration 23781, Loss: 0.0556691512465477\n",
      "Iteration 23782, Loss: 0.05579555407166481\n",
      "Iteration 23783, Loss: 0.05590701103210449\n",
      "Iteration 23784, Loss: 0.05591297149658203\n",
      "Iteration 23785, Loss: 0.05582364648580551\n",
      "Iteration 23786, Loss: 0.05565079301595688\n",
      "Iteration 23787, Loss: 0.05589469522237778\n",
      "Iteration 23788, Loss: 0.05602574720978737\n",
      "Iteration 23789, Loss: 0.05596999451518059\n",
      "Iteration 23790, Loss: 0.0557478666305542\n",
      "Iteration 23791, Loss: 0.055783748626708984\n",
      "Iteration 23792, Loss: 0.05593693256378174\n",
      "Iteration 23793, Loss: 0.05598068609833717\n",
      "Iteration 23794, Loss: 0.055925171822309494\n",
      "Iteration 23795, Loss: 0.055780570954084396\n",
      "Iteration 23796, Loss: 0.05568341538310051\n",
      "Iteration 23797, Loss: 0.055783629417419434\n",
      "Iteration 23798, Loss: 0.055702608078718185\n",
      "Iteration 23799, Loss: 0.0557231530547142\n",
      "Iteration 23800, Loss: 0.05579177662730217\n",
      "Iteration 23801, Loss: 0.055759232491254807\n",
      "Iteration 23802, Loss: 0.055636607110500336\n",
      "Iteration 23803, Loss: 0.0558520182967186\n",
      "Iteration 23804, Loss: 0.055928509682416916\n",
      "Iteration 23805, Loss: 0.05582491680979729\n",
      "Iteration 23806, Loss: 0.05564948171377182\n",
      "Iteration 23807, Loss: 0.055736105889081955\n",
      "Iteration 23808, Loss: 0.05572275444865227\n",
      "Iteration 23809, Loss: 0.05561622232198715\n",
      "Iteration 23810, Loss: 0.05584939569234848\n",
      "Iteration 23811, Loss: 0.05590669438242912\n",
      "Iteration 23812, Loss: 0.05579718202352524\n",
      "Iteration 23813, Loss: 0.05567344278097153\n",
      "Iteration 23814, Loss: 0.05576614663004875\n",
      "Iteration 23815, Loss: 0.05575740709900856\n",
      "Iteration 23816, Loss: 0.0556441955268383\n",
      "Iteration 23817, Loss: 0.05582241341471672\n",
      "Iteration 23818, Loss: 0.05589409917593002\n",
      "Iteration 23819, Loss: 0.05580723658204079\n",
      "Iteration 23820, Loss: 0.05565778538584709\n",
      "Iteration 23821, Loss: 0.05576912686228752\n",
      "Iteration 23822, Loss: 0.05578605458140373\n",
      "Iteration 23823, Loss: 0.05567614361643791\n",
      "Iteration 23824, Loss: 0.05577540770173073\n",
      "Iteration 23825, Loss: 0.055849313735961914\n",
      "Iteration 23826, Loss: 0.05578235909342766\n",
      "Iteration 23827, Loss: 0.05565603822469711\n",
      "Iteration 23828, Loss: 0.05576741695404053\n",
      "Iteration 23829, Loss: 0.05577210709452629\n",
      "Iteration 23830, Loss: 0.05562949180603027\n",
      "Iteration 23831, Loss: 0.05583226680755615\n",
      "Iteration 23832, Loss: 0.05593454837799072\n",
      "Iteration 23833, Loss: 0.0559137687087059\n",
      "Iteration 23834, Loss: 0.05578958988189697\n",
      "Iteration 23835, Loss: 0.055684011429548264\n",
      "Iteration 23836, Loss: 0.055803898721933365\n",
      "Iteration 23837, Loss: 0.05579284951090813\n",
      "Iteration 23838, Loss: 0.05565377324819565\n",
      "Iteration 23839, Loss: 0.05576412007212639\n",
      "Iteration 23840, Loss: 0.05582460016012192\n",
      "Iteration 23841, Loss: 0.05578780546784401\n",
      "Iteration 23842, Loss: 0.05566633120179176\n",
      "Iteration 23843, Loss: 0.05581244081258774\n",
      "Iteration 23844, Loss: 0.055885277688503265\n",
      "Iteration 23845, Loss: 0.05578140541911125\n",
      "Iteration 23846, Loss: 0.05568297952413559\n",
      "Iteration 23847, Loss: 0.05576745793223381\n",
      "Iteration 23848, Loss: 0.05575430393218994\n",
      "Iteration 23849, Loss: 0.055649399757385254\n",
      "Iteration 23850, Loss: 0.055812399834394455\n",
      "Iteration 23851, Loss: 0.05587053298950195\n",
      "Iteration 23852, Loss: 0.05575970932841301\n",
      "Iteration 23853, Loss: 0.055700741708278656\n",
      "Iteration 23854, Loss: 0.05578712746500969\n",
      "Iteration 23855, Loss: 0.05577012151479721\n",
      "Iteration 23856, Loss: 0.05565512552857399\n",
      "Iteration 23857, Loss: 0.055814266204833984\n",
      "Iteration 23858, Loss: 0.0558849573135376\n",
      "Iteration 23859, Loss: 0.055787406861782074\n",
      "Iteration 23860, Loss: 0.055671535432338715\n",
      "Iteration 23861, Loss: 0.0557558573782444\n",
      "Iteration 23862, Loss: 0.05573805421590805\n",
      "Iteration 23863, Loss: 0.05561717599630356\n",
      "Iteration 23864, Loss: 0.05585074797272682\n",
      "Iteration 23865, Loss: 0.05592620372772217\n",
      "Iteration 23866, Loss: 0.05584963411092758\n",
      "Iteration 23867, Loss: 0.05567014589905739\n",
      "Iteration 23868, Loss: 0.0558190755546093\n",
      "Iteration 23869, Loss: 0.05592143535614014\n",
      "Iteration 23870, Loss: 0.05588364601135254\n",
      "Iteration 23871, Loss: 0.0557224377989769\n",
      "Iteration 23872, Loss: 0.05576924607157707\n",
      "Iteration 23873, Loss: 0.05588480085134506\n",
      "Iteration 23874, Loss: 0.055853407829999924\n",
      "Iteration 23875, Loss: 0.05569370836019516\n",
      "Iteration 23876, Loss: 0.0557938814163208\n",
      "Iteration 23877, Loss: 0.0559055432677269\n",
      "Iteration 23878, Loss: 0.05587379261851311\n",
      "Iteration 23879, Loss: 0.05572168156504631\n",
      "Iteration 23880, Loss: 0.05576201528310776\n",
      "Iteration 23881, Loss: 0.05586898326873779\n",
      "Iteration 23882, Loss: 0.05583139508962631\n",
      "Iteration 23883, Loss: 0.05566398426890373\n",
      "Iteration 23884, Loss: 0.05583139508962631\n",
      "Iteration 23885, Loss: 0.05594825744628906\n",
      "Iteration 23886, Loss: 0.0559210404753685\n",
      "Iteration 23887, Loss: 0.055774252861738205\n",
      "Iteration 23888, Loss: 0.05571548268198967\n",
      "Iteration 23889, Loss: 0.05582825466990471\n",
      "Iteration 23890, Loss: 0.055810850113630295\n",
      "Iteration 23891, Loss: 0.05565913766622543\n",
      "Iteration 23892, Loss: 0.055822454392910004\n",
      "Iteration 23893, Loss: 0.05593065544962883\n",
      "Iteration 23894, Loss: 0.05591186136007309\n",
      "Iteration 23895, Loss: 0.05579002946615219\n",
      "Iteration 23896, Loss: 0.05569450184702873\n",
      "Iteration 23897, Loss: 0.05580759048461914\n",
      "Iteration 23898, Loss: 0.05580977723002434\n",
      "Iteration 23899, Loss: 0.055673759430646896\n",
      "Iteration 23900, Loss: 0.05578267574310303\n",
      "Iteration 23901, Loss: 0.05587319657206535\n",
      "Iteration 23902, Loss: 0.055860161781311035\n",
      "Iteration 23903, Loss: 0.0557582788169384\n",
      "Iteration 23904, Loss: 0.055682938545942307\n",
      "Iteration 23905, Loss: 0.055768173187971115\n",
      "Iteration 23906, Loss: 0.055714093148708344\n",
      "Iteration 23907, Loss: 0.055703602731227875\n",
      "Iteration 23908, Loss: 0.05575307458639145\n",
      "Iteration 23909, Loss: 0.0557151660323143\n",
      "Iteration 23910, Loss: 0.055658817291259766\n",
      "Iteration 23911, Loss: 0.055695533752441406\n",
      "Iteration 23912, Loss: 0.05565913766622543\n",
      "Iteration 23913, Loss: 0.05568917840719223\n",
      "Iteration 23914, Loss: 0.05567082017660141\n",
      "Iteration 23915, Loss: 0.05569823831319809\n",
      "Iteration 23916, Loss: 0.05569283291697502\n",
      "Iteration 23917, Loss: 0.05566609278321266\n",
      "Iteration 23918, Loss: 0.05567614361643791\n",
      "Iteration 23919, Loss: 0.05566354840993881\n",
      "Iteration 23920, Loss: 0.05564844608306885\n",
      "Iteration 23921, Loss: 0.055681586265563965\n",
      "Iteration 23922, Loss: 0.055642012506723404\n",
      "Iteration 23923, Loss: 0.055743854492902756\n",
      "Iteration 23924, Loss: 0.05577035993337631\n",
      "Iteration 23925, Loss: 0.05568715184926987\n",
      "Iteration 23926, Loss: 0.05573221296072006\n",
      "Iteration 23927, Loss: 0.05577290058135986\n",
      "Iteration 23928, Loss: 0.05566263571381569\n",
      "Iteration 23929, Loss: 0.055780887603759766\n",
      "Iteration 23930, Loss: 0.05586155503988266\n",
      "Iteration 23931, Loss: 0.05582396313548088\n",
      "Iteration 23932, Loss: 0.055694662034511566\n",
      "Iteration 23933, Loss: 0.05577608197927475\n",
      "Iteration 23934, Loss: 0.05585658550262451\n",
      "Iteration 23935, Loss: 0.05577560514211655\n",
      "Iteration 23936, Loss: 0.055664222687482834\n",
      "Iteration 23937, Loss: 0.0557279996573925\n",
      "Iteration 23938, Loss: 0.05568385124206543\n",
      "Iteration 23939, Loss: 0.0556945838034153\n",
      "Iteration 23940, Loss: 0.05569601058959961\n",
      "Iteration 23941, Loss: 0.05566946789622307\n",
      "Iteration 23942, Loss: 0.055679917335510254\n",
      "Iteration 23943, Loss: 0.055643480271101\n",
      "Iteration 23944, Loss: 0.055661045014858246\n",
      "Iteration 23945, Loss: 0.05565870180726051\n",
      "Iteration 23946, Loss: 0.05564502999186516\n",
      "Iteration 23947, Loss: 0.05570419877767563\n",
      "Iteration 23948, Loss: 0.05566013231873512\n",
      "Iteration 23949, Loss: 0.05572708696126938\n",
      "Iteration 23950, Loss: 0.055766426026821136\n",
      "Iteration 23951, Loss: 0.05570153519511223\n",
      "Iteration 23952, Loss: 0.05569557473063469\n",
      "Iteration 23953, Loss: 0.055714093148708344\n",
      "Iteration 23954, Loss: 0.055646657943725586\n",
      "Iteration 23955, Loss: 0.055651985108852386\n",
      "Iteration 23956, Loss: 0.055675387382507324\n",
      "Iteration 23957, Loss: 0.055615901947021484\n",
      "Iteration 23958, Loss: 0.055774811655282974\n",
      "Iteration 23959, Loss: 0.055828217417001724\n",
      "Iteration 23960, Loss: 0.0557783842086792\n",
      "Iteration 23961, Loss: 0.05564606189727783\n",
      "Iteration 23962, Loss: 0.05584168806672096\n",
      "Iteration 23963, Loss: 0.05591515824198723\n",
      "Iteration 23964, Loss: 0.05580707639455795\n",
      "Iteration 23965, Loss: 0.05566597357392311\n",
      "Iteration 23966, Loss: 0.05575176328420639\n",
      "Iteration 23967, Loss: 0.05573701858520508\n",
      "Iteration 23968, Loss: 0.05563473701477051\n",
      "Iteration 23969, Loss: 0.05582340806722641\n",
      "Iteration 23970, Loss: 0.05587097257375717\n",
      "Iteration 23971, Loss: 0.05574377626180649\n",
      "Iteration 23972, Loss: 0.05572374910116196\n",
      "Iteration 23973, Loss: 0.055819712579250336\n",
      "Iteration 23974, Loss: 0.05581235885620117\n",
      "Iteration 23975, Loss: 0.05571091175079346\n",
      "Iteration 23976, Loss: 0.05572585389018059\n",
      "Iteration 23977, Loss: 0.05578009411692619\n",
      "Iteration 23978, Loss: 0.05566028878092766\n",
      "Iteration 23979, Loss: 0.055779337882995605\n",
      "Iteration 23980, Loss: 0.05586898326873779\n",
      "Iteration 23981, Loss: 0.055853329598903656\n",
      "Iteration 23982, Loss: 0.05574401468038559\n",
      "Iteration 23983, Loss: 0.05569203943014145\n",
      "Iteration 23984, Loss: 0.055758796632289886\n",
      "Iteration 23985, Loss: 0.055654171854257584\n",
      "Iteration 23986, Loss: 0.0557740144431591\n",
      "Iteration 23987, Loss: 0.05585523694753647\n",
      "Iteration 23988, Loss: 0.05583445355296135\n",
      "Iteration 23989, Loss: 0.055722158402204514\n",
      "Iteration 23990, Loss: 0.055723946541547775\n",
      "Iteration 23991, Loss: 0.055789630860090256\n",
      "Iteration 23992, Loss: 0.05567757412791252\n",
      "Iteration 23993, Loss: 0.05576268956065178\n",
      "Iteration 23994, Loss: 0.05585018917918205\n",
      "Iteration 23995, Loss: 0.055834971368312836\n",
      "Iteration 23996, Loss: 0.05572736635804176\n",
      "Iteration 23997, Loss: 0.055710673332214355\n",
      "Iteration 23998, Loss: 0.0557708740234375\n",
      "Iteration 23999, Loss: 0.05565258115530014\n",
      "Iteration 24000, Loss: 0.05578502267599106\n",
      "Iteration 24001, Loss: 0.05587625876069069\n",
      "Iteration 24002, Loss: 0.055864136666059494\n",
      "Iteration 24003, Loss: 0.05575891584157944\n",
      "Iteration 24004, Loss: 0.05566469952464104\n",
      "Iteration 24005, Loss: 0.05572223663330078\n",
      "Iteration 24006, Loss: 0.055618010461330414\n",
      "Iteration 24007, Loss: 0.05563386529684067\n",
      "Iteration 24008, Loss: 0.05569617077708244\n",
      "Iteration 24009, Loss: 0.05567511171102524\n",
      "Iteration 24010, Loss: 0.05568560212850571\n",
      "Iteration 24011, Loss: 0.0556771382689476\n",
      "Iteration 24012, Loss: 0.0556865930557251\n",
      "Iteration 24013, Loss: 0.055691640824079514\n",
      "Iteration 24014, Loss: 0.05564681813120842\n",
      "Iteration 24015, Loss: 0.05565277859568596\n",
      "Iteration 24016, Loss: 0.05568607896566391\n",
      "Iteration 24017, Loss: 0.0556815080344677\n",
      "Iteration 24018, Loss: 0.055654726922512054\n",
      "Iteration 24019, Loss: 0.05564944073557854\n",
      "Iteration 24020, Loss: 0.055683378130197525\n",
      "Iteration 24021, Loss: 0.055680595338344574\n",
      "Iteration 24022, Loss: 0.0556463822722435\n",
      "Iteration 24023, Loss: 0.055640820413827896\n",
      "Iteration 24024, Loss: 0.05566362664103508\n",
      "Iteration 24025, Loss: 0.055614035576581955\n",
      "Iteration 24026, Loss: 0.05577787011861801\n",
      "Iteration 24027, Loss: 0.05580493062734604\n",
      "Iteration 24028, Loss: 0.05570932477712631\n",
      "Iteration 24029, Loss: 0.055721841752529144\n",
      "Iteration 24030, Loss: 0.055782318115234375\n",
      "Iteration 24031, Loss: 0.05570010468363762\n",
      "Iteration 24032, Loss: 0.055724821984767914\n",
      "Iteration 24033, Loss: 0.05578223988413811\n",
      "Iteration 24034, Loss: 0.05571429058909416\n",
      "Iteration 24035, Loss: 0.05569100379943848\n",
      "Iteration 24036, Loss: 0.055731695145368576\n",
      "Iteration 24037, Loss: 0.05563191697001457\n",
      "Iteration 24038, Loss: 0.05579634755849838\n",
      "Iteration 24039, Loss: 0.055867355316877365\n",
      "Iteration 24040, Loss: 0.055819910019636154\n",
      "Iteration 24041, Loss: 0.05568445101380348\n",
      "Iteration 24042, Loss: 0.05579400062561035\n",
      "Iteration 24043, Loss: 0.05587534233927727\n",
      "Iteration 24044, Loss: 0.055786848068237305\n",
      "Iteration 24045, Loss: 0.055661678314208984\n",
      "Iteration 24046, Loss: 0.05573209375143051\n",
      "Iteration 24047, Loss: 0.055693309754133224\n",
      "Iteration 24048, Loss: 0.05567685887217522\n",
      "Iteration 24049, Loss: 0.055675946176052094\n",
      "Iteration 24050, Loss: 0.055687230080366135\n",
      "Iteration 24051, Loss: 0.05569986626505852\n",
      "Iteration 24052, Loss: 0.055635929107666016\n",
      "Iteration 24053, Loss: 0.055740438401699066\n",
      "Iteration 24054, Loss: 0.05570916458964348\n",
      "Iteration 24055, Loss: 0.05568520352244377\n",
      "Iteration 24056, Loss: 0.05572418496012688\n",
      "Iteration 24057, Loss: 0.055668435990810394\n",
      "Iteration 24058, Loss: 0.05572656914591789\n",
      "Iteration 24059, Loss: 0.0557299479842186\n",
      "Iteration 24060, Loss: 0.055647414177656174\n",
      "Iteration 24061, Loss: 0.05567292496562004\n",
      "Iteration 24062, Loss: 0.05562667176127434\n",
      "Iteration 24063, Loss: 0.05570463463664055\n",
      "Iteration 24064, Loss: 0.0556870698928833\n",
      "Iteration 24065, Loss: 0.055667679756879807\n",
      "Iteration 24066, Loss: 0.05566398426890373\n",
      "Iteration 24067, Loss: 0.055694542825222015\n",
      "Iteration 24068, Loss: 0.05569799989461899\n",
      "Iteration 24069, Loss: 0.055644989013671875\n",
      "Iteration 24070, Loss: 0.05569601431488991\n",
      "Iteration 24071, Loss: 0.05564054101705551\n",
      "Iteration 24072, Loss: 0.05571937561035156\n",
      "Iteration 24073, Loss: 0.055738888680934906\n",
      "Iteration 24074, Loss: 0.055658578872680664\n",
      "Iteration 24075, Loss: 0.05576737970113754\n",
      "Iteration 24076, Loss: 0.05580031871795654\n",
      "Iteration 24077, Loss: 0.05566704645752907\n",
      "Iteration 24078, Loss: 0.05578271672129631\n",
      "Iteration 24079, Loss: 0.055878642946481705\n",
      "Iteration 24080, Loss: 0.0558648519217968\n",
      "Iteration 24081, Loss: 0.055754583328962326\n",
      "Iteration 24082, Loss: 0.05568265914916992\n",
      "Iteration 24083, Loss: 0.05576050654053688\n",
      "Iteration 24084, Loss: 0.05567789450287819\n",
      "Iteration 24085, Loss: 0.05574222654104233\n",
      "Iteration 24086, Loss: 0.0558089017868042\n",
      "Iteration 24087, Loss: 0.055775921791791916\n",
      "Iteration 24088, Loss: 0.05565822124481201\n",
      "Iteration 24089, Loss: 0.055812954902648926\n",
      "Iteration 24090, Loss: 0.05587848275899887\n",
      "Iteration 24091, Loss: 0.055764757096767426\n",
      "Iteration 24092, Loss: 0.055698953568935394\n",
      "Iteration 24093, Loss: 0.05578768253326416\n",
      "Iteration 24094, Loss: 0.05577461048960686\n",
      "Iteration 24095, Loss: 0.05566934868693352\n",
      "Iteration 24096, Loss: 0.055784665048122406\n",
      "Iteration 24097, Loss: 0.05584140866994858\n",
      "Iteration 24098, Loss: 0.05571905896067619\n",
      "Iteration 24099, Loss: 0.05573773384094238\n",
      "Iteration 24100, Loss: 0.05583171173930168\n",
      "Iteration 24101, Loss: 0.055822134017944336\n",
      "Iteration 24102, Loss: 0.05571961775422096\n",
      "Iteration 24103, Loss: 0.055713336914777756\n",
      "Iteration 24104, Loss: 0.05576765537261963\n",
      "Iteration 24105, Loss: 0.055643681436777115\n",
      "Iteration 24106, Loss: 0.055794041603803635\n",
      "Iteration 24107, Loss: 0.05588873475790024\n",
      "Iteration 24108, Loss: 0.05587943643331528\n",
      "Iteration 24109, Loss: 0.05577675625681877\n",
      "Iteration 24110, Loss: 0.05563577264547348\n",
      "Iteration 24111, Loss: 0.05569171905517578\n",
      "Iteration 24112, Loss: 0.05563962459564209\n",
      "Iteration 24113, Loss: 0.05563263222575188\n",
      "Iteration 24114, Loss: 0.05570995807647705\n",
      "Iteration 24115, Loss: 0.05566227436065674\n",
      "Iteration 24116, Loss: 0.05572677031159401\n",
      "Iteration 24117, Loss: 0.055768609046936035\n",
      "Iteration 24118, Loss: 0.05570710077881813\n",
      "Iteration 24119, Loss: 0.055683497339487076\n",
      "Iteration 24120, Loss: 0.05570220947265625\n",
      "Iteration 24121, Loss: 0.05565281957387924\n",
      "Iteration 24122, Loss: 0.055656831711530685\n",
      "Iteration 24123, Loss: 0.05566795915365219\n",
      "Iteration 24124, Loss: 0.0556187629699707\n",
      "Iteration 24125, Loss: 0.05573177710175514\n",
      "Iteration 24126, Loss: 0.055750809609889984\n",
      "Iteration 24127, Loss: 0.05567225068807602\n",
      "Iteration 24128, Loss: 0.055747393518686295\n",
      "Iteration 24129, Loss: 0.055774930864572525\n",
      "Iteration 24130, Loss: 0.05562814325094223\n",
      "Iteration 24131, Loss: 0.05582273006439209\n",
      "Iteration 24132, Loss: 0.05593077465891838\n",
      "Iteration 24133, Loss: 0.05592910572886467\n",
      "Iteration 24134, Loss: 0.05583028122782707\n",
      "Iteration 24135, Loss: 0.05565623566508293\n",
      "Iteration 24136, Loss: 0.05588182061910629\n",
      "Iteration 24137, Loss: 0.056003015488386154\n",
      "Iteration 24138, Loss: 0.05593765154480934\n",
      "Iteration 24139, Loss: 0.055706582963466644\n",
      "Iteration 24140, Loss: 0.05581919476389885\n",
      "Iteration 24141, Loss: 0.055977463722229004\n",
      "Iteration 24142, Loss: 0.0560249499976635\n",
      "Iteration 24143, Loss: 0.05597182363271713\n",
      "Iteration 24144, Loss: 0.05582968518137932\n",
      "Iteration 24145, Loss: 0.05563024803996086\n",
      "Iteration 24146, Loss: 0.055871009826660156\n",
      "Iteration 24147, Loss: 0.05594559758901596\n",
      "Iteration 24148, Loss: 0.05584895610809326\n",
      "Iteration 24149, Loss: 0.05565369501709938\n",
      "Iteration 24150, Loss: 0.05579519271850586\n",
      "Iteration 24151, Loss: 0.05586433410644531\n",
      "Iteration 24152, Loss: 0.05581788346171379\n",
      "Iteration 24153, Loss: 0.05566811561584473\n",
      "Iteration 24154, Loss: 0.055825792253017426\n",
      "Iteration 24155, Loss: 0.0559290274977684\n",
      "Iteration 24156, Loss: 0.05586771294474602\n",
      "Iteration 24157, Loss: 0.05567038431763649\n",
      "Iteration 24158, Loss: 0.055828772485256195\n",
      "Iteration 24159, Loss: 0.05595918744802475\n",
      "Iteration 24160, Loss: 0.05596109479665756\n",
      "Iteration 24161, Loss: 0.0558498315513134\n",
      "Iteration 24162, Loss: 0.055668752640485764\n",
      "Iteration 24163, Loss: 0.055853407829999924\n",
      "Iteration 24164, Loss: 0.05596490949392319\n",
      "Iteration 24165, Loss: 0.05590824410319328\n",
      "Iteration 24166, Loss: 0.05569871515035629\n",
      "Iteration 24167, Loss: 0.05581764504313469\n",
      "Iteration 24168, Loss: 0.05596526712179184\n",
      "Iteration 24169, Loss: 0.05599324032664299\n",
      "Iteration 24170, Loss: 0.0559154748916626\n",
      "Iteration 24171, Loss: 0.05575140565633774\n",
      "Iteration 24172, Loss: 0.055754028260707855\n",
      "Iteration 24173, Loss: 0.05587848275899887\n",
      "Iteration 24174, Loss: 0.05583878606557846\n",
      "Iteration 24175, Loss: 0.055646222084760666\n",
      "Iteration 24176, Loss: 0.05583580583333969\n",
      "Iteration 24177, Loss: 0.0559646300971508\n",
      "Iteration 24178, Loss: 0.05598429962992668\n",
      "Iteration 24179, Loss: 0.055907052010297775\n",
      "Iteration 24180, Loss: 0.05574588105082512\n",
      "Iteration 24181, Loss: 0.05575426667928696\n",
      "Iteration 24182, Loss: 0.05587514489889145\n",
      "Iteration 24183, Loss: 0.05581824108958244\n",
      "Iteration 24184, Loss: 0.055631041526794434\n",
      "Iteration 24185, Loss: 0.055742066353559494\n",
      "Iteration 24186, Loss: 0.055765990167856216\n",
      "Iteration 24187, Loss: 0.055690646171569824\n",
      "Iteration 24188, Loss: 0.05572057142853737\n",
      "Iteration 24189, Loss: 0.055746834725141525\n",
      "Iteration 24190, Loss: 0.05562242120504379\n",
      "Iteration 24191, Loss: 0.05571254342794418\n",
      "Iteration 24192, Loss: 0.05568742752075195\n",
      "Iteration 24193, Loss: 0.0556790828704834\n",
      "Iteration 24194, Loss: 0.05568147078156471\n",
      "Iteration 24195, Loss: 0.0556689128279686\n",
      "Iteration 24196, Loss: 0.0556614026427269\n",
      "Iteration 24197, Loss: 0.055688463151454926\n",
      "Iteration 24198, Loss: 0.055672090500593185\n",
      "Iteration 24199, Loss: 0.055691998451948166\n",
      "Iteration 24200, Loss: 0.05569756031036377\n",
      "Iteration 24201, Loss: 0.05564197152853012\n",
      "Iteration 24202, Loss: 0.055647097527980804\n",
      "Iteration 24203, Loss: 0.0556923970580101\n",
      "Iteration 24204, Loss: 0.05568961426615715\n",
      "Iteration 24205, Loss: 0.05564530938863754\n",
      "Iteration 24206, Loss: 0.05566736310720444\n",
      "Iteration 24207, Loss: 0.05565718933939934\n",
      "Iteration 24208, Loss: 0.05565067380666733\n",
      "Iteration 24209, Loss: 0.055682223290205\n",
      "Iteration 24210, Loss: 0.055634062737226486\n",
      "Iteration 24211, Loss: 0.05572410672903061\n",
      "Iteration 24212, Loss: 0.05573757737874985\n",
      "Iteration 24213, Loss: 0.05564745515584946\n",
      "Iteration 24214, Loss: 0.055784426629543304\n",
      "Iteration 24215, Loss: 0.05582277104258537\n",
      "Iteration 24216, Loss: 0.05569756403565407\n",
      "Iteration 24217, Loss: 0.05575522035360336\n",
      "Iteration 24218, Loss: 0.05584748834371567\n",
      "Iteration 24219, Loss: 0.05582793802022934\n",
      "Iteration 24220, Loss: 0.05571095272898674\n",
      "Iteration 24221, Loss: 0.05574154853820801\n",
      "Iteration 24222, Loss: 0.05581502243876457\n",
      "Iteration 24223, Loss: 0.05572068691253662\n",
      "Iteration 24224, Loss: 0.05571727082133293\n",
      "Iteration 24225, Loss: 0.055791258811950684\n",
      "Iteration 24226, Loss: 0.0557580403983593\n",
      "Iteration 24227, Loss: 0.05564232915639877\n",
      "Iteration 24228, Loss: 0.05581458657979965\n",
      "Iteration 24229, Loss: 0.05586155503988266\n",
      "Iteration 24230, Loss: 0.05573304742574692\n",
      "Iteration 24231, Loss: 0.055731456726789474\n",
      "Iteration 24232, Loss: 0.05582885071635246\n",
      "Iteration 24233, Loss: 0.05582181736826897\n",
      "Iteration 24234, Loss: 0.05572168156504631\n",
      "Iteration 24235, Loss: 0.055708132684230804\n",
      "Iteration 24236, Loss: 0.0557611808180809\n",
      "Iteration 24237, Loss: 0.05563922971487045\n",
      "Iteration 24238, Loss: 0.05579535290598869\n",
      "Iteration 24239, Loss: 0.055887818336486816\n",
      "Iteration 24240, Loss: 0.05587669461965561\n",
      "Iteration 24241, Loss: 0.055772505700588226\n",
      "Iteration 24242, Loss: 0.05564387887716293\n",
      "Iteration 24243, Loss: 0.05570507049560547\n",
      "Iteration 24244, Loss: 0.05562862008810043\n",
      "Iteration 24245, Loss: 0.05566271394491196\n",
      "Iteration 24246, Loss: 0.05562591552734375\n",
      "Iteration 24247, Loss: 0.055665574967861176\n",
      "Iteration 24248, Loss: 0.05562345311045647\n",
      "Iteration 24249, Loss: 0.05575243756175041\n",
      "Iteration 24250, Loss: 0.05572589486837387\n",
      "Iteration 24251, Loss: 0.05566803738474846\n",
      "Iteration 24252, Loss: 0.05570368096232414\n",
      "Iteration 24253, Loss: 0.05563930794596672\n",
      "Iteration 24254, Loss: 0.05577504634857178\n",
      "Iteration 24255, Loss: 0.05578824132680893\n",
      "Iteration 24256, Loss: 0.05563342571258545\n",
      "Iteration 24257, Loss: 0.055818360298871994\n",
      "Iteration 24258, Loss: 0.055920522660017014\n",
      "Iteration 24259, Loss: 0.055907491594552994\n",
      "Iteration 24260, Loss: 0.05579420179128647\n",
      "Iteration 24261, Loss: 0.05565667524933815\n",
      "Iteration 24262, Loss: 0.055801670998334885\n",
      "Iteration 24263, Loss: 0.05580497160553932\n",
      "Iteration 24264, Loss: 0.055656515061855316\n",
      "Iteration 24265, Loss: 0.055783193558454514\n",
      "Iteration 24266, Loss: 0.05587081238627434\n",
      "Iteration 24267, Loss: 0.055855393409729004\n",
      "Iteration 24268, Loss: 0.055747032165527344\n",
      "Iteration 24269, Loss: 0.0556841716170311\n",
      "Iteration 24270, Loss: 0.05574600026011467\n",
      "Iteration 24271, Loss: 0.05563565343618393\n",
      "Iteration 24272, Loss: 0.05578502267599106\n",
      "Iteration 24273, Loss: 0.05586151406168938\n",
      "Iteration 24274, Loss: 0.05583147332072258\n",
      "Iteration 24275, Loss: 0.05570904538035393\n",
      "Iteration 24276, Loss: 0.05575128644704819\n",
      "Iteration 24277, Loss: 0.05582885071635246\n",
      "Iteration 24278, Loss: 0.05573391914367676\n",
      "Iteration 24279, Loss: 0.055708371102809906\n",
      "Iteration 24280, Loss: 0.05578446388244629\n",
      "Iteration 24281, Loss: 0.05575625225901604\n",
      "Iteration 24282, Loss: 0.055642250925302505\n",
      "Iteration 24283, Loss: 0.05582225322723389\n",
      "Iteration 24284, Loss: 0.05587757006287575\n",
      "Iteration 24285, Loss: 0.05575522035360336\n",
      "Iteration 24286, Loss: 0.05571095272898674\n",
      "Iteration 24287, Loss: 0.05580481141805649\n",
      "Iteration 24288, Loss: 0.05579563230276108\n",
      "Iteration 24289, Loss: 0.05569326877593994\n",
      "Iteration 24290, Loss: 0.05574977770447731\n",
      "Iteration 24291, Loss: 0.0558040551841259\n",
      "Iteration 24292, Loss: 0.055680833756923676\n",
      "Iteration 24293, Loss: 0.05576714128255844\n",
      "Iteration 24294, Loss: 0.05586131662130356\n",
      "Iteration 24295, Loss: 0.055851977318525314\n",
      "Iteration 24296, Loss: 0.05574953928589821\n",
      "Iteration 24297, Loss: 0.05567403882741928\n",
      "Iteration 24298, Loss: 0.055729907006025314\n",
      "Iteration 24299, Loss: 0.05561637878417969\n",
      "Iteration 24300, Loss: 0.05573109909892082\n",
      "Iteration 24301, Loss: 0.055733680725097656\n",
      "Iteration 24302, Loss: 0.05563982576131821\n",
      "Iteration 24303, Loss: 0.05578859895467758\n",
      "Iteration 24304, Loss: 0.05581212043762207\n",
      "Iteration 24305, Loss: 0.05566513538360596\n",
      "Iteration 24306, Loss: 0.055797938257455826\n",
      "Iteration 24307, Loss: 0.055905066430568695\n",
      "Iteration 24308, Loss: 0.05589882656931877\n",
      "Iteration 24309, Loss: 0.05579300969839096\n",
      "Iteration 24310, Loss: 0.05565150827169418\n",
      "Iteration 24311, Loss: 0.05580779165029526\n",
      "Iteration 24312, Loss: 0.0558215007185936\n",
      "Iteration 24313, Loss: 0.055673997849226\n",
      "Iteration 24314, Loss: 0.055782437324523926\n",
      "Iteration 24315, Loss: 0.05588182061910629\n",
      "Iteration 24316, Loss: 0.05587748810648918\n",
      "Iteration 24317, Loss: 0.05577925965189934\n",
      "Iteration 24318, Loss: 0.05563044920563698\n",
      "Iteration 24319, Loss: 0.055692158639431\n",
      "Iteration 24320, Loss: 0.05563513562083244\n",
      "Iteration 24321, Loss: 0.05567193031311035\n",
      "Iteration 24322, Loss: 0.055621348321437836\n",
      "Iteration 24323, Loss: 0.05568262189626694\n",
      "Iteration 24324, Loss: 0.05565520375967026\n",
      "Iteration 24325, Loss: 0.05571047589182854\n",
      "Iteration 24326, Loss: 0.055688779801130295\n",
      "Iteration 24327, Loss: 0.05569104477763176\n",
      "Iteration 24328, Loss: 0.05571989342570305\n",
      "Iteration 24329, Loss: 0.05564828962087631\n",
      "Iteration 24330, Loss: 0.055769603699445724\n",
      "Iteration 24331, Loss: 0.0557909831404686\n",
      "Iteration 24332, Loss: 0.055646102875471115\n",
      "Iteration 24333, Loss: 0.05580691620707512\n",
      "Iteration 24334, Loss: 0.05590832233428955\n",
      "Iteration 24335, Loss: 0.055899541825056076\n",
      "Iteration 24336, Loss: 0.05579324811697006\n",
      "Iteration 24337, Loss: 0.05565059185028076\n",
      "Iteration 24338, Loss: 0.05580679699778557\n",
      "Iteration 24339, Loss: 0.05582018941640854\n",
      "Iteration 24340, Loss: 0.05566863343119621\n",
      "Iteration 24341, Loss: 0.05579090118408203\n",
      "Iteration 24342, Loss: 0.055896125733852386\n",
      "Iteration 24343, Loss: 0.0558956079185009\n",
      "Iteration 24344, Loss: 0.05579976364970207\n",
      "Iteration 24345, Loss: 0.05561975762248039\n",
      "Iteration 24346, Loss: 0.05594528093934059\n",
      "Iteration 24347, Loss: 0.0560842789709568\n",
      "Iteration 24348, Loss: 0.05603337660431862\n",
      "Iteration 24349, Loss: 0.05581235885620117\n",
      "Iteration 24350, Loss: 0.055735033005476\n",
      "Iteration 24351, Loss: 0.0558878593146801\n",
      "Iteration 24352, Loss: 0.05593041703104973\n",
      "Iteration 24353, Loss: 0.05587303638458252\n",
      "Iteration 24354, Loss: 0.055726926773786545\n",
      "Iteration 24355, Loss: 0.05575776472687721\n",
      "Iteration 24356, Loss: 0.055859487503767014\n",
      "Iteration 24357, Loss: 0.05577751249074936\n",
      "Iteration 24358, Loss: 0.055669788271188736\n",
      "Iteration 24359, Loss: 0.05573980137705803\n",
      "Iteration 24360, Loss: 0.05571043863892555\n",
      "Iteration 24361, Loss: 0.05563986301422119\n",
      "Iteration 24362, Loss: 0.055655837059020996\n",
      "Iteration 24363, Loss: 0.05566760152578354\n",
      "Iteration 24364, Loss: 0.05565023794770241\n",
      "Iteration 24365, Loss: 0.055709563195705414\n",
      "Iteration 24366, Loss: 0.05568492412567139\n",
      "Iteration 24367, Loss: 0.055692434310913086\n",
      "Iteration 24368, Loss: 0.0557178258895874\n",
      "Iteration 24369, Loss: 0.05563676357269287\n",
      "Iteration 24370, Loss: 0.05579320713877678\n",
      "Iteration 24371, Loss: 0.0558268241584301\n",
      "Iteration 24372, Loss: 0.05570002645254135\n",
      "Iteration 24373, Loss: 0.055754583328962326\n",
      "Iteration 24374, Loss: 0.0558469332754612\n",
      "Iteration 24375, Loss: 0.05582630634307861\n",
      "Iteration 24376, Loss: 0.055706463754177094\n",
      "Iteration 24377, Loss: 0.05575088784098625\n",
      "Iteration 24378, Loss: 0.05582690238952637\n",
      "Iteration 24379, Loss: 0.055736105889081955\n",
      "Iteration 24380, Loss: 0.055702369660139084\n",
      "Iteration 24381, Loss: 0.05577389523386955\n",
      "Iteration 24382, Loss: 0.05573650449514389\n",
      "Iteration 24383, Loss: 0.05563465878367424\n",
      "Iteration 24384, Loss: 0.055744610726833344\n",
      "Iteration 24385, Loss: 0.05570479482412338\n",
      "Iteration 24386, Loss: 0.05569517984986305\n",
      "Iteration 24387, Loss: 0.05573912709951401\n",
      "Iteration 24388, Loss: 0.05568671226501465\n",
      "Iteration 24389, Loss: 0.05569935217499733\n",
      "Iteration 24390, Loss: 0.05569911003112793\n",
      "Iteration 24391, Loss: 0.055672287940979004\n",
      "Iteration 24392, Loss: 0.05569326877593994\n",
      "Iteration 24393, Loss: 0.055621188133955\n",
      "Iteration 24394, Loss: 0.055802106857299805\n",
      "Iteration 24395, Loss: 0.05581788346171379\n",
      "Iteration 24396, Loss: 0.055669307708740234\n",
      "Iteration 24397, Loss: 0.05579225346446037\n",
      "Iteration 24398, Loss: 0.05589604750275612\n",
      "Iteration 24399, Loss: 0.05588344857096672\n",
      "Iteration 24400, Loss: 0.05576924607157707\n",
      "Iteration 24401, Loss: 0.05567371845245361\n",
      "Iteration 24402, Loss: 0.055764876306056976\n",
      "Iteration 24403, Loss: 0.055699191987514496\n",
      "Iteration 24404, Loss: 0.055718742311000824\n",
      "Iteration 24405, Loss: 0.055775128304958344\n",
      "Iteration 24406, Loss: 0.05573415756225586\n",
      "Iteration 24407, Loss: 0.05564550682902336\n",
      "Iteration 24408, Loss: 0.055746953934431076\n",
      "Iteration 24409, Loss: 0.05572069063782692\n",
      "Iteration 24410, Loss: 0.055675506591796875\n",
      "Iteration 24411, Loss: 0.05571325868368149\n",
      "Iteration 24412, Loss: 0.055664580315351486\n",
      "Iteration 24413, Loss: 0.0557301864027977\n",
      "Iteration 24414, Loss: 0.05573491379618645\n",
      "Iteration 24415, Loss: 0.05564757436513901\n",
      "Iteration 24416, Loss: 0.05570809170603752\n",
      "Iteration 24417, Loss: 0.05567812919616699\n",
      "Iteration 24418, Loss: 0.05570177361369133\n",
      "Iteration 24419, Loss: 0.055711984634399414\n",
      "Iteration 24420, Loss: 0.055643677711486816\n",
      "Iteration 24421, Loss: 0.05570376291871071\n",
      "Iteration 24422, Loss: 0.05564781278371811\n",
      "Iteration 24423, Loss: 0.05574631690979004\n",
      "Iteration 24424, Loss: 0.05579257011413574\n",
      "Iteration 24425, Loss: 0.05573570728302002\n",
      "Iteration 24426, Loss: 0.05565425008535385\n",
      "Iteration 24427, Loss: 0.05572100728750229\n",
      "Iteration 24428, Loss: 0.055662475526332855\n",
      "Iteration 24429, Loss: 0.05572279542684555\n",
      "Iteration 24430, Loss: 0.05575990676879883\n",
      "Iteration 24431, Loss: 0.05569855496287346\n",
      "Iteration 24432, Loss: 0.05569565296173096\n",
      "Iteration 24433, Loss: 0.055708371102809906\n",
      "Iteration 24434, Loss: 0.05565500259399414\n",
      "Iteration 24435, Loss: 0.055668991059064865\n",
      "Iteration 24436, Loss: 0.055644869804382324\n",
      "Iteration 24437, Loss: 0.05564538761973381\n",
      "Iteration 24438, Loss: 0.05563509464263916\n",
      "Iteration 24439, Loss: 0.05568341538310051\n",
      "Iteration 24440, Loss: 0.05564538761973381\n",
      "Iteration 24441, Loss: 0.05572501942515373\n",
      "Iteration 24442, Loss: 0.05572076886892319\n",
      "Iteration 24443, Loss: 0.05564320087432861\n",
      "Iteration 24444, Loss: 0.055667322129011154\n",
      "Iteration 24445, Loss: 0.05565778538584709\n",
      "Iteration 24446, Loss: 0.055636171251535416\n",
      "Iteration 24447, Loss: 0.05569911003112793\n",
      "Iteration 24448, Loss: 0.05564793199300766\n",
      "Iteration 24449, Loss: 0.05574524402618408\n",
      "Iteration 24450, Loss: 0.05578911677002907\n",
      "Iteration 24451, Loss: 0.05572688952088356\n",
      "Iteration 24452, Loss: 0.055667560547590256\n",
      "Iteration 24453, Loss: 0.055709801614284515\n",
      "Iteration 24454, Loss: 0.05564284324645996\n",
      "Iteration 24455, Loss: 0.0556974820792675\n",
      "Iteration 24456, Loss: 0.05569366738200188\n",
      "Iteration 24457, Loss: 0.05563656613230705\n",
      "Iteration 24458, Loss: 0.055649518966674805\n",
      "Iteration 24459, Loss: 0.055645786225795746\n",
      "Iteration 24460, Loss: 0.05565154552459717\n",
      "Iteration 24461, Loss: 0.055651187896728516\n",
      "Iteration 24462, Loss: 0.05562468618154526\n",
      "Iteration 24463, Loss: 0.05573539063334465\n",
      "Iteration 24464, Loss: 0.05572068691253662\n",
      "Iteration 24465, Loss: 0.05565468594431877\n",
      "Iteration 24466, Loss: 0.0556923970580101\n",
      "Iteration 24467, Loss: 0.05563465878367424\n",
      "Iteration 24468, Loss: 0.05572366714477539\n",
      "Iteration 24469, Loss: 0.05574226379394531\n",
      "Iteration 24470, Loss: 0.05566819757223129\n",
      "Iteration 24471, Loss: 0.05574759095907211\n",
      "Iteration 24472, Loss: 0.05576881021261215\n",
      "Iteration 24473, Loss: 0.055619798600673676\n",
      "Iteration 24474, Loss: 0.0558164156973362\n",
      "Iteration 24475, Loss: 0.05591416358947754\n",
      "Iteration 24476, Loss: 0.05590685456991196\n",
      "Iteration 24477, Loss: 0.05580528825521469\n",
      "Iteration 24478, Loss: 0.05562357231974602\n",
      "Iteration 24479, Loss: 0.05592723935842514\n",
      "Iteration 24480, Loss: 0.05605395883321762\n",
      "Iteration 24481, Loss: 0.0559951476752758\n",
      "Iteration 24482, Loss: 0.05577143281698227\n",
      "Iteration 24483, Loss: 0.05576741695404053\n",
      "Iteration 24484, Loss: 0.05592147633433342\n",
      "Iteration 24485, Loss: 0.055966101586818695\n",
      "Iteration 24486, Loss: 0.0559113435447216\n",
      "Iteration 24487, Loss: 0.05576729774475098\n",
      "Iteration 24488, Loss: 0.055700622498989105\n",
      "Iteration 24489, Loss: 0.05580083653330803\n",
      "Iteration 24490, Loss: 0.055720292031764984\n",
      "Iteration 24491, Loss: 0.055709563195705414\n",
      "Iteration 24492, Loss: 0.05577782914042473\n",
      "Iteration 24493, Loss: 0.055744968354701996\n",
      "Iteration 24494, Loss: 0.05562170594930649\n",
      "Iteration 24495, Loss: 0.055869024246931076\n",
      "Iteration 24496, Loss: 0.05594277381896973\n",
      "Iteration 24497, Loss: 0.05583604425191879\n",
      "Iteration 24498, Loss: 0.05564316362142563\n",
      "Iteration 24499, Loss: 0.05573185533285141\n",
      "Iteration 24500, Loss: 0.05572013184428215\n",
      "Iteration 24501, Loss: 0.05561518669128418\n",
      "Iteration 24502, Loss: 0.055853091180324554\n",
      "Iteration 24503, Loss: 0.055910151451826096\n",
      "Iteration 24504, Loss: 0.05579730123281479\n",
      "Iteration 24505, Loss: 0.055675070732831955\n",
      "Iteration 24506, Loss: 0.05576733872294426\n",
      "Iteration 24507, Loss: 0.05575820058584213\n",
      "Iteration 24508, Loss: 0.05564860627055168\n",
      "Iteration 24509, Loss: 0.05581498146057129\n",
      "Iteration 24510, Loss: 0.05588114634156227\n",
      "Iteration 24511, Loss: 0.05578406900167465\n",
      "Iteration 24512, Loss: 0.055673401802778244\n",
      "Iteration 24513, Loss: 0.055758796632289886\n",
      "Iteration 24514, Loss: 0.05574103444814682\n",
      "Iteration 24515, Loss: 0.05561674013733864\n",
      "Iteration 24516, Loss: 0.055832069367170334\n",
      "Iteration 24517, Loss: 0.05590172857046127\n",
      "Iteration 24518, Loss: 0.055832624435424805\n",
      "Iteration 24519, Loss: 0.05567284673452377\n",
      "Iteration 24520, Loss: 0.05581176280975342\n",
      "Iteration 24521, Loss: 0.055900733917951584\n",
      "Iteration 24522, Loss: 0.055836718529462814\n",
      "Iteration 24523, Loss: 0.055641453713178635\n",
      "Iteration 24524, Loss: 0.05586473271250725\n",
      "Iteration 24525, Loss: 0.0559922456741333\n",
      "Iteration 24526, Loss: 0.05597484111785889\n",
      "Iteration 24527, Loss: 0.055831991136074066\n",
      "Iteration 24528, Loss: 0.05567439645528793\n",
      "Iteration 24529, Loss: 0.05582471936941147\n",
      "Iteration 24530, Loss: 0.0558655671775341\n",
      "Iteration 24531, Loss: 0.05575327202677727\n",
      "Iteration 24532, Loss: 0.05570737645030022\n",
      "Iteration 24533, Loss: 0.05579042434692383\n",
      "Iteration 24534, Loss: 0.05575438588857651\n",
      "Iteration 24535, Loss: 0.05565222352743149\n",
      "Iteration 24536, Loss: 0.05577155202627182\n",
      "Iteration 24537, Loss: 0.055777352303266525\n",
      "Iteration 24538, Loss: 0.0556282214820385\n",
      "Iteration 24539, Loss: 0.0557788647711277\n",
      "Iteration 24540, Loss: 0.05584593862295151\n",
      "Iteration 24541, Loss: 0.05581224337220192\n",
      "Iteration 24542, Loss: 0.05568818375468254\n",
      "Iteration 24543, Loss: 0.05578351393342018\n",
      "Iteration 24544, Loss: 0.055861592292785645\n",
      "Iteration 24545, Loss: 0.05575970932841301\n",
      "Iteration 24546, Loss: 0.05569517984986305\n",
      "Iteration 24547, Loss: 0.05577671900391579\n",
      "Iteration 24548, Loss: 0.05575660988688469\n",
      "Iteration 24549, Loss: 0.05564502999186516\n",
      "Iteration 24550, Loss: 0.05582654848694801\n",
      "Iteration 24551, Loss: 0.05589119717478752\n",
      "Iteration 24552, Loss: 0.05577810853719711\n",
      "Iteration 24553, Loss: 0.05568870157003403\n",
      "Iteration 24554, Loss: 0.055777113884687424\n",
      "Iteration 24555, Loss: 0.05576356500387192\n",
      "Iteration 24556, Loss: 0.05565715208649635\n",
      "Iteration 24557, Loss: 0.05580369755625725\n",
      "Iteration 24558, Loss: 0.05586334317922592\n",
      "Iteration 24559, Loss: 0.055747948586940765\n",
      "Iteration 24560, Loss: 0.0557120256125927\n",
      "Iteration 24561, Loss: 0.055801354348659515\n",
      "Iteration 24562, Loss: 0.05578704923391342\n",
      "Iteration 24563, Loss: 0.0556788444519043\n",
      "Iteration 24564, Loss: 0.055775921791791916\n",
      "Iteration 24565, Loss: 0.05583759397268295\n",
      "Iteration 24566, Loss: 0.055724382400512695\n",
      "Iteration 24567, Loss: 0.05572764202952385\n",
      "Iteration 24568, Loss: 0.055814944207668304\n",
      "Iteration 24569, Loss: 0.05579805374145508\n",
      "Iteration 24570, Loss: 0.0556873083114624\n",
      "Iteration 24571, Loss: 0.05576702207326889\n",
      "Iteration 24572, Loss: 0.05583115667104721\n",
      "Iteration 24573, Loss: 0.055720530450344086\n",
      "Iteration 24574, Loss: 0.05572919175028801\n",
      "Iteration 24575, Loss: 0.055815260857343674\n",
      "Iteration 24576, Loss: 0.05579730123281479\n",
      "Iteration 24577, Loss: 0.05568603798747063\n",
      "Iteration 24578, Loss: 0.055769484490156174\n",
      "Iteration 24579, Loss: 0.05583401769399643\n",
      "Iteration 24580, Loss: 0.05572303384542465\n",
      "Iteration 24581, Loss: 0.05572783946990967\n",
      "Iteration 24582, Loss: 0.05581434816122055\n",
      "Iteration 24583, Loss: 0.0557967834174633\n",
      "Iteration 24584, Loss: 0.05568627640604973\n",
      "Iteration 24585, Loss: 0.05576789751648903\n",
      "Iteration 24586, Loss: 0.05583127588033676\n",
      "Iteration 24587, Loss: 0.05571862310171127\n",
      "Iteration 24588, Loss: 0.05573217198252678\n",
      "Iteration 24589, Loss: 0.05581963062286377\n",
      "Iteration 24590, Loss: 0.05580329895019531\n",
      "Iteration 24591, Loss: 0.05569450184702873\n",
      "Iteration 24592, Loss: 0.05575565621256828\n",
      "Iteration 24593, Loss: 0.05581788346171379\n",
      "Iteration 24594, Loss: 0.05570328235626221\n",
      "Iteration 24595, Loss: 0.055745046585798264\n",
      "Iteration 24596, Loss: 0.05583366006612778\n",
      "Iteration 24597, Loss: 0.05581867694854736\n",
      "Iteration 24598, Loss: 0.05571087449789047\n",
      "Iteration 24599, Loss: 0.05573248863220215\n",
      "Iteration 24600, Loss: 0.05579332634806633\n",
      "Iteration 24601, Loss: 0.05567765608429909\n",
      "Iteration 24602, Loss: 0.05576467514038086\n",
      "Iteration 24603, Loss: 0.055854201316833496\n",
      "Iteration 24604, Loss: 0.05584069341421127\n",
      "Iteration 24605, Loss: 0.05573407933115959\n",
      "Iteration 24606, Loss: 0.055701497942209244\n",
      "Iteration 24607, Loss: 0.05576233193278313\n",
      "Iteration 24608, Loss: 0.055647533386945724\n",
      "Iteration 24609, Loss: 0.055786292999982834\n",
      "Iteration 24610, Loss: 0.05587446689605713\n",
      "Iteration 24611, Loss: 0.05585968494415283\n",
      "Iteration 24612, Loss: 0.05575219914317131\n",
      "Iteration 24613, Loss: 0.0556773766875267\n",
      "Iteration 24614, Loss: 0.055737774819135666\n",
      "Iteration 24615, Loss: 0.05562182515859604\n",
      "Iteration 24616, Loss: 0.055799007415771484\n",
      "Iteration 24617, Loss: 0.05587999150156975\n",
      "Iteration 24618, Loss: 0.055855873972177505\n",
      "Iteration 24619, Loss: 0.05573892593383789\n",
      "Iteration 24620, Loss: 0.05570896714925766\n",
      "Iteration 24621, Loss: 0.055780570954084396\n",
      "Iteration 24622, Loss: 0.05567697808146477\n",
      "Iteration 24623, Loss: 0.05575796216726303\n",
      "Iteration 24624, Loss: 0.05583910271525383\n",
      "Iteration 24625, Loss: 0.05581764504313469\n",
      "Iteration 24626, Loss: 0.055705588310956955\n",
      "Iteration 24627, Loss: 0.05574759095907211\n",
      "Iteration 24628, Loss: 0.05581232160329819\n",
      "Iteration 24629, Loss: 0.05569875240325928\n",
      "Iteration 24630, Loss: 0.05574874207377434\n",
      "Iteration 24631, Loss: 0.0558367595076561\n",
      "Iteration 24632, Loss: 0.0558219775557518\n",
      "Iteration 24633, Loss: 0.0557154044508934\n",
      "Iteration 24634, Loss: 0.055727045983076096\n",
      "Iteration 24635, Loss: 0.05578573793172836\n",
      "Iteration 24636, Loss: 0.05566736310720444\n",
      "Iteration 24637, Loss: 0.05577373504638672\n",
      "Iteration 24638, Loss: 0.05586361885070801\n",
      "Iteration 24639, Loss: 0.05585058778524399\n",
      "Iteration 24640, Loss: 0.055744729936122894\n",
      "Iteration 24641, Loss: 0.05568584054708481\n",
      "Iteration 24642, Loss: 0.05574413388967514\n",
      "Iteration 24643, Loss: 0.05562559887766838\n",
      "Iteration 24644, Loss: 0.05580298230051994\n",
      "Iteration 24645, Loss: 0.055890560150146484\n",
      "Iteration 24646, Loss: 0.055873993784189224\n",
      "Iteration 24647, Loss: 0.055764518678188324\n",
      "Iteration 24648, Loss: 0.055666130036115646\n",
      "Iteration 24649, Loss: 0.05573999881744385\n",
      "Iteration 24650, Loss: 0.0556488074362278\n",
      "Iteration 24651, Loss: 0.05576157942414284\n",
      "Iteration 24652, Loss: 0.05582702159881592\n",
      "Iteration 24653, Loss: 0.05579022690653801\n",
      "Iteration 24654, Loss: 0.05566287040710449\n",
      "Iteration 24655, Loss: 0.05582253262400627\n",
      "Iteration 24656, Loss: 0.05590371415019035\n",
      "Iteration 24657, Loss: 0.05580238625407219\n",
      "Iteration 24658, Loss: 0.055664461106061935\n",
      "Iteration 24659, Loss: 0.05574604123830795\n",
      "Iteration 24660, Loss: 0.055725060403347015\n",
      "Iteration 24661, Loss: 0.055615704506635666\n",
      "Iteration 24662, Loss: 0.05581343546509743\n",
      "Iteration 24663, Loss: 0.05583202838897705\n",
      "Iteration 24664, Loss: 0.05568552389740944\n",
      "Iteration 24665, Loss: 0.05577898025512695\n",
      "Iteration 24666, Loss: 0.055883608758449554\n",
      "Iteration 24667, Loss: 0.05587955564260483\n",
      "Iteration 24668, Loss: 0.05577782914042473\n",
      "Iteration 24669, Loss: 0.05564737319946289\n",
      "Iteration 24670, Loss: 0.05575374886393547\n",
      "Iteration 24671, Loss: 0.05570268630981445\n",
      "Iteration 24672, Loss: 0.05570530891418457\n",
      "Iteration 24673, Loss: 0.05575525760650635\n",
      "Iteration 24674, Loss: 0.055709365755319595\n",
      "Iteration 24675, Loss: 0.05566362664103508\n",
      "Iteration 24676, Loss: 0.0556669645011425\n",
      "Iteration 24677, Loss: 0.055688146501779556\n",
      "Iteration 24678, Loss: 0.055699825286865234\n",
      "Iteration 24679, Loss: 0.05562027543783188\n",
      "Iteration 24680, Loss: 0.0557984933257103\n",
      "Iteration 24681, Loss: 0.05581573769450188\n",
      "Iteration 24682, Loss: 0.05568019673228264\n",
      "Iteration 24683, Loss: 0.055776000022888184\n",
      "Iteration 24684, Loss: 0.055867474526166916\n",
      "Iteration 24685, Loss: 0.055836521089076996\n",
      "Iteration 24686, Loss: 0.055699270218610764\n",
      "Iteration 24687, Loss: 0.05577763170003891\n",
      "Iteration 24688, Loss: 0.055868905037641525\n",
      "Iteration 24689, Loss: 0.05579376593232155\n",
      "Iteration 24690, Loss: 0.05564689636230469\n",
      "Iteration 24691, Loss: 0.055726371705532074\n",
      "Iteration 24692, Loss: 0.0556894950568676\n",
      "Iteration 24693, Loss: 0.05569124221801758\n",
      "Iteration 24694, Loss: 0.05569986626505852\n",
      "Iteration 24695, Loss: 0.05565138906240463\n",
      "Iteration 24696, Loss: 0.05565973371267319\n",
      "Iteration 24697, Loss: 0.05567936226725578\n",
      "Iteration 24698, Loss: 0.05565552040934563\n",
      "Iteration 24699, Loss: 0.05570368096232414\n",
      "Iteration 24700, Loss: 0.05569537729024887\n",
      "Iteration 24701, Loss: 0.055665016174316406\n",
      "Iteration 24702, Loss: 0.0556643046438694\n",
      "Iteration 24703, Loss: 0.05568333715200424\n",
      "Iteration 24704, Loss: 0.05566597357392311\n",
      "Iteration 24705, Loss: 0.055698834359645844\n",
      "Iteration 24706, Loss: 0.05570109933614731\n",
      "Iteration 24707, Loss: 0.0556461438536644\n",
      "Iteration 24708, Loss: 0.055654287338256836\n",
      "Iteration 24709, Loss: 0.055687349289655685\n",
      "Iteration 24710, Loss: 0.05567801371216774\n",
      "Iteration 24711, Loss: 0.05566728115081787\n",
      "Iteration 24712, Loss: 0.05565103143453598\n",
      "Iteration 24713, Loss: 0.055713020265102386\n",
      "Iteration 24714, Loss: 0.05572772026062012\n",
      "Iteration 24715, Loss: 0.055651627480983734\n",
      "Iteration 24716, Loss: 0.05576245114207268\n",
      "Iteration 24717, Loss: 0.05577627941966057\n",
      "Iteration 24718, Loss: 0.055621545761823654\n",
      "Iteration 24719, Loss: 0.055834852159023285\n",
      "Iteration 24720, Loss: 0.055947624146938324\n",
      "Iteration 24721, Loss: 0.05595302954316139\n",
      "Iteration 24722, Loss: 0.05586262792348862\n",
      "Iteration 24723, Loss: 0.055690012872219086\n",
      "Iteration 24724, Loss: 0.05584033578634262\n",
      "Iteration 24725, Loss: 0.0559699572622776\n",
      "Iteration 24726, Loss: 0.0559157133102417\n",
      "Iteration 24727, Loss: 0.05569581314921379\n",
      "Iteration 24728, Loss: 0.05582181736826897\n",
      "Iteration 24729, Loss: 0.05597412958741188\n",
      "Iteration 24730, Loss: 0.056016288697719574\n",
      "Iteration 24731, Loss: 0.055958788841962814\n",
      "Iteration 24732, Loss: 0.05581247806549072\n",
      "Iteration 24733, Loss: 0.05564828962087631\n",
      "Iteration 24734, Loss: 0.05578160285949707\n",
      "Iteration 24735, Loss: 0.055743299424648285\n",
      "Iteration 24736, Loss: 0.055669866502285004\n",
      "Iteration 24737, Loss: 0.05571727082133293\n",
      "Iteration 24738, Loss: 0.05567499250173569\n",
      "Iteration 24739, Loss: 0.05570733919739723\n",
      "Iteration 24740, Loss: 0.05569803714752197\n",
      "Iteration 24741, Loss: 0.05567892640829086\n",
      "Iteration 24742, Loss: 0.05570375919342041\n",
      "Iteration 24743, Loss: 0.055633507668972015\n",
      "Iteration 24744, Loss: 0.055790942162275314\n",
      "Iteration 24745, Loss: 0.05581120774149895\n",
      "Iteration 24746, Loss: 0.055669110268354416\n",
      "Iteration 24747, Loss: 0.05578816309571266\n",
      "Iteration 24748, Loss: 0.05588619038462639\n",
      "Iteration 24749, Loss: 0.055866800248622894\n",
      "Iteration 24750, Loss: 0.05574524402618408\n",
      "Iteration 24751, Loss: 0.05570860952138901\n",
      "Iteration 24752, Loss: 0.05579058453440666\n",
      "Iteration 24753, Loss: 0.05570606514811516\n",
      "Iteration 24754, Loss: 0.05572676658630371\n",
      "Iteration 24755, Loss: 0.05579479783773422\n",
      "Iteration 24756, Loss: 0.05575617402791977\n",
      "Iteration 24757, Loss: 0.05564681813120842\n",
      "Iteration 24758, Loss: 0.05579809471964836\n",
      "Iteration 24759, Loss: 0.05582427978515625\n",
      "Iteration 24760, Loss: 0.055678848177194595\n",
      "Iteration 24761, Loss: 0.0557832345366478\n",
      "Iteration 24762, Loss: 0.055888257920742035\n",
      "Iteration 24763, Loss: 0.05588885396718979\n",
      "Iteration 24764, Loss: 0.055795393884181976\n",
      "Iteration 24765, Loss: 0.05562500283122063\n",
      "Iteration 24766, Loss: 0.05589338392019272\n",
      "Iteration 24767, Loss: 0.05599125474691391\n",
      "Iteration 24768, Loss: 0.05591229721903801\n",
      "Iteration 24769, Loss: 0.05568492412567139\n",
      "Iteration 24770, Loss: 0.05583282560110092\n",
      "Iteration 24771, Loss: 0.05598298832774162\n",
      "Iteration 24772, Loss: 0.05601867288351059\n",
      "Iteration 24773, Loss: 0.05595092102885246\n",
      "Iteration 24774, Loss: 0.055792056024074554\n",
      "Iteration 24775, Loss: 0.055693309754133224\n",
      "Iteration 24776, Loss: 0.05582030862569809\n",
      "Iteration 24777, Loss: 0.055778902024030685\n",
      "Iteration 24778, Loss: 0.055645350366830826\n",
      "Iteration 24779, Loss: 0.05570181459188461\n",
      "Iteration 24780, Loss: 0.055676184594631195\n",
      "Iteration 24781, Loss: 0.055683497339487076\n",
      "Iteration 24782, Loss: 0.05565790459513664\n",
      "Iteration 24783, Loss: 0.05571373552083969\n",
      "Iteration 24784, Loss: 0.055742185562849045\n",
      "Iteration 24785, Loss: 0.055673323571681976\n",
      "Iteration 24786, Loss: 0.055737853050231934\n",
      "Iteration 24787, Loss: 0.05575573816895485\n",
      "Iteration 24788, Loss: 0.055621903389692307\n",
      "Iteration 24789, Loss: 0.055692676454782486\n",
      "Iteration 24790, Loss: 0.05565055459737778\n",
      "Iteration 24791, Loss: 0.055735111236572266\n",
      "Iteration 24792, Loss: 0.05574166774749756\n",
      "Iteration 24793, Loss: 0.05563469976186752\n",
      "Iteration 24794, Loss: 0.0557405948638916\n",
      "Iteration 24795, Loss: 0.05572780221700668\n",
      "Iteration 24796, Loss: 0.05564538761973381\n",
      "Iteration 24797, Loss: 0.055661797523498535\n",
      "Iteration 24798, Loss: 0.05566847324371338\n",
      "Iteration 24799, Loss: 0.05563521757721901\n",
      "Iteration 24800, Loss: 0.0557352714240551\n",
      "Iteration 24801, Loss: 0.05573388189077377\n",
      "Iteration 24802, Loss: 0.05563763901591301\n",
      "Iteration 24803, Loss: 0.05572740361094475\n",
      "Iteration 24804, Loss: 0.05569291114807129\n",
      "Iteration 24805, Loss: 0.05569744482636452\n",
      "Iteration 24806, Loss: 0.05572883412241936\n",
      "Iteration 24807, Loss: 0.05565802380442619\n",
      "Iteration 24808, Loss: 0.05575207993388176\n",
      "Iteration 24809, Loss: 0.055770598351955414\n",
      "Iteration 24810, Loss: 0.05562814325094223\n",
      "Iteration 24811, Loss: 0.05582293123006821\n",
      "Iteration 24812, Loss: 0.05591897293925285\n",
      "Iteration 24813, Loss: 0.055898070335388184\n",
      "Iteration 24814, Loss: 0.05577854439616203\n",
      "Iteration 24815, Loss: 0.05568389222025871\n",
      "Iteration 24816, Loss: 0.05579241365194321\n",
      "Iteration 24817, Loss: 0.055765312165021896\n",
      "Iteration 24818, Loss: 0.05565802380442619\n",
      "Iteration 24819, Loss: 0.05571568012237549\n",
      "Iteration 24820, Loss: 0.05571170896291733\n",
      "Iteration 24821, Loss: 0.05564689636230469\n",
      "Iteration 24822, Loss: 0.0557459220290184\n",
      "Iteration 24823, Loss: 0.0557357482612133\n",
      "Iteration 24824, Loss: 0.05565885826945305\n",
      "Iteration 24825, Loss: 0.0557100772857666\n",
      "Iteration 24826, Loss: 0.055677495896816254\n",
      "Iteration 24827, Loss: 0.05570761486887932\n",
      "Iteration 24828, Loss: 0.05571603775024414\n",
      "Iteration 24829, Loss: 0.055651430040597916\n",
      "Iteration 24830, Loss: 0.055708806961774826\n",
      "Iteration 24831, Loss: 0.05566827580332756\n",
      "Iteration 24832, Loss: 0.05572307109832764\n",
      "Iteration 24833, Loss: 0.055756449699401855\n",
      "Iteration 24834, Loss: 0.055689774453639984\n",
      "Iteration 24835, Loss: 0.05571349710226059\n",
      "Iteration 24836, Loss: 0.05573586747050285\n",
      "Iteration 24837, Loss: 0.055628299713134766\n",
      "Iteration 24838, Loss: 0.05569251626729965\n",
      "Iteration 24839, Loss: 0.055675946176052094\n",
      "Iteration 24840, Loss: 0.05567244812846184\n",
      "Iteration 24841, Loss: 0.055637042969465256\n",
      "Iteration 24842, Loss: 0.05573960393667221\n",
      "Iteration 24843, Loss: 0.055773936212062836\n",
      "Iteration 24844, Loss: 0.05570543184876442\n",
      "Iteration 24845, Loss: 0.05569581314921379\n",
      "Iteration 24846, Loss: 0.05571858212351799\n",
      "Iteration 24847, Loss: 0.05564220994710922\n",
      "Iteration 24848, Loss: 0.05565246194601059\n",
      "Iteration 24849, Loss: 0.055661480873823166\n",
      "Iteration 24850, Loss: 0.055637042969465256\n",
      "Iteration 24851, Loss: 0.05565444752573967\n",
      "Iteration 24852, Loss: 0.05563434213399887\n",
      "Iteration 24853, Loss: 0.05566128343343735\n",
      "Iteration 24854, Loss: 0.05562051385641098\n",
      "Iteration 24855, Loss: 0.055753033608198166\n",
      "Iteration 24856, Loss: 0.05572160333395004\n",
      "Iteration 24857, Loss: 0.055676382035017014\n",
      "Iteration 24858, Loss: 0.055715326219797134\n",
      "Iteration 24859, Loss: 0.055656276643276215\n",
      "Iteration 24860, Loss: 0.0557481050491333\n",
      "Iteration 24861, Loss: 0.055756013840436935\n",
      "Iteration 24862, Loss: 0.05562679097056389\n",
      "Iteration 24863, Loss: 0.05567721650004387\n",
      "Iteration 24864, Loss: 0.05562194436788559\n",
      "Iteration 24865, Loss: 0.055783312767744064\n",
      "Iteration 24866, Loss: 0.0557987317442894\n",
      "Iteration 24867, Loss: 0.055671416223049164\n",
      "Iteration 24868, Loss: 0.05577751249074936\n",
      "Iteration 24869, Loss: 0.05586008355021477\n",
      "Iteration 24870, Loss: 0.05581708997488022\n",
      "Iteration 24871, Loss: 0.05567189306020737\n",
      "Iteration 24872, Loss: 0.05581136792898178\n",
      "Iteration 24873, Loss: 0.05590411275625229\n",
      "Iteration 24874, Loss: 0.0558367595076561\n",
      "Iteration 24875, Loss: 0.05563918873667717\n",
      "Iteration 24876, Loss: 0.055846892297267914\n",
      "Iteration 24877, Loss: 0.05595707893371582\n",
      "Iteration 24878, Loss: 0.05593069642782211\n",
      "Iteration 24879, Loss: 0.055789314210414886\n",
      "Iteration 24880, Loss: 0.05569859594106674\n",
      "Iteration 24881, Loss: 0.05581573769450188\n",
      "Iteration 24882, Loss: 0.05580481141805649\n",
      "Iteration 24883, Loss: 0.05565552040934563\n",
      "Iteration 24884, Loss: 0.05581315606832504\n",
      "Iteration 24885, Loss: 0.055916350334882736\n",
      "Iteration 24886, Loss: 0.055905066430568695\n",
      "Iteration 24887, Loss: 0.05579773709177971\n",
      "Iteration 24888, Loss: 0.05567566677927971\n",
      "Iteration 24889, Loss: 0.05581045150756836\n",
      "Iteration 24890, Loss: 0.05583556741476059\n",
      "Iteration 24891, Loss: 0.05570201203227043\n",
      "Iteration 24892, Loss: 0.055762529373168945\n",
      "Iteration 24893, Loss: 0.05585885047912598\n",
      "Iteration 24894, Loss: 0.055853843688964844\n",
      "Iteration 24895, Loss: 0.05575760453939438\n",
      "Iteration 24896, Loss: 0.0556664876639843\n",
      "Iteration 24897, Loss: 0.055741190910339355\n",
      "Iteration 24898, Loss: 0.05567324161529541\n",
      "Iteration 24899, Loss: 0.055727604776620865\n",
      "Iteration 24900, Loss: 0.0557740144431591\n",
      "Iteration 24901, Loss: 0.05571834370493889\n",
      "Iteration 24902, Loss: 0.05566549301147461\n",
      "Iteration 24903, Loss: 0.05567371845245361\n",
      "Iteration 24904, Loss: 0.05568333715200424\n",
      "Iteration 24905, Loss: 0.05569418519735336\n",
      "Iteration 24906, Loss: 0.05561995506286621\n",
      "Iteration 24907, Loss: 0.055691324174404144\n",
      "Iteration 24908, Loss: 0.05563418194651604\n",
      "Iteration 24909, Loss: 0.055738095194101334\n",
      "Iteration 24910, Loss: 0.0557384118437767\n",
      "Iteration 24911, Loss: 0.055633544921875\n",
      "Iteration 24912, Loss: 0.055747710168361664\n",
      "Iteration 24913, Loss: 0.055732689797878265\n",
      "Iteration 24914, Loss: 0.05564868450164795\n",
      "Iteration 24915, Loss: 0.05567018315196037\n",
      "Iteration 24916, Loss: 0.055645786225795746\n",
      "Iteration 24917, Loss: 0.055627308785915375\n",
      "Iteration 24918, Loss: 0.05567093938589096\n",
      "Iteration 24919, Loss: 0.05562937632203102\n",
      "Iteration 24920, Loss: 0.05566231533885002\n",
      "Iteration 24921, Loss: 0.05565353482961655\n",
      "Iteration 24922, Loss: 0.05563279241323471\n",
      "Iteration 24923, Loss: 0.05572708696126938\n",
      "Iteration 24924, Loss: 0.055685125291347504\n",
      "Iteration 24925, Loss: 0.05571131035685539\n",
      "Iteration 24926, Loss: 0.05575716868042946\n",
      "Iteration 24927, Loss: 0.055704712867736816\n",
      "Iteration 24928, Loss: 0.05567439645528793\n",
      "Iteration 24929, Loss: 0.05567439645528793\n",
      "Iteration 24930, Loss: 0.055690888315439224\n",
      "Iteration 24931, Loss: 0.05571138858795166\n",
      "Iteration 24932, Loss: 0.05563744157552719\n",
      "Iteration 24933, Loss: 0.05579129979014397\n",
      "Iteration 24934, Loss: 0.055813077837228775\n",
      "Iteration 24935, Loss: 0.055660609155893326\n",
      "Iteration 24936, Loss: 0.0558016300201416\n",
      "Iteration 24937, Loss: 0.05591261386871338\n",
      "Iteration 24938, Loss: 0.05591817945241928\n",
      "Iteration 24939, Loss: 0.055828653275966644\n",
      "Iteration 24940, Loss: 0.05565452575683594\n",
      "Iteration 24941, Loss: 0.05589115619659424\n",
      "Iteration 24942, Loss: 0.056023918092250824\n",
      "Iteration 24943, Loss: 0.05596967786550522\n",
      "Iteration 24944, Loss: 0.05574822798371315\n",
      "Iteration 24945, Loss: 0.05578315630555153\n",
      "Iteration 24946, Loss: 0.05593609809875488\n",
      "Iteration 24947, Loss: 0.05597945302724838\n",
      "Iteration 24948, Loss: 0.05592334643006325\n",
      "Iteration 24949, Loss: 0.055778466165065765\n",
      "Iteration 24950, Loss: 0.05568687245249748\n",
      "Iteration 24951, Loss: 0.05578760430216789\n",
      "Iteration 24952, Loss: 0.05570642277598381\n",
      "Iteration 24953, Loss: 0.05572120472788811\n",
      "Iteration 24954, Loss: 0.05579022690653801\n",
      "Iteration 24955, Loss: 0.055758439004421234\n",
      "Iteration 24956, Loss: 0.05563640594482422\n",
      "Iteration 24957, Loss: 0.05585170164704323\n",
      "Iteration 24958, Loss: 0.05592719838023186\n",
      "Iteration 24959, Loss: 0.055822014808654785\n",
      "Iteration 24960, Loss: 0.055652741342782974\n",
      "Iteration 24961, Loss: 0.055738091468811035\n",
      "Iteration 24962, Loss: 0.055722832679748535\n",
      "Iteration 24963, Loss: 0.055616021156311035\n",
      "Iteration 24964, Loss: 0.05584903806447983\n",
      "Iteration 24965, Loss: 0.055903397500514984\n",
      "Iteration 24966, Loss: 0.05578971281647682\n",
      "Iteration 24967, Loss: 0.0556817464530468\n",
      "Iteration 24968, Loss: 0.055773377418518066\n",
      "Iteration 24969, Loss: 0.05576229467988014\n",
      "Iteration 24970, Loss: 0.055650435388088226\n",
      "Iteration 24971, Loss: 0.055815815925598145\n",
      "Iteration 24972, Loss: 0.05588475987315178\n",
      "Iteration 24973, Loss: 0.05579014867544174\n",
      "Iteration 24974, Loss: 0.055668555200099945\n",
      "Iteration 24975, Loss: 0.055756013840436935\n",
      "Iteration 24976, Loss: 0.05574178695678711\n",
      "Iteration 24977, Loss: 0.0556185245513916\n",
      "Iteration 24978, Loss: 0.0558367595076561\n",
      "Iteration 24979, Loss: 0.055911265313625336\n",
      "Iteration 24980, Loss: 0.05584562197327614\n",
      "Iteration 24981, Loss: 0.055682502686977386\n",
      "Iteration 24982, Loss: 0.055809300392866135\n",
      "Iteration 24983, Loss: 0.055909596383571625\n",
      "Iteration 24984, Loss: 0.05585912987589836\n",
      "Iteration 24985, Loss: 0.05567499250173569\n",
      "Iteration 24986, Loss: 0.055831313133239746\n",
      "Iteration 24987, Loss: 0.05596137046813965\n",
      "Iteration 24988, Loss: 0.05594861879944801\n",
      "Iteration 24989, Loss: 0.05581232160329819\n",
      "Iteration 24990, Loss: 0.0556764230132103\n",
      "Iteration 24991, Loss: 0.0558091402053833\n",
      "Iteration 24992, Loss: 0.05581891909241676\n",
      "Iteration 24993, Loss: 0.055678609758615494\n",
      "Iteration 24994, Loss: 0.05579320713877678\n",
      "Iteration 24995, Loss: 0.0558951310813427\n",
      "Iteration 24996, Loss: 0.055875301361083984\n",
      "Iteration 24997, Loss: 0.05575498193502426\n",
      "Iteration 24998, Loss: 0.05570916458964348\n",
      "Iteration 24999, Loss: 0.05579940602183342\n",
      "Iteration 25000, Loss: 0.055743537843227386\n",
      "Iteration 25001, Loss: 0.05568647384643555\n",
      "Iteration 25002, Loss: 0.05573614686727524\n",
      "Iteration 25003, Loss: 0.055699944496154785\n",
      "Iteration 25004, Loss: 0.05567936226725578\n",
      "Iteration 25005, Loss: 0.05569378659129143\n",
      "Iteration 25006, Loss: 0.055670224130153656\n",
      "Iteration 25007, Loss: 0.05567912384867668\n",
      "Iteration 25008, Loss: 0.05566215515136719\n",
      "Iteration 25009, Loss: 0.055687468498945236\n",
      "Iteration 25010, Loss: 0.05565623566508293\n",
      "Iteration 25011, Loss: 0.055695973336696625\n",
      "Iteration 25012, Loss: 0.05569462105631828\n",
      "Iteration 25013, Loss: 0.055644236505031586\n",
      "Iteration 25014, Loss: 0.05562726780772209\n",
      "Iteration 25015, Loss: 0.05570328235626221\n",
      "Iteration 25016, Loss: 0.05564924329519272\n",
      "Iteration 25017, Loss: 0.05574536696076393\n",
      "Iteration 25018, Loss: 0.055784307420253754\n",
      "Iteration 25019, Loss: 0.05570916458964348\n",
      "Iteration 25020, Loss: 0.055701855570077896\n",
      "Iteration 25021, Loss: 0.05573984235525131\n",
      "Iteration 25022, Loss: 0.05562392994761467\n",
      "Iteration 25023, Loss: 0.055808067321777344\n",
      "Iteration 25024, Loss: 0.0558907613158226\n",
      "Iteration 25025, Loss: 0.055860720574855804\n",
      "Iteration 25026, Loss: 0.05573638528585434\n",
      "Iteration 25027, Loss: 0.05572644993662834\n",
      "Iteration 25028, Loss: 0.0558096207678318\n",
      "Iteration 25029, Loss: 0.055726926773786545\n",
      "Iteration 25030, Loss: 0.055712103843688965\n",
      "Iteration 25031, Loss: 0.05577941983938217\n",
      "Iteration 25032, Loss: 0.055745601654052734\n",
      "Iteration 25033, Loss: 0.05564955994486809\n",
      "Iteration 25034, Loss: 0.055780649185180664\n",
      "Iteration 25035, Loss: 0.055790744721889496\n",
      "Iteration 25036, Loss: 0.05564840883016586\n",
      "Iteration 25037, Loss: 0.055781327188014984\n",
      "Iteration 25038, Loss: 0.05585901066660881\n",
      "Iteration 25039, Loss: 0.05583099648356438\n",
      "Iteration 25040, Loss: 0.05570828914642334\n",
      "Iteration 25041, Loss: 0.05575386807322502\n",
      "Iteration 25042, Loss: 0.05583195015788078\n",
      "Iteration 25043, Loss: 0.05573678016662598\n",
      "Iteration 25044, Loss: 0.05570678040385246\n",
      "Iteration 25045, Loss: 0.05578271672129631\n",
      "Iteration 25046, Loss: 0.05575351044535637\n",
      "Iteration 25047, Loss: 0.055634621530771255\n",
      "Iteration 25048, Loss: 0.05583866685628891\n",
      "Iteration 25049, Loss: 0.055899981409311295\n",
      "Iteration 25050, Loss: 0.05578367039561272\n",
      "Iteration 25051, Loss: 0.0556870736181736\n",
      "Iteration 25052, Loss: 0.055777233093976974\n",
      "Iteration 25053, Loss: 0.05576404184103012\n",
      "Iteration 25054, Loss: 0.05565798655152321\n",
      "Iteration 25055, Loss: 0.05580250546336174\n",
      "Iteration 25056, Loss: 0.055861275643110275\n",
      "Iteration 25057, Loss: 0.055743418633937836\n",
      "Iteration 25058, Loss: 0.055717747658491135\n",
      "Iteration 25059, Loss: 0.055808424949645996\n",
      "Iteration 25060, Loss: 0.05579555034637451\n",
      "Iteration 25061, Loss: 0.055689454078674316\n",
      "Iteration 25062, Loss: 0.05576002597808838\n",
      "Iteration 25063, Loss: 0.05581911653280258\n",
      "Iteration 25064, Loss: 0.05570201203227043\n",
      "Iteration 25065, Loss: 0.055747829377651215\n",
      "Iteration 25066, Loss: 0.05583786964416504\n",
      "Iteration 25067, Loss: 0.0558239221572876\n",
      "Iteration 25068, Loss: 0.05571671575307846\n",
      "Iteration 25069, Loss: 0.05572529882192612\n",
      "Iteration 25070, Loss: 0.055785857141017914\n",
      "Iteration 25071, Loss: 0.05567038431763649\n",
      "Iteration 25072, Loss: 0.05577099695801735\n",
      "Iteration 25073, Loss: 0.05586032196879387\n",
      "Iteration 25074, Loss: 0.055845897644758224\n",
      "Iteration 25075, Loss: 0.05573868751525879\n",
      "Iteration 25076, Loss: 0.055696092545986176\n",
      "Iteration 25077, Loss: 0.05575831979513168\n",
      "Iteration 25078, Loss: 0.0556461438536644\n",
      "Iteration 25079, Loss: 0.055784426629543304\n",
      "Iteration 25080, Loss: 0.0558701790869236\n",
      "Iteration 25081, Loss: 0.05585316941142082\n",
      "Iteration 25082, Loss: 0.05574365705251694\n",
      "Iteration 25083, Loss: 0.055690646171569824\n",
      "Iteration 25084, Loss: 0.055753193795681\n",
      "Iteration 25085, Loss: 0.05563696473836899\n",
      "Iteration 25086, Loss: 0.05579483509063721\n",
      "Iteration 25087, Loss: 0.055884283035993576\n",
      "Iteration 25088, Loss: 0.05586973950266838\n",
      "Iteration 25089, Loss: 0.05576205626130104\n",
      "Iteration 25090, Loss: 0.055663786828517914\n",
      "Iteration 25091, Loss: 0.055724941194057465\n",
      "Iteration 25092, Loss: 0.0556209497153759\n",
      "Iteration 25093, Loss: 0.05573197454214096\n",
      "Iteration 25094, Loss: 0.05573694035410881\n",
      "Iteration 25095, Loss: 0.055642448365688324\n",
      "Iteration 25096, Loss: 0.05578843876719475\n",
      "Iteration 25097, Loss: 0.05581923574209213\n",
      "Iteration 25098, Loss: 0.05568544194102287\n",
      "Iteration 25099, Loss: 0.05577286332845688\n",
      "Iteration 25100, Loss: 0.055870771408081055\n",
      "Iteration 25101, Loss: 0.055857740342617035\n",
      "Iteration 25102, Loss: 0.055747631937265396\n",
      "Iteration 25103, Loss: 0.055694106966257095\n",
      "Iteration 25104, Loss: 0.05576908960938454\n",
      "Iteration 25105, Loss: 0.055684249848127365\n",
      "Iteration 25106, Loss: 0.055741988122463226\n",
      "Iteration 25107, Loss: 0.05581025406718254\n",
      "Iteration 25108, Loss: 0.05577739328145981\n",
      "Iteration 25109, Loss: 0.055661801248788834\n",
      "Iteration 25110, Loss: 0.0558067187666893\n",
      "Iteration 25111, Loss: 0.05586842820048332\n",
      "Iteration 25112, Loss: 0.05575243756175041\n",
      "Iteration 25113, Loss: 0.055711112916469574\n",
      "Iteration 25114, Loss: 0.0558011569082737\n",
      "Iteration 25115, Loss: 0.05578883737325668\n",
      "Iteration 25116, Loss: 0.055684689432382584\n",
      "Iteration 25117, Loss: 0.055764954537153244\n",
      "Iteration 25118, Loss: 0.05582074448466301\n",
      "Iteration 25119, Loss: 0.05569855496287346\n",
      "Iteration 25120, Loss: 0.05575438588857651\n",
      "Iteration 25121, Loss: 0.05584832280874252\n",
      "Iteration 25122, Loss: 0.05583874508738518\n",
      "Iteration 25123, Loss: 0.055736225098371506\n",
      "Iteration 25124, Loss: 0.05569283291697502\n",
      "Iteration 25125, Loss: 0.05574766919016838\n",
      "Iteration 25126, Loss: 0.05562448501586914\n",
      "Iteration 25127, Loss: 0.05580934137105942\n",
      "Iteration 25128, Loss: 0.0559033565223217\n",
      "Iteration 25129, Loss: 0.05589306727051735\n",
      "Iteration 25130, Loss: 0.055789511650800705\n",
      "Iteration 25131, Loss: 0.05562285706400871\n",
      "Iteration 25132, Loss: 0.0557052306830883\n",
      "Iteration 25133, Loss: 0.05563148111104965\n",
      "Iteration 25134, Loss: 0.05574711412191391\n",
      "Iteration 25135, Loss: 0.055769603699445724\n",
      "Iteration 25136, Loss: 0.05567912384867668\n",
      "Iteration 25137, Loss: 0.055748503655195236\n",
      "Iteration 25138, Loss: 0.05579579249024391\n",
      "Iteration 25139, Loss: 0.05569040775299072\n",
      "Iteration 25140, Loss: 0.05575096607208252\n",
      "Iteration 25141, Loss: 0.05582737922668457\n",
      "Iteration 25142, Loss: 0.055783711373806\n",
      "Iteration 25143, Loss: 0.05565345659852028\n",
      "Iteration 25144, Loss: 0.055810533463954926\n",
      "Iteration 25145, Loss: 0.05587109178304672\n",
      "Iteration 25146, Loss: 0.05576547235250473\n",
      "Iteration 25147, Loss: 0.055690567940473557\n",
      "Iteration 25148, Loss: 0.05577246472239494\n",
      "Iteration 25149, Loss: 0.05574437230825424\n",
      "Iteration 25150, Loss: 0.055631160736083984\n",
      "Iteration 25151, Loss: 0.0558093786239624\n",
      "Iteration 25152, Loss: 0.05583576485514641\n",
      "Iteration 25153, Loss: 0.055687546730041504\n",
      "Iteration 25154, Loss: 0.05577985569834709\n",
      "Iteration 25155, Loss: 0.05588909238576889\n",
      "Iteration 25156, Loss: 0.055892907083034515\n",
      "Iteration 25157, Loss: 0.05580202862620354\n",
      "Iteration 25158, Loss: 0.05562957376241684\n",
      "Iteration 25159, Loss: 0.0559157133102417\n",
      "Iteration 25160, Loss: 0.05603782460093498\n",
      "Iteration 25161, Loss: 0.05597277730703354\n",
      "Iteration 25162, Loss: 0.05573992058634758\n",
      "Iteration 25163, Loss: 0.055797021836042404\n",
      "Iteration 25164, Loss: 0.05595676228404045\n",
      "Iteration 25165, Loss: 0.05600595474243164\n",
      "Iteration 25166, Loss: 0.0559546984732151\n",
      "Iteration 25167, Loss: 0.055813394486904144\n",
      "Iteration 25168, Loss: 0.05563712120056152\n",
      "Iteration 25169, Loss: 0.05575188249349594\n",
      "Iteration 25170, Loss: 0.05569378659129143\n",
      "Iteration 25171, Loss: 0.05571329593658447\n",
      "Iteration 25172, Loss: 0.05576551333069801\n",
      "Iteration 25173, Loss: 0.0557178296148777\n",
      "Iteration 25174, Loss: 0.05565325543284416\n",
      "Iteration 25175, Loss: 0.055650752037763596\n",
      "Iteration 25176, Loss: 0.05570876598358154\n",
      "Iteration 25177, Loss: 0.0557275228202343\n",
      "Iteration 25178, Loss: 0.05564991757273674\n",
      "Iteration 25179, Loss: 0.05577707663178444\n",
      "Iteration 25180, Loss: 0.05580314248800278\n",
      "Iteration 25181, Loss: 0.05565730854868889\n",
      "Iteration 25182, Loss: 0.05580031871795654\n",
      "Iteration 25183, Loss: 0.055906496942043304\n",
      "Iteration 25184, Loss: 0.0559062585234642\n",
      "Iteration 25185, Loss: 0.055810533463954926\n",
      "Iteration 25186, Loss: 0.05563883110880852\n",
      "Iteration 25187, Loss: 0.05589219182729721\n",
      "Iteration 25188, Loss: 0.05600106716156006\n",
      "Iteration 25189, Loss: 0.05592513456940651\n",
      "Iteration 25190, Loss: 0.05568385496735573\n",
      "Iteration 25191, Loss: 0.055844228714704514\n",
      "Iteration 25192, Loss: 0.056009650230407715\n",
      "Iteration 25193, Loss: 0.05606353282928467\n",
      "Iteration 25194, Loss: 0.056016210466623306\n",
      "Iteration 25195, Loss: 0.05587844178080559\n",
      "Iteration 25196, Loss: 0.055661480873823166\n",
      "Iteration 25197, Loss: 0.05593455210328102\n",
      "Iteration 25198, Loss: 0.056114714592695236\n",
      "Iteration 25199, Loss: 0.05610227584838867\n",
      "Iteration 25200, Loss: 0.055916666984558105\n",
      "Iteration 25201, Loss: 0.05563930794596672\n",
      "Iteration 25202, Loss: 0.055782757699489594\n",
      "Iteration 25203, Loss: 0.05582352727651596\n",
      "Iteration 25204, Loss: 0.05576543137431145\n",
      "Iteration 25205, Loss: 0.05561737343668938\n",
      "Iteration 25206, Loss: 0.05590355396270752\n",
      "Iteration 25207, Loss: 0.05600516125559807\n",
      "Iteration 25208, Loss: 0.055925171822309494\n",
      "Iteration 25209, Loss: 0.05568552017211914\n",
      "Iteration 25210, Loss: 0.0558398962020874\n",
      "Iteration 25211, Loss: 0.0560019426047802\n",
      "Iteration 25212, Loss: 0.05605181306600571\n",
      "Iteration 25213, Loss: 0.056000471115112305\n",
      "Iteration 25214, Loss: 0.05585877224802971\n",
      "Iteration 25215, Loss: 0.05564228817820549\n",
      "Iteration 25216, Loss: 0.05594766512513161\n",
      "Iteration 25217, Loss: 0.05611483380198479\n",
      "Iteration 25218, Loss: 0.05609186738729477\n",
      "Iteration 25219, Loss: 0.0558977946639061\n",
      "Iteration 25220, Loss: 0.055654291063547134\n",
      "Iteration 25221, Loss: 0.055792491883039474\n",
      "Iteration 25222, Loss: 0.05582396313548088\n",
      "Iteration 25223, Loss: 0.0557578019797802\n",
      "Iteration 25224, Loss: 0.05561896413564682\n",
      "Iteration 25225, Loss: 0.055664144456386566\n",
      "Iteration 25226, Loss: 0.0556614026427269\n",
      "Iteration 25227, Loss: 0.05564812943339348\n",
      "Iteration 25228, Loss: 0.055703405290842056\n",
      "Iteration 25229, Loss: 0.05566462129354477\n",
      "Iteration 25230, Loss: 0.055721841752529144\n",
      "Iteration 25231, Loss: 0.05576157569885254\n",
      "Iteration 25232, Loss: 0.05569835752248764\n",
      "Iteration 25233, Loss: 0.05569605156779289\n",
      "Iteration 25234, Loss: 0.05571131035685539\n",
      "Iteration 25235, Loss: 0.055650752037763596\n",
      "Iteration 25236, Loss: 0.0556592158973217\n",
      "Iteration 25237, Loss: 0.055660925805568695\n",
      "Iteration 25238, Loss: 0.055624209344387054\n",
      "Iteration 25239, Loss: 0.055682264268398285\n",
      "Iteration 25240, Loss: 0.055639468133449554\n",
      "Iteration 25241, Loss: 0.05574246495962143\n",
      "Iteration 25242, Loss: 0.055745165795087814\n",
      "Iteration 25243, Loss: 0.055632513016462326\n",
      "Iteration 25244, Loss: 0.05574246495962143\n",
      "Iteration 25245, Loss: 0.055731140077114105\n",
      "Iteration 25246, Loss: 0.05564054101705551\n",
      "Iteration 25247, Loss: 0.05566597357392311\n",
      "Iteration 25248, Loss: 0.055657029151916504\n",
      "Iteration 25249, Loss: 0.05563100427389145\n",
      "Iteration 25250, Loss: 0.05571170896291733\n",
      "Iteration 25251, Loss: 0.055667322129011154\n",
      "Iteration 25252, Loss: 0.05572287365794182\n",
      "Iteration 25253, Loss: 0.05576181784272194\n",
      "Iteration 25254, Loss: 0.05569490045309067\n",
      "Iteration 25255, Loss: 0.055705271661281586\n",
      "Iteration 25256, Loss: 0.05572974681854248\n",
      "Iteration 25257, Loss: 0.05562424659729004\n",
      "Iteration 25258, Loss: 0.05563410371541977\n",
      "Iteration 25259, Loss: 0.05568218231201172\n",
      "Iteration 25260, Loss: 0.055618010461330414\n",
      "Iteration 25261, Loss: 0.055694304406642914\n",
      "Iteration 25262, Loss: 0.05563966557383537\n",
      "Iteration 25263, Loss: 0.055750053375959396\n",
      "Iteration 25264, Loss: 0.055773019790649414\n",
      "Iteration 25265, Loss: 0.055672090500593185\n",
      "Iteration 25266, Loss: 0.055760424584150314\n",
      "Iteration 25267, Loss: 0.05581788346171379\n",
      "Iteration 25268, Loss: 0.05573185533285141\n",
      "Iteration 25269, Loss: 0.05569517984986305\n",
      "Iteration 25270, Loss: 0.055756889283657074\n",
      "Iteration 25271, Loss: 0.055689454078674316\n",
      "Iteration 25272, Loss: 0.05571715161204338\n",
      "Iteration 25273, Loss: 0.055753909051418304\n",
      "Iteration 25274, Loss: 0.05565401166677475\n",
      "Iteration 25275, Loss: 0.055777233093976974\n",
      "Iteration 25276, Loss: 0.05583679676055908\n",
      "Iteration 25277, Loss: 0.05576074495911598\n",
      "Iteration 25278, Loss: 0.0556647814810276\n",
      "Iteration 25279, Loss: 0.05574202910065651\n",
      "Iteration 25280, Loss: 0.055697403848171234\n",
      "Iteration 25281, Loss: 0.055702608078718185\n",
      "Iteration 25282, Loss: 0.05573515221476555\n",
      "Iteration 25283, Loss: 0.05566783994436264\n",
      "Iteration 25284, Loss: 0.055732011795043945\n",
      "Iteration 25285, Loss: 0.05574667453765869\n",
      "Iteration 25286, Loss: 0.05561637878417969\n",
      "Iteration 25287, Loss: 0.05569573491811752\n",
      "Iteration 25288, Loss: 0.055679839104413986\n",
      "Iteration 25289, Loss: 0.055663350969552994\n",
      "Iteration 25290, Loss: 0.05562492460012436\n",
      "Iteration 25291, Loss: 0.05575196072459221\n",
      "Iteration 25292, Loss: 0.05579432100057602\n",
      "Iteration 25293, Loss: 0.05573729798197746\n",
      "Iteration 25294, Loss: 0.05563819408416748\n",
      "Iteration 25295, Loss: 0.05568575859069824\n",
      "Iteration 25296, Loss: 0.05564502999186516\n",
      "Iteration 25297, Loss: 0.05566529557108879\n",
      "Iteration 25298, Loss: 0.05563521385192871\n",
      "Iteration 25299, Loss: 0.055687908083200455\n",
      "Iteration 25300, Loss: 0.055678050965070724\n",
      "Iteration 25301, Loss: 0.05565861985087395\n",
      "Iteration 25302, Loss: 0.055633388459682465\n",
      "Iteration 25303, Loss: 0.05570634454488754\n",
      "Iteration 25304, Loss: 0.05570884793996811\n",
      "Iteration 25305, Loss: 0.05562099069356918\n",
      "Iteration 25306, Loss: 0.055781684815883636\n",
      "Iteration 25307, Loss: 0.05577341839671135\n",
      "Iteration 25308, Loss: 0.05562552064657211\n",
      "Iteration 25309, Loss: 0.05568253993988037\n",
      "Iteration 25310, Loss: 0.055638790130615234\n",
      "Iteration 25311, Loss: 0.05575219914317131\n",
      "Iteration 25312, Loss: 0.0557527169585228\n",
      "Iteration 25313, Loss: 0.05563509464263916\n",
      "Iteration 25314, Loss: 0.05572895333170891\n",
      "Iteration 25315, Loss: 0.05571448802947998\n",
      "Iteration 25316, Loss: 0.055649518966674805\n",
      "Iteration 25317, Loss: 0.05565405264496803\n",
      "Iteration 25318, Loss: 0.05569136142730713\n",
      "Iteration 25319, Loss: 0.05567558854818344\n",
      "Iteration 25320, Loss: 0.05568337440490723\n",
      "Iteration 25321, Loss: 0.055680595338344574\n",
      "Iteration 25322, Loss: 0.05567161366343498\n",
      "Iteration 25323, Loss: 0.055662911385297775\n",
      "Iteration 25324, Loss: 0.055690646171569824\n",
      "Iteration 25325, Loss: 0.05568067356944084\n",
      "Iteration 25326, Loss: 0.05567694082856178\n",
      "Iteration 25327, Loss: 0.055674515664577484\n",
      "Iteration 25328, Loss: 0.05567467585206032\n",
      "Iteration 25329, Loss: 0.055661242455244064\n",
      "Iteration 25330, Loss: 0.055697839707136154\n",
      "Iteration 25331, Loss: 0.055694859474897385\n",
      "Iteration 25332, Loss: 0.05565603822469711\n",
      "Iteration 25333, Loss: 0.05565270036458969\n",
      "Iteration 25334, Loss: 0.055701177567243576\n",
      "Iteration 25335, Loss: 0.0556941032409668\n",
      "Iteration 25336, Loss: 0.05565845966339111\n",
      "Iteration 25337, Loss: 0.05566398426890373\n",
      "Iteration 25338, Loss: 0.05568305775523186\n",
      "Iteration 25339, Loss: 0.05567201226949692\n",
      "Iteration 25340, Loss: 0.055676184594631195\n",
      "Iteration 25341, Loss: 0.05566084757447243\n",
      "Iteration 25342, Loss: 0.0557069405913353\n",
      "Iteration 25343, Loss: 0.05571556091308594\n",
      "Iteration 25344, Loss: 0.05564085766673088\n",
      "Iteration 25345, Loss: 0.05573570728302002\n",
      "Iteration 25346, Loss: 0.05570551007986069\n",
      "Iteration 25347, Loss: 0.05568750947713852\n",
      "Iteration 25348, Loss: 0.055723074823617935\n",
      "Iteration 25349, Loss: 0.05566207692027092\n",
      "Iteration 25350, Loss: 0.055738769471645355\n",
      "Iteration 25351, Loss: 0.05574476718902588\n",
      "Iteration 25352, Loss: 0.05563422292470932\n",
      "Iteration 25353, Loss: 0.055654171854257584\n",
      "Iteration 25354, Loss: 0.055650435388088226\n",
      "Iteration 25355, Loss: 0.05564530938863754\n",
      "Iteration 25356, Loss: 0.05562961474061012\n",
      "Iteration 25357, Loss: 0.05569211766123772\n",
      "Iteration 25358, Loss: 0.05564197152853012\n",
      "Iteration 25359, Loss: 0.05573705956339836\n",
      "Iteration 25360, Loss: 0.055754225701093674\n",
      "Iteration 25361, Loss: 0.0556536540389061\n",
      "Iteration 25362, Loss: 0.0557791031897068\n",
      "Iteration 25363, Loss: 0.055829644203186035\n",
      "Iteration 25364, Loss: 0.05573030561208725\n",
      "Iteration 25365, Loss: 0.055708568543195724\n",
      "Iteration 25366, Loss: 0.055781763046979904\n",
      "Iteration 25367, Loss: 0.05573185533285141\n",
      "Iteration 25368, Loss: 0.055654168128967285\n",
      "Iteration 25369, Loss: 0.05568739026784897\n",
      "Iteration 25370, Loss: 0.0556461438536644\n",
      "Iteration 25371, Loss: 0.05564066022634506\n",
      "Iteration 25372, Loss: 0.05567912384867668\n",
      "Iteration 25373, Loss: 0.0556187629699707\n",
      "Iteration 25374, Loss: 0.055707816034555435\n",
      "Iteration 25375, Loss: 0.05569084733724594\n",
      "Iteration 25376, Loss: 0.05566160008311272\n",
      "Iteration 25377, Loss: 0.05565265938639641\n",
      "Iteration 25378, Loss: 0.05570821091532707\n",
      "Iteration 25379, Loss: 0.05571993440389633\n",
      "Iteration 25380, Loss: 0.05564209073781967\n",
      "Iteration 25381, Loss: 0.05576666444540024\n",
      "Iteration 25382, Loss: 0.05576976388692856\n",
      "Iteration 25383, Loss: 0.05561816692352295\n",
      "Iteration 25384, Loss: 0.055677615106105804\n",
      "Iteration 25385, Loss: 0.05563906952738762\n",
      "Iteration 25386, Loss: 0.055744290351867676\n",
      "Iteration 25387, Loss: 0.05573372170329094\n",
      "Iteration 25388, Loss: 0.05565003678202629\n",
      "Iteration 25389, Loss: 0.055679403245449066\n",
      "Iteration 25390, Loss: 0.05561904236674309\n",
      "Iteration 25391, Loss: 0.05571158975362778\n",
      "Iteration 25392, Loss: 0.05570300668478012\n",
      "Iteration 25393, Loss: 0.05564224720001221\n",
      "Iteration 25394, Loss: 0.055690132081508636\n",
      "Iteration 25395, Loss: 0.05564066022634506\n",
      "Iteration 25396, Loss: 0.0556919202208519\n",
      "Iteration 25397, Loss: 0.05567312613129616\n",
      "Iteration 25398, Loss: 0.05568627640604973\n",
      "Iteration 25399, Loss: 0.055672526359558105\n",
      "Iteration 25400, Loss: 0.05569152161478996\n",
      "Iteration 25401, Loss: 0.055702291429042816\n",
      "Iteration 25402, Loss: 0.05562695115804672\n",
      "Iteration 25403, Loss: 0.055694226175546646\n",
      "Iteration 25404, Loss: 0.05562500283122063\n",
      "Iteration 25405, Loss: 0.05572211742401123\n",
      "Iteration 25406, Loss: 0.05571905896067619\n",
      "Iteration 25407, Loss: 0.05563155934214592\n",
      "Iteration 25408, Loss: 0.05573320761322975\n",
      "Iteration 25409, Loss: 0.055687230080366135\n",
      "Iteration 25410, Loss: 0.05571170896291733\n",
      "Iteration 25411, Loss: 0.055758994072675705\n",
      "Iteration 25412, Loss: 0.05570797249674797\n",
      "Iteration 25413, Loss: 0.055669110268354416\n",
      "Iteration 25414, Loss: 0.055674076080322266\n",
      "Iteration 25415, Loss: 0.0556846484541893\n",
      "Iteration 25416, Loss: 0.05569915100932121\n",
      "Iteration 25417, Loss: 0.055628739297389984\n",
      "Iteration 25418, Loss: 0.055778663605451584\n",
      "Iteration 25419, Loss: 0.05578228086233139\n",
      "Iteration 25420, Loss: 0.055638354271650314\n",
      "Iteration 25421, Loss: 0.05578549951314926\n",
      "Iteration 25422, Loss: 0.05585245415568352\n",
      "Iteration 25423, Loss: 0.05580366030335426\n",
      "Iteration 25424, Loss: 0.05566255375742912\n",
      "Iteration 25425, Loss: 0.05582042783498764\n",
      "Iteration 25426, Loss: 0.055906180292367935\n",
      "Iteration 25427, Loss: 0.05582388490438461\n",
      "Iteration 25428, Loss: 0.055631399154663086\n",
      "Iteration 25429, Loss: 0.05574842542409897\n",
      "Iteration 25430, Loss: 0.05574989691376686\n",
      "Iteration 25431, Loss: 0.05563700199127197\n",
      "Iteration 25432, Loss: 0.055808186531066895\n",
      "Iteration 25433, Loss: 0.055862508714199066\n",
      "Iteration 25434, Loss: 0.055761419236660004\n",
      "Iteration 25435, Loss: 0.055686913430690765\n",
      "Iteration 25436, Loss: 0.055767178535461426\n",
      "Iteration 25437, Loss: 0.055731695145368576\n",
      "Iteration 25438, Loss: 0.05563775822520256\n",
      "Iteration 25439, Loss: 0.055647097527980804\n",
      "Iteration 25440, Loss: 0.05569974705576897\n",
      "Iteration 25441, Loss: 0.05570177361369133\n",
      "Iteration 25442, Loss: 0.05563501641154289\n",
      "Iteration 25443, Loss: 0.05570829287171364\n",
      "Iteration 25444, Loss: 0.05564673990011215\n",
      "Iteration 25445, Loss: 0.05574151128530502\n",
      "Iteration 25446, Loss: 0.05578676983714104\n",
      "Iteration 25447, Loss: 0.055728595703840256\n",
      "Iteration 25448, Loss: 0.05565134808421135\n",
      "Iteration 25449, Loss: 0.055673640221357346\n",
      "Iteration 25450, Loss: 0.055672649294137955\n",
      "Iteration 25451, Loss: 0.05567558854818344\n",
      "Iteration 25452, Loss: 0.055647335946559906\n",
      "Iteration 25453, Loss: 0.055651746690273285\n",
      "Iteration 25454, Loss: 0.0556560754776001\n",
      "Iteration 25455, Loss: 0.0556306466460228\n",
      "Iteration 25456, Loss: 0.055728476494550705\n",
      "Iteration 25457, Loss: 0.05572100728750229\n",
      "Iteration 25458, Loss: 0.05564769357442856\n",
      "Iteration 25459, Loss: 0.05570531263947487\n",
      "Iteration 25460, Loss: 0.05564836785197258\n",
      "Iteration 25461, Loss: 0.05574536323547363\n",
      "Iteration 25462, Loss: 0.055789947509765625\n",
      "Iteration 25463, Loss: 0.05573185533285141\n",
      "Iteration 25464, Loss: 0.055659692734479904\n",
      "Iteration 25465, Loss: 0.055715203285217285\n",
      "Iteration 25466, Loss: 0.05565468594431877\n",
      "Iteration 25467, Loss: 0.05571707338094711\n",
      "Iteration 25468, Loss: 0.055740635842084885\n",
      "Iteration 25469, Loss: 0.05566565319895744\n",
      "Iteration 25470, Loss: 0.05575565621256828\n",
      "Iteration 25471, Loss: 0.05578450486063957\n",
      "Iteration 25472, Loss: 0.05565369129180908\n",
      "Iteration 25473, Loss: 0.05578792095184326\n",
      "Iteration 25474, Loss: 0.055874112993478775\n",
      "Iteration 25475, Loss: 0.055847328156232834\n",
      "Iteration 25476, Loss: 0.05572303384542465\n",
      "Iteration 25477, Loss: 0.05573749542236328\n",
      "Iteration 25478, Loss: 0.05581943318247795\n",
      "Iteration 25479, Loss: 0.05573650449514389\n",
      "Iteration 25480, Loss: 0.055698953568935394\n",
      "Iteration 25481, Loss: 0.05576551333069801\n",
      "Iteration 25482, Loss: 0.0557255782186985\n",
      "Iteration 25483, Loss: 0.055648211389780045\n",
      "Iteration 25484, Loss: 0.05571027845144272\n",
      "Iteration 25485, Loss: 0.0556495226919651\n",
      "Iteration 25486, Loss: 0.055726610124111176\n",
      "Iteration 25487, Loss: 0.05575696751475334\n",
      "Iteration 25488, Loss: 0.05568504333496094\n",
      "Iteration 25489, Loss: 0.055725812911987305\n",
      "Iteration 25490, Loss: 0.05574989318847656\n",
      "Iteration 25491, Loss: 0.05562281608581543\n",
      "Iteration 25492, Loss: 0.05573304742574692\n",
      "Iteration 25493, Loss: 0.05572346970438957\n",
      "Iteration 25494, Loss: 0.05563831701874733\n",
      "Iteration 25495, Loss: 0.055696092545986176\n",
      "Iteration 25496, Loss: 0.05562722682952881\n",
      "Iteration 25497, Loss: 0.055734436959028244\n",
      "Iteration 25498, Loss: 0.05576149746775627\n",
      "Iteration 25499, Loss: 0.0556919202208519\n",
      "Iteration 25500, Loss: 0.055712465196847916\n",
      "Iteration 25501, Loss: 0.05573141947388649\n",
      "Iteration 25502, Loss: 0.05563601106405258\n",
      "Iteration 25503, Loss: 0.05564860627055168\n",
      "Iteration 25504, Loss: 0.05567149445414543\n",
      "Iteration 25505, Loss: 0.05562174692749977\n",
      "Iteration 25506, Loss: 0.055696289986371994\n",
      "Iteration 25507, Loss: 0.05563827604055405\n",
      "Iteration 25508, Loss: 0.055755615234375\n",
      "Iteration 25509, Loss: 0.05578593537211418\n",
      "Iteration 25510, Loss: 0.05569462105631828\n",
      "Iteration 25511, Loss: 0.055733565241098404\n",
      "Iteration 25512, Loss: 0.05578700825572014\n",
      "Iteration 25513, Loss: 0.05569314956665039\n",
      "Iteration 25514, Loss: 0.055740080773830414\n",
      "Iteration 25515, Loss: 0.05580584332346916\n",
      "Iteration 25516, Loss: 0.055744290351867676\n",
      "Iteration 25517, Loss: 0.05566056817770004\n",
      "Iteration 25518, Loss: 0.05572120472788811\n",
      "Iteration 25519, Loss: 0.05565011873841286\n",
      "Iteration 25520, Loss: 0.05575374886393547\n",
      "Iteration 25521, Loss: 0.05580596253275871\n",
      "Iteration 25522, Loss: 0.05575398728251457\n",
      "Iteration 25523, Loss: 0.05564801022410393\n",
      "Iteration 25524, Loss: 0.055778466165065765\n",
      "Iteration 25525, Loss: 0.05577830597758293\n",
      "Iteration 25526, Loss: 0.05564216896891594\n",
      "Iteration 25527, Loss: 0.055752240121364594\n",
      "Iteration 25528, Loss: 0.05579312890768051\n",
      "Iteration 25529, Loss: 0.055729031562805176\n",
      "Iteration 25530, Loss: 0.05566120520234108\n",
      "Iteration 25531, Loss: 0.05568397417664528\n",
      "Iteration 25532, Loss: 0.055664896965026855\n",
      "Iteration 25533, Loss: 0.05566521733999252\n",
      "Iteration 25534, Loss: 0.055665891617536545\n",
      "Iteration 25535, Loss: 0.0556207112967968\n",
      "Iteration 25536, Loss: 0.05574822425842285\n",
      "Iteration 25537, Loss: 0.05578505992889404\n",
      "Iteration 25538, Loss: 0.05572434514760971\n",
      "Iteration 25539, Loss: 0.05565730854868889\n",
      "Iteration 25540, Loss: 0.0556667260825634\n",
      "Iteration 25541, Loss: 0.05569016933441162\n",
      "Iteration 25542, Loss: 0.055705271661281586\n",
      "Iteration 25543, Loss: 0.05562595650553703\n",
      "Iteration 25544, Loss: 0.0558137521147728\n",
      "Iteration 25545, Loss: 0.05584120750427246\n",
      "Iteration 25546, Loss: 0.05569255352020264\n",
      "Iteration 25547, Loss: 0.05577604100108147\n",
      "Iteration 25548, Loss: 0.05588575452566147\n",
      "Iteration 25549, Loss: 0.05588928982615471\n",
      "Iteration 25550, Loss: 0.05579753965139389\n",
      "Iteration 25551, Loss: 0.05562500283122063\n",
      "Iteration 25552, Loss: 0.0559161901473999\n",
      "Iteration 25553, Loss: 0.05603397265076637\n",
      "Iteration 25554, Loss: 0.05596689507365227\n",
      "Iteration 25555, Loss: 0.05573650449514389\n",
      "Iteration 25556, Loss: 0.05579789727926254\n",
      "Iteration 25557, Loss: 0.05595577135682106\n",
      "Iteration 25558, Loss: 0.056003570556640625\n",
      "Iteration 25559, Loss: 0.05595123767852783\n",
      "Iteration 25560, Loss: 0.05580949783325195\n",
      "Iteration 25561, Loss: 0.0556415319442749\n",
      "Iteration 25562, Loss: 0.05574139207601547\n",
      "Iteration 25563, Loss: 0.05565989017486572\n",
      "Iteration 25564, Loss: 0.055755339562892914\n",
      "Iteration 25565, Loss: 0.055824678391218185\n",
      "Iteration 25566, Loss: 0.05579312890768051\n",
      "Iteration 25567, Loss: 0.05567117780447006\n",
      "Iteration 25568, Loss: 0.05580409616231918\n",
      "Iteration 25569, Loss: 0.05587955564260483\n",
      "Iteration 25570, Loss: 0.055774252861738205\n",
      "Iteration 25571, Loss: 0.055687110871076584\n",
      "Iteration 25572, Loss: 0.05577067658305168\n",
      "Iteration 25573, Loss: 0.05575255677103996\n",
      "Iteration 25574, Loss: 0.055642686784267426\n",
      "Iteration 25575, Loss: 0.05582857131958008\n",
      "Iteration 25576, Loss: 0.05589083954691887\n",
      "Iteration 25577, Loss: 0.05577409267425537\n",
      "Iteration 25578, Loss: 0.05569490045309067\n",
      "Iteration 25579, Loss: 0.05578533932566643\n",
      "Iteration 25580, Loss: 0.05577345937490463\n",
      "Iteration 25581, Loss: 0.055669188499450684\n",
      "Iteration 25582, Loss: 0.05578577518463135\n",
      "Iteration 25583, Loss: 0.05584228038787842\n",
      "Iteration 25584, Loss: 0.05572132393717766\n",
      "Iteration 25585, Loss: 0.055735986679792404\n",
      "Iteration 25586, Loss: 0.055828534066677094\n",
      "Iteration 25587, Loss: 0.05581776425242424\n",
      "Iteration 25588, Loss: 0.05571389198303223\n",
      "Iteration 25589, Loss: 0.055724065750837326\n",
      "Iteration 25590, Loss: 0.055780332535505295\n",
      "Iteration 25591, Loss: 0.05565985292196274\n",
      "Iteration 25592, Loss: 0.0557810477912426\n",
      "Iteration 25593, Loss: 0.055873118340969086\n",
      "Iteration 25594, Loss: 0.05586107820272446\n",
      "Iteration 25595, Loss: 0.05575593560934067\n",
      "Iteration 25596, Loss: 0.05566978454589844\n",
      "Iteration 25597, Loss: 0.05573130026459694\n",
      "Iteration 25598, Loss: 0.05562448501586914\n",
      "Iteration 25599, Loss: 0.0557706356048584\n",
      "Iteration 25600, Loss: 0.055825233459472656\n",
      "Iteration 25601, Loss: 0.05577544495463371\n",
      "Iteration 25602, Loss: 0.05564109608530998\n",
      "Iteration 25603, Loss: 0.05584828183054924\n",
      "Iteration 25604, Loss: 0.05592203140258789\n",
      "Iteration 25605, Loss: 0.05581454560160637\n",
      "Iteration 25606, Loss: 0.055659614503383636\n",
      "Iteration 25607, Loss: 0.055744968354701996\n",
      "Iteration 25608, Loss: 0.05572875589132309\n",
      "Iteration 25609, Loss: 0.055625878274440765\n",
      "Iteration 25610, Loss: 0.055825989693403244\n",
      "Iteration 25611, Loss: 0.05586513131856918\n",
      "Iteration 25612, Loss: 0.055732451379299164\n",
      "Iteration 25613, Loss: 0.05573558807373047\n",
      "Iteration 25614, Loss: 0.05583453178405762\n",
      "Iteration 25615, Loss: 0.05582880973815918\n",
      "Iteration 25616, Loss: 0.05572796240448952\n",
      "Iteration 25617, Loss: 0.0557023286819458\n",
      "Iteration 25618, Loss: 0.05575660988688469\n",
      "Iteration 25619, Loss: 0.055637720972299576\n",
      "Iteration 25620, Loss: 0.05579650402069092\n",
      "Iteration 25621, Loss: 0.05588483810424805\n",
      "Iteration 25622, Loss: 0.055866919457912445\n",
      "Iteration 25623, Loss: 0.055755458772182465\n",
      "Iteration 25624, Loss: 0.055683378130197525\n",
      "Iteration 25625, Loss: 0.05576006695628166\n",
      "Iteration 25626, Loss: 0.055673997849226\n",
      "Iteration 25627, Loss: 0.055747032165527344\n",
      "Iteration 25628, Loss: 0.055815618485212326\n",
      "Iteration 25629, Loss: 0.055784426629543304\n",
      "Iteration 25630, Loss: 0.055665336549282074\n",
      "Iteration 25631, Loss: 0.055809300392866135\n",
      "Iteration 25632, Loss: 0.055880073457956314\n",
      "Iteration 25633, Loss: 0.05577067658305168\n",
      "Iteration 25634, Loss: 0.055693190544843674\n",
      "Iteration 25635, Loss: 0.055779457092285156\n",
      "Iteration 25636, Loss: 0.0557638444006443\n",
      "Iteration 25637, Loss: 0.055656395852565765\n",
      "Iteration 25638, Loss: 0.05580699443817139\n",
      "Iteration 25639, Loss: 0.05586620420217514\n",
      "Iteration 25640, Loss: 0.0557456836104393\n",
      "Iteration 25641, Loss: 0.055718183517456055\n",
      "Iteration 25642, Loss: 0.055810969322919846\n",
      "Iteration 25643, Loss: 0.05580032244324684\n",
      "Iteration 25644, Loss: 0.05569688603281975\n",
      "Iteration 25645, Loss: 0.05574687570333481\n",
      "Iteration 25646, Loss: 0.055801872164011\n",
      "Iteration 25647, Loss: 0.05567821115255356\n",
      "Iteration 25648, Loss: 0.05576996132731438\n",
      "Iteration 25649, Loss: 0.0558646135032177\n",
      "Iteration 25650, Loss: 0.055855195969343185\n",
      "Iteration 25651, Loss: 0.055752478539943695\n",
      "Iteration 25652, Loss: 0.05567058175802231\n",
      "Iteration 25653, Loss: 0.05572609230875969\n",
      "Iteration 25654, Loss: 0.055622778832912445\n",
      "Iteration 25655, Loss: 0.05571683496236801\n",
      "Iteration 25656, Loss: 0.05570673942565918\n",
      "Iteration 25657, Loss: 0.05564248934388161\n",
      "Iteration 25658, Loss: 0.05566990375518799\n",
      "Iteration 25659, Loss: 0.05566128343343735\n",
      "Iteration 25660, Loss: 0.05565226078033447\n",
      "Iteration 25661, Loss: 0.0556841716170311\n",
      "Iteration 25662, Loss: 0.055632513016462326\n",
      "Iteration 25663, Loss: 0.055747274309396744\n",
      "Iteration 25664, Loss: 0.05579078570008278\n",
      "Iteration 25665, Loss: 0.05573582649230957\n",
      "Iteration 25666, Loss: 0.05563489720225334\n",
      "Iteration 25667, Loss: 0.05563783645629883\n",
      "Iteration 25668, Loss: 0.05571544170379639\n",
      "Iteration 25669, Loss: 0.055733561515808105\n",
      "Iteration 25670, Loss: 0.055656157433986664\n",
      "Iteration 25671, Loss: 0.05577031895518303\n",
      "Iteration 25672, Loss: 0.0557965449988842\n",
      "Iteration 25673, Loss: 0.055646657943725586\n",
      "Iteration 25674, Loss: 0.055810850113630295\n",
      "Iteration 25675, Loss: 0.05592111870646477\n",
      "Iteration 25676, Loss: 0.05592449754476547\n",
      "Iteration 25677, Loss: 0.05583222955465317\n",
      "Iteration 25678, Loss: 0.05565766617655754\n",
      "Iteration 25679, Loss: 0.055887024849653244\n",
      "Iteration 25680, Loss: 0.05601823329925537\n",
      "Iteration 25681, Loss: 0.05596164986491203\n",
      "Iteration 25682, Loss: 0.055737536400556564\n",
      "Iteration 25683, Loss: 0.05579281225800514\n",
      "Iteration 25684, Loss: 0.05594726651906967\n",
      "Iteration 25685, Loss: 0.05599188804626465\n",
      "Iteration 25686, Loss: 0.05593669414520264\n",
      "Iteration 25687, Loss: 0.05579289048910141\n",
      "Iteration 25688, Loss: 0.055667243897914886\n",
      "Iteration 25689, Loss: 0.05577008053660393\n",
      "Iteration 25690, Loss: 0.05569605156779289\n",
      "Iteration 25691, Loss: 0.0557229146361351\n",
      "Iteration 25692, Loss: 0.055786095559597015\n",
      "Iteration 25693, Loss: 0.055749181658029556\n",
      "Iteration 25694, Loss: 0.05562245845794678\n",
      "Iteration 25695, Loss: 0.055873237550258636\n",
      "Iteration 25696, Loss: 0.05595390126109123\n",
      "Iteration 25697, Loss: 0.05585765838623047\n",
      "Iteration 25698, Loss: 0.05563708394765854\n",
      "Iteration 25699, Loss: 0.05580127239227295\n",
      "Iteration 25700, Loss: 0.05587426945567131\n",
      "Iteration 25701, Loss: 0.055833302438259125\n",
      "Iteration 25702, Loss: 0.05569426342844963\n",
      "Iteration 25703, Loss: 0.055786214768886566\n",
      "Iteration 25704, Loss: 0.0558798722922802\n",
      "Iteration 25705, Loss: 0.05580659955739975\n",
      "Iteration 25706, Loss: 0.05563601106405258\n",
      "Iteration 25707, Loss: 0.05572545528411865\n",
      "Iteration 25708, Loss: 0.055699270218610764\n",
      "Iteration 25709, Loss: 0.05566859245300293\n",
      "Iteration 25710, Loss: 0.05567002296447754\n",
      "Iteration 25711, Loss: 0.055682383477687836\n",
      "Iteration 25712, Loss: 0.055677931755781174\n",
      "Iteration 25713, Loss: 0.05566835403442383\n",
      "Iteration 25714, Loss: 0.055650632828474045\n",
      "Iteration 25715, Loss: 0.05571802705526352\n",
      "Iteration 25716, Loss: 0.0557275228202343\n",
      "Iteration 25717, Loss: 0.05564181134104729\n",
      "Iteration 25718, Loss: 0.055759869515895844\n",
      "Iteration 25719, Loss: 0.05576233193278313\n",
      "Iteration 25720, Loss: 0.05561721697449684\n",
      "Iteration 25721, Loss: 0.055689334869384766\n",
      "Iteration 25722, Loss: 0.055629096925258636\n",
      "Iteration 25723, Loss: 0.05576106160879135\n",
      "Iteration 25724, Loss: 0.05577334016561508\n",
      "Iteration 25725, Loss: 0.05565500259399414\n",
      "Iteration 25726, Loss: 0.055786095559597015\n",
      "Iteration 25727, Loss: 0.05585245415568352\n",
      "Iteration 25728, Loss: 0.055780768394470215\n",
      "Iteration 25729, Loss: 0.05565015599131584\n",
      "Iteration 25730, Loss: 0.05576086416840553\n",
      "Iteration 25731, Loss: 0.05575084686279297\n",
      "Iteration 25732, Loss: 0.05562977120280266\n",
      "Iteration 25733, Loss: 0.0556592158973217\n",
      "Iteration 25734, Loss: 0.05563386529684067\n",
      "Iteration 25735, Loss: 0.05568687245249748\n",
      "Iteration 25736, Loss: 0.05564594268798828\n",
      "Iteration 25737, Loss: 0.05572299286723137\n",
      "Iteration 25738, Loss: 0.05572033300995827\n",
      "Iteration 25739, Loss: 0.05563942715525627\n",
      "Iteration 25740, Loss: 0.055661797523498535\n",
      "Iteration 25741, Loss: 0.05566815659403801\n",
      "Iteration 25742, Loss: 0.055646300315856934\n",
      "Iteration 25743, Loss: 0.055708568543195724\n",
      "Iteration 25744, Loss: 0.05568262189626694\n",
      "Iteration 25745, Loss: 0.05569704622030258\n",
      "Iteration 25746, Loss: 0.05572017282247543\n",
      "Iteration 25747, Loss: 0.05563728138804436\n",
      "Iteration 25748, Loss: 0.055783431977033615\n",
      "Iteration 25749, Loss: 0.05580413341522217\n",
      "Iteration 25750, Loss: 0.05565977096557617\n",
      "Iteration 25751, Loss: 0.05580019950866699\n",
      "Iteration 25752, Loss: 0.05590256303548813\n",
      "Iteration 25753, Loss: 0.05588650703430176\n",
      "Iteration 25754, Loss: 0.05576789751648903\n",
      "Iteration 25755, Loss: 0.05568408966064453\n",
      "Iteration 25756, Loss: 0.055778782814741135\n",
      "Iteration 25757, Loss: 0.0557204894721508\n",
      "Iteration 25758, Loss: 0.05570141598582268\n",
      "Iteration 25759, Loss: 0.055753033608198166\n",
      "Iteration 25760, Loss: 0.05571138858795166\n",
      "Iteration 25761, Loss: 0.05566815659403801\n",
      "Iteration 25762, Loss: 0.05569835752248764\n",
      "Iteration 25763, Loss: 0.05566072836518288\n",
      "Iteration 25764, Loss: 0.05568253993988037\n",
      "Iteration 25765, Loss: 0.05566736310720444\n",
      "Iteration 25766, Loss: 0.05569084733724594\n",
      "Iteration 25767, Loss: 0.05566692352294922\n",
      "Iteration 25768, Loss: 0.05569803714752197\n",
      "Iteration 25769, Loss: 0.05571107193827629\n",
      "Iteration 25770, Loss: 0.05561510846018791\n",
      "Iteration 25771, Loss: 0.05582714453339577\n",
      "Iteration 25772, Loss: 0.05586318299174309\n",
      "Iteration 25773, Loss: 0.05573515221476555\n",
      "Iteration 25774, Loss: 0.05572931095957756\n",
      "Iteration 25775, Loss: 0.05582452192902565\n",
      "Iteration 25776, Loss: 0.055810730904340744\n",
      "Iteration 25777, Loss: 0.055697642266750336\n",
      "Iteration 25778, Loss: 0.05575522035360336\n",
      "Iteration 25779, Loss: 0.055823925882577896\n",
      "Iteration 25780, Loss: 0.055724263191223145\n",
      "Iteration 25781, Loss: 0.055718064308166504\n",
      "Iteration 25782, Loss: 0.05579551309347153\n",
      "Iteration 25783, Loss: 0.05576467514038086\n",
      "Iteration 25784, Loss: 0.05564173310995102\n",
      "Iteration 25785, Loss: 0.05583211034536362\n",
      "Iteration 25786, Loss: 0.05589902400970459\n",
      "Iteration 25787, Loss: 0.05579157918691635\n",
      "Iteration 25788, Loss: 0.05567384138703346\n",
      "Iteration 25789, Loss: 0.055759429931640625\n",
      "Iteration 25790, Loss: 0.05574079602956772\n",
      "Iteration 25791, Loss: 0.05562695115804672\n",
      "Iteration 25792, Loss: 0.05584871768951416\n",
      "Iteration 25793, Loss: 0.055913448333740234\n",
      "Iteration 25794, Loss: 0.05580393597483635\n",
      "Iteration 25795, Loss: 0.05566795915365219\n",
      "Iteration 25796, Loss: 0.05575760453939438\n",
      "Iteration 25797, Loss: 0.05574576184153557\n",
      "Iteration 25798, Loss: 0.05563684552907944\n",
      "Iteration 25799, Loss: 0.05583111569285393\n",
      "Iteration 25800, Loss: 0.05589564889669418\n",
      "Iteration 25801, Loss: 0.05579197779297829\n",
      "Iteration 25802, Loss: 0.05567292496562004\n",
      "Iteration 25803, Loss: 0.05576102063059807\n",
      "Iteration 25804, Loss: 0.05574766919016838\n",
      "Iteration 25805, Loss: 0.05563080310821533\n",
      "Iteration 25806, Loss: 0.055841684341430664\n",
      "Iteration 25807, Loss: 0.055916547775268555\n",
      "Iteration 25808, Loss: 0.05583258718252182\n",
      "Iteration 25809, Loss: 0.05565556138753891\n",
      "Iteration 25810, Loss: 0.055802345275878906\n",
      "Iteration 25811, Loss: 0.055870335549116135\n",
      "Iteration 25812, Loss: 0.05580544471740723\n",
      "Iteration 25813, Loss: 0.05563613027334213\n",
      "Iteration 25814, Loss: 0.05584653466939926\n",
      "Iteration 25815, Loss: 0.055931251496076584\n",
      "Iteration 25816, Loss: 0.055856745690107346\n",
      "Iteration 25817, Loss: 0.05565718933939934\n",
      "Iteration 25818, Loss: 0.05584057420492172\n",
      "Iteration 25819, Loss: 0.05596248432993889\n",
      "Iteration 25820, Loss: 0.05594853684306145\n",
      "Iteration 25821, Loss: 0.05581708997488022\n",
      "Iteration 25822, Loss: 0.055670857429504395\n",
      "Iteration 25823, Loss: 0.055813513696193695\n",
      "Iteration 25824, Loss: 0.055839937180280685\n",
      "Iteration 25825, Loss: 0.05571286007761955\n",
      "Iteration 25826, Loss: 0.055752914398908615\n",
      "Iteration 25827, Loss: 0.05584633722901344\n",
      "Iteration 25828, Loss: 0.05582781881093979\n",
      "Iteration 25829, Loss: 0.05571850389242172\n",
      "Iteration 25830, Loss: 0.05573264881968498\n",
      "Iteration 25831, Loss: 0.055800557136535645\n",
      "Iteration 25832, Loss: 0.05571027845144272\n",
      "Iteration 25833, Loss: 0.0557279996573925\n",
      "Iteration 25834, Loss: 0.05579877272248268\n",
      "Iteration 25835, Loss: 0.05576726049184799\n",
      "Iteration 25836, Loss: 0.055660128593444824\n",
      "Iteration 25837, Loss: 0.05579432100057602\n",
      "Iteration 25838, Loss: 0.05583862587809563\n",
      "Iteration 25839, Loss: 0.055709365755319595\n",
      "Iteration 25840, Loss: 0.055751923471689224\n",
      "Iteration 25841, Loss: 0.05584947392344475\n",
      "Iteration 25842, Loss: 0.05584359169006348\n",
      "Iteration 25843, Loss: 0.05574500933289528\n",
      "Iteration 25844, Loss: 0.05567769333720207\n",
      "Iteration 25845, Loss: 0.0557328462600708\n",
      "Iteration 25846, Loss: 0.05563056468963623\n",
      "Iteration 25847, Loss: 0.05574798583984375\n",
      "Iteration 25848, Loss: 0.05578148737549782\n",
      "Iteration 25849, Loss: 0.05570908635854721\n",
      "Iteration 25850, Loss: 0.055695533752441406\n",
      "Iteration 25851, Loss: 0.05572374910116196\n",
      "Iteration 25852, Loss: 0.05563092231750488\n",
      "Iteration 25853, Loss: 0.05563771724700928\n",
      "Iteration 25854, Loss: 0.0556819848716259\n",
      "Iteration 25855, Loss: 0.05561785027384758\n",
      "Iteration 25856, Loss: 0.05570439621806145\n",
      "Iteration 25857, Loss: 0.055667243897914886\n",
      "Iteration 25858, Loss: 0.05571051687002182\n",
      "Iteration 25859, Loss: 0.055720966309309006\n",
      "Iteration 25860, Loss: 0.05562838166952133\n",
      "Iteration 25861, Loss: 0.055697206407785416\n",
      "Iteration 25862, Loss: 0.055620115250349045\n",
      "Iteration 25863, Loss: 0.055777113884687424\n",
      "Iteration 25864, Loss: 0.05583743378520012\n",
      "Iteration 25865, Loss: 0.05579555034637451\n",
      "Iteration 25866, Loss: 0.05566581338644028\n",
      "Iteration 25867, Loss: 0.055820148438215256\n",
      "Iteration 25868, Loss: 0.05590144917368889\n",
      "Iteration 25869, Loss: 0.055800680071115494\n",
      "Iteration 25870, Loss: 0.05566513538360596\n",
      "Iteration 25871, Loss: 0.055746398866176605\n",
      "Iteration 25872, Loss: 0.05572668835520744\n",
      "Iteration 25873, Loss: 0.05562349408864975\n",
      "Iteration 25874, Loss: 0.05581112951040268\n",
      "Iteration 25875, Loss: 0.055836521089076996\n",
      "Iteration 25876, Loss: 0.05569855496287346\n",
      "Iteration 25877, Loss: 0.05576292797923088\n",
      "Iteration 25878, Loss: 0.055862270295619965\n",
      "Iteration 25879, Loss: 0.055852536112070084\n",
      "Iteration 25880, Loss: 0.055744729936122894\n",
      "Iteration 25881, Loss: 0.055690448731184006\n",
      "Iteration 25882, Loss: 0.05575728416442871\n",
      "Iteration 25883, Loss: 0.055655837059020996\n",
      "Iteration 25884, Loss: 0.055773377418518066\n",
      "Iteration 25885, Loss: 0.05585312843322754\n",
      "Iteration 25886, Loss: 0.05582857131958008\n",
      "Iteration 25887, Loss: 0.05571293830871582\n",
      "Iteration 25888, Loss: 0.055740755051374435\n",
      "Iteration 25889, Loss: 0.05581124871969223\n",
      "Iteration 25890, Loss: 0.0557078942656517\n",
      "Iteration 25891, Loss: 0.05573570728302002\n",
      "Iteration 25892, Loss: 0.055817168205976486\n",
      "Iteration 25893, Loss: 0.05579610913991928\n",
      "Iteration 25894, Loss: 0.05568496510386467\n",
      "Iteration 25895, Loss: 0.05577230453491211\n",
      "Iteration 25896, Loss: 0.055835526436567307\n",
      "Iteration 25897, Loss: 0.05572191998362541\n",
      "Iteration 25898, Loss: 0.05573117733001709\n",
      "Iteration 25899, Loss: 0.05581963062286377\n",
      "Iteration 25900, Loss: 0.055805008858442307\n",
      "Iteration 25901, Loss: 0.055698394775390625\n",
      "Iteration 25902, Loss: 0.05574862286448479\n",
      "Iteration 25903, Loss: 0.05580791085958481\n",
      "Iteration 25904, Loss: 0.05568941682577133\n",
      "Iteration 25905, Loss: 0.0557582788169384\n",
      "Iteration 25906, Loss: 0.05584951490163803\n",
      "Iteration 25907, Loss: 0.05583759397268295\n",
      "Iteration 25908, Loss: 0.05573276802897453\n",
      "Iteration 25909, Loss: 0.05569986626505852\n",
      "Iteration 25910, Loss: 0.055757444351911545\n",
      "Iteration 25911, Loss: 0.05563831329345703\n",
      "Iteration 25912, Loss: 0.055795591324567795\n",
      "Iteration 25913, Loss: 0.055886827409267426\n",
      "Iteration 25914, Loss: 0.05587458983063698\n",
      "Iteration 25915, Loss: 0.05576920509338379\n",
      "Iteration 25916, Loss: 0.05565115064382553\n",
      "Iteration 25917, Loss: 0.05570884793996811\n",
      "Iteration 25918, Loss: 0.05562750622630119\n",
      "Iteration 25919, Loss: 0.05561685562133789\n",
      "Iteration 25920, Loss: 0.055736344307661057\n",
      "Iteration 25921, Loss: 0.05569744110107422\n",
      "Iteration 25922, Loss: 0.05569446086883545\n",
      "Iteration 25923, Loss: 0.05573026463389397\n",
      "Iteration 25924, Loss: 0.055657029151916504\n",
      "Iteration 25925, Loss: 0.05576189607381821\n",
      "Iteration 25926, Loss: 0.05579010769724846\n",
      "Iteration 25927, Loss: 0.05566044896841049\n",
      "Iteration 25928, Loss: 0.055787961930036545\n",
      "Iteration 25929, Loss: 0.05587538331747055\n",
      "Iteration 25930, Loss: 0.055840931832790375\n",
      "Iteration 25931, Loss: 0.05570423603057861\n",
      "Iteration 25932, Loss: 0.05577174946665764\n",
      "Iteration 25933, Loss: 0.055862270295619965\n",
      "Iteration 25934, Loss: 0.05578545853495598\n",
      "Iteration 25935, Loss: 0.055653057992458344\n",
      "Iteration 25936, Loss: 0.05571838468313217\n",
      "Iteration 25937, Loss: 0.055668435990810394\n",
      "Iteration 25938, Loss: 0.05572247505187988\n",
      "Iteration 25939, Loss: 0.05573193356394768\n",
      "Iteration 25940, Loss: 0.05563497543334961\n",
      "Iteration 25941, Loss: 0.05568607896566391\n",
      "Iteration 25942, Loss: 0.05561971664428711\n",
      "Iteration 25943, Loss: 0.055771153420209885\n",
      "Iteration 25944, Loss: 0.0558164119720459\n",
      "Iteration 25945, Loss: 0.055755697190761566\n",
      "Iteration 25946, Loss: 0.05565115064382553\n",
      "Iteration 25947, Loss: 0.05576400086283684\n",
      "Iteration 25948, Loss: 0.05574619770050049\n",
      "Iteration 25949, Loss: 0.05565730854868889\n",
      "Iteration 25950, Loss: 0.05569887161254883\n",
      "Iteration 25951, Loss: 0.05566593259572983\n",
      "Iteration 25952, Loss: 0.05570685863494873\n",
      "Iteration 25953, Loss: 0.05568560212850571\n",
      "Iteration 25954, Loss: 0.055694226175546646\n",
      "Iteration 25955, Loss: 0.055723510682582855\n",
      "Iteration 25956, Loss: 0.05565520375967026\n",
      "Iteration 25957, Loss: 0.05576181411743164\n",
      "Iteration 25958, Loss: 0.05578061193227768\n",
      "Iteration 25959, Loss: 0.05563624948263168\n",
      "Iteration 25960, Loss: 0.05580366030335426\n",
      "Iteration 25961, Loss: 0.05589008703827858\n",
      "Iteration 25962, Loss: 0.05585968494415283\n",
      "Iteration 25963, Loss: 0.055729590356349945\n",
      "Iteration 25964, Loss: 0.05573682114481926\n",
      "Iteration 25965, Loss: 0.055823683738708496\n",
      "Iteration 25966, Loss: 0.05574214830994606\n",
      "Iteration 25967, Loss: 0.05569569393992424\n",
      "Iteration 25968, Loss: 0.055761538445949554\n",
      "Iteration 25969, Loss: 0.05572100728750229\n",
      "Iteration 25970, Loss: 0.0556543692946434\n",
      "Iteration 25971, Loss: 0.05570264905691147\n",
      "Iteration 25972, Loss: 0.055649399757385254\n",
      "Iteration 25973, Loss: 0.05570189282298088\n",
      "Iteration 25974, Loss: 0.05570431798696518\n",
      "Iteration 25975, Loss: 0.05562027543783188\n",
      "Iteration 25976, Loss: 0.05567356199026108\n",
      "Iteration 25977, Loss: 0.05563851445913315\n",
      "Iteration 25978, Loss: 0.05573562905192375\n",
      "Iteration 25979, Loss: 0.05570976063609123\n",
      "Iteration 25980, Loss: 0.05568166822195053\n",
      "Iteration 25981, Loss: 0.055717192590236664\n",
      "Iteration 25982, Loss: 0.055654607713222504\n",
      "Iteration 25983, Loss: 0.055753789842128754\n",
      "Iteration 25984, Loss: 0.05576356500387192\n",
      "Iteration 25985, Loss: 0.055618766695261\n",
      "Iteration 25986, Loss: 0.05564439669251442\n",
      "Iteration 25987, Loss: 0.055667441338300705\n",
      "Iteration 25988, Loss: 0.05562261864542961\n",
      "Iteration 25989, Loss: 0.05568850040435791\n",
      "Iteration 25990, Loss: 0.05561649799346924\n",
      "Iteration 25991, Loss: 0.0556720532476902\n",
      "Iteration 25992, Loss: 0.05564431473612785\n",
      "Iteration 25993, Loss: 0.05561983585357666\n",
      "Iteration 25994, Loss: 0.055754899978637695\n",
      "Iteration 25995, Loss: 0.05572931095957756\n",
      "Iteration 25996, Loss: 0.055665019899606705\n",
      "Iteration 25997, Loss: 0.055700305849313736\n",
      "Iteration 25998, Loss: 0.0556335486471653\n",
      "Iteration 25999, Loss: 0.05578649416565895\n",
      "Iteration 26000, Loss: 0.05580854415893555\n",
      "Iteration 26001, Loss: 0.05567511171102524\n",
      "Iteration 26002, Loss: 0.05577826499938965\n",
      "Iteration 26003, Loss: 0.05586886405944824\n",
      "Iteration 26004, Loss: 0.05583791062235832\n",
      "Iteration 26005, Loss: 0.055702608078718185\n",
      "Iteration 26006, Loss: 0.05577155202627182\n",
      "Iteration 26007, Loss: 0.055861156433820724\n",
      "Iteration 26008, Loss: 0.055783115327358246\n",
      "Iteration 26009, Loss: 0.05565595626831055\n",
      "Iteration 26010, Loss: 0.05572442337870598\n",
      "Iteration 26011, Loss: 0.05567769333720207\n",
      "Iteration 26012, Loss: 0.055710118263959885\n",
      "Iteration 26013, Loss: 0.05571969598531723\n",
      "Iteration 26014, Loss: 0.055641572922468185\n",
      "Iteration 26015, Loss: 0.055671095848083496\n",
      "Iteration 26016, Loss: 0.05564340204000473\n",
      "Iteration 26017, Loss: 0.0556490421295166\n",
      "Iteration 26018, Loss: 0.05563271418213844\n",
      "Iteration 26019, Loss: 0.05569211766123772\n",
      "Iteration 26020, Loss: 0.05567368119955063\n",
      "Iteration 26021, Loss: 0.0556795597076416\n",
      "Iteration 26022, Loss: 0.05566056817770004\n",
      "Iteration 26023, Loss: 0.05571186915040016\n",
      "Iteration 26024, Loss: 0.05573193356394768\n",
      "Iteration 26025, Loss: 0.055654846131801605\n",
      "Iteration 26026, Loss: 0.05576006695628166\n",
      "Iteration 26027, Loss: 0.055780135095119476\n",
      "Iteration 26028, Loss: 0.05563533678650856\n",
      "Iteration 26029, Loss: 0.05582018941640854\n",
      "Iteration 26030, Loss: 0.05592437833547592\n",
      "Iteration 26031, Loss: 0.05591702461242676\n",
      "Iteration 26032, Loss: 0.05581236258149147\n",
      "Iteration 26033, Loss: 0.05565690994262695\n",
      "Iteration 26034, Loss: 0.055843714624643326\n",
      "Iteration 26035, Loss: 0.055911265313625336\n",
      "Iteration 26036, Loss: 0.05580250546336174\n",
      "Iteration 26037, Loss: 0.055673085153102875\n",
      "Iteration 26038, Loss: 0.055759549140930176\n",
      "Iteration 26039, Loss: 0.055748824030160904\n",
      "Iteration 26040, Loss: 0.05565214157104492\n",
      "Iteration 26041, Loss: 0.055799245834350586\n",
      "Iteration 26042, Loss: 0.05584220215678215\n",
      "Iteration 26043, Loss: 0.05570892617106438\n",
      "Iteration 26044, Loss: 0.055754583328962326\n",
      "Iteration 26045, Loss: 0.055854640901088715\n",
      "Iteration 26046, Loss: 0.055848877876996994\n",
      "Iteration 26047, Loss: 0.055748067796230316\n",
      "Iteration 26048, Loss: 0.05567602440714836\n",
      "Iteration 26049, Loss: 0.055729471147060394\n",
      "Iteration 26050, Loss: 0.05561817064881325\n",
      "Iteration 26051, Loss: 0.05570121854543686\n",
      "Iteration 26052, Loss: 0.05567427724599838\n",
      "Iteration 26053, Loss: 0.055691562592983246\n",
      "Iteration 26054, Loss: 0.05568305775523186\n",
      "Iteration 26055, Loss: 0.055683255195617676\n",
      "Iteration 26056, Loss: 0.055692993104457855\n",
      "Iteration 26057, Loss: 0.055638473480939865\n",
      "Iteration 26058, Loss: 0.05566931143403053\n",
      "Iteration 26059, Loss: 0.0556543692946434\n",
      "Iteration 26060, Loss: 0.05564475059509277\n",
      "Iteration 26061, Loss: 0.055693548172712326\n",
      "Iteration 26062, Loss: 0.05563763901591301\n",
      "Iteration 26063, Loss: 0.05574735254049301\n",
      "Iteration 26064, Loss: 0.05578875541687012\n",
      "Iteration 26065, Loss: 0.05572656914591789\n",
      "Iteration 26066, Loss: 0.05566362664103508\n",
      "Iteration 26067, Loss: 0.055691562592983246\n",
      "Iteration 26068, Loss: 0.05565989390015602\n",
      "Iteration 26069, Loss: 0.05566342920064926\n",
      "Iteration 26070, Loss: 0.05566032975912094\n",
      "Iteration 26071, Loss: 0.05564646050333977\n",
      "Iteration 26072, Loss: 0.055667124688625336\n",
      "Iteration 26073, Loss: 0.05563974380493164\n",
      "Iteration 26074, Loss: 0.05573427677154541\n",
      "Iteration 26075, Loss: 0.055721085518598557\n",
      "Iteration 26076, Loss: 0.05565878003835678\n",
      "Iteration 26077, Loss: 0.05568766966462135\n",
      "Iteration 26078, Loss: 0.0556262731552124\n",
      "Iteration 26079, Loss: 0.055716753005981445\n",
      "Iteration 26080, Loss: 0.055717550218105316\n",
      "Iteration 26081, Loss: 0.055638037621974945\n",
      "Iteration 26082, Loss: 0.05576678365468979\n",
      "Iteration 26083, Loss: 0.05575895681977272\n",
      "Iteration 26084, Loss: 0.055638670921325684\n",
      "Iteration 26085, Loss: 0.05568341538310051\n",
      "Iteration 26086, Loss: 0.05564316362142563\n",
      "Iteration 26087, Loss: 0.05574452877044678\n",
      "Iteration 26088, Loss: 0.05573869124054909\n",
      "Iteration 26089, Loss: 0.055646900087594986\n",
      "Iteration 26090, Loss: 0.055693864822387695\n",
      "Iteration 26091, Loss: 0.055643558502197266\n",
      "Iteration 26092, Loss: 0.05575406551361084\n",
      "Iteration 26093, Loss: 0.055777471512556076\n",
      "Iteration 26094, Loss: 0.0556793212890625\n",
      "Iteration 26095, Loss: 0.055752240121364594\n",
      "Iteration 26096, Loss: 0.055806875228881836\n",
      "Iteration 26097, Loss: 0.055719535797834396\n",
      "Iteration 26098, Loss: 0.055710118263959885\n",
      "Iteration 26099, Loss: 0.055770598351955414\n",
      "Iteration 26100, Loss: 0.05570022389292717\n",
      "Iteration 26101, Loss: 0.05570987984538078\n",
      "Iteration 26102, Loss: 0.055749814957380295\n",
      "Iteration 26103, Loss: 0.055650949478149414\n",
      "Iteration 26104, Loss: 0.05578363314270973\n",
      "Iteration 26105, Loss: 0.05584514141082764\n",
      "Iteration 26106, Loss: 0.05577119439840317\n",
      "Iteration 26107, Loss: 0.05565885826945305\n",
      "Iteration 26108, Loss: 0.05575132742524147\n",
      "Iteration 26109, Loss: 0.05572203919291496\n",
      "Iteration 26110, Loss: 0.05567300319671631\n",
      "Iteration 26111, Loss: 0.055698953568935394\n",
      "Iteration 26112, Loss: 0.055645864456892014\n",
      "Iteration 26113, Loss: 0.0557255782186985\n",
      "Iteration 26114, Loss: 0.05569108575582504\n",
      "Iteration 26115, Loss: 0.05570320412516594\n",
      "Iteration 26116, Loss: 0.055742423981428146\n",
      "Iteration 26117, Loss: 0.055685125291347504\n",
      "Iteration 26118, Loss: 0.05570777505636215\n",
      "Iteration 26119, Loss: 0.05571305751800537\n",
      "Iteration 26120, Loss: 0.055659692734479904\n",
      "Iteration 26121, Loss: 0.05567800998687744\n",
      "Iteration 26122, Loss: 0.055632829666137695\n",
      "Iteration 26123, Loss: 0.05570165440440178\n",
      "Iteration 26124, Loss: 0.055661797523498535\n",
      "Iteration 26125, Loss: 0.055713776499032974\n",
      "Iteration 26126, Loss: 0.05572863668203354\n",
      "Iteration 26127, Loss: 0.05562635511159897\n",
      "Iteration 26128, Loss: 0.05580306053161621\n",
      "Iteration 26129, Loss: 0.05583802983164787\n",
      "Iteration 26130, Loss: 0.05571790784597397\n",
      "Iteration 26131, Loss: 0.05573571100831032\n",
      "Iteration 26132, Loss: 0.05582348629832268\n",
      "Iteration 26133, Loss: 0.055794280022382736\n",
      "Iteration 26134, Loss: 0.05566350743174553\n",
      "Iteration 26135, Loss: 0.05581128969788551\n",
      "Iteration 26136, Loss: 0.05589374154806137\n",
      "Iteration 26137, Loss: 0.05581136792898178\n",
      "Iteration 26138, Loss: 0.05564260482788086\n",
      "Iteration 26139, Loss: 0.05575069040060043\n",
      "Iteration 26140, Loss: 0.055750250816345215\n",
      "Iteration 26141, Loss: 0.05562957376241684\n",
      "Iteration 26142, Loss: 0.05583040043711662\n",
      "Iteration 26143, Loss: 0.05590061470866203\n",
      "Iteration 26144, Loss: 0.05581800267100334\n",
      "Iteration 26145, Loss: 0.05565301701426506\n",
      "Iteration 26146, Loss: 0.05579376220703125\n",
      "Iteration 26147, Loss: 0.05584176629781723\n",
      "Iteration 26148, Loss: 0.05575060844421387\n",
      "Iteration 26149, Loss: 0.05568202584981918\n",
      "Iteration 26150, Loss: 0.05574735254049301\n",
      "Iteration 26151, Loss: 0.05567837134003639\n",
      "Iteration 26152, Loss: 0.05573109909892082\n",
      "Iteration 26153, Loss: 0.05577047914266586\n",
      "Iteration 26154, Loss: 0.05567729473114014\n",
      "Iteration 26155, Loss: 0.05574989318847656\n",
      "Iteration 26156, Loss: 0.055806003510951996\n",
      "Iteration 26157, Loss: 0.05572418496012688\n",
      "Iteration 26158, Loss: 0.05569760128855705\n",
      "Iteration 26159, Loss: 0.05575323477387428\n",
      "Iteration 26160, Loss: 0.05567491427063942\n",
      "Iteration 26161, Loss: 0.05574337765574455\n",
      "Iteration 26162, Loss: 0.055791061371564865\n",
      "Iteration 26163, Loss: 0.05570407956838608\n",
      "Iteration 26164, Loss: 0.0557200126349926\n",
      "Iteration 26165, Loss: 0.05577600374817848\n",
      "Iteration 26166, Loss: 0.055696409195661545\n",
      "Iteration 26167, Loss: 0.0557231530547142\n",
      "Iteration 26168, Loss: 0.05577385425567627\n",
      "Iteration 26169, Loss: 0.05569104477763176\n",
      "Iteration 26170, Loss: 0.05572883412241936\n",
      "Iteration 26171, Loss: 0.05577961727976799\n",
      "Iteration 26172, Loss: 0.05569315329194069\n",
      "Iteration 26173, Loss: 0.055730901658535004\n",
      "Iteration 26174, Loss: 0.05578633397817612\n",
      "Iteration 26175, Loss: 0.055707018822431564\n",
      "Iteration 26176, Loss: 0.05571075528860092\n",
      "Iteration 26177, Loss: 0.0557609423995018\n",
      "Iteration 26178, Loss: 0.055674634873867035\n",
      "Iteration 26179, Loss: 0.05574953556060791\n",
      "Iteration 26180, Loss: 0.055803459137678146\n",
      "Iteration 26181, Loss: 0.05572263523936272\n",
      "Iteration 26182, Loss: 0.055697642266750336\n",
      "Iteration 26183, Loss: 0.05575243756175041\n",
      "Iteration 26184, Loss: 0.055671416223049164\n",
      "Iteration 26185, Loss: 0.05575156211853027\n",
      "Iteration 26186, Loss: 0.05580437183380127\n",
      "Iteration 26187, Loss: 0.055726807564496994\n",
      "Iteration 26188, Loss: 0.05569152161478996\n",
      "Iteration 26189, Loss: 0.05574576184153557\n",
      "Iteration 26190, Loss: 0.05566469952464104\n",
      "Iteration 26191, Loss: 0.055759869515895844\n",
      "Iteration 26192, Loss: 0.0558161735534668\n",
      "Iteration 26193, Loss: 0.055749181658029556\n",
      "Iteration 26194, Loss: 0.05566791817545891\n",
      "Iteration 26195, Loss: 0.05574103444814682\n",
      "Iteration 26196, Loss: 0.055692315101623535\n",
      "Iteration 26197, Loss: 0.055713772773742676\n",
      "Iteration 26198, Loss: 0.055754028260707855\n",
      "Iteration 26199, Loss: 0.055696528404951096\n",
      "Iteration 26200, Loss: 0.05569823831319809\n",
      "Iteration 26201, Loss: 0.055715203285217285\n",
      "Iteration 26202, Loss: 0.05565281957387924\n",
      "Iteration 26203, Loss: 0.05567499250173569\n",
      "Iteration 26204, Loss: 0.05565170571208\n",
      "Iteration 26205, Loss: 0.05570245161652565\n",
      "Iteration 26206, Loss: 0.055664580315351486\n",
      "Iteration 26207, Loss: 0.05571305751800537\n",
      "Iteration 26208, Loss: 0.05574282258749008\n",
      "Iteration 26209, Loss: 0.0556691512465477\n",
      "Iteration 26210, Loss: 0.05574687570333481\n",
      "Iteration 26211, Loss: 0.05577500909566879\n",
      "Iteration 26212, Loss: 0.05564447492361069\n",
      "Iteration 26213, Loss: 0.0557965449988842\n",
      "Iteration 26214, Loss: 0.05588182061910629\n",
      "Iteration 26215, Loss: 0.055851224809885025\n",
      "Iteration 26216, Loss: 0.055723391473293304\n",
      "Iteration 26217, Loss: 0.05574158951640129\n",
      "Iteration 26218, Loss: 0.05582758039236069\n",
      "Iteration 26219, Loss: 0.05575200170278549\n",
      "Iteration 26220, Loss: 0.055680714547634125\n",
      "Iteration 26221, Loss: 0.05574123188853264\n",
      "Iteration 26222, Loss: 0.05569661036133766\n",
      "Iteration 26223, Loss: 0.05568063631653786\n",
      "Iteration 26224, Loss: 0.05568814277648926\n",
      "Iteration 26225, Loss: 0.05567260831594467\n",
      "Iteration 26226, Loss: 0.055679801851511\n",
      "Iteration 26227, Loss: 0.05564836785197258\n",
      "Iteration 26228, Loss: 0.055673085153102875\n",
      "Iteration 26229, Loss: 0.05565015599131584\n",
      "Iteration 26230, Loss: 0.05565802380442619\n",
      "Iteration 26231, Loss: 0.05564912408590317\n",
      "Iteration 26232, Loss: 0.055666010826826096\n",
      "Iteration 26233, Loss: 0.05564805120229721\n",
      "Iteration 26234, Loss: 0.055702291429042816\n",
      "Iteration 26235, Loss: 0.05565730854868889\n",
      "Iteration 26236, Loss: 0.055731695145368576\n",
      "Iteration 26237, Loss: 0.05577818676829338\n",
      "Iteration 26238, Loss: 0.05572609230875969\n",
      "Iteration 26239, Loss: 0.055643480271101\n",
      "Iteration 26240, Loss: 0.05564336106181145\n",
      "Iteration 26241, Loss: 0.05571238324046135\n",
      "Iteration 26242, Loss: 0.0557323694229126\n",
      "Iteration 26243, Loss: 0.055656515061855316\n",
      "Iteration 26244, Loss: 0.05576733872294426\n",
      "Iteration 26245, Loss: 0.05579213425517082\n",
      "Iteration 26246, Loss: 0.05564085766673088\n",
      "Iteration 26247, Loss: 0.055815380066633224\n",
      "Iteration 26248, Loss: 0.055926404893398285\n",
      "Iteration 26249, Loss: 0.05593057721853256\n",
      "Iteration 26250, Loss: 0.05583886429667473\n",
      "Iteration 26251, Loss: 0.0556643009185791\n",
      "Iteration 26252, Loss: 0.05587729066610336\n",
      "Iteration 26253, Loss: 0.056008897721767426\n",
      "Iteration 26254, Loss: 0.05595263093709946\n",
      "Iteration 26255, Loss: 0.05572863668203354\n",
      "Iteration 26256, Loss: 0.05579861253499985\n",
      "Iteration 26257, Loss: 0.05595310777425766\n",
      "Iteration 26258, Loss: 0.055997334420681\n",
      "Iteration 26259, Loss: 0.05594193935394287\n",
      "Iteration 26260, Loss: 0.055797696113586426\n",
      "Iteration 26261, Loss: 0.055660806596279144\n",
      "Iteration 26262, Loss: 0.05576670169830322\n",
      "Iteration 26263, Loss: 0.05569712445139885\n",
      "Iteration 26264, Loss: 0.05571834370493889\n",
      "Iteration 26265, Loss: 0.05577826499938965\n",
      "Iteration 26266, Loss: 0.05573872849345207\n",
      "Iteration 26267, Loss: 0.0556158646941185\n",
      "Iteration 26268, Loss: 0.05573364347219467\n",
      "Iteration 26269, Loss: 0.05570971965789795\n",
      "Iteration 26270, Loss: 0.055666327476501465\n",
      "Iteration 26271, Loss: 0.05569092556834221\n",
      "Iteration 26272, Loss: 0.05563676357269287\n",
      "Iteration 26273, Loss: 0.055668435990810394\n",
      "Iteration 26274, Loss: 0.055643003433942795\n",
      "Iteration 26275, Loss: 0.05571989342570305\n",
      "Iteration 26276, Loss: 0.05568544194102287\n",
      "Iteration 26277, Loss: 0.055703405290842056\n",
      "Iteration 26278, Loss: 0.055743258446455\n",
      "Iteration 26279, Loss: 0.05568500608205795\n",
      "Iteration 26280, Loss: 0.0557076558470726\n",
      "Iteration 26281, Loss: 0.05571460723876953\n",
      "Iteration 26282, Loss: 0.05565508455038071\n",
      "Iteration 26283, Loss: 0.05567173287272453\n",
      "Iteration 26284, Loss: 0.05563529580831528\n",
      "Iteration 26285, Loss: 0.05564948171377182\n",
      "Iteration 26286, Loss: 0.05561864376068115\n",
      "Iteration 26287, Loss: 0.05571063607931137\n",
      "Iteration 26288, Loss: 0.05568818375468254\n",
      "Iteration 26289, Loss: 0.0556744746863842\n",
      "Iteration 26290, Loss: 0.055674634873867035\n",
      "Iteration 26291, Loss: 0.055682145059108734\n",
      "Iteration 26292, Loss: 0.055679164826869965\n",
      "Iteration 26293, Loss: 0.055664658546447754\n",
      "Iteration 26294, Loss: 0.05565170571208\n",
      "Iteration 26295, Loss: 0.05571099370718002\n",
      "Iteration 26296, Loss: 0.055717550218105316\n",
      "Iteration 26297, Loss: 0.05564193055033684\n",
      "Iteration 26298, Loss: 0.05574222654104233\n",
      "Iteration 26299, Loss: 0.05571846291422844\n",
      "Iteration 26300, Loss: 0.055674195289611816\n",
      "Iteration 26301, Loss: 0.055706702172756195\n",
      "Iteration 26302, Loss: 0.05564701557159424\n",
      "Iteration 26303, Loss: 0.05575510114431381\n",
      "Iteration 26304, Loss: 0.05575498193502426\n",
      "Iteration 26305, Loss: 0.05563263222575188\n",
      "Iteration 26306, Loss: 0.05566342920064926\n",
      "Iteration 26307, Loss: 0.05562647432088852\n",
      "Iteration 26308, Loss: 0.055687468498945236\n",
      "Iteration 26309, Loss: 0.055664144456386566\n",
      "Iteration 26310, Loss: 0.05569350719451904\n",
      "Iteration 26311, Loss: 0.05566907301545143\n",
      "Iteration 26312, Loss: 0.055709920823574066\n",
      "Iteration 26313, Loss: 0.05573801323771477\n",
      "Iteration 26314, Loss: 0.05566438287496567\n",
      "Iteration 26315, Loss: 0.055749617516994476\n",
      "Iteration 26316, Loss: 0.0557711161673069\n",
      "Iteration 26317, Loss: 0.05562512204051018\n",
      "Iteration 26318, Loss: 0.055828891694545746\n",
      "Iteration 26319, Loss: 0.05593228340148926\n",
      "Iteration 26320, Loss: 0.05591794103384018\n",
      "Iteration 26321, Loss: 0.05580230802297592\n",
      "Iteration 26322, Loss: 0.05566295236349106\n",
      "Iteration 26323, Loss: 0.055816058069467545\n",
      "Iteration 26324, Loss: 0.05583862587809563\n",
      "Iteration 26325, Loss: 0.055696964263916016\n",
      "Iteration 26326, Loss: 0.05576853081583977\n",
      "Iteration 26327, Loss: 0.05586938187479973\n",
      "Iteration 26328, Loss: 0.05586683750152588\n",
      "Iteration 26329, Loss: 0.05577218532562256\n",
      "Iteration 26330, Loss: 0.05564812943339348\n",
      "Iteration 26331, Loss: 0.055759869515895844\n",
      "Iteration 26332, Loss: 0.05573054403066635\n",
      "Iteration 26333, Loss: 0.055673204362392426\n",
      "Iteration 26334, Loss: 0.055716633796691895\n",
      "Iteration 26335, Loss: 0.055678170174360275\n",
      "Iteration 26336, Loss: 0.05570729821920395\n",
      "Iteration 26337, Loss: 0.05570761486887932\n",
      "Iteration 26338, Loss: 0.05566100403666496\n",
      "Iteration 26339, Loss: 0.055686477571725845\n",
      "Iteration 26340, Loss: 0.055637799203395844\n",
      "Iteration 26341, Loss: 0.05571750923991203\n",
      "Iteration 26342, Loss: 0.055722516030073166\n",
      "Iteration 26343, Loss: 0.055646657943725586\n",
      "Iteration 26344, Loss: 0.05575649067759514\n",
      "Iteration 26345, Loss: 0.05575140565633774\n",
      "Iteration 26346, Loss: 0.055639468133449554\n",
      "Iteration 26347, Loss: 0.055673997849226\n",
      "Iteration 26348, Loss: 0.05563223361968994\n",
      "Iteration 26349, Loss: 0.05574381351470947\n",
      "Iteration 26350, Loss: 0.0557275228202343\n",
      "Iteration 26351, Loss: 0.05565798282623291\n",
      "Iteration 26352, Loss: 0.055693548172712326\n",
      "Iteration 26353, Loss: 0.055633544921875\n",
      "Iteration 26354, Loss: 0.05576447769999504\n",
      "Iteration 26355, Loss: 0.05578958988189697\n",
      "Iteration 26356, Loss: 0.05569219961762428\n",
      "Iteration 26357, Loss: 0.05573912709951401\n",
      "Iteration 26358, Loss: 0.05579710006713867\n",
      "Iteration 26359, Loss: 0.05571429058909416\n",
      "Iteration 26360, Loss: 0.055710792541503906\n",
      "Iteration 26361, Loss: 0.055767420679330826\n",
      "Iteration 26362, Loss: 0.05569501966238022\n",
      "Iteration 26363, Loss: 0.055714886635541916\n",
      "Iteration 26364, Loss: 0.05575522035360336\n",
      "Iteration 26365, Loss: 0.05565500259399414\n",
      "Iteration 26366, Loss: 0.05578061193227768\n",
      "Iteration 26367, Loss: 0.05584629625082016\n",
      "Iteration 26368, Loss: 0.05577842518687248\n",
      "Iteration 26369, Loss: 0.055652618408203125\n",
      "Iteration 26370, Loss: 0.055769406259059906\n",
      "Iteration 26371, Loss: 0.05576865002512932\n",
      "Iteration 26372, Loss: 0.05561959743499756\n",
      "Iteration 26373, Loss: 0.055775921791791916\n",
      "Iteration 26374, Loss: 0.05583437532186508\n",
      "Iteration 26375, Loss: 0.05579201504588127\n",
      "Iteration 26376, Loss: 0.055662356317043304\n",
      "Iteration 26377, Loss: 0.05582475662231445\n",
      "Iteration 26378, Loss: 0.055906496942043304\n",
      "Iteration 26379, Loss: 0.05580612272024155\n",
      "Iteration 26380, Loss: 0.055661123245954514\n",
      "Iteration 26381, Loss: 0.055742740631103516\n",
      "Iteration 26382, Loss: 0.05572478100657463\n",
      "Iteration 26383, Loss: 0.05562249943614006\n",
      "Iteration 26384, Loss: 0.055811405181884766\n",
      "Iteration 26385, Loss: 0.05583973973989487\n",
      "Iteration 26386, Loss: 0.05570809170603752\n",
      "Iteration 26387, Loss: 0.05575120449066162\n",
      "Iteration 26388, Loss: 0.05584649369120598\n",
      "Iteration 26389, Loss: 0.05583079904317856\n",
      "Iteration 26390, Loss: 0.05571528524160385\n",
      "Iteration 26391, Loss: 0.055735234171152115\n",
      "Iteration 26392, Loss: 0.05580735206604004\n",
      "Iteration 26393, Loss: 0.05571242421865463\n",
      "Iteration 26394, Loss: 0.05572410672903061\n",
      "Iteration 26395, Loss: 0.05579829216003418\n",
      "Iteration 26396, Loss: 0.05576400086283684\n",
      "Iteration 26397, Loss: 0.055642448365688324\n",
      "Iteration 26398, Loss: 0.05582404136657715\n",
      "Iteration 26399, Loss: 0.05588182061910629\n",
      "Iteration 26400, Loss: 0.055764954537153244\n",
      "Iteration 26401, Loss: 0.055699948221445084\n",
      "Iteration 26402, Loss: 0.05579010769724846\n",
      "Iteration 26403, Loss: 0.05577604100108147\n",
      "Iteration 26404, Loss: 0.055668435990810394\n",
      "Iteration 26405, Loss: 0.05578792095184326\n",
      "Iteration 26406, Loss: 0.055848121643066406\n",
      "Iteration 26407, Loss: 0.0557323694229126\n",
      "Iteration 26408, Loss: 0.055723391473293304\n",
      "Iteration 26409, Loss: 0.05581275746226311\n",
      "Iteration 26410, Loss: 0.05579789727926254\n",
      "Iteration 26411, Loss: 0.055689893662929535\n",
      "Iteration 26412, Loss: 0.0557604655623436\n",
      "Iteration 26413, Loss: 0.05582161992788315\n",
      "Iteration 26414, Loss: 0.05570678040385246\n",
      "Iteration 26415, Loss: 0.055742185562849045\n",
      "Iteration 26416, Loss: 0.055830877274274826\n",
      "Iteration 26417, Loss: 0.055815618485212326\n",
      "Iteration 26418, Loss: 0.0557071790099144\n",
      "Iteration 26419, Loss: 0.05573761463165283\n",
      "Iteration 26420, Loss: 0.05579936504364014\n",
      "Iteration 26421, Loss: 0.0556848868727684\n",
      "Iteration 26422, Loss: 0.05575859546661377\n",
      "Iteration 26423, Loss: 0.05584728717803955\n",
      "Iteration 26424, Loss: 0.055832188576459885\n",
      "Iteration 26425, Loss: 0.05572470277547836\n",
      "Iteration 26426, Loss: 0.05571405217051506\n",
      "Iteration 26427, Loss: 0.05577576160430908\n",
      "Iteration 26428, Loss: 0.05566168203949928\n",
      "Iteration 26429, Loss: 0.05577552318572998\n",
      "Iteration 26430, Loss: 0.055864058434963226\n",
      "Iteration 26431, Loss: 0.0558493547141552\n",
      "Iteration 26432, Loss: 0.055742502212524414\n",
      "Iteration 26433, Loss: 0.0556894950568676\n",
      "Iteration 26434, Loss: 0.05575069040060043\n",
      "Iteration 26435, Loss: 0.05563759803771973\n",
      "Iteration 26436, Loss: 0.0557883195579052\n",
      "Iteration 26437, Loss: 0.055872559547424316\n",
      "Iteration 26438, Loss: 0.05585348606109619\n",
      "Iteration 26439, Loss: 0.05574170872569084\n",
      "Iteration 26440, Loss: 0.055695418268442154\n",
      "Iteration 26441, Loss: 0.05575982853770256\n",
      "Iteration 26442, Loss: 0.05564558506011963\n",
      "Iteration 26443, Loss: 0.0557863712310791\n",
      "Iteration 26444, Loss: 0.055874746292829514\n",
      "Iteration 26445, Loss: 0.055859923362731934\n",
      "Iteration 26446, Loss: 0.05575228109955788\n",
      "Iteration 26447, Loss: 0.05567582696676254\n",
      "Iteration 26448, Loss: 0.05573626607656479\n",
      "Iteration 26449, Loss: 0.05562218278646469\n",
      "Iteration 26450, Loss: 0.05578688904643059\n",
      "Iteration 26451, Loss: 0.05585595220327377\n",
      "Iteration 26452, Loss: 0.05582062527537346\n",
      "Iteration 26453, Loss: 0.055694662034511566\n",
      "Iteration 26454, Loss: 0.05577496811747551\n",
      "Iteration 26455, Loss: 0.05585499852895737\n",
      "Iteration 26456, Loss: 0.055759549140930176\n",
      "Iteration 26457, Loss: 0.05568981170654297\n",
      "Iteration 26458, Loss: 0.05576638504862785\n",
      "Iteration 26459, Loss: 0.055739644914865494\n",
      "Iteration 26460, Loss: 0.055627863854169846\n",
      "Iteration 26461, Loss: 0.055825673043727875\n",
      "Iteration 26462, Loss: 0.05586453527212143\n",
      "Iteration 26463, Loss: 0.055725812911987305\n",
      "Iteration 26464, Loss: 0.05574425309896469\n",
      "Iteration 26465, Loss: 0.05584792420268059\n",
      "Iteration 26466, Loss: 0.0558466911315918\n",
      "Iteration 26467, Loss: 0.05575132369995117\n",
      "Iteration 26468, Loss: 0.055662356317043304\n",
      "Iteration 26469, Loss: 0.05570932477712631\n",
      "Iteration 26470, Loss: 0.05563386529684067\n",
      "Iteration 26471, Loss: 0.055630408227443695\n",
      "Iteration 26472, Loss: 0.055711548775434494\n",
      "Iteration 26473, Loss: 0.05565742775797844\n",
      "Iteration 26474, Loss: 0.05573686212301254\n",
      "Iteration 26475, Loss: 0.055786214768886566\n",
      "Iteration 26476, Loss: 0.05573463812470436\n",
      "Iteration 26477, Loss: 0.055638354271650314\n",
      "Iteration 26478, Loss: 0.05568961426615715\n",
      "Iteration 26479, Loss: 0.05564193055033684\n",
      "Iteration 26480, Loss: 0.05567062273621559\n",
      "Iteration 26481, Loss: 0.05562746897339821\n",
      "Iteration 26482, Loss: 0.055742859840393066\n",
      "Iteration 26483, Loss: 0.055758874863386154\n",
      "Iteration 26484, Loss: 0.055672209709882736\n",
      "Iteration 26485, Loss: 0.055749498307704926\n",
      "Iteration 26486, Loss: 0.0557863712310791\n",
      "Iteration 26487, Loss: 0.05566617101430893\n",
      "Iteration 26488, Loss: 0.055782318115234375\n",
      "Iteration 26489, Loss: 0.055870652198791504\n",
      "Iteration 26490, Loss: 0.05584331601858139\n",
      "Iteration 26491, Loss: 0.05572044849395752\n",
      "Iteration 26492, Loss: 0.05574079602956772\n",
      "Iteration 26493, Loss: 0.055821698158979416\n",
      "Iteration 26494, Loss: 0.055742304772138596\n",
      "Iteration 26495, Loss: 0.055692873895168304\n",
      "Iteration 26496, Loss: 0.0557558573782444\n",
      "Iteration 26497, Loss: 0.05571480840444565\n",
      "Iteration 26498, Loss: 0.05566143989562988\n",
      "Iteration 26499, Loss: 0.05569811910390854\n",
      "Iteration 26500, Loss: 0.055652063339948654\n",
      "Iteration 26501, Loss: 0.055683135986328125\n",
      "Iteration 26502, Loss: 0.055667877197265625\n",
      "Iteration 26503, Loss: 0.055686675012111664\n",
      "Iteration 26504, Loss: 0.055658381432294846\n",
      "Iteration 26505, Loss: 0.05571063607931137\n",
      "Iteration 26506, Loss: 0.055730223655700684\n",
      "Iteration 26507, Loss: 0.05564316362142563\n",
      "Iteration 26508, Loss: 0.05579185485839844\n",
      "Iteration 26509, Loss: 0.05582984536886215\n",
      "Iteration 26510, Loss: 0.05570264905691147\n",
      "Iteration 26511, Loss: 0.05575406551361084\n",
      "Iteration 26512, Loss: 0.05584736913442612\n",
      "Iteration 26513, Loss: 0.05582432076334953\n",
      "Iteration 26514, Loss: 0.0557001456618309\n",
      "Iteration 26515, Loss: 0.0557636022567749\n",
      "Iteration 26516, Loss: 0.05584188550710678\n",
      "Iteration 26517, Loss: 0.05574846267700195\n",
      "Iteration 26518, Loss: 0.055695537477731705\n",
      "Iteration 26519, Loss: 0.05576956272125244\n",
      "Iteration 26520, Loss: 0.05573241040110588\n",
      "Iteration 26521, Loss: 0.055635612457990646\n",
      "Iteration 26522, Loss: 0.05571667477488518\n",
      "Iteration 26523, Loss: 0.05565377324819565\n",
      "Iteration 26524, Loss: 0.055737853050231934\n",
      "Iteration 26525, Loss: 0.05578533932566643\n",
      "Iteration 26526, Loss: 0.0557326078414917\n",
      "Iteration 26527, Loss: 0.0556383952498436\n",
      "Iteration 26528, Loss: 0.055652063339948654\n",
      "Iteration 26529, Loss: 0.055693626403808594\n",
      "Iteration 26530, Loss: 0.05570245161652565\n",
      "Iteration 26531, Loss: 0.05561884492635727\n",
      "Iteration 26532, Loss: 0.05582010746002197\n",
      "Iteration 26533, Loss: 0.05584665387868881\n",
      "Iteration 26534, Loss: 0.0556994304060936\n",
      "Iteration 26535, Loss: 0.05576964467763901\n",
      "Iteration 26536, Loss: 0.055877648293972015\n",
      "Iteration 26537, Loss: 0.05587633699178696\n",
      "Iteration 26538, Loss: 0.05577731132507324\n",
      "Iteration 26539, Loss: 0.055641334503889084\n",
      "Iteration 26540, Loss: 0.05575009435415268\n",
      "Iteration 26541, Loss: 0.05570157617330551\n",
      "Iteration 26542, Loss: 0.05570153519511223\n",
      "Iteration 26543, Loss: 0.05574830621480942\n",
      "Iteration 26544, Loss: 0.05569903180003166\n",
      "Iteration 26545, Loss: 0.05568067356944084\n",
      "Iteration 26546, Loss: 0.055680714547634125\n",
      "Iteration 26547, Loss: 0.05568242445588112\n",
      "Iteration 26548, Loss: 0.05569986626505852\n",
      "Iteration 26549, Loss: 0.05562226101756096\n",
      "Iteration 26550, Loss: 0.05580902099609375\n",
      "Iteration 26551, Loss: 0.0558374747633934\n",
      "Iteration 26552, Loss: 0.055705033242702484\n",
      "Iteration 26553, Loss: 0.05575426667928696\n",
      "Iteration 26554, Loss: 0.05585038661956787\n",
      "Iteration 26555, Loss: 0.055835090577602386\n",
      "Iteration 26556, Loss: 0.0557204894721508\n",
      "Iteration 26557, Loss: 0.05572788044810295\n",
      "Iteration 26558, Loss: 0.05579948425292969\n",
      "Iteration 26559, Loss: 0.05570387840270996\n",
      "Iteration 26560, Loss: 0.05573197454214096\n",
      "Iteration 26561, Loss: 0.055806756019592285\n",
      "Iteration 26562, Loss: 0.05577349662780762\n",
      "Iteration 26563, Loss: 0.05565277859568596\n",
      "Iteration 26564, Loss: 0.05581565946340561\n",
      "Iteration 26565, Loss: 0.05587947741150856\n",
      "Iteration 26566, Loss: 0.05576900765299797\n",
      "Iteration 26567, Loss: 0.05569251626729965\n",
      "Iteration 26568, Loss: 0.0557786226272583\n",
      "Iteration 26569, Loss: 0.05576014518737793\n",
      "Iteration 26570, Loss: 0.055649399757385254\n",
      "Iteration 26571, Loss: 0.05581601709127426\n",
      "Iteration 26572, Loss: 0.05587736889719963\n",
      "Iteration 26573, Loss: 0.05576233193278313\n",
      "Iteration 26574, Loss: 0.05570109933614731\n",
      "Iteration 26575, Loss: 0.05579010769724846\n",
      "Iteration 26576, Loss: 0.055775802582502365\n",
      "Iteration 26577, Loss: 0.05566803738474846\n",
      "Iteration 26578, Loss: 0.055789511650800705\n",
      "Iteration 26579, Loss: 0.05585015192627907\n",
      "Iteration 26580, Loss: 0.055734675377607346\n",
      "Iteration 26581, Loss: 0.05572164058685303\n",
      "Iteration 26582, Loss: 0.05581093207001686\n",
      "Iteration 26583, Loss: 0.05579646676778793\n",
      "Iteration 26584, Loss: 0.055688582360744476\n",
      "Iteration 26585, Loss: 0.055761855095624924\n",
      "Iteration 26586, Loss: 0.055822812020778656\n",
      "Iteration 26587, Loss: 0.05570761486887932\n",
      "Iteration 26588, Loss: 0.055741433054208755\n",
      "Iteration 26589, Loss: 0.05583035945892334\n",
      "Iteration 26590, Loss: 0.05581510066986084\n",
      "Iteration 26591, Loss: 0.05570698156952858\n",
      "Iteration 26592, Loss: 0.05573829263448715\n",
      "Iteration 26593, Loss: 0.05580008029937744\n",
      "Iteration 26594, Loss: 0.05568532273173332\n",
      "Iteration 26595, Loss: 0.05575820058584213\n",
      "Iteration 26596, Loss: 0.05584685131907463\n",
      "Iteration 26597, Loss: 0.055831871926784515\n",
      "Iteration 26598, Loss: 0.05572390928864479\n",
      "Iteration 26599, Loss: 0.05571524426341057\n",
      "Iteration 26600, Loss: 0.055777035653591156\n",
      "Iteration 26601, Loss: 0.05566307157278061\n",
      "Iteration 26602, Loss: 0.055774372071027756\n",
      "Iteration 26603, Loss: 0.05586306378245354\n",
      "Iteration 26604, Loss: 0.05584832280874252\n",
      "Iteration 26605, Loss: 0.05574115365743637\n",
      "Iteration 26606, Loss: 0.05569136142730713\n",
      "Iteration 26607, Loss: 0.05575283616781235\n",
      "Iteration 26608, Loss: 0.055639903992414474\n",
      "Iteration 26609, Loss: 0.055786967277526855\n",
      "Iteration 26610, Loss: 0.05587152764201164\n",
      "Iteration 26611, Loss: 0.055853210389614105\n",
      "Iteration 26612, Loss: 0.05574226751923561\n",
      "Iteration 26613, Loss: 0.05569378659129143\n",
      "Iteration 26614, Loss: 0.055757246911525726\n",
      "Iteration 26615, Loss: 0.05564185231924057\n",
      "Iteration 26616, Loss: 0.05579022690653801\n",
      "Iteration 26617, Loss: 0.05587971210479736\n",
      "Iteration 26618, Loss: 0.055865686386823654\n",
      "Iteration 26619, Loss: 0.05575871467590332\n",
      "Iteration 26620, Loss: 0.05566640943288803\n",
      "Iteration 26621, Loss: 0.05572589486837387\n",
      "Iteration 26622, Loss: 0.0556187629699707\n",
      "Iteration 26623, Loss: 0.0557304248213768\n",
      "Iteration 26624, Loss: 0.05573086068034172\n",
      "Iteration 26625, Loss: 0.05563720315694809\n",
      "Iteration 26626, Loss: 0.05577818676829338\n",
      "Iteration 26627, Loss: 0.05579086393117905\n",
      "Iteration 26628, Loss: 0.05563775822520256\n",
      "Iteration 26629, Loss: 0.05582305043935776\n",
      "Iteration 26630, Loss: 0.05593224614858627\n",
      "Iteration 26631, Loss: 0.05593010038137436\n",
      "Iteration 26632, Loss: 0.05582992359995842\n",
      "Iteration 26633, Loss: 0.0556621178984642\n",
      "Iteration 26634, Loss: 0.05586306378245354\n",
      "Iteration 26635, Loss: 0.05596904084086418\n",
      "Iteration 26636, Loss: 0.05589529126882553\n",
      "Iteration 26637, Loss: 0.05565889924764633\n",
      "Iteration 26638, Loss: 0.05586076155304909\n",
      "Iteration 26639, Loss: 0.056023161858320236\n",
      "Iteration 26640, Loss: 0.05607418343424797\n",
      "Iteration 26641, Loss: 0.05602451413869858\n",
      "Iteration 26642, Loss: 0.05588475987315178\n",
      "Iteration 26643, Loss: 0.05566927045583725\n",
      "Iteration 26644, Loss: 0.0559215173125267\n",
      "Iteration 26645, Loss: 0.056097548454999924\n",
      "Iteration 26646, Loss: 0.05608205124735832\n",
      "Iteration 26647, Loss: 0.05589469522237778\n",
      "Iteration 26648, Loss: 0.055653057992458344\n",
      "Iteration 26649, Loss: 0.05578633397817612\n",
      "Iteration 26650, Loss: 0.055812638252973557\n",
      "Iteration 26651, Loss: 0.05574238300323486\n",
      "Iteration 26652, Loss: 0.055645786225795746\n",
      "Iteration 26653, Loss: 0.05566728115081787\n",
      "Iteration 26654, Loss: 0.05568206310272217\n",
      "Iteration 26655, Loss: 0.05568965524435043\n",
      "Iteration 26656, Loss: 0.05562270060181618\n",
      "Iteration 26657, Loss: 0.055670224130153656\n",
      "Iteration 26658, Loss: 0.05563163757324219\n",
      "Iteration 26659, Loss: 0.05571949481964111\n",
      "Iteration 26660, Loss: 0.055669866502285004\n",
      "Iteration 26661, Loss: 0.05572740361094475\n",
      "Iteration 26662, Loss: 0.055776678025722504\n",
      "Iteration 26663, Loss: 0.05572644993662834\n",
      "Iteration 26664, Loss: 0.055645864456892014\n",
      "Iteration 26665, Loss: 0.05567304417490959\n",
      "Iteration 26666, Loss: 0.05566640943288803\n",
      "Iteration 26667, Loss: 0.055664580315351486\n",
      "Iteration 26668, Loss: 0.05566748231649399\n",
      "Iteration 26669, Loss: 0.05563187599182129\n",
      "Iteration 26670, Loss: 0.05570181459188461\n",
      "Iteration 26671, Loss: 0.05568588152527809\n",
      "Iteration 26672, Loss: 0.055669307708740234\n",
      "Iteration 26673, Loss: 0.055655281990766525\n",
      "Iteration 26674, Loss: 0.05571230500936508\n",
      "Iteration 26675, Loss: 0.05572446435689926\n",
      "Iteration 26676, Loss: 0.055639706552028656\n",
      "Iteration 26677, Loss: 0.05577019974589348\n",
      "Iteration 26678, Loss: 0.05577616021037102\n",
      "Iteration 26679, Loss: 0.055617135018110275\n",
      "Iteration 26680, Loss: 0.05582781881093979\n",
      "Iteration 26681, Loss: 0.05591488257050514\n",
      "Iteration 26682, Loss: 0.05587884038686752\n",
      "Iteration 26683, Loss: 0.055741988122463226\n",
      "Iteration 26684, Loss: 0.055734436959028244\n",
      "Iteration 26685, Loss: 0.05583139508962631\n",
      "Iteration 26686, Loss: 0.055769842118024826\n",
      "Iteration 26687, Loss: 0.05566120520234108\n",
      "Iteration 26688, Loss: 0.05571473017334938\n",
      "Iteration 26689, Loss: 0.05567439645528793\n",
      "Iteration 26690, Loss: 0.05570089817047119\n",
      "Iteration 26691, Loss: 0.05568961426615715\n",
      "Iteration 26692, Loss: 0.055687904357910156\n",
      "Iteration 26693, Loss: 0.05570952221751213\n",
      "Iteration 26694, Loss: 0.05564550682902336\n",
      "Iteration 26695, Loss: 0.05575581640005112\n",
      "Iteration 26696, Loss: 0.05575072765350342\n",
      "Iteration 26697, Loss: 0.05564141273498535\n",
      "Iteration 26698, Loss: 0.055675387382507324\n",
      "Iteration 26699, Loss: 0.05562937259674072\n",
      "Iteration 26700, Loss: 0.055751048028469086\n",
      "Iteration 26701, Loss: 0.05574178695678711\n",
      "Iteration 26702, Loss: 0.055646419525146484\n",
      "Iteration 26703, Loss: 0.05571075528860092\n",
      "Iteration 26704, Loss: 0.055673640221357346\n",
      "Iteration 26705, Loss: 0.05571480840444565\n",
      "Iteration 26706, Loss: 0.05573471635580063\n",
      "Iteration 26707, Loss: 0.05564860627055168\n",
      "Iteration 26708, Loss: 0.055759988725185394\n",
      "Iteration 26709, Loss: 0.0557764396071434\n",
      "Iteration 26710, Loss: 0.05563982576131821\n",
      "Iteration 26711, Loss: 0.055817604064941406\n",
      "Iteration 26712, Loss: 0.055910151451826096\n",
      "Iteration 26713, Loss: 0.05587426945567131\n",
      "Iteration 26714, Loss: 0.0557328499853611\n",
      "Iteration 26715, Loss: 0.05574667453765869\n",
      "Iteration 26716, Loss: 0.055845778435468674\n",
      "Iteration 26717, Loss: 0.05578804016113281\n",
      "Iteration 26718, Loss: 0.05563565343618393\n",
      "Iteration 26719, Loss: 0.055682938545942307\n",
      "Iteration 26720, Loss: 0.05563247203826904\n",
      "Iteration 26721, Loss: 0.05574337765574455\n",
      "Iteration 26722, Loss: 0.05571440979838371\n",
      "Iteration 26723, Loss: 0.05568107217550278\n",
      "Iteration 26724, Loss: 0.05571870133280754\n",
      "Iteration 26725, Loss: 0.055658817291259766\n",
      "Iteration 26726, Loss: 0.05574524402618408\n",
      "Iteration 26727, Loss: 0.055751603096723557\n",
      "Iteration 26728, Loss: 0.055630724877119064\n",
      "Iteration 26729, Loss: 0.05564872547984123\n",
      "Iteration 26730, Loss: 0.055663347244262695\n",
      "Iteration 26731, Loss: 0.05563124269247055\n",
      "Iteration 26732, Loss: 0.05564085766673088\n",
      "Iteration 26733, Loss: 0.05567765235900879\n",
      "Iteration 26734, Loss: 0.05565985292196274\n",
      "Iteration 26735, Loss: 0.055693984031677246\n",
      "Iteration 26736, Loss: 0.05565989017486572\n",
      "Iteration 26737, Loss: 0.0557248592376709\n",
      "Iteration 26738, Loss: 0.055762212723493576\n",
      "Iteration 26739, Loss: 0.0556977204978466\n",
      "Iteration 26740, Loss: 0.05570070073008537\n",
      "Iteration 26741, Loss: 0.05571810528635979\n",
      "Iteration 26742, Loss: 0.055646542459726334\n",
      "Iteration 26743, Loss: 0.0556565523147583\n",
      "Iteration 26744, Loss: 0.05566072836518288\n",
      "Iteration 26745, Loss: 0.05563342571258545\n",
      "Iteration 26746, Loss: 0.05566652864217758\n",
      "Iteration 26747, Loss: 0.055616579949855804\n",
      "Iteration 26748, Loss: 0.0557708777487278\n",
      "Iteration 26749, Loss: 0.05575951188802719\n",
      "Iteration 26750, Loss: 0.055636726319789886\n",
      "Iteration 26751, Loss: 0.0557129792869091\n",
      "Iteration 26752, Loss: 0.05568365380167961\n",
      "Iteration 26753, Loss: 0.0556919202208519\n",
      "Iteration 26754, Loss: 0.05569835752248764\n",
      "Iteration 26755, Loss: 0.05565083026885986\n",
      "Iteration 26756, Loss: 0.0556567907333374\n",
      "Iteration 26757, Loss: 0.05568651482462883\n",
      "Iteration 26758, Loss: 0.05566903203725815\n",
      "Iteration 26759, Loss: 0.05568818375468254\n",
      "Iteration 26760, Loss: 0.055678725242614746\n",
      "Iteration 26761, Loss: 0.05568452924489975\n",
      "Iteration 26762, Loss: 0.05568520352244377\n",
      "Iteration 26763, Loss: 0.0556594543159008\n",
      "Iteration 26764, Loss: 0.0556492805480957\n",
      "Iteration 26765, Loss: 0.05571150779724121\n",
      "Iteration 26766, Loss: 0.05571349710226059\n",
      "Iteration 26767, Loss: 0.05564240738749504\n",
      "Iteration 26768, Loss: 0.055719178169965744\n",
      "Iteration 26769, Loss: 0.05566950887441635\n",
      "Iteration 26770, Loss: 0.05572756379842758\n",
      "Iteration 26771, Loss: 0.05577453225851059\n",
      "Iteration 26772, Loss: 0.0557224377989769\n",
      "Iteration 26773, Loss: 0.05565711110830307\n",
      "Iteration 26774, Loss: 0.05568786710500717\n",
      "Iteration 26775, Loss: 0.05565830320119858\n",
      "Iteration 26776, Loss: 0.05566617101430893\n",
      "Iteration 26777, Loss: 0.05565166473388672\n",
      "Iteration 26778, Loss: 0.05567602440714836\n",
      "Iteration 26779, Loss: 0.05565333738923073\n",
      "Iteration 26780, Loss: 0.05569501966238022\n",
      "Iteration 26781, Loss: 0.05565985292196274\n",
      "Iteration 26782, Loss: 0.05572621151804924\n",
      "Iteration 26783, Loss: 0.05575764179229736\n",
      "Iteration 26784, Loss: 0.055683813989162445\n",
      "Iteration 26785, Loss: 0.055727601051330566\n",
      "Iteration 26786, Loss: 0.055754583328962326\n",
      "Iteration 26787, Loss: 0.05561932176351547\n",
      "Iteration 26788, Loss: 0.055827897042036057\n",
      "Iteration 26789, Loss: 0.05592691898345947\n",
      "Iteration 26790, Loss: 0.05591174215078354\n",
      "Iteration 26791, Loss: 0.05579761788249016\n",
      "Iteration 26792, Loss: 0.05566176027059555\n",
      "Iteration 26793, Loss: 0.055810730904340744\n",
      "Iteration 26794, Loss: 0.05582452192902565\n",
      "Iteration 26795, Loss: 0.055680159479379654\n",
      "Iteration 26796, Loss: 0.055777791887521744\n",
      "Iteration 26797, Loss: 0.055874984711408615\n",
      "Iteration 26798, Loss: 0.05586950108408928\n",
      "Iteration 26799, Loss: 0.055771034210920334\n",
      "Iteration 26800, Loss: 0.05564574524760246\n",
      "Iteration 26801, Loss: 0.055718980729579926\n",
      "Iteration 26802, Loss: 0.05565190315246582\n",
      "Iteration 26803, Loss: 0.05573205277323723\n",
      "Iteration 26804, Loss: 0.05576026812195778\n",
      "Iteration 26805, Loss: 0.055675867944955826\n",
      "Iteration 26806, Loss: 0.05574945732951164\n",
      "Iteration 26807, Loss: 0.05579245090484619\n",
      "Iteration 26808, Loss: 0.0556819848716259\n",
      "Iteration 26809, Loss: 0.05575951188802719\n",
      "Iteration 26810, Loss: 0.055837713181972504\n",
      "Iteration 26811, Loss: 0.05579598993062973\n",
      "Iteration 26812, Loss: 0.05566032975912094\n",
      "Iteration 26813, Loss: 0.05581486225128174\n",
      "Iteration 26814, Loss: 0.05589219182729721\n",
      "Iteration 26815, Loss: 0.05580425262451172\n",
      "Iteration 26816, Loss: 0.055647335946559906\n",
      "Iteration 26817, Loss: 0.055728595703840256\n",
      "Iteration 26818, Loss: 0.05569767951965332\n",
      "Iteration 26819, Loss: 0.055670104920864105\n",
      "Iteration 26820, Loss: 0.055668871849775314\n",
      "Iteration 26821, Loss: 0.055686913430690765\n",
      "Iteration 26822, Loss: 0.05569028854370117\n",
      "Iteration 26823, Loss: 0.055644553154706955\n",
      "Iteration 26824, Loss: 0.055645786225795746\n",
      "Iteration 26825, Loss: 0.05569104477763176\n",
      "Iteration 26826, Loss: 0.05569159984588623\n",
      "Iteration 26827, Loss: 0.055632948875427246\n",
      "Iteration 26828, Loss: 0.055685997009277344\n",
      "Iteration 26829, Loss: 0.055640220642089844\n",
      "Iteration 26830, Loss: 0.05571214482188225\n",
      "Iteration 26831, Loss: 0.05569303035736084\n",
      "Iteration 26832, Loss: 0.05567610636353493\n",
      "Iteration 26833, Loss: 0.05568496510386467\n",
      "Iteration 26834, Loss: 0.055650435388088226\n",
      "Iteration 26835, Loss: 0.05563322827219963\n",
      "Iteration 26836, Loss: 0.055727165192365646\n",
      "Iteration 26837, Loss: 0.05573869124054909\n",
      "Iteration 26838, Loss: 0.055653613060712814\n",
      "Iteration 26839, Loss: 0.05576598644256592\n",
      "Iteration 26840, Loss: 0.05579165741801262\n",
      "Iteration 26841, Loss: 0.055653929710388184\n",
      "Iteration 26842, Loss: 0.055800795555114746\n",
      "Iteration 26843, Loss: 0.055901527404785156\n",
      "Iteration 26844, Loss: 0.055889926850795746\n",
      "Iteration 26845, Loss: 0.055781327188014984\n",
      "Iteration 26846, Loss: 0.05566481873393059\n",
      "Iteration 26847, Loss: 0.05578426644206047\n",
      "Iteration 26848, Loss: 0.05576527491211891\n",
      "Iteration 26849, Loss: 0.05565309524536133\n",
      "Iteration 26850, Loss: 0.05571512505412102\n",
      "Iteration 26851, Loss: 0.0557122640311718\n",
      "Iteration 26852, Loss: 0.05563513562083244\n",
      "Iteration 26853, Loss: 0.05575720593333244\n",
      "Iteration 26854, Loss: 0.05576102063059807\n",
      "Iteration 26855, Loss: 0.055657386779785156\n",
      "Iteration 26856, Loss: 0.05575680732727051\n",
      "Iteration 26857, Loss: 0.055789314210414886\n",
      "Iteration 26858, Loss: 0.05568341538310051\n",
      "Iteration 26859, Loss: 0.05576018616557121\n",
      "Iteration 26860, Loss: 0.055833183228969574\n",
      "Iteration 26861, Loss: 0.05577472969889641\n",
      "Iteration 26862, Loss: 0.0556538924574852\n",
      "Iteration 26863, Loss: 0.055772703140974045\n",
      "Iteration 26864, Loss: 0.05578383058309555\n",
      "Iteration 26865, Loss: 0.055641137063503265\n",
      "Iteration 26866, Loss: 0.05581589788198471\n",
      "Iteration 26867, Loss: 0.05591913312673569\n",
      "Iteration 26868, Loss: 0.05591074749827385\n",
      "Iteration 26869, Loss: 0.05580584332346916\n",
      "Iteration 26870, Loss: 0.05566096678376198\n",
      "Iteration 26871, Loss: 0.055829524993896484\n",
      "Iteration 26872, Loss: 0.05588114261627197\n",
      "Iteration 26873, Loss: 0.05576034635305405\n",
      "Iteration 26874, Loss: 0.0557105578482151\n",
      "Iteration 26875, Loss: 0.055802784860134125\n",
      "Iteration 26876, Loss: 0.055795472115278244\n",
      "Iteration 26877, Loss: 0.05569863319396973\n",
      "Iteration 26878, Loss: 0.05573849007487297\n",
      "Iteration 26879, Loss: 0.05578688904643059\n",
      "Iteration 26880, Loss: 0.055662672966718674\n",
      "Iteration 26881, Loss: 0.055778346955776215\n",
      "Iteration 26882, Loss: 0.0558701753616333\n",
      "Iteration 26883, Loss: 0.05585753917694092\n",
      "Iteration 26884, Loss: 0.0557510070502758\n",
      "Iteration 26885, Loss: 0.05567570775747299\n",
      "Iteration 26886, Loss: 0.05573451519012451\n",
      "Iteration 26887, Loss: 0.055615704506635666\n",
      "Iteration 26888, Loss: 0.05580306053161621\n",
      "Iteration 26889, Loss: 0.05588619038462639\n",
      "Iteration 26890, Loss: 0.05586656183004379\n",
      "Iteration 26891, Loss: 0.05575430765748024\n",
      "Iteration 26892, Loss: 0.05567849054932594\n",
      "Iteration 26893, Loss: 0.05574604123830795\n",
      "Iteration 26894, Loss: 0.055638473480939865\n",
      "Iteration 26895, Loss: 0.055784229189157486\n",
      "Iteration 26896, Loss: 0.05586584657430649\n",
      "Iteration 26897, Loss: 0.05584454536437988\n",
      "Iteration 26898, Loss: 0.055731020867824554\n",
      "Iteration 26899, Loss: 0.055711351335048676\n",
      "Iteration 26900, Loss: 0.055777788162231445\n",
      "Iteration 26901, Loss: 0.055664461106061935\n",
      "Iteration 26902, Loss: 0.05577230453491211\n",
      "Iteration 26903, Loss: 0.05586063861846924\n",
      "Iteration 26904, Loss: 0.055845700204372406\n",
      "Iteration 26905, Loss: 0.0557381734251976\n",
      "Iteration 26906, Loss: 0.055694859474897385\n",
      "Iteration 26907, Loss: 0.055754899978637695\n",
      "Iteration 26908, Loss: 0.055637918412685394\n",
      "Iteration 26909, Loss: 0.05579141899943352\n",
      "Iteration 26910, Loss: 0.05587884038686752\n",
      "Iteration 26911, Loss: 0.05586310476064682\n",
      "Iteration 26912, Loss: 0.055755022913217545\n",
      "Iteration 26913, Loss: 0.05567280575633049\n",
      "Iteration 26914, Loss: 0.05573463439941406\n",
      "Iteration 26915, Loss: 0.055619239807128906\n",
      "Iteration 26916, Loss: 0.05580350011587143\n",
      "Iteration 26917, Loss: 0.05588964745402336\n",
      "Iteration 26918, Loss: 0.05587232485413551\n",
      "Iteration 26919, Loss: 0.05576189607381821\n",
      "Iteration 26920, Loss: 0.05566652864217758\n",
      "Iteration 26921, Loss: 0.05573185533285141\n",
      "Iteration 26922, Loss: 0.05562806501984596\n",
      "Iteration 26923, Loss: 0.05577008053660393\n",
      "Iteration 26924, Loss: 0.05582718178629875\n",
      "Iteration 26925, Loss: 0.05578116700053215\n",
      "Iteration 26926, Loss: 0.05564801022410393\n",
      "Iteration 26927, Loss: 0.05583914369344711\n",
      "Iteration 26928, Loss: 0.05591849610209465\n",
      "Iteration 26929, Loss: 0.05581967160105705\n",
      "Iteration 26930, Loss: 0.05564745515584946\n",
      "Iteration 26931, Loss: 0.05572911351919174\n",
      "Iteration 26932, Loss: 0.05570809170603752\n",
      "Iteration 26933, Loss: 0.05563346669077873\n",
      "Iteration 26934, Loss: 0.05562623590230942\n",
      "Iteration 26935, Loss: 0.05570507049560547\n",
      "Iteration 26936, Loss: 0.05569152161478996\n",
      "Iteration 26937, Loss: 0.055658500641584396\n",
      "Iteration 26938, Loss: 0.055650752037763596\n",
      "Iteration 26939, Loss: 0.055707573890686035\n",
      "Iteration 26940, Loss: 0.05571739003062248\n",
      "Iteration 26941, Loss: 0.05564061924815178\n",
      "Iteration 26942, Loss: 0.05576455593109131\n",
      "Iteration 26943, Loss: 0.05576249212026596\n",
      "Iteration 26944, Loss: 0.05562806501984596\n",
      "Iteration 26945, Loss: 0.055668119341135025\n",
      "Iteration 26946, Loss: 0.055621188133955\n",
      "Iteration 26947, Loss: 0.055766504257917404\n",
      "Iteration 26948, Loss: 0.05576511472463608\n",
      "Iteration 26949, Loss: 0.055640820413827896\n",
      "Iteration 26950, Loss: 0.05576769635081291\n",
      "Iteration 26951, Loss: 0.05580143257975578\n",
      "Iteration 26952, Loss: 0.05570324510335922\n",
      "Iteration 26953, Loss: 0.05573074147105217\n",
      "Iteration 26954, Loss: 0.05579475685954094\n",
      "Iteration 26955, Loss: 0.055719614028930664\n",
      "Iteration 26956, Loss: 0.055695656687021255\n",
      "Iteration 26957, Loss: 0.05574604123830795\n",
      "Iteration 26958, Loss: 0.05566398426890373\n",
      "Iteration 26959, Loss: 0.055756449699401855\n",
      "Iteration 26960, Loss: 0.05580472946166992\n",
      "Iteration 26961, Loss: 0.05571572110056877\n",
      "Iteration 26962, Loss: 0.055711034685373306\n",
      "Iteration 26963, Loss: 0.05577167123556137\n",
      "Iteration 26964, Loss: 0.05569931119680405\n",
      "Iteration 26965, Loss: 0.05571393296122551\n",
      "Iteration 26966, Loss: 0.05575823783874512\n",
      "Iteration 26967, Loss: 0.05566966533660889\n",
      "Iteration 26968, Loss: 0.05575327202677727\n",
      "Iteration 26969, Loss: 0.05580469220876694\n",
      "Iteration 26970, Loss: 0.05571846291422844\n",
      "Iteration 26971, Loss: 0.05570586770772934\n",
      "Iteration 26972, Loss: 0.055764321237802505\n",
      "Iteration 26973, Loss: 0.05568882077932358\n",
      "Iteration 26974, Loss: 0.0557277612388134\n",
      "Iteration 26975, Loss: 0.05577429383993149\n",
      "Iteration 26976, Loss: 0.0556892566382885\n",
      "Iteration 26977, Loss: 0.05573074147105217\n",
      "Iteration 26978, Loss: 0.05578184500336647\n",
      "Iteration 26979, Loss: 0.05569521710276604\n",
      "Iteration 26980, Loss: 0.055730145424604416\n",
      "Iteration 26981, Loss: 0.0557868517935276\n",
      "Iteration 26982, Loss: 0.055711470544338226\n",
      "Iteration 26983, Loss: 0.05570197477936745\n",
      "Iteration 26984, Loss: 0.055749379098415375\n",
      "Iteration 26985, Loss: 0.05565973371267319\n",
      "Iteration 26986, Loss: 0.05576980113983154\n",
      "Iteration 26987, Loss: 0.055829524993896484\n",
      "Iteration 26988, Loss: 0.05575975030660629\n",
      "Iteration 26989, Loss: 0.05566474050283432\n",
      "Iteration 26990, Loss: 0.05574866384267807\n",
      "Iteration 26991, Loss: 0.05571834370493889\n",
      "Iteration 26992, Loss: 0.05567821115255356\n",
      "Iteration 26993, Loss: 0.055706143379211426\n",
      "Iteration 26994, Loss: 0.05565742775797844\n",
      "Iteration 26995, Loss: 0.0557197742164135\n",
      "Iteration 26996, Loss: 0.05569935217499733\n",
      "Iteration 26997, Loss: 0.05568588152527809\n",
      "Iteration 26998, Loss: 0.055715322494506836\n",
      "Iteration 26999, Loss: 0.055654965341091156\n",
      "Iteration 27000, Loss: 0.05574564263224602\n",
      "Iteration 27001, Loss: 0.05574798583984375\n",
      "Iteration 27002, Loss: 0.055635254830121994\n",
      "Iteration 27003, Loss: 0.05566227436065674\n",
      "Iteration 27004, Loss: 0.05563163757324219\n",
      "Iteration 27005, Loss: 0.055676620453596115\n",
      "Iteration 27006, Loss: 0.055649518966674805\n",
      "Iteration 27007, Loss: 0.055711667984724045\n",
      "Iteration 27008, Loss: 0.055685047060251236\n",
      "Iteration 27009, Loss: 0.0556974820792675\n",
      "Iteration 27010, Loss: 0.05572883412241936\n",
      "Iteration 27011, Loss: 0.055660050362348557\n",
      "Iteration 27012, Loss: 0.05575025454163551\n",
      "Iteration 27013, Loss: 0.05576884746551514\n",
      "Iteration 27014, Loss: 0.05562281608581543\n",
      "Iteration 27015, Loss: 0.05581840127706528\n",
      "Iteration 27016, Loss: 0.05590852349996567\n",
      "Iteration 27017, Loss: 0.05588452145457268\n",
      "Iteration 27018, Loss: 0.05576344579458237\n",
      "Iteration 27019, Loss: 0.055694304406642914\n",
      "Iteration 27020, Loss: 0.055789947509765625\n",
      "Iteration 27021, Loss: 0.055739205330610275\n",
      "Iteration 27022, Loss: 0.05568317696452141\n",
      "Iteration 27023, Loss: 0.05573129653930664\n",
      "Iteration 27024, Loss: 0.05569557473063469\n",
      "Iteration 27025, Loss: 0.055677734315395355\n",
      "Iteration 27026, Loss: 0.05568576231598854\n",
      "Iteration 27027, Loss: 0.055673956871032715\n",
      "Iteration 27028, Loss: 0.05568158999085426\n",
      "Iteration 27029, Loss: 0.05565270036458969\n",
      "Iteration 27030, Loss: 0.05569549649953842\n",
      "Iteration 27031, Loss: 0.055658262223005295\n",
      "Iteration 27032, Loss: 0.055705588310956955\n",
      "Iteration 27033, Loss: 0.05571393296122551\n",
      "Iteration 27034, Loss: 0.05561431497335434\n",
      "Iteration 27035, Loss: 0.055702053010463715\n",
      "Iteration 27036, Loss: 0.05562138557434082\n",
      "Iteration 27037, Loss: 0.05576491355895996\n",
      "Iteration 27038, Loss: 0.05580691620707512\n",
      "Iteration 27039, Loss: 0.05574166774749756\n",
      "Iteration 27040, Loss: 0.055657707154750824\n",
      "Iteration 27041, Loss: 0.05572573468089104\n",
      "Iteration 27042, Loss: 0.055661559104919434\n",
      "Iteration 27043, Loss: 0.05573384091258049\n",
      "Iteration 27044, Loss: 0.055782120674848557\n",
      "Iteration 27045, Loss: 0.05573328584432602\n",
      "Iteration 27046, Loss: 0.05563529580831528\n",
      "Iteration 27047, Loss: 0.0556952990591526\n",
      "Iteration 27048, Loss: 0.055644236505031586\n",
      "Iteration 27049, Loss: 0.055711351335048676\n",
      "Iteration 27050, Loss: 0.05570026487112045\n",
      "Iteration 27051, Loss: 0.05565846338868141\n",
      "Iteration 27052, Loss: 0.05565512180328369\n",
      "Iteration 27053, Loss: 0.05569561570882797\n",
      "Iteration 27054, Loss: 0.055684249848127365\n",
      "Iteration 27055, Loss: 0.05567284673452377\n",
      "Iteration 27056, Loss: 0.055670302361249924\n",
      "Iteration 27057, Loss: 0.0556815080344677\n",
      "Iteration 27058, Loss: 0.055670738220214844\n",
      "Iteration 27059, Loss: 0.05568420886993408\n",
      "Iteration 27060, Loss: 0.055677931755781174\n",
      "Iteration 27061, Loss: 0.055676620453596115\n",
      "Iteration 27062, Loss: 0.0556693896651268\n",
      "Iteration 27063, Loss: 0.055683694779872894\n",
      "Iteration 27064, Loss: 0.05567502975463867\n",
      "Iteration 27065, Loss: 0.05568035691976547\n",
      "Iteration 27066, Loss: 0.0556747131049633\n",
      "Iteration 27067, Loss: 0.05567777156829834\n",
      "Iteration 27068, Loss: 0.0556689128279686\n",
      "Iteration 27069, Loss: 0.05568655580282211\n",
      "Iteration 27070, Loss: 0.055680036544799805\n",
      "Iteration 27071, Loss: 0.055672965943813324\n",
      "Iteration 27072, Loss: 0.055665694177150726\n",
      "Iteration 27073, Loss: 0.05568897724151611\n",
      "Iteration 27074, Loss: 0.05568122863769531\n",
      "Iteration 27075, Loss: 0.05567280575633049\n",
      "Iteration 27076, Loss: 0.055667441338300705\n",
      "Iteration 27077, Loss: 0.055686235427856445\n",
      "Iteration 27078, Loss: 0.0556768998503685\n",
      "Iteration 27079, Loss: 0.05567789450287819\n",
      "Iteration 27080, Loss: 0.05567284673452377\n",
      "Iteration 27081, Loss: 0.05568031594157219\n",
      "Iteration 27082, Loss: 0.05567082017660141\n",
      "Iteration 27083, Loss: 0.055684130638837814\n",
      "Iteration 27084, Loss: 0.055677931755781174\n",
      "Iteration 27085, Loss: 0.05567526817321777\n",
      "Iteration 27086, Loss: 0.055667243897914886\n",
      "Iteration 27087, Loss: 0.05568774789571762\n",
      "Iteration 27088, Loss: 0.055680595338344574\n",
      "Iteration 27089, Loss: 0.05567300319671631\n",
      "Iteration 27090, Loss: 0.05566680431365967\n",
      "Iteration 27091, Loss: 0.05568794533610344\n",
      "Iteration 27092, Loss: 0.055679481476545334\n",
      "Iteration 27093, Loss: 0.05567435547709465\n",
      "Iteration 27094, Loss: 0.055669307708740234\n",
      "Iteration 27095, Loss: 0.05568476766347885\n",
      "Iteration 27096, Loss: 0.05567554756999016\n",
      "Iteration 27097, Loss: 0.055678289383649826\n",
      "Iteration 27098, Loss: 0.05567260831594467\n",
      "Iteration 27099, Loss: 0.055681824684143066\n",
      "Iteration 27100, Loss: 0.05567284673452377\n",
      "Iteration 27101, Loss: 0.05568079277873039\n",
      "Iteration 27102, Loss: 0.05567439645528793\n",
      "Iteration 27103, Loss: 0.05568035691976547\n",
      "Iteration 27104, Loss: 0.05567189306020737\n",
      "Iteration 27105, Loss: 0.0556817464530468\n",
      "Iteration 27106, Loss: 0.055674951523542404\n",
      "Iteration 27107, Loss: 0.05567944049835205\n",
      "Iteration 27108, Loss: 0.055671337991952896\n",
      "Iteration 27109, Loss: 0.055682700127363205\n",
      "Iteration 27110, Loss: 0.05567586421966553\n",
      "Iteration 27111, Loss: 0.05567837134003639\n",
      "Iteration 27112, Loss: 0.05567046254873276\n",
      "Iteration 27113, Loss: 0.05568385124206543\n",
      "Iteration 27114, Loss: 0.055676817893981934\n",
      "Iteration 27115, Loss: 0.05567709729075432\n",
      "Iteration 27116, Loss: 0.055669549852609634\n",
      "Iteration 27117, Loss: 0.0556846484541893\n",
      "Iteration 27118, Loss: 0.055677175521850586\n",
      "Iteration 27119, Loss: 0.055676739662885666\n",
      "Iteration 27120, Loss: 0.055669866502285004\n",
      "Iteration 27121, Loss: 0.055684130638837814\n",
      "Iteration 27122, Loss: 0.05567602440714836\n",
      "Iteration 27123, Loss: 0.05567824840545654\n",
      "Iteration 27124, Loss: 0.055671773850917816\n",
      "Iteration 27125, Loss: 0.0556819848716259\n",
      "Iteration 27126, Loss: 0.055673521012067795\n",
      "Iteration 27127, Loss: 0.05568091198801994\n",
      "Iteration 27128, Loss: 0.05567443370819092\n",
      "Iteration 27129, Loss: 0.05567912384867668\n",
      "Iteration 27130, Loss: 0.055670857429504395\n",
      "Iteration 27131, Loss: 0.055683694779872894\n",
      "Iteration 27132, Loss: 0.055676739662885666\n",
      "Iteration 27133, Loss: 0.05567697808146477\n",
      "Iteration 27134, Loss: 0.05566966533660889\n",
      "Iteration 27135, Loss: 0.05568492412567139\n",
      "Iteration 27136, Loss: 0.055677056312561035\n",
      "Iteration 27137, Loss: 0.05567685887217522\n",
      "Iteration 27138, Loss: 0.05567018315196037\n",
      "Iteration 27139, Loss: 0.055684011429548264\n",
      "Iteration 27140, Loss: 0.05567554756999016\n",
      "Iteration 27141, Loss: 0.05567856878042221\n",
      "Iteration 27142, Loss: 0.055672526359558105\n",
      "Iteration 27143, Loss: 0.055681467056274414\n",
      "Iteration 27144, Loss: 0.05567268654704094\n",
      "Iteration 27145, Loss: 0.05568142980337143\n",
      "Iteration 27146, Loss: 0.055675070732831955\n",
      "Iteration 27147, Loss: 0.05567868798971176\n",
      "Iteration 27148, Loss: 0.05567034333944321\n",
      "Iteration 27149, Loss: 0.05568373203277588\n",
      "Iteration 27150, Loss: 0.05567669868469238\n",
      "Iteration 27151, Loss: 0.05567721650004387\n",
      "Iteration 27152, Loss: 0.05566970631480217\n",
      "Iteration 27153, Loss: 0.055684369057416916\n",
      "Iteration 27154, Loss: 0.05567682161927223\n",
      "Iteration 27155, Loss: 0.05567725747823715\n",
      "Iteration 27156, Loss: 0.05567038059234619\n",
      "Iteration 27157, Loss: 0.05568365380167961\n",
      "Iteration 27158, Loss: 0.05567558854818344\n",
      "Iteration 27159, Loss: 0.05567868798971176\n",
      "Iteration 27160, Loss: 0.0556720495223999\n",
      "Iteration 27161, Loss: 0.05568186566233635\n",
      "Iteration 27162, Loss: 0.05567372217774391\n",
      "Iteration 27163, Loss: 0.05568067356944084\n",
      "Iteration 27164, Loss: 0.055673997849226\n",
      "Iteration 27165, Loss: 0.05567964166402817\n",
      "Iteration 27166, Loss: 0.05567173287272453\n",
      "Iteration 27167, Loss: 0.055682700127363205\n",
      "Iteration 27168, Loss: 0.055675625801086426\n",
      "Iteration 27169, Loss: 0.055677853524684906\n",
      "Iteration 27170, Loss: 0.05567038059234619\n",
      "Iteration 27171, Loss: 0.05568397045135498\n",
      "Iteration 27172, Loss: 0.05567634478211403\n",
      "Iteration 27173, Loss: 0.0556771382689476\n",
      "Iteration 27174, Loss: 0.05567034333944321\n",
      "Iteration 27175, Loss: 0.05568405240774155\n",
      "Iteration 27176, Loss: 0.055676065385341644\n",
      "Iteration 27177, Loss: 0.05567769333720207\n",
      "Iteration 27178, Loss: 0.05567129701375961\n",
      "Iteration 27179, Loss: 0.05568317696452141\n",
      "Iteration 27180, Loss: 0.055674754083156586\n",
      "Iteration 27181, Loss: 0.05567924305796623\n",
      "Iteration 27182, Loss: 0.05567300319671631\n",
      "Iteration 27183, Loss: 0.05568111315369606\n",
      "Iteration 27184, Loss: 0.05567248910665512\n",
      "Iteration 27185, Loss: 0.05568162724375725\n",
      "Iteration 27186, Loss: 0.05567502975463867\n",
      "Iteration 27187, Loss: 0.05567900463938713\n",
      "Iteration 27188, Loss: 0.05567086115479469\n",
      "Iteration 27189, Loss: 0.05568321794271469\n",
      "Iteration 27190, Loss: 0.05567602440714836\n",
      "Iteration 27191, Loss: 0.055677931755781174\n",
      "Iteration 27192, Loss: 0.05567062273621559\n",
      "Iteration 27193, Loss: 0.05568337440490723\n",
      "Iteration 27194, Loss: 0.05567566677927971\n",
      "Iteration 27195, Loss: 0.05567844957113266\n",
      "Iteration 27196, Loss: 0.055671416223049164\n",
      "Iteration 27197, Loss: 0.0556824617087841\n",
      "Iteration 27198, Loss: 0.05567459389567375\n",
      "Iteration 27199, Loss: 0.055679600685834885\n",
      "Iteration 27200, Loss: 0.05567260831594467\n",
      "Iteration 27201, Loss: 0.05568091198801994\n",
      "Iteration 27202, Loss: 0.05567300692200661\n",
      "Iteration 27203, Loss: 0.0556815080344677\n",
      "Iteration 27204, Loss: 0.055674634873867035\n",
      "Iteration 27205, Loss: 0.05567900463938713\n",
      "Iteration 27206, Loss: 0.05567121505737305\n",
      "Iteration 27207, Loss: 0.05568329617381096\n",
      "Iteration 27208, Loss: 0.05567598342895508\n",
      "Iteration 27209, Loss: 0.05567753314971924\n",
      "Iteration 27210, Loss: 0.05567046254873276\n",
      "Iteration 27211, Loss: 0.0556841716170311\n",
      "Iteration 27212, Loss: 0.055676184594631195\n",
      "Iteration 27213, Loss: 0.0556773766875267\n",
      "Iteration 27214, Loss: 0.05567077919840813\n",
      "Iteration 27215, Loss: 0.05568353459239006\n",
      "Iteration 27216, Loss: 0.05567542836070061\n",
      "Iteration 27217, Loss: 0.05567852780222893\n",
      "Iteration 27218, Loss: 0.05567217245697975\n",
      "Iteration 27219, Loss: 0.0556819848716259\n",
      "Iteration 27220, Loss: 0.05567356199026108\n",
      "Iteration 27221, Loss: 0.05568035691976547\n",
      "Iteration 27222, Loss: 0.05567396059632301\n",
      "Iteration 27223, Loss: 0.05568031594157219\n",
      "Iteration 27224, Loss: 0.05567213147878647\n",
      "Iteration 27225, Loss: 0.055681705474853516\n",
      "Iteration 27226, Loss: 0.05567499250173569\n",
      "Iteration 27227, Loss: 0.05567920207977295\n",
      "Iteration 27228, Loss: 0.05567137524485588\n",
      "Iteration 27229, Loss: 0.0556824617087841\n",
      "Iteration 27230, Loss: 0.055675189942121506\n",
      "Iteration 27231, Loss: 0.05567888543009758\n",
      "Iteration 27232, Loss: 0.055671416223049164\n",
      "Iteration 27233, Loss: 0.05568262189626694\n",
      "Iteration 27234, Loss: 0.05567499250173569\n",
      "Iteration 27235, Loss: 0.0556793212890625\n",
      "Iteration 27236, Loss: 0.05567213147878647\n",
      "Iteration 27237, Loss: 0.05568162724375725\n",
      "Iteration 27238, Loss: 0.05567387863993645\n",
      "Iteration 27239, Loss: 0.05568051338195801\n",
      "Iteration 27240, Loss: 0.05567356199026108\n",
      "Iteration 27241, Loss: 0.05568023771047592\n",
      "Iteration 27242, Loss: 0.05567248910665512\n",
      "Iteration 27243, Loss: 0.055682066828012466\n",
      "Iteration 27244, Loss: 0.055674951523542404\n",
      "Iteration 27245, Loss: 0.05567880719900131\n",
      "Iteration 27246, Loss: 0.055671218782663345\n",
      "Iteration 27247, Loss: 0.05568317696452141\n",
      "Iteration 27248, Loss: 0.055675748735666275\n",
      "Iteration 27249, Loss: 0.05567781254649162\n",
      "Iteration 27250, Loss: 0.055670857429504395\n",
      "Iteration 27251, Loss: 0.055683694779872894\n",
      "Iteration 27252, Loss: 0.05567582696676254\n",
      "Iteration 27253, Loss: 0.055677734315395355\n",
      "Iteration 27254, Loss: 0.055671099573373795\n",
      "Iteration 27255, Loss: 0.05568321794271469\n",
      "Iteration 27256, Loss: 0.055675189942121506\n",
      "Iteration 27257, Loss: 0.055678606033325195\n",
      "Iteration 27258, Loss: 0.05567201226949692\n",
      "Iteration 27259, Loss: 0.05568210408091545\n",
      "Iteration 27260, Loss: 0.05567380040884018\n",
      "Iteration 27261, Loss: 0.05568023771047592\n",
      "Iteration 27262, Loss: 0.055673759430646896\n",
      "Iteration 27263, Loss: 0.05568019673228264\n",
      "Iteration 27264, Loss: 0.055671971291303635\n",
      "Iteration 27265, Loss: 0.05568234249949455\n",
      "Iteration 27266, Loss: 0.05567554756999016\n",
      "Iteration 27267, Loss: 0.055678367614746094\n",
      "Iteration 27268, Loss: 0.055670421570539474\n",
      "Iteration 27269, Loss: 0.05568373203277588\n",
      "Iteration 27270, Loss: 0.0556764230132103\n",
      "Iteration 27271, Loss: 0.05567733570933342\n",
      "Iteration 27272, Loss: 0.05567026510834694\n",
      "Iteration 27273, Loss: 0.05568397417664528\n",
      "Iteration 27274, Loss: 0.055676382035017014\n",
      "Iteration 27275, Loss: 0.055677615106105804\n",
      "Iteration 27276, Loss: 0.05567082017660141\n",
      "Iteration 27277, Loss: 0.055683135986328125\n",
      "Iteration 27278, Loss: 0.05567515268921852\n",
      "Iteration 27279, Loss: 0.055679045617580414\n",
      "Iteration 27280, Loss: 0.05567232891917229\n",
      "Iteration 27281, Loss: 0.05568142980337143\n",
      "Iteration 27282, Loss: 0.05567324161529541\n",
      "Iteration 27283, Loss: 0.05568111315369606\n",
      "Iteration 27284, Loss: 0.05567431449890137\n",
      "Iteration 27285, Loss: 0.05567924305796623\n",
      "Iteration 27286, Loss: 0.05567145347595215\n",
      "Iteration 27287, Loss: 0.05568297952413559\n",
      "Iteration 27288, Loss: 0.05567582696676254\n",
      "Iteration 27289, Loss: 0.05567813292145729\n",
      "Iteration 27290, Loss: 0.05567082017660141\n",
      "Iteration 27291, Loss: 0.05568353459239006\n",
      "Iteration 27292, Loss: 0.055675625801086426\n",
      "Iteration 27293, Loss: 0.055678289383649826\n",
      "Iteration 27294, Loss: 0.055671773850917816\n",
      "Iteration 27295, Loss: 0.05568242073059082\n",
      "Iteration 27296, Loss: 0.05567415803670883\n",
      "Iteration 27297, Loss: 0.05567976087331772\n",
      "Iteration 27298, Loss: 0.055673085153102875\n",
      "Iteration 27299, Loss: 0.05568079277873039\n",
      "Iteration 27300, Loss: 0.05567272752523422\n",
      "Iteration 27301, Loss: 0.05568123236298561\n",
      "Iteration 27302, Loss: 0.05567439645528793\n",
      "Iteration 27303, Loss: 0.05567967891693115\n",
      "Iteration 27304, Loss: 0.0556718111038208\n",
      "Iteration 27305, Loss: 0.05568230524659157\n",
      "Iteration 27306, Loss: 0.05567514896392822\n",
      "Iteration 27307, Loss: 0.05567892640829086\n",
      "Iteration 27308, Loss: 0.0556715726852417\n",
      "Iteration 27309, Loss: 0.05568242445588112\n",
      "Iteration 27310, Loss: 0.05567499250173569\n",
      "Iteration 27311, Loss: 0.05567944049835205\n",
      "Iteration 27312, Loss: 0.05567213147878647\n",
      "Iteration 27313, Loss: 0.05568162724375725\n",
      "Iteration 27314, Loss: 0.055674076080322266\n",
      "Iteration 27315, Loss: 0.05568039417266846\n",
      "Iteration 27316, Loss: 0.055673401802778244\n",
      "Iteration 27317, Loss: 0.05568043515086174\n",
      "Iteration 27318, Loss: 0.05567268654704094\n",
      "Iteration 27319, Loss: 0.05568178743124008\n",
      "Iteration 27320, Loss: 0.05567455664277077\n",
      "Iteration 27321, Loss: 0.05567896366119385\n",
      "Iteration 27322, Loss: 0.055671654641628265\n",
      "Iteration 27323, Loss: 0.05568274110555649\n",
      "Iteration 27324, Loss: 0.05567511171102524\n",
      "Iteration 27325, Loss: 0.05567864701151848\n",
      "Iteration 27326, Loss: 0.0556718148291111\n",
      "Iteration 27327, Loss: 0.055682502686977386\n",
      "Iteration 27328, Loss: 0.05567439645528793\n",
      "Iteration 27329, Loss: 0.05567924305796623\n",
      "Iteration 27330, Loss: 0.05567244812846184\n",
      "Iteration 27331, Loss: 0.05568190664052963\n",
      "Iteration 27332, Loss: 0.055673837661743164\n",
      "Iteration 27333, Loss: 0.05567967891693115\n",
      "Iteration 27334, Loss: 0.05567280575633049\n",
      "Iteration 27335, Loss: 0.0556815080344677\n",
      "Iteration 27336, Loss: 0.055673837661743164\n",
      "Iteration 27337, Loss: 0.055679719895124435\n",
      "Iteration 27338, Loss: 0.05567272752523422\n",
      "Iteration 27339, Loss: 0.0556819848716259\n",
      "Iteration 27340, Loss: 0.05567427724599838\n",
      "Iteration 27341, Loss: 0.055679284036159515\n",
      "Iteration 27342, Loss: 0.05567236989736557\n",
      "Iteration 27343, Loss: 0.05568210408091545\n",
      "Iteration 27344, Loss: 0.055674199014902115\n",
      "Iteration 27345, Loss: 0.055679403245449066\n",
      "Iteration 27346, Loss: 0.05567260831594467\n",
      "Iteration 27347, Loss: 0.05568186566233635\n",
      "Iteration 27348, Loss: 0.05567368119955063\n",
      "Iteration 27349, Loss: 0.05567995831370354\n",
      "Iteration 27350, Loss: 0.05567316338419914\n",
      "Iteration 27351, Loss: 0.05568099021911621\n",
      "Iteration 27352, Loss: 0.05567284673452377\n",
      "Iteration 27353, Loss: 0.055680833756923676\n",
      "Iteration 27354, Loss: 0.05567403882741928\n",
      "Iteration 27355, Loss: 0.05568031594157219\n",
      "Iteration 27356, Loss: 0.05567236989736557\n",
      "Iteration 27357, Loss: 0.055681269615888596\n",
      "Iteration 27358, Loss: 0.055674079805612564\n",
      "Iteration 27359, Loss: 0.055680155754089355\n",
      "Iteration 27360, Loss: 0.055672649294137955\n",
      "Iteration 27361, Loss: 0.05568099021911621\n",
      "Iteration 27362, Loss: 0.05567372217774391\n",
      "Iteration 27363, Loss: 0.05568079277873039\n",
      "Iteration 27364, Loss: 0.05567336454987526\n",
      "Iteration 27365, Loss: 0.055680274963378906\n",
      "Iteration 27366, Loss: 0.055672887712717056\n",
      "Iteration 27367, Loss: 0.0556817464530468\n",
      "Iteration 27368, Loss: 0.05567411705851555\n",
      "Iteration 27369, Loss: 0.055679600685834885\n",
      "Iteration 27370, Loss: 0.05567225068807602\n",
      "Iteration 27371, Loss: 0.055682145059108734\n",
      "Iteration 27372, Loss: 0.0556744746863842\n",
      "Iteration 27373, Loss: 0.055679164826869965\n",
      "Iteration 27374, Loss: 0.05567225068807602\n",
      "Iteration 27375, Loss: 0.05568210408091545\n",
      "Iteration 27376, Loss: 0.05567411705851555\n",
      "Iteration 27377, Loss: 0.055679481476545334\n",
      "Iteration 27378, Loss: 0.05567292496562004\n",
      "Iteration 27379, Loss: 0.055681388825178146\n",
      "Iteration 27380, Loss: 0.05567324161529541\n",
      "Iteration 27381, Loss: 0.055680595338344574\n",
      "Iteration 27382, Loss: 0.05567371845245361\n",
      "Iteration 27383, Loss: 0.05568019673228264\n",
      "Iteration 27384, Loss: 0.05567217245697975\n",
      "Iteration 27385, Loss: 0.05568206310272217\n",
      "Iteration 27386, Loss: 0.05567527189850807\n",
      "Iteration 27387, Loss: 0.055678606033325195\n",
      "Iteration 27388, Loss: 0.05567077919840813\n",
      "Iteration 27389, Loss: 0.05568341538310051\n",
      "Iteration 27390, Loss: 0.055676303803920746\n",
      "Iteration 27391, Loss: 0.05567769333720207\n",
      "Iteration 27392, Loss: 0.05567034333944321\n",
      "Iteration 27393, Loss: 0.05568373203277588\n",
      "Iteration 27394, Loss: 0.055675946176052094\n",
      "Iteration 27395, Loss: 0.055677853524684906\n",
      "Iteration 27396, Loss: 0.055670976638793945\n",
      "Iteration 27397, Loss: 0.05568297952413559\n",
      "Iteration 27398, Loss: 0.055674951523542404\n",
      "Iteration 27399, Loss: 0.0556790865957737\n",
      "Iteration 27400, Loss: 0.05567236989736557\n",
      "Iteration 27401, Loss: 0.05568122863769531\n",
      "Iteration 27402, Loss: 0.05567324534058571\n",
      "Iteration 27403, Loss: 0.055680952966213226\n",
      "Iteration 27404, Loss: 0.05567403882741928\n",
      "Iteration 27405, Loss: 0.0556795634329319\n",
      "Iteration 27406, Loss: 0.055671773850917816\n",
      "Iteration 27407, Loss: 0.055682700127363205\n",
      "Iteration 27408, Loss: 0.05567535012960434\n",
      "Iteration 27409, Loss: 0.055678170174360275\n",
      "Iteration 27410, Loss: 0.055670976638793945\n",
      "Iteration 27411, Loss: 0.05568345636129379\n",
      "Iteration 27412, Loss: 0.05567574501037598\n",
      "Iteration 27413, Loss: 0.055677931755781174\n",
      "Iteration 27414, Loss: 0.05567113682627678\n",
      "Iteration 27415, Loss: 0.05568297952413559\n",
      "Iteration 27416, Loss: 0.05567502975463867\n",
      "Iteration 27417, Loss: 0.055678848177194595\n",
      "Iteration 27418, Loss: 0.05567256733775139\n",
      "Iteration 27419, Loss: 0.05568154901266098\n",
      "Iteration 27420, Loss: 0.05567300692200661\n",
      "Iteration 27421, Loss: 0.055681031197309494\n",
      "Iteration 27422, Loss: 0.055674437433481216\n",
      "Iteration 27423, Loss: 0.055679600685834885\n",
      "Iteration 27424, Loss: 0.05567125603556633\n",
      "Iteration 27425, Loss: 0.05568253993988037\n",
      "Iteration 27426, Loss: 0.055675506591796875\n",
      "Iteration 27427, Loss: 0.055678486824035645\n",
      "Iteration 27428, Loss: 0.05567089840769768\n",
      "Iteration 27429, Loss: 0.05568286031484604\n",
      "Iteration 27430, Loss: 0.05567542836070061\n",
      "Iteration 27431, Loss: 0.05567900463938713\n",
      "Iteration 27432, Loss: 0.055671773850917816\n",
      "Iteration 27433, Loss: 0.05568178743124008\n",
      "Iteration 27434, Loss: 0.0556742362678051\n",
      "Iteration 27435, Loss: 0.05568007752299309\n",
      "Iteration 27436, Loss: 0.055673085153102875\n",
      "Iteration 27437, Loss: 0.05568055436015129\n",
      "Iteration 27438, Loss: 0.05567304417490959\n",
      "Iteration 27439, Loss: 0.055681467056274414\n",
      "Iteration 27440, Loss: 0.05567411705851555\n",
      "Iteration 27441, Loss: 0.05567944422364235\n",
      "Iteration 27442, Loss: 0.055672090500593185\n",
      "Iteration 27443, Loss: 0.055682145059108734\n",
      "Iteration 27444, Loss: 0.055674515664577484\n",
      "Iteration 27445, Loss: 0.05567912384867668\n",
      "Iteration 27446, Loss: 0.055672090500593185\n",
      "Iteration 27447, Loss: 0.05568242073059082\n",
      "Iteration 27448, Loss: 0.055674515664577484\n",
      "Iteration 27449, Loss: 0.05567900463938713\n",
      "Iteration 27450, Loss: 0.05567189306020737\n",
      "Iteration 27451, Loss: 0.055682383477687836\n",
      "Iteration 27452, Loss: 0.0556744746863842\n",
      "Iteration 27453, Loss: 0.055678967386484146\n",
      "Iteration 27454, Loss: 0.05567213147878647\n",
      "Iteration 27455, Loss: 0.055682145059108734\n",
      "Iteration 27456, Loss: 0.05567411705851555\n",
      "Iteration 27457, Loss: 0.0556795597076416\n",
      "Iteration 27458, Loss: 0.055672887712717056\n",
      "Iteration 27459, Loss: 0.0556817464530468\n",
      "Iteration 27460, Loss: 0.05567356199026108\n",
      "Iteration 27461, Loss: 0.05568142980337143\n",
      "Iteration 27462, Loss: 0.05567467212677002\n",
      "Iteration 27463, Loss: 0.0556800402700901\n",
      "Iteration 27464, Loss: 0.055671852082014084\n",
      "Iteration 27465, Loss: 0.05568333715200424\n",
      "Iteration 27466, Loss: 0.05567646399140358\n",
      "Iteration 27467, Loss: 0.05567833036184311\n",
      "Iteration 27468, Loss: 0.05567070096731186\n",
      "Iteration 27469, Loss: 0.05568429082632065\n",
      "Iteration 27470, Loss: 0.05567701905965805\n",
      "Iteration 27471, Loss: 0.05567797273397446\n",
      "Iteration 27472, Loss: 0.05567086115479469\n",
      "Iteration 27473, Loss: 0.05568397417664528\n",
      "Iteration 27474, Loss: 0.05567602440714836\n",
      "Iteration 27475, Loss: 0.055679164826869965\n",
      "Iteration 27476, Loss: 0.05567248910665512\n",
      "Iteration 27477, Loss: 0.05568206310272217\n",
      "Iteration 27478, Loss: 0.055673837661743164\n",
      "Iteration 27479, Loss: 0.05568166822195053\n",
      "Iteration 27480, Loss: 0.055675189942121506\n",
      "Iteration 27481, Loss: 0.05567912384867668\n",
      "Iteration 27482, Loss: 0.05567101761698723\n",
      "Iteration 27483, Loss: 0.055684804916381836\n",
      "Iteration 27484, Loss: 0.055677931755781174\n",
      "Iteration 27485, Loss: 0.05567610636353493\n",
      "Iteration 27486, Loss: 0.0556689128279686\n",
      "Iteration 27487, Loss: 0.05568715184926987\n",
      "Iteration 27488, Loss: 0.055679600685834885\n",
      "Iteration 27489, Loss: 0.05567467212677002\n",
      "Iteration 27490, Loss: 0.05566871166229248\n",
      "Iteration 27491, Loss: 0.05568675324320793\n",
      "Iteration 27492, Loss: 0.055678170174360275\n",
      "Iteration 27493, Loss: 0.055676620453596115\n",
      "Iteration 27494, Loss: 0.05567105859518051\n",
      "Iteration 27495, Loss: 0.055683933198451996\n",
      "Iteration 27496, Loss: 0.055674754083156586\n",
      "Iteration 27497, Loss: 0.05568023771047592\n",
      "Iteration 27498, Loss: 0.05567427724599838\n",
      "Iteration 27499, Loss: 0.05568087100982666\n",
      "Iteration 27500, Loss: 0.05567213147878647\n",
      "Iteration 27501, Loss: 0.055682502686977386\n",
      "Iteration 27502, Loss: 0.05567582696676254\n",
      "Iteration 27503, Loss: 0.05567944049835205\n",
      "Iteration 27504, Loss: 0.055671535432338715\n",
      "Iteration 27505, Loss: 0.05568305775523186\n",
      "Iteration 27506, Loss: 0.05567574501037598\n",
      "Iteration 27507, Loss: 0.055679403245449066\n",
      "Iteration 27508, Loss: 0.05567201226949692\n",
      "Iteration 27509, Loss: 0.05568210408091545\n",
      "Iteration 27510, Loss: 0.05567455291748047\n",
      "Iteration 27511, Loss: 0.055680952966213226\n",
      "Iteration 27512, Loss: 0.055673759430646896\n",
      "Iteration 27513, Loss: 0.05568075180053711\n",
      "Iteration 27514, Loss: 0.05567316338419914\n",
      "Iteration 27515, Loss: 0.055682264268398285\n",
      "Iteration 27516, Loss: 0.05567499250173569\n",
      "Iteration 27517, Loss: 0.055679600685834885\n",
      "Iteration 27518, Loss: 0.05567232891917229\n",
      "Iteration 27519, Loss: 0.055683016777038574\n",
      "Iteration 27520, Loss: 0.05567542836070061\n",
      "Iteration 27521, Loss: 0.055679403245449066\n",
      "Iteration 27522, Loss: 0.05567236989736557\n",
      "Iteration 27523, Loss: 0.05568286031484604\n",
      "Iteration 27524, Loss: 0.0556747131049633\n",
      "Iteration 27525, Loss: 0.05568019673228264\n",
      "Iteration 27526, Loss: 0.05567356199026108\n",
      "Iteration 27527, Loss: 0.05568142980337143\n",
      "Iteration 27528, Loss: 0.05567316338419914\n",
      "Iteration 27529, Loss: 0.055681828409433365\n",
      "Iteration 27530, Loss: 0.05567483231425285\n",
      "Iteration 27531, Loss: 0.0556800402700901\n",
      "Iteration 27532, Loss: 0.055671971291303635\n",
      "Iteration 27533, Loss: 0.055682819336652756\n",
      "Iteration 27534, Loss: 0.05567574501037598\n",
      "Iteration 27535, Loss: 0.05567920580506325\n",
      "Iteration 27536, Loss: 0.05567169189453125\n",
      "Iteration 27537, Loss: 0.055683258920907974\n",
      "Iteration 27538, Loss: 0.05567582696676254\n",
      "Iteration 27539, Loss: 0.05567936226725578\n",
      "Iteration 27540, Loss: 0.055672209709882736\n",
      "Iteration 27541, Loss: 0.05568253993988037\n",
      "Iteration 27542, Loss: 0.05567467212677002\n",
      "Iteration 27543, Loss: 0.055680714547634125\n",
      "Iteration 27544, Loss: 0.05567371845245361\n",
      "Iteration 27545, Loss: 0.05568079277873039\n",
      "Iteration 27546, Loss: 0.055672887712717056\n",
      "Iteration 27547, Loss: 0.05568274110555649\n",
      "Iteration 27548, Loss: 0.05567582696676254\n",
      "Iteration 27549, Loss: 0.055678486824035645\n",
      "Iteration 27550, Loss: 0.05567070096731186\n",
      "Iteration 27551, Loss: 0.05568496510386467\n",
      "Iteration 27552, Loss: 0.055677734315395355\n",
      "Iteration 27553, Loss: 0.05567657947540283\n",
      "Iteration 27554, Loss: 0.05566950887441635\n",
      "Iteration 27555, Loss: 0.05568595975637436\n",
      "Iteration 27556, Loss: 0.05567777156829834\n",
      "Iteration 27557, Loss: 0.055676739662885666\n",
      "Iteration 27558, Loss: 0.055670659989118576\n",
      "Iteration 27559, Loss: 0.055684447288513184\n",
      "Iteration 27560, Loss: 0.05567558854818344\n",
      "Iteration 27561, Loss: 0.05567920580506325\n",
      "Iteration 27562, Loss: 0.05567328259348869\n",
      "Iteration 27563, Loss: 0.055681705474853516\n",
      "Iteration 27564, Loss: 0.05567300319671631\n",
      "Iteration 27565, Loss: 0.05568186566233635\n",
      "Iteration 27566, Loss: 0.05567526817321777\n",
      "Iteration 27567, Loss: 0.05568007752299309\n",
      "Iteration 27568, Loss: 0.055671971291303635\n",
      "Iteration 27569, Loss: 0.05568274110555649\n",
      "Iteration 27570, Loss: 0.055675625801086426\n",
      "Iteration 27571, Loss: 0.05567976087331772\n",
      "Iteration 27572, Loss: 0.055671971291303635\n",
      "Iteration 27573, Loss: 0.055682700127363205\n",
      "Iteration 27574, Loss: 0.05567526817321777\n",
      "Iteration 27575, Loss: 0.055679839104413986\n",
      "Iteration 27576, Loss: 0.05567248910665512\n",
      "Iteration 27577, Loss: 0.05568218603730202\n",
      "Iteration 27578, Loss: 0.055674873292446136\n",
      "Iteration 27579, Loss: 0.055680274963378906\n",
      "Iteration 27580, Loss: 0.055672768503427505\n",
      "Iteration 27581, Loss: 0.05568210408091545\n",
      "Iteration 27582, Loss: 0.055674754083156586\n",
      "Iteration 27583, Loss: 0.05568051338195801\n",
      "Iteration 27584, Loss: 0.055673204362392426\n",
      "Iteration 27585, Loss: 0.05568166822195053\n",
      "Iteration 27586, Loss: 0.05567415803670883\n",
      "Iteration 27587, Loss: 0.055681150406599045\n",
      "Iteration 27588, Loss: 0.05567387863993645\n",
      "Iteration 27589, Loss: 0.055680952966213226\n",
      "Iteration 27590, Loss: 0.055673401802778244\n",
      "Iteration 27591, Loss: 0.05568178743124008\n",
      "Iteration 27592, Loss: 0.05567455291748047\n",
      "Iteration 27593, Loss: 0.055680036544799805\n",
      "Iteration 27594, Loss: 0.05567260831594467\n",
      "Iteration 27595, Loss: 0.05568274110555649\n",
      "Iteration 27596, Loss: 0.05567523092031479\n",
      "Iteration 27597, Loss: 0.05567944422364235\n",
      "Iteration 27598, Loss: 0.05567236989736557\n",
      "Iteration 27599, Loss: 0.05568286031484604\n",
      "Iteration 27600, Loss: 0.05567502975463867\n",
      "Iteration 27601, Loss: 0.05567968264222145\n",
      "Iteration 27602, Loss: 0.05567288398742676\n",
      "Iteration 27603, Loss: 0.055682223290205\n",
      "Iteration 27604, Loss: 0.05567411705851555\n",
      "Iteration 27605, Loss: 0.05568075180053711\n",
      "Iteration 27606, Loss: 0.05567403882741928\n",
      "Iteration 27607, Loss: 0.055681031197309494\n",
      "Iteration 27608, Loss: 0.05567300319671631\n",
      "Iteration 27609, Loss: 0.0556817464530468\n",
      "Iteration 27610, Loss: 0.05567459389567375\n",
      "Iteration 27611, Loss: 0.05568039417266846\n",
      "Iteration 27612, Loss: 0.05567276477813721\n",
      "Iteration 27613, Loss: 0.05568202584981918\n",
      "Iteration 27614, Loss: 0.055674754083156586\n",
      "Iteration 27615, Loss: 0.05568043515086174\n",
      "Iteration 27616, Loss: 0.05567304417490959\n",
      "Iteration 27617, Loss: 0.05568166822195053\n",
      "Iteration 27618, Loss: 0.05567427724599838\n",
      "Iteration 27619, Loss: 0.05568122863769531\n",
      "Iteration 27620, Loss: 0.05567371845245361\n",
      "Iteration 27621, Loss: 0.05568099021911621\n",
      "Iteration 27622, Loss: 0.05567368119955063\n",
      "Iteration 27623, Loss: 0.05568162724375725\n",
      "Iteration 27624, Loss: 0.055673997849226\n",
      "Iteration 27625, Loss: 0.055680595338344574\n",
      "Iteration 27626, Loss: 0.055673323571681976\n",
      "Iteration 27627, Loss: 0.0556819848716259\n",
      "Iteration 27628, Loss: 0.05567431449890137\n",
      "Iteration 27629, Loss: 0.05568035691976547\n",
      "Iteration 27630, Loss: 0.055673085153102875\n",
      "Iteration 27631, Loss: 0.055682066828012466\n",
      "Iteration 27632, Loss: 0.05567435547709465\n",
      "Iteration 27633, Loss: 0.055680278688669205\n",
      "Iteration 27634, Loss: 0.055673401802778244\n",
      "Iteration 27635, Loss: 0.05568190664052963\n",
      "Iteration 27636, Loss: 0.055673956871032715\n",
      "Iteration 27637, Loss: 0.055680952966213226\n",
      "Iteration 27638, Loss: 0.05567415803670883\n",
      "Iteration 27639, Loss: 0.05568087100982666\n",
      "Iteration 27640, Loss: 0.05567300319671631\n",
      "Iteration 27641, Loss: 0.05568194389343262\n",
      "Iteration 27642, Loss: 0.055674754083156586\n",
      "Iteration 27643, Loss: 0.05568039417266846\n",
      "Iteration 27644, Loss: 0.055672768503427505\n",
      "Iteration 27645, Loss: 0.05568202584981918\n",
      "Iteration 27646, Loss: 0.05567467212677002\n",
      "Iteration 27647, Loss: 0.05568087100982666\n",
      "Iteration 27648, Loss: 0.055673401802778244\n",
      "Iteration 27649, Loss: 0.05568122863769531\n",
      "Iteration 27650, Loss: 0.055673759430646896\n",
      "Iteration 27651, Loss: 0.0556819848716259\n",
      "Iteration 27652, Loss: 0.05567455291748047\n",
      "Iteration 27653, Loss: 0.05567999929189682\n",
      "Iteration 27654, Loss: 0.055672530084848404\n",
      "Iteration 27655, Loss: 0.05568286031484604\n",
      "Iteration 27656, Loss: 0.05567527189850807\n",
      "Iteration 27657, Loss: 0.055679284036159515\n",
      "Iteration 27658, Loss: 0.05567236989736557\n",
      "Iteration 27659, Loss: 0.055683016777038574\n",
      "Iteration 27660, Loss: 0.05567511171102524\n",
      "Iteration 27661, Loss: 0.055679917335510254\n",
      "Iteration 27662, Loss: 0.055673204362392426\n",
      "Iteration 27663, Loss: 0.05568202584981918\n",
      "Iteration 27664, Loss: 0.05567391961812973\n",
      "Iteration 27665, Loss: 0.05568135157227516\n",
      "Iteration 27666, Loss: 0.0556744746863842\n",
      "Iteration 27667, Loss: 0.05568051338195801\n",
      "Iteration 27668, Loss: 0.055672526359558105\n",
      "Iteration 27669, Loss: 0.055682700127363205\n",
      "Iteration 27670, Loss: 0.05567578598856926\n",
      "Iteration 27671, Loss: 0.055679403245449066\n",
      "Iteration 27672, Loss: 0.05567169189453125\n",
      "Iteration 27673, Loss: 0.05568345636129379\n",
      "Iteration 27674, Loss: 0.05567602440714836\n",
      "Iteration 27675, Loss: 0.05567924305796623\n",
      "Iteration 27676, Loss: 0.05567193031311035\n",
      "Iteration 27677, Loss: 0.05568297952413559\n",
      "Iteration 27678, Loss: 0.055675070732831955\n",
      "Iteration 27679, Loss: 0.055680274963378906\n",
      "Iteration 27680, Loss: 0.055673401802778244\n",
      "Iteration 27681, Loss: 0.05568142980337143\n",
      "Iteration 27682, Loss: 0.055673640221357346\n",
      "Iteration 27683, Loss: 0.055681828409433365\n",
      "Iteration 27684, Loss: 0.05567455291748047\n",
      "Iteration 27685, Loss: 0.055680155754089355\n",
      "Iteration 27686, Loss: 0.05567276477813721\n",
      "Iteration 27687, Loss: 0.05568265914916992\n",
      "Iteration 27688, Loss: 0.05567503347992897\n",
      "Iteration 27689, Loss: 0.055679719895124435\n",
      "Iteration 27690, Loss: 0.05567288398742676\n",
      "Iteration 27691, Loss: 0.05568253993988037\n",
      "Iteration 27692, Loss: 0.05567455291748047\n",
      "Iteration 27693, Loss: 0.05568051338195801\n",
      "Iteration 27694, Loss: 0.05567380040884018\n",
      "Iteration 27695, Loss: 0.0556815080344677\n",
      "Iteration 27696, Loss: 0.05567344278097153\n",
      "Iteration 27697, Loss: 0.055681586265563965\n",
      "Iteration 27698, Loss: 0.055674873292446136\n",
      "Iteration 27699, Loss: 0.05568031594157219\n",
      "Iteration 27700, Loss: 0.05567248910665512\n",
      "Iteration 27701, Loss: 0.055682700127363205\n",
      "Iteration 27702, Loss: 0.055675506591796875\n",
      "Iteration 27703, Loss: 0.05567976087331772\n",
      "Iteration 27704, Loss: 0.055672407150268555\n",
      "Iteration 27705, Loss: 0.0556824617087841\n",
      "Iteration 27706, Loss: 0.05567491427063942\n",
      "Iteration 27707, Loss: 0.05568055436015129\n",
      "Iteration 27708, Loss: 0.055673323571681976\n",
      "Iteration 27709, Loss: 0.05568131059408188\n",
      "Iteration 27710, Loss: 0.05567384138703346\n",
      "Iteration 27711, Loss: 0.05568162724375725\n",
      "Iteration 27712, Loss: 0.05567368119955063\n",
      "Iteration 27713, Loss: 0.055680952966213226\n",
      "Iteration 27714, Loss: 0.055673640221357346\n",
      "Iteration 27715, Loss: 0.05568142980337143\n",
      "Iteration 27716, Loss: 0.05567387863993645\n",
      "Iteration 27717, Loss: 0.05568123236298561\n",
      "Iteration 27718, Loss: 0.055674076080322266\n",
      "Iteration 27719, Loss: 0.05568087100982666\n",
      "Iteration 27720, Loss: 0.055673085153102875\n",
      "Iteration 27721, Loss: 0.055682145059108734\n",
      "Iteration 27722, Loss: 0.055674951523542404\n",
      "Iteration 27723, Loss: 0.05567999929189682\n",
      "Iteration 27724, Loss: 0.05567232891917229\n",
      "Iteration 27725, Loss: 0.05568277835845947\n",
      "Iteration 27726, Loss: 0.05567570775747299\n",
      "Iteration 27727, Loss: 0.05567924305796623\n",
      "Iteration 27728, Loss: 0.055671773850917816\n",
      "Iteration 27729, Loss: 0.05568305775523186\n",
      "Iteration 27730, Loss: 0.05567574501037598\n",
      "Iteration 27731, Loss: 0.0556793212890625\n",
      "Iteration 27732, Loss: 0.05567217245697975\n",
      "Iteration 27733, Loss: 0.05568278208374977\n",
      "Iteration 27734, Loss: 0.05567502975463867\n",
      "Iteration 27735, Loss: 0.05568011850118637\n",
      "Iteration 27736, Loss: 0.055673204362392426\n",
      "Iteration 27737, Loss: 0.05568162724375725\n",
      "Iteration 27738, Loss: 0.05567368119955063\n",
      "Iteration 27739, Loss: 0.05568162724375725\n",
      "Iteration 27740, Loss: 0.05567455291748047\n",
      "Iteration 27741, Loss: 0.05567995831370354\n",
      "Iteration 27742, Loss: 0.05567232891917229\n",
      "Iteration 27743, Loss: 0.05568309873342514\n",
      "Iteration 27744, Loss: 0.05567590519785881\n",
      "Iteration 27745, Loss: 0.055678728967905045\n",
      "Iteration 27746, Loss: 0.055671654641628265\n",
      "Iteration 27747, Loss: 0.05568373203277588\n",
      "Iteration 27748, Loss: 0.05567590519785881\n",
      "Iteration 27749, Loss: 0.05567892640829086\n",
      "Iteration 27750, Loss: 0.055672407150268555\n",
      "Iteration 27751, Loss: 0.055682580918073654\n",
      "Iteration 27752, Loss: 0.05567415803670883\n",
      "Iteration 27753, Loss: 0.05568091198801994\n",
      "Iteration 27754, Loss: 0.0556744746863842\n",
      "Iteration 27755, Loss: 0.055680278688669205\n",
      "Iteration 27756, Loss: 0.055671773850917816\n",
      "Iteration 27757, Loss: 0.05568321794271469\n",
      "Iteration 27758, Loss: 0.05567654222249985\n",
      "Iteration 27759, Loss: 0.05567821115255356\n",
      "Iteration 27760, Loss: 0.05567049980163574\n",
      "Iteration 27761, Loss: 0.055684566497802734\n",
      "Iteration 27762, Loss: 0.05567729473114014\n",
      "Iteration 27763, Loss: 0.05567777529358864\n",
      "Iteration 27764, Loss: 0.05567070096731186\n",
      "Iteration 27765, Loss: 0.05568429082632065\n",
      "Iteration 27766, Loss: 0.05567626282572746\n",
      "Iteration 27767, Loss: 0.05567892640829086\n",
      "Iteration 27768, Loss: 0.05567225068807602\n",
      "Iteration 27769, Loss: 0.055682264268398285\n",
      "Iteration 27770, Loss: 0.05567415803670883\n",
      "Iteration 27771, Loss: 0.05568134784698486\n",
      "Iteration 27772, Loss: 0.05567479133605957\n",
      "Iteration 27773, Loss: 0.05567964166402817\n",
      "Iteration 27774, Loss: 0.055671654641628265\n",
      "Iteration 27775, Loss: 0.055683933198451996\n",
      "Iteration 27776, Loss: 0.05567685887217522\n",
      "Iteration 27777, Loss: 0.05567753687500954\n",
      "Iteration 27778, Loss: 0.055670421570539474\n",
      "Iteration 27779, Loss: 0.05568528547883034\n",
      "Iteration 27780, Loss: 0.055677495896816254\n",
      "Iteration 27781, Loss: 0.05567701905965805\n",
      "Iteration 27782, Loss: 0.055670738220214844\n",
      "Iteration 27783, Loss: 0.05568452924489975\n",
      "Iteration 27784, Loss: 0.05567602440714836\n",
      "Iteration 27785, Loss: 0.05567876622080803\n",
      "Iteration 27786, Loss: 0.05567256733775139\n",
      "Iteration 27787, Loss: 0.055682502686977386\n",
      "Iteration 27788, Loss: 0.05567391961812973\n",
      "Iteration 27789, Loss: 0.05568107217550278\n",
      "Iteration 27790, Loss: 0.055674754083156586\n",
      "Iteration 27791, Loss: 0.05568035691976547\n",
      "Iteration 27792, Loss: 0.055671971291303635\n",
      "Iteration 27793, Loss: 0.05568297952413559\n",
      "Iteration 27794, Loss: 0.05567626282572746\n",
      "Iteration 27795, Loss: 0.05567880719900131\n",
      "Iteration 27796, Loss: 0.05567093938589096\n",
      "Iteration 27797, Loss: 0.05568448826670647\n",
      "Iteration 27798, Loss: 0.05567709729075432\n",
      "Iteration 27799, Loss: 0.05567912384867668\n",
      "Iteration 27800, Loss: 0.055671852082014084\n",
      "Iteration 27801, Loss: 0.0556841716170311\n",
      "Iteration 27802, Loss: 0.055676501244306564\n",
      "Iteration 27803, Loss: 0.05567964166402817\n",
      "Iteration 27804, Loss: 0.05567268654704094\n",
      "Iteration 27805, Loss: 0.05568305775523186\n",
      "Iteration 27806, Loss: 0.05567523092031479\n",
      "Iteration 27807, Loss: 0.055681031197309494\n",
      "Iteration 27808, Loss: 0.05567391961812973\n",
      "Iteration 27809, Loss: 0.05568166822195053\n",
      "Iteration 27810, Loss: 0.05567380040884018\n",
      "Iteration 27811, Loss: 0.05568262189626694\n",
      "Iteration 27812, Loss: 0.05567551031708717\n",
      "Iteration 27813, Loss: 0.05568011850118637\n",
      "Iteration 27814, Loss: 0.055672645568847656\n",
      "Iteration 27815, Loss: 0.055683694779872894\n",
      "Iteration 27816, Loss: 0.05567622184753418\n",
      "Iteration 27817, Loss: 0.05567936226725578\n",
      "Iteration 27818, Loss: 0.05567225068807602\n",
      "Iteration 27819, Loss: 0.05568408966064453\n",
      "Iteration 27820, Loss: 0.055676184594631195\n",
      "Iteration 27821, Loss: 0.05567976087331772\n",
      "Iteration 27822, Loss: 0.05567316338419914\n",
      "Iteration 27823, Loss: 0.055682938545942307\n",
      "Iteration 27824, Loss: 0.05567459389567375\n",
      "Iteration 27825, Loss: 0.0556815080344677\n",
      "Iteration 27826, Loss: 0.05567499250173569\n",
      "Iteration 27827, Loss: 0.055680595338344574\n",
      "Iteration 27828, Loss: 0.05567241087555885\n",
      "Iteration 27829, Loss: 0.055683813989162445\n",
      "Iteration 27830, Loss: 0.055676937103271484\n",
      "Iteration 27831, Loss: 0.05567880719900131\n",
      "Iteration 27832, Loss: 0.055671095848083496\n",
      "Iteration 27833, Loss: 0.05568500608205795\n",
      "Iteration 27834, Loss: 0.05567765608429909\n",
      "Iteration 27835, Loss: 0.05567852780222893\n",
      "Iteration 27836, Loss: 0.05567129701375961\n",
      "Iteration 27837, Loss: 0.055684566497802734\n",
      "Iteration 27838, Loss: 0.0556766614317894\n",
      "Iteration 27839, Loss: 0.05567952245473862\n",
      "Iteration 27840, Loss: 0.055672965943813324\n",
      "Iteration 27841, Loss: 0.055682819336652756\n",
      "Iteration 27842, Loss: 0.055674634873867035\n",
      "Iteration 27843, Loss: 0.05568186566233635\n",
      "Iteration 27844, Loss: 0.05567514896392822\n",
      "Iteration 27845, Loss: 0.05568043515086174\n",
      "Iteration 27846, Loss: 0.05567256733775139\n",
      "Iteration 27847, Loss: 0.05568389222025871\n",
      "Iteration 27848, Loss: 0.0556766614317894\n",
      "Iteration 27849, Loss: 0.055679045617580414\n",
      "Iteration 27850, Loss: 0.05567189306020737\n",
      "Iteration 27851, Loss: 0.055684249848127365\n",
      "Iteration 27852, Loss: 0.055676501244306564\n",
      "Iteration 27853, Loss: 0.0556795634329319\n",
      "Iteration 27854, Loss: 0.05567288398742676\n",
      "Iteration 27855, Loss: 0.055683255195617676\n",
      "Iteration 27856, Loss: 0.05567499250173569\n",
      "Iteration 27857, Loss: 0.055681031197309494\n",
      "Iteration 27858, Loss: 0.05567435547709465\n",
      "Iteration 27859, Loss: 0.0556817464530468\n",
      "Iteration 27860, Loss: 0.055673640221357346\n",
      "Iteration 27861, Loss: 0.05568234249949455\n",
      "Iteration 27862, Loss: 0.05567554756999016\n",
      "Iteration 27863, Loss: 0.055680714547634125\n",
      "Iteration 27864, Loss: 0.05567272752523422\n",
      "Iteration 27865, Loss: 0.05568333715200424\n",
      "Iteration 27866, Loss: 0.05567634105682373\n",
      "Iteration 27867, Loss: 0.05567964166402817\n",
      "Iteration 27868, Loss: 0.0556720495223999\n",
      "Iteration 27869, Loss: 0.05568405240774155\n",
      "Iteration 27870, Loss: 0.055676739662885666\n",
      "Iteration 27871, Loss: 0.055679403245449066\n",
      "Iteration 27872, Loss: 0.05567213147878647\n",
      "Iteration 27873, Loss: 0.055683933198451996\n",
      "Iteration 27874, Loss: 0.05567622557282448\n",
      "Iteration 27875, Loss: 0.05568019673228264\n",
      "Iteration 27876, Loss: 0.05567316338419914\n",
      "Iteration 27877, Loss: 0.05568262189626694\n",
      "Iteration 27878, Loss: 0.0556747131049633\n",
      "Iteration 27879, Loss: 0.05568162724375725\n",
      "Iteration 27880, Loss: 0.05567483231425285\n",
      "Iteration 27881, Loss: 0.05568079277873039\n",
      "Iteration 27882, Loss: 0.05567284673452377\n",
      "Iteration 27883, Loss: 0.05568377301096916\n",
      "Iteration 27884, Loss: 0.055676817893981934\n",
      "Iteration 27885, Loss: 0.05567868798971176\n",
      "Iteration 27886, Loss: 0.05567129701375961\n",
      "Iteration 27887, Loss: 0.055685244500637054\n",
      "Iteration 27888, Loss: 0.05567753314971924\n",
      "Iteration 27889, Loss: 0.05567825213074684\n",
      "Iteration 27890, Loss: 0.05567169189453125\n",
      "Iteration 27891, Loss: 0.05568445101380348\n",
      "Iteration 27892, Loss: 0.05567598342895508\n",
      "Iteration 27893, Loss: 0.05568007752299309\n",
      "Iteration 27894, Loss: 0.05567380040884018\n",
      "Iteration 27895, Loss: 0.05568206310272217\n",
      "Iteration 27896, Loss: 0.05567348003387451\n",
      "Iteration 27897, Loss: 0.0556824617087841\n",
      "Iteration 27898, Loss: 0.055675946176052094\n",
      "Iteration 27899, Loss: 0.055679917335510254\n",
      "Iteration 27900, Loss: 0.05567169189453125\n",
      "Iteration 27901, Loss: 0.05568409338593483\n",
      "Iteration 27902, Loss: 0.055677056312561035\n",
      "Iteration 27903, Loss: 0.05567920580506325\n",
      "Iteration 27904, Loss: 0.05567169561982155\n",
      "Iteration 27905, Loss: 0.055684011429548264\n",
      "Iteration 27906, Loss: 0.055676382035017014\n",
      "Iteration 27907, Loss: 0.0556797981262207\n",
      "Iteration 27908, Loss: 0.05567284673452377\n",
      "Iteration 27909, Loss: 0.05568262189626694\n",
      "Iteration 27910, Loss: 0.05567483231425285\n",
      "Iteration 27911, Loss: 0.0556815080344677\n",
      "Iteration 27912, Loss: 0.05567455291748047\n",
      "Iteration 27913, Loss: 0.05568067356944084\n",
      "Iteration 27914, Loss: 0.05567288398742676\n",
      "Iteration 27915, Loss: 0.05568361654877663\n",
      "Iteration 27916, Loss: 0.05567646026611328\n",
      "Iteration 27917, Loss: 0.05567880719900131\n",
      "Iteration 27918, Loss: 0.055671535432338715\n",
      "Iteration 27919, Loss: 0.05568500608205795\n",
      "Iteration 27920, Loss: 0.05567725747823715\n",
      "Iteration 27921, Loss: 0.05567821115255356\n",
      "Iteration 27922, Loss: 0.05567169189453125\n",
      "Iteration 27923, Loss: 0.05568452924489975\n",
      "Iteration 27924, Loss: 0.05567626282572746\n",
      "Iteration 27925, Loss: 0.05567976087331772\n",
      "Iteration 27926, Loss: 0.055673640221357346\n",
      "Iteration 27927, Loss: 0.055682223290205\n",
      "Iteration 27928, Loss: 0.05567324161529541\n",
      "Iteration 27929, Loss: 0.055682819336652756\n",
      "Iteration 27930, Loss: 0.0556764230132103\n",
      "Iteration 27931, Loss: 0.055679284036159515\n",
      "Iteration 27932, Loss: 0.05567113682627678\n",
      "Iteration 27933, Loss: 0.0556848868727684\n",
      "Iteration 27934, Loss: 0.05567789077758789\n",
      "Iteration 27935, Loss: 0.05567800998687744\n",
      "Iteration 27936, Loss: 0.05567046254873276\n",
      "Iteration 27937, Loss: 0.05568563938140869\n",
      "Iteration 27938, Loss: 0.055677931755781174\n",
      "Iteration 27939, Loss: 0.05567812919616699\n",
      "Iteration 27940, Loss: 0.05567129701375961\n",
      "Iteration 27941, Loss: 0.0556844100356102\n",
      "Iteration 27942, Loss: 0.055676382035017014\n",
      "Iteration 27943, Loss: 0.05568011850118637\n",
      "Iteration 27944, Loss: 0.05567368119955063\n",
      "Iteration 27945, Loss: 0.05568166822195053\n",
      "Iteration 27946, Loss: 0.055673401802778244\n",
      "Iteration 27947, Loss: 0.05568321794271469\n",
      "Iteration 27948, Loss: 0.05567657947540283\n",
      "Iteration 27949, Loss: 0.055678728967905045\n",
      "Iteration 27950, Loss: 0.05567074194550514\n",
      "Iteration 27951, Loss: 0.05568575859069824\n",
      "Iteration 27952, Loss: 0.05567840859293938\n",
      "Iteration 27953, Loss: 0.055677060037851334\n",
      "Iteration 27954, Loss: 0.055670224130153656\n",
      "Iteration 27955, Loss: 0.05568603798747063\n",
      "Iteration 27956, Loss: 0.05567797273397446\n",
      "Iteration 27957, Loss: 0.05567777529358864\n",
      "Iteration 27958, Loss: 0.05567169189453125\n",
      "Iteration 27959, Loss: 0.055684369057416916\n",
      "Iteration 27960, Loss: 0.05567566677927971\n",
      "Iteration 27961, Loss: 0.05568023771047592\n",
      "Iteration 27962, Loss: 0.05567415803670883\n",
      "Iteration 27963, Loss: 0.05568202584981918\n",
      "Iteration 27964, Loss: 0.05567312240600586\n",
      "Iteration 27965, Loss: 0.055682700127363205\n",
      "Iteration 27966, Loss: 0.05567622184753418\n",
      "Iteration 27967, Loss: 0.05567988008260727\n",
      "Iteration 27968, Loss: 0.05567161366343498\n",
      "Iteration 27969, Loss: 0.055684249848127365\n",
      "Iteration 27970, Loss: 0.0556773766875267\n",
      "Iteration 27971, Loss: 0.05567849054932594\n",
      "Iteration 27972, Loss: 0.05567077919840813\n",
      "Iteration 27973, Loss: 0.05568508431315422\n",
      "Iteration 27974, Loss: 0.05567781254649162\n",
      "Iteration 27975, Loss: 0.05567821115255356\n",
      "Iteration 27976, Loss: 0.05567105859518051\n",
      "Iteration 27977, Loss: 0.05568508431315422\n",
      "Iteration 27978, Loss: 0.05567733570933342\n",
      "Iteration 27979, Loss: 0.05567868798971176\n",
      "Iteration 27980, Loss: 0.055671773850917816\n",
      "Iteration 27981, Loss: 0.05568408966064453\n",
      "Iteration 27982, Loss: 0.05567598342895508\n",
      "Iteration 27983, Loss: 0.05568011850118637\n",
      "Iteration 27984, Loss: 0.055673401802778244\n",
      "Iteration 27985, Loss: 0.0556819848716259\n",
      "Iteration 27986, Loss: 0.05567380040884018\n",
      "Iteration 27987, Loss: 0.055682502686977386\n",
      "Iteration 27988, Loss: 0.05567590519785881\n",
      "Iteration 27989, Loss: 0.05567944422364235\n",
      "Iteration 27990, Loss: 0.05567137524485588\n",
      "Iteration 27991, Loss: 0.05568544194102287\n",
      "Iteration 27992, Loss: 0.055678289383649826\n",
      "Iteration 27993, Loss: 0.05567685887217522\n",
      "Iteration 27994, Loss: 0.05566978454589844\n",
      "Iteration 27995, Loss: 0.05568699166178703\n",
      "Iteration 27996, Loss: 0.05567920580506325\n",
      "Iteration 27997, Loss: 0.0556764230132103\n",
      "Iteration 27998, Loss: 0.055670421570539474\n",
      "Iteration 27999, Loss: 0.05568575859069824\n",
      "Iteration 28000, Loss: 0.05567701905965805\n",
      "Iteration 28001, Loss: 0.055678848177194595\n",
      "Iteration 28002, Loss: 0.05567312613129616\n",
      "Iteration 28003, Loss: 0.05568290129303932\n",
      "Iteration 28004, Loss: 0.055674076080322266\n",
      "Iteration 28005, Loss: 0.05568158999085426\n",
      "Iteration 28006, Loss: 0.055675309151411057\n",
      "Iteration 28007, Loss: 0.055681031197309494\n",
      "Iteration 28008, Loss: 0.05567268654704094\n",
      "Iteration 28009, Loss: 0.055682819336652756\n",
      "Iteration 28010, Loss: 0.055675946176052094\n",
      "Iteration 28011, Loss: 0.05568011850118637\n",
      "Iteration 28012, Loss: 0.05567232891917229\n",
      "Iteration 28013, Loss: 0.05568329617381096\n",
      "Iteration 28014, Loss: 0.05567614361643791\n",
      "Iteration 28015, Loss: 0.0556797981262207\n",
      "Iteration 28016, Loss: 0.0556720532476902\n",
      "Iteration 28017, Loss: 0.05568373203277588\n",
      "Iteration 28018, Loss: 0.0556764230132103\n",
      "Iteration 28019, Loss: 0.05567968264222145\n",
      "Iteration 28020, Loss: 0.05567232891917229\n",
      "Iteration 28021, Loss: 0.055683575570583344\n",
      "Iteration 28022, Loss: 0.05567598715424538\n",
      "Iteration 28023, Loss: 0.055680274963378906\n",
      "Iteration 28024, Loss: 0.05567316338419914\n",
      "Iteration 28025, Loss: 0.055682502686977386\n",
      "Iteration 28026, Loss: 0.055674951523542404\n",
      "Iteration 28027, Loss: 0.055681388825178146\n",
      "Iteration 28028, Loss: 0.05567435547709465\n",
      "Iteration 28029, Loss: 0.05568131059408188\n",
      "Iteration 28030, Loss: 0.05567368119955063\n",
      "Iteration 28031, Loss: 0.055682580918073654\n",
      "Iteration 28032, Loss: 0.05567535012960434\n",
      "Iteration 28033, Loss: 0.05568011850118637\n",
      "Iteration 28034, Loss: 0.05567276477813721\n",
      "Iteration 28035, Loss: 0.055683575570583344\n",
      "Iteration 28036, Loss: 0.05567614361643791\n",
      "Iteration 28037, Loss: 0.0556793212890625\n",
      "Iteration 28038, Loss: 0.05567241087555885\n",
      "Iteration 28039, Loss: 0.055684011429548264\n",
      "Iteration 28040, Loss: 0.05567614361643791\n",
      "Iteration 28041, Loss: 0.055679403245449066\n",
      "Iteration 28042, Loss: 0.05567300319671631\n",
      "Iteration 28043, Loss: 0.05568321794271469\n",
      "Iteration 28044, Loss: 0.05567515268921852\n",
      "Iteration 28045, Loss: 0.05568063259124756\n",
      "Iteration 28046, Loss: 0.055673956871032715\n",
      "Iteration 28047, Loss: 0.05568218603730202\n",
      "Iteration 28048, Loss: 0.055673997849226\n",
      "Iteration 28049, Loss: 0.05568178743124008\n",
      "Iteration 28050, Loss: 0.05567514896392822\n",
      "Iteration 28051, Loss: 0.05568063259124756\n",
      "Iteration 28052, Loss: 0.05567248910665512\n",
      "Iteration 28053, Loss: 0.05568349361419678\n",
      "Iteration 28054, Loss: 0.05567657947540283\n",
      "Iteration 28055, Loss: 0.05567924305796623\n",
      "Iteration 28056, Loss: 0.05567149445414543\n",
      "Iteration 28057, Loss: 0.055684447288513184\n",
      "Iteration 28058, Loss: 0.05567721650004387\n",
      "Iteration 28059, Loss: 0.05567868798971176\n",
      "Iteration 28060, Loss: 0.05567137524485588\n",
      "Iteration 28061, Loss: 0.05568448826670647\n",
      "Iteration 28062, Loss: 0.05567682161927223\n",
      "Iteration 28063, Loss: 0.05567924305796623\n",
      "Iteration 28064, Loss: 0.05567260831594467\n",
      "Iteration 28065, Loss: 0.05568309873342514\n",
      "Iteration 28066, Loss: 0.05567511171102524\n",
      "Iteration 28067, Loss: 0.05568134784698486\n",
      "Iteration 28068, Loss: 0.055674634873867035\n",
      "Iteration 28069, Loss: 0.055680952966213226\n",
      "Iteration 28070, Loss: 0.05567292496562004\n",
      "Iteration 28071, Loss: 0.05568353459239006\n",
      "Iteration 28072, Loss: 0.0556766614317894\n",
      "Iteration 28073, Loss: 0.05567888543009758\n",
      "Iteration 28074, Loss: 0.055671535432338715\n",
      "Iteration 28075, Loss: 0.05568492412567139\n",
      "Iteration 28076, Loss: 0.05567729473114014\n",
      "Iteration 28077, Loss: 0.055678170174360275\n",
      "Iteration 28078, Loss: 0.05567145347595215\n",
      "Iteration 28079, Loss: 0.05568496510386467\n",
      "Iteration 28080, Loss: 0.05567697808146477\n",
      "Iteration 28081, Loss: 0.05567880719900131\n",
      "Iteration 28082, Loss: 0.055672526359558105\n",
      "Iteration 28083, Loss: 0.055683575570583344\n",
      "Iteration 28084, Loss: 0.05567499250173569\n",
      "Iteration 28085, Loss: 0.05568079277873039\n",
      "Iteration 28086, Loss: 0.0556744746863842\n",
      "Iteration 28087, Loss: 0.05568162724375725\n",
      "Iteration 28088, Loss: 0.05567312240600586\n",
      "Iteration 28089, Loss: 0.055682819336652756\n",
      "Iteration 28090, Loss: 0.05567614361643791\n",
      "Iteration 28091, Loss: 0.05567988008260727\n",
      "Iteration 28092, Loss: 0.05567189306020737\n",
      "Iteration 28093, Loss: 0.055684011429548264\n",
      "Iteration 28094, Loss: 0.05567670240998268\n",
      "Iteration 28095, Loss: 0.05567952245473862\n",
      "Iteration 28096, Loss: 0.05567213147878647\n",
      "Iteration 28097, Loss: 0.05568373575806618\n",
      "Iteration 28098, Loss: 0.05567590519785881\n",
      "Iteration 28099, Loss: 0.05568051338195801\n",
      "Iteration 28100, Loss: 0.05567344278097153\n",
      "Iteration 28101, Loss: 0.05568218603730202\n",
      "Iteration 28102, Loss: 0.05567439645528793\n",
      "Iteration 28103, Loss: 0.05568210408091545\n",
      "Iteration 28104, Loss: 0.055674873292446136\n",
      "Iteration 28105, Loss: 0.055680595338344574\n",
      "Iteration 28106, Loss: 0.05567300319671631\n",
      "Iteration 28107, Loss: 0.055683575570583344\n",
      "Iteration 28108, Loss: 0.055676303803920746\n",
      "Iteration 28109, Loss: 0.055679284036159515\n",
      "Iteration 28110, Loss: 0.055671971291303635\n",
      "Iteration 28111, Loss: 0.05568448826670647\n",
      "Iteration 28112, Loss: 0.055676620453596115\n",
      "Iteration 28113, Loss: 0.05567896366119385\n",
      "Iteration 28114, Loss: 0.05567232891917229\n",
      "Iteration 28115, Loss: 0.055683694779872894\n",
      "Iteration 28116, Loss: 0.05567546933889389\n",
      "Iteration 28117, Loss: 0.05568035691976547\n",
      "Iteration 28118, Loss: 0.05567411705851555\n",
      "Iteration 28119, Loss: 0.05568190664052963\n",
      "Iteration 28120, Loss: 0.055673401802778244\n",
      "Iteration 28121, Loss: 0.05568242445588112\n",
      "Iteration 28122, Loss: 0.05567590519785881\n",
      "Iteration 28123, Loss: 0.05567995831370354\n",
      "Iteration 28124, Loss: 0.05567193403840065\n",
      "Iteration 28125, Loss: 0.05568397045135498\n",
      "Iteration 28126, Loss: 0.05567678064107895\n",
      "Iteration 28127, Loss: 0.055679284036159515\n",
      "Iteration 28128, Loss: 0.05567173287272453\n",
      "Iteration 28129, Loss: 0.055684130638837814\n",
      "Iteration 28130, Loss: 0.05567657947540283\n",
      "Iteration 28131, Loss: 0.0556793212890625\n",
      "Iteration 28132, Loss: 0.05567213147878647\n",
      "Iteration 28133, Loss: 0.05568361654877663\n",
      "Iteration 28134, Loss: 0.05567582696676254\n",
      "Iteration 28135, Loss: 0.05568023771047592\n",
      "Iteration 28136, Loss: 0.05567344278097153\n",
      "Iteration 28137, Loss: 0.055682264268398285\n",
      "Iteration 28138, Loss: 0.05567439645528793\n",
      "Iteration 28139, Loss: 0.05568206310272217\n",
      "Iteration 28140, Loss: 0.05567502975463867\n",
      "Iteration 28141, Loss: 0.05568047612905502\n",
      "Iteration 28142, Loss: 0.05567280575633049\n",
      "Iteration 28143, Loss: 0.05568377301096916\n",
      "Iteration 28144, Loss: 0.0556766614317894\n",
      "Iteration 28145, Loss: 0.05567876622080803\n",
      "Iteration 28146, Loss: 0.05567137524485588\n",
      "Iteration 28147, Loss: 0.055685244500637054\n",
      "Iteration 28148, Loss: 0.055677734315395355\n",
      "Iteration 28149, Loss: 0.055677853524684906\n",
      "Iteration 28150, Loss: 0.05567125603556633\n",
      "Iteration 28151, Loss: 0.05568484589457512\n",
      "Iteration 28152, Loss: 0.05567654222249985\n",
      "Iteration 28153, Loss: 0.05567920580506325\n",
      "Iteration 28154, Loss: 0.05567292496562004\n",
      "Iteration 28155, Loss: 0.05568305775523186\n",
      "Iteration 28156, Loss: 0.055674754083156586\n",
      "Iteration 28157, Loss: 0.05568134784698486\n",
      "Iteration 28158, Loss: 0.05567483231425285\n",
      "Iteration 28159, Loss: 0.05568122863769531\n",
      "Iteration 28160, Loss: 0.055673085153102875\n",
      "Iteration 28161, Loss: 0.05568274110555649\n",
      "Iteration 28162, Loss: 0.05567566677927971\n",
      "Iteration 28163, Loss: 0.05568055436015129\n",
      "Iteration 28164, Loss: 0.05567288398742676\n",
      "Iteration 28165, Loss: 0.055682819336652756\n",
      "Iteration 28166, Loss: 0.05567558854818344\n",
      "Iteration 28167, Loss: 0.05568079277873039\n",
      "Iteration 28168, Loss: 0.055673323571681976\n",
      "Iteration 28169, Loss: 0.055682223290205\n",
      "Iteration 28170, Loss: 0.05567467585206032\n",
      "Iteration 28171, Loss: 0.055681586265563965\n",
      "Iteration 28172, Loss: 0.05567435547709465\n",
      "Iteration 28173, Loss: 0.05568122863769531\n",
      "Iteration 28174, Loss: 0.05567368119955063\n",
      "Iteration 28175, Loss: 0.05568274110555649\n",
      "Iteration 28176, Loss: 0.05567527189850807\n",
      "Iteration 28177, Loss: 0.05568023771047592\n",
      "Iteration 28178, Loss: 0.055672965943813324\n",
      "Iteration 28179, Loss: 0.05568337440490723\n",
      "Iteration 28180, Loss: 0.05567578598856926\n",
      "Iteration 28181, Loss: 0.05567999929189682\n",
      "Iteration 28182, Loss: 0.05567284673452377\n",
      "Iteration 28183, Loss: 0.05568337440490723\n",
      "Iteration 28184, Loss: 0.05567542836070061\n",
      "Iteration 28185, Loss: 0.055680274963378906\n",
      "Iteration 28186, Loss: 0.05567348003387451\n",
      "Iteration 28187, Loss: 0.05568278208374977\n",
      "Iteration 28188, Loss: 0.055674634873867035\n",
      "Iteration 28189, Loss: 0.05568111315369606\n",
      "Iteration 28190, Loss: 0.05567431449890137\n",
      "Iteration 28191, Loss: 0.05568162724375725\n",
      "Iteration 28192, Loss: 0.05567360296845436\n",
      "Iteration 28193, Loss: 0.0556819848716259\n",
      "Iteration 28194, Loss: 0.05567502975463867\n",
      "Iteration 28195, Loss: 0.055681150406599045\n",
      "Iteration 28196, Loss: 0.05567344278097153\n",
      "Iteration 28197, Loss: 0.05568234249949455\n",
      "Iteration 28198, Loss: 0.05567499250173569\n",
      "Iteration 28199, Loss: 0.055681150406599045\n",
      "Iteration 28200, Loss: 0.05567371845245361\n",
      "Iteration 28201, Loss: 0.05568210408091545\n",
      "Iteration 28202, Loss: 0.055674873292446136\n",
      "Iteration 28203, Loss: 0.05568119138479233\n",
      "Iteration 28204, Loss: 0.05567359924316406\n",
      "Iteration 28205, Loss: 0.055682264268398285\n",
      "Iteration 28206, Loss: 0.055675070732831955\n",
      "Iteration 28207, Loss: 0.055680952966213226\n",
      "Iteration 28208, Loss: 0.05567312613129616\n",
      "Iteration 28209, Loss: 0.055682700127363205\n",
      "Iteration 28210, Loss: 0.055675309151411057\n",
      "Iteration 28211, Loss: 0.055680595338344574\n",
      "Iteration 28212, Loss: 0.05567324534058571\n",
      "Iteration 28213, Loss: 0.05568234249949455\n",
      "Iteration 28214, Loss: 0.055674754083156586\n",
      "Iteration 28215, Loss: 0.05568079277873039\n",
      "Iteration 28216, Loss: 0.05567368119955063\n",
      "Iteration 28217, Loss: 0.05568039417266846\n",
      "Iteration 28218, Loss: 0.05567276477813721\n",
      "Iteration 28219, Loss: 0.055682223290205\n",
      "Iteration 28220, Loss: 0.055674873292446136\n",
      "Iteration 28221, Loss: 0.0556790828704834\n",
      "Iteration 28222, Loss: 0.05567169189453125\n",
      "Iteration 28223, Loss: 0.055683255195617676\n",
      "Iteration 28224, Loss: 0.05567574501037598\n",
      "Iteration 28225, Loss: 0.05567844957113266\n",
      "Iteration 28226, Loss: 0.0556713342666626\n",
      "Iteration 28227, Loss: 0.055683497339487076\n",
      "Iteration 28228, Loss: 0.05567570775747299\n",
      "Iteration 28229, Loss: 0.055678486824035645\n",
      "Iteration 28230, Loss: 0.055671654641628265\n",
      "Iteration 28231, Loss: 0.05568297952413559\n",
      "Iteration 28232, Loss: 0.05567511171102524\n",
      "Iteration 28233, Loss: 0.05567920207977295\n",
      "Iteration 28234, Loss: 0.05567232891917229\n",
      "Iteration 28235, Loss: 0.05568234249949455\n",
      "Iteration 28236, Loss: 0.05567431449890137\n",
      "Iteration 28237, Loss: 0.05567988008260727\n",
      "Iteration 28238, Loss: 0.05567312240600586\n",
      "Iteration 28239, Loss: 0.05568142980337143\n",
      "Iteration 28240, Loss: 0.05567328259348869\n",
      "Iteration 28241, Loss: 0.05568111315369606\n",
      "Iteration 28242, Loss: 0.05567439645528793\n",
      "Iteration 28243, Loss: 0.05567999929189682\n",
      "Iteration 28244, Loss: 0.05567189306020737\n",
      "Iteration 28245, Loss: 0.05568274110555649\n",
      "Iteration 28246, Loss: 0.05567586421966553\n",
      "Iteration 28247, Loss: 0.05567849054932594\n",
      "Iteration 28248, Loss: 0.05567070096731186\n",
      "Iteration 28249, Loss: 0.055683694779872894\n",
      "Iteration 28250, Loss: 0.055676382035017014\n",
      "Iteration 28251, Loss: 0.05567809194326401\n",
      "Iteration 28252, Loss: 0.055670857429504395\n",
      "Iteration 28253, Loss: 0.055683575570583344\n",
      "Iteration 28254, Loss: 0.05567582696676254\n",
      "Iteration 28255, Loss: 0.05567876622080803\n",
      "Iteration 28256, Loss: 0.05567201226949692\n",
      "Iteration 28257, Loss: 0.05568230152130127\n",
      "Iteration 28258, Loss: 0.05567443370819092\n",
      "Iteration 28259, Loss: 0.05568043515086174\n",
      "Iteration 28260, Loss: 0.05567380040884018\n",
      "Iteration 28261, Loss: 0.05568023771047592\n",
      "Iteration 28262, Loss: 0.055672209709882736\n",
      "Iteration 28263, Loss: 0.05568277835845947\n",
      "Iteration 28264, Loss: 0.05567578598856926\n",
      "Iteration 28265, Loss: 0.05567812919616699\n",
      "Iteration 28266, Loss: 0.05567061901092529\n",
      "Iteration 28267, Loss: 0.05568448826670647\n",
      "Iteration 28268, Loss: 0.05567697808146477\n",
      "Iteration 28269, Loss: 0.05567721650004387\n",
      "Iteration 28270, Loss: 0.05567038431763649\n",
      "Iteration 28271, Loss: 0.05568445101380348\n",
      "Iteration 28272, Loss: 0.055676382035017014\n",
      "Iteration 28273, Loss: 0.05567797273397446\n",
      "Iteration 28274, Loss: 0.05567173287272453\n",
      "Iteration 28275, Loss: 0.055682819336652756\n",
      "Iteration 28276, Loss: 0.055674199014902115\n",
      "Iteration 28277, Loss: 0.05568035691976547\n",
      "Iteration 28278, Loss: 0.05567403882741928\n",
      "Iteration 28279, Loss: 0.05568055436015129\n",
      "Iteration 28280, Loss: 0.05567213147878647\n",
      "Iteration 28281, Loss: 0.055682145059108734\n",
      "Iteration 28282, Loss: 0.05567535012960434\n",
      "Iteration 28283, Loss: 0.055679481476545334\n",
      "Iteration 28284, Loss: 0.0556715726852417\n",
      "Iteration 28285, Loss: 0.05568262189626694\n",
      "Iteration 28286, Loss: 0.055675189942121506\n",
      "Iteration 28287, Loss: 0.055679801851511\n",
      "Iteration 28288, Loss: 0.05567236989736557\n",
      "Iteration 28289, Loss: 0.0556815080344677\n",
      "Iteration 28290, Loss: 0.055673997849226\n",
      "Iteration 28291, Loss: 0.05568091198801994\n",
      "Iteration 28292, Loss: 0.05567380040884018\n",
      "Iteration 28293, Loss: 0.05568011850118637\n",
      "Iteration 28294, Loss: 0.055672645568847656\n",
      "Iteration 28295, Loss: 0.05568218231201172\n",
      "Iteration 28296, Loss: 0.055674951523542404\n",
      "Iteration 28297, Loss: 0.055679045617580414\n",
      "Iteration 28298, Loss: 0.055671971291303635\n",
      "Iteration 28299, Loss: 0.055682819336652756\n",
      "Iteration 28300, Loss: 0.055675070732831955\n",
      "Iteration 28301, Loss: 0.0556790828704834\n",
      "Iteration 28302, Loss: 0.05567213147878647\n",
      "Iteration 28303, Loss: 0.05568230152130127\n",
      "Iteration 28304, Loss: 0.05567427724599838\n",
      "Iteration 28305, Loss: 0.05568007752299309\n",
      "Iteration 28306, Loss: 0.05567356199026108\n",
      "Iteration 28307, Loss: 0.05568079277873039\n",
      "Iteration 28308, Loss: 0.055672526359558105\n",
      "Iteration 28309, Loss: 0.055682223290205\n",
      "Iteration 28310, Loss: 0.05567566677927971\n",
      "Iteration 28311, Loss: 0.05567840859293938\n",
      "Iteration 28312, Loss: 0.05567026138305664\n",
      "Iteration 28313, Loss: 0.0556844100356102\n",
      "Iteration 28314, Loss: 0.05567729473114014\n",
      "Iteration 28315, Loss: 0.05567685887217522\n",
      "Iteration 28316, Loss: 0.05566970631480217\n",
      "Iteration 28317, Loss: 0.05568484589457512\n",
      "Iteration 28318, Loss: 0.05567721650004387\n",
      "Iteration 28319, Loss: 0.055677495896816254\n",
      "Iteration 28320, Loss: 0.05567105859518051\n",
      "Iteration 28321, Loss: 0.05568313971161842\n",
      "Iteration 28322, Loss: 0.055674754083156586\n",
      "Iteration 28323, Loss: 0.05568023771047592\n",
      "Iteration 28324, Loss: 0.05567387863993645\n",
      "Iteration 28325, Loss: 0.0556797981262207\n",
      "Iteration 28326, Loss: 0.0556713342666626\n",
      "Iteration 28327, Loss: 0.055684011429548264\n",
      "Iteration 28328, Loss: 0.0556771382689476\n",
      "Iteration 28329, Loss: 0.05567646026611328\n",
      "Iteration 28330, Loss: 0.05566895008087158\n",
      "Iteration 28331, Loss: 0.05568647384643555\n",
      "Iteration 28332, Loss: 0.055678967386484146\n",
      "Iteration 28333, Loss: 0.0556747131049633\n",
      "Iteration 28334, Loss: 0.05566835775971413\n",
      "Iteration 28335, Loss: 0.055686794221401215\n",
      "Iteration 28336, Loss: 0.05567844957113266\n",
      "Iteration 28337, Loss: 0.05567574501037598\n",
      "Iteration 28338, Loss: 0.05567026510834694\n",
      "Iteration 28339, Loss: 0.055684249848127365\n",
      "Iteration 28340, Loss: 0.055675070732831955\n",
      "Iteration 28341, Loss: 0.0556793250143528\n",
      "Iteration 28342, Loss: 0.055673759430646896\n",
      "Iteration 28343, Loss: 0.055680714547634125\n",
      "Iteration 28344, Loss: 0.05567161366343498\n",
      "Iteration 28345, Loss: 0.05568274110555649\n",
      "Iteration 28346, Loss: 0.05567626282572746\n",
      "Iteration 28347, Loss: 0.05567824840545654\n",
      "Iteration 28348, Loss: 0.055669985711574554\n",
      "Iteration 28349, Loss: 0.0556844100356102\n",
      "Iteration 28350, Loss: 0.05567741394042969\n",
      "Iteration 28351, Loss: 0.05567685887217522\n",
      "Iteration 28352, Loss: 0.05566919222474098\n",
      "Iteration 28353, Loss: 0.05568532273173332\n",
      "Iteration 28354, Loss: 0.055677853524684906\n",
      "Iteration 28355, Loss: 0.055676739662885666\n",
      "Iteration 28356, Loss: 0.05566990375518799\n",
      "Iteration 28357, Loss: 0.05568432807922363\n",
      "Iteration 28358, Loss: 0.05567614361643791\n",
      "Iteration 28359, Loss: 0.05567868798971176\n",
      "Iteration 28360, Loss: 0.05567244812846184\n",
      "Iteration 28361, Loss: 0.0556815080344677\n",
      "Iteration 28362, Loss: 0.055673085153102875\n",
      "Iteration 28363, Loss: 0.055682145059108734\n",
      "Iteration 28364, Loss: 0.055675625801086426\n",
      "Iteration 28365, Loss: 0.05567797273397446\n",
      "Iteration 28366, Loss: 0.055669985711574554\n",
      "Iteration 28367, Loss: 0.05568528547883034\n",
      "Iteration 28368, Loss: 0.05567809194326401\n",
      "Iteration 28369, Loss: 0.05567574501037598\n",
      "Iteration 28370, Loss: 0.05566895380616188\n",
      "Iteration 28371, Loss: 0.05568587779998779\n",
      "Iteration 28372, Loss: 0.05567789077758789\n",
      "Iteration 28373, Loss: 0.05567614361643791\n",
      "Iteration 28374, Loss: 0.05567026138305664\n",
      "Iteration 28375, Loss: 0.05568433180451393\n",
      "Iteration 28376, Loss: 0.05567578598856926\n",
      "Iteration 28377, Loss: 0.05567840859293938\n",
      "Iteration 28378, Loss: 0.055672287940979004\n",
      "Iteration 28379, Loss: 0.05568253993988037\n",
      "Iteration 28380, Loss: 0.055673997849226\n",
      "Iteration 28381, Loss: 0.05568011850118637\n",
      "Iteration 28382, Loss: 0.05567371845245361\n",
      "Iteration 28383, Loss: 0.05568087473511696\n",
      "Iteration 28384, Loss: 0.055672526359558105\n",
      "Iteration 28385, Loss: 0.05568178743124008\n",
      "Iteration 28386, Loss: 0.05567499250173569\n",
      "Iteration 28387, Loss: 0.05567944422364235\n",
      "Iteration 28388, Loss: 0.05567125603556633\n",
      "Iteration 28389, Loss: 0.05568297952413559\n",
      "Iteration 28390, Loss: 0.05567602440714836\n",
      "Iteration 28391, Loss: 0.055678289383649826\n",
      "Iteration 28392, Loss: 0.055670540779829025\n",
      "Iteration 28393, Loss: 0.055683933198451996\n",
      "Iteration 28394, Loss: 0.055676739662885666\n",
      "Iteration 28395, Loss: 0.055677495896816254\n",
      "Iteration 28396, Loss: 0.05567002668976784\n",
      "Iteration 28397, Loss: 0.055684447288513184\n",
      "Iteration 28398, Loss: 0.055676817893981934\n",
      "Iteration 28399, Loss: 0.05567741394042969\n",
      "Iteration 28400, Loss: 0.055670540779829025\n",
      "Iteration 28401, Loss: 0.05568389222025871\n",
      "Iteration 28402, Loss: 0.05567574501037598\n",
      "Iteration 28403, Loss: 0.05567876622080803\n",
      "Iteration 28404, Loss: 0.05567232891917229\n",
      "Iteration 28405, Loss: 0.055681709200143814\n",
      "Iteration 28406, Loss: 0.055673323571681976\n",
      "Iteration 28407, Loss: 0.055681586265563965\n",
      "Iteration 28408, Loss: 0.05567503347992897\n",
      "Iteration 28409, Loss: 0.05567888543009758\n",
      "Iteration 28410, Loss: 0.05567086115479469\n",
      "Iteration 28411, Loss: 0.05568420886993408\n",
      "Iteration 28412, Loss: 0.05567709729075432\n",
      "Iteration 28413, Loss: 0.05567657947540283\n",
      "Iteration 28414, Loss: 0.055669426918029785\n",
      "Iteration 28415, Loss: 0.05568572133779526\n",
      "Iteration 28416, Loss: 0.05567789077758789\n",
      "Iteration 28417, Loss: 0.05567602440714836\n",
      "Iteration 28418, Loss: 0.05566970631480217\n",
      "Iteration 28419, Loss: 0.055684804916381836\n",
      "Iteration 28420, Loss: 0.05567626282572746\n",
      "Iteration 28421, Loss: 0.05567781254649162\n",
      "Iteration 28422, Loss: 0.05567201226949692\n",
      "Iteration 28423, Loss: 0.05568242445588112\n",
      "Iteration 28424, Loss: 0.05567359924316406\n",
      "Iteration 28425, Loss: 0.05568047612905502\n",
      "Iteration 28426, Loss: 0.05567415803670883\n",
      "Iteration 28427, Loss: 0.05568051338195801\n",
      "Iteration 28428, Loss: 0.055671416223049164\n",
      "Iteration 28429, Loss: 0.055681269615888596\n",
      "Iteration 28430, Loss: 0.055674318224191666\n",
      "Iteration 28431, Loss: 0.055679164826869965\n",
      "Iteration 28432, Loss: 0.05567113682627678\n",
      "Iteration 28433, Loss: 0.05568154901266098\n",
      "Iteration 28434, Loss: 0.05567427724599838\n",
      "Iteration 28435, Loss: 0.0556788444519043\n",
      "Iteration 28436, Loss: 0.055671416223049164\n",
      "Iteration 28437, Loss: 0.055681388825178146\n",
      "Iteration 28438, Loss: 0.05567403882741928\n",
      "Iteration 28439, Loss: 0.055679045617580414\n",
      "Iteration 28440, Loss: 0.05567161366343498\n",
      "Iteration 28441, Loss: 0.05568122863769531\n",
      "Iteration 28442, Loss: 0.05567384138703346\n",
      "Iteration 28443, Loss: 0.0556793212890625\n",
      "Iteration 28444, Loss: 0.0556720532476902\n",
      "Iteration 28445, Loss: 0.05568063259124756\n",
      "Iteration 28446, Loss: 0.05567304417490959\n",
      "Iteration 28447, Loss: 0.05568031594157219\n",
      "Iteration 28448, Loss: 0.055673204362392426\n",
      "Iteration 28449, Loss: 0.0556793250143528\n",
      "Iteration 28450, Loss: 0.055671773850917816\n",
      "Iteration 28451, Loss: 0.0556817464530468\n",
      "Iteration 28452, Loss: 0.05567431449890137\n",
      "Iteration 28453, Loss: 0.05567797273397446\n",
      "Iteration 28454, Loss: 0.055670659989118576\n",
      "Iteration 28455, Loss: 0.055682700127363205\n",
      "Iteration 28456, Loss: 0.05567502975463867\n",
      "Iteration 28457, Loss: 0.05567757412791252\n",
      "Iteration 28458, Loss: 0.055670659989118576\n",
      "Iteration 28459, Loss: 0.05568253993988037\n",
      "Iteration 28460, Loss: 0.05567455291748047\n",
      "Iteration 28461, Loss: 0.055678050965070724\n",
      "Iteration 28462, Loss: 0.05567173287272453\n",
      "Iteration 28463, Loss: 0.055681269615888596\n",
      "Iteration 28464, Loss: 0.055672965943813324\n",
      "Iteration 28465, Loss: 0.05567999929189682\n",
      "Iteration 28466, Loss: 0.05567328259348869\n",
      "Iteration 28467, Loss: 0.055679481476545334\n",
      "Iteration 28468, Loss: 0.05567129701375961\n",
      "Iteration 28469, Loss: 0.055681467056274414\n",
      "Iteration 28470, Loss: 0.055674754083156586\n",
      "Iteration 28471, Loss: 0.05567840859293938\n",
      "Iteration 28472, Loss: 0.05567049980163574\n",
      "Iteration 28473, Loss: 0.055682223290205\n",
      "Iteration 28474, Loss: 0.05567479133605957\n",
      "Iteration 28475, Loss: 0.05567840859293938\n",
      "Iteration 28476, Loss: 0.05567121505737305\n",
      "Iteration 28477, Loss: 0.055681467056274414\n",
      "Iteration 28478, Loss: 0.05567380040884018\n",
      "Iteration 28479, Loss: 0.05567964166402817\n",
      "Iteration 28480, Loss: 0.055672526359558105\n",
      "Iteration 28481, Loss: 0.05567999929189682\n",
      "Iteration 28482, Loss: 0.055672287940979004\n",
      "Iteration 28483, Loss: 0.05568087100982666\n",
      "Iteration 28484, Loss: 0.055673640221357346\n",
      "Iteration 28485, Loss: 0.0556790865957737\n",
      "Iteration 28486, Loss: 0.05567169189453125\n",
      "Iteration 28487, Loss: 0.05568134784698486\n",
      "Iteration 28488, Loss: 0.055673640221357346\n",
      "Iteration 28489, Loss: 0.05567912384867668\n",
      "Iteration 28490, Loss: 0.05567213147878647\n",
      "Iteration 28491, Loss: 0.05568087100982666\n",
      "Iteration 28492, Loss: 0.055672965943813324\n",
      "Iteration 28493, Loss: 0.055679917335510254\n",
      "Iteration 28494, Loss: 0.05567312613129616\n",
      "Iteration 28495, Loss: 0.05567976087331772\n",
      "Iteration 28496, Loss: 0.055671773850917816\n",
      "Iteration 28497, Loss: 0.05568122863769531\n",
      "Iteration 28498, Loss: 0.055674199014902115\n",
      "Iteration 28499, Loss: 0.05567864701151848\n",
      "Iteration 28500, Loss: 0.05567089840769768\n",
      "Iteration 28501, Loss: 0.05568194389343262\n",
      "Iteration 28502, Loss: 0.0556744746863842\n",
      "Iteration 28503, Loss: 0.05567856878042221\n",
      "Iteration 28504, Loss: 0.055671218782663345\n",
      "Iteration 28505, Loss: 0.05568135157227516\n",
      "Iteration 28506, Loss: 0.05567356199026108\n",
      "Iteration 28507, Loss: 0.055679719895124435\n",
      "Iteration 28508, Loss: 0.05567280575633049\n",
      "Iteration 28509, Loss: 0.05567964166402817\n",
      "Iteration 28510, Loss: 0.055671852082014084\n",
      "Iteration 28511, Loss: 0.055681705474853516\n",
      "Iteration 28512, Loss: 0.055674515664577484\n",
      "Iteration 28513, Loss: 0.05567781254649162\n",
      "Iteration 28514, Loss: 0.05567026138305664\n",
      "Iteration 28515, Loss: 0.05568309873342514\n",
      "Iteration 28516, Loss: 0.05567558854818344\n",
      "Iteration 28517, Loss: 0.05567669868469238\n",
      "Iteration 28518, Loss: 0.055669866502285004\n",
      "Iteration 28519, Loss: 0.055683694779872894\n",
      "Iteration 28520, Loss: 0.05567566677927971\n",
      "Iteration 28521, Loss: 0.05567685887217522\n",
      "Iteration 28522, Loss: 0.05567058175802231\n",
      "Iteration 28523, Loss: 0.055682700127363205\n",
      "Iteration 28524, Loss: 0.055674199014902115\n",
      "Iteration 28525, Loss: 0.055678606033325195\n",
      "Iteration 28526, Loss: 0.055672209709882736\n",
      "Iteration 28527, Loss: 0.05568075180053711\n",
      "Iteration 28528, Loss: 0.05567225068807602\n",
      "Iteration 28529, Loss: 0.05568075180053711\n",
      "Iteration 28530, Loss: 0.05567427724599838\n",
      "Iteration 28531, Loss: 0.055678606033325195\n",
      "Iteration 28532, Loss: 0.05567018315196037\n",
      "Iteration 28533, Loss: 0.05568278208374977\n",
      "Iteration 28534, Loss: 0.05567582696676254\n",
      "Iteration 28535, Loss: 0.055677056312561035\n",
      "Iteration 28536, Loss: 0.05566946789622307\n",
      "Iteration 28537, Loss: 0.05568345636129379\n",
      "Iteration 28538, Loss: 0.05567590519785881\n",
      "Iteration 28539, Loss: 0.055677060037851334\n",
      "Iteration 28540, Loss: 0.05567002668976784\n",
      "Iteration 28541, Loss: 0.05568265914916992\n",
      "Iteration 28542, Loss: 0.05567483231425285\n",
      "Iteration 28543, Loss: 0.05567844957113266\n",
      "Iteration 28544, Loss: 0.05567161366343498\n",
      "Iteration 28545, Loss: 0.055681031197309494\n",
      "Iteration 28546, Loss: 0.05567288398742676\n",
      "Iteration 28547, Loss: 0.05568039417266846\n",
      "Iteration 28548, Loss: 0.05567359924316406\n",
      "Iteration 28549, Loss: 0.05567880719900131\n",
      "Iteration 28550, Loss: 0.05567105859518051\n",
      "Iteration 28551, Loss: 0.05568242445588112\n",
      "Iteration 28552, Loss: 0.05567526817321777\n",
      "Iteration 28553, Loss: 0.055677179247140884\n",
      "Iteration 28554, Loss: 0.055669985711574554\n",
      "Iteration 28555, Loss: 0.055683255195617676\n",
      "Iteration 28556, Loss: 0.05567554756999016\n",
      "Iteration 28557, Loss: 0.05567697808146477\n",
      "Iteration 28558, Loss: 0.055670421570539474\n",
      "Iteration 28559, Loss: 0.05568277835845947\n",
      "Iteration 28560, Loss: 0.05567467212677002\n",
      "Iteration 28561, Loss: 0.055677853524684906\n",
      "Iteration 28562, Loss: 0.05567129701375961\n",
      "Iteration 28563, Loss: 0.05568166822195053\n",
      "Iteration 28564, Loss: 0.055673401802778244\n",
      "Iteration 28565, Loss: 0.055679284036159515\n",
      "Iteration 28566, Loss: 0.05567268654704094\n",
      "Iteration 28567, Loss: 0.05568031594157219\n",
      "Iteration 28568, Loss: 0.055672090500593185\n",
      "Iteration 28569, Loss: 0.05568075180053711\n",
      "Iteration 28570, Loss: 0.05567415803670883\n",
      "Iteration 28571, Loss: 0.055678967386484146\n",
      "Iteration 28572, Loss: 0.055670738220214844\n",
      "Iteration 28573, Loss: 0.05568190664052963\n",
      "Iteration 28574, Loss: 0.05567491054534912\n",
      "Iteration 28575, Loss: 0.05567813292145729\n",
      "Iteration 28576, Loss: 0.055670540779829025\n",
      "Iteration 28577, Loss: 0.05568210408091545\n",
      "Iteration 28578, Loss: 0.055674754083156586\n",
      "Iteration 28579, Loss: 0.05567833036184311\n",
      "Iteration 28580, Loss: 0.05567117780447006\n",
      "Iteration 28581, Loss: 0.0556815080344677\n",
      "Iteration 28582, Loss: 0.055673640221357346\n",
      "Iteration 28583, Loss: 0.0556795634329319\n",
      "Iteration 28584, Loss: 0.05567260831594467\n",
      "Iteration 28585, Loss: 0.05567988008260727\n",
      "Iteration 28586, Loss: 0.05567236989736557\n",
      "Iteration 28587, Loss: 0.05568099394440651\n",
      "Iteration 28588, Loss: 0.05567371845245361\n",
      "Iteration 28589, Loss: 0.055678848177194595\n",
      "Iteration 28590, Loss: 0.05567149445414543\n",
      "Iteration 28591, Loss: 0.05568178743124008\n",
      "Iteration 28592, Loss: 0.055674076080322266\n",
      "Iteration 28593, Loss: 0.05567864701151848\n",
      "Iteration 28594, Loss: 0.05567169189453125\n",
      "Iteration 28595, Loss: 0.05568134784698486\n",
      "Iteration 28596, Loss: 0.05567344278097153\n",
      "Iteration 28597, Loss: 0.05567936226725578\n",
      "Iteration 28598, Loss: 0.05567260831594467\n",
      "Iteration 28599, Loss: 0.05568035691976547\n",
      "Iteration 28600, Loss: 0.05567217245697975\n",
      "Iteration 28601, Loss: 0.05568047612905502\n",
      "Iteration 28602, Loss: 0.055673759430646896\n",
      "Iteration 28603, Loss: 0.05567924305796623\n",
      "Iteration 28604, Loss: 0.0556713342666626\n",
      "Iteration 28605, Loss: 0.055681388825178146\n",
      "Iteration 28606, Loss: 0.05567427724599838\n",
      "Iteration 28607, Loss: 0.0556788444519043\n",
      "Iteration 28608, Loss: 0.05567129701375961\n",
      "Iteration 28609, Loss: 0.055681586265563965\n",
      "Iteration 28610, Loss: 0.05567411705851555\n",
      "Iteration 28611, Loss: 0.055679164826869965\n",
      "Iteration 28612, Loss: 0.05567173287272453\n",
      "Iteration 28613, Loss: 0.05568091198801994\n",
      "Iteration 28614, Loss: 0.05567344278097153\n",
      "Iteration 28615, Loss: 0.055680036544799805\n",
      "Iteration 28616, Loss: 0.05567272752523422\n",
      "Iteration 28617, Loss: 0.05567995831370354\n",
      "Iteration 28618, Loss: 0.05567280575633049\n",
      "Iteration 28619, Loss: 0.055680274963378906\n",
      "Iteration 28620, Loss: 0.05567272752523422\n",
      "Iteration 28621, Loss: 0.055680036544799805\n",
      "Iteration 28622, Loss: 0.05567292496562004\n",
      "Iteration 28623, Loss: 0.055680155754089355\n",
      "Iteration 28624, Loss: 0.05567256733775139\n",
      "Iteration 28625, Loss: 0.05568047612905502\n",
      "Iteration 28626, Loss: 0.055673401802778244\n",
      "Iteration 28627, Loss: 0.055679719895124435\n",
      "Iteration 28628, Loss: 0.0556720495223999\n",
      "Iteration 28629, Loss: 0.055680833756923676\n",
      "Iteration 28630, Loss: 0.05567372217774391\n",
      "Iteration 28631, Loss: 0.0556793250143528\n",
      "Iteration 28632, Loss: 0.055671654641628265\n",
      "Iteration 28633, Loss: 0.05568107217550278\n",
      "Iteration 28634, Loss: 0.055673837661743164\n",
      "Iteration 28635, Loss: 0.0556793250143528\n",
      "Iteration 28636, Loss: 0.05567193403840065\n",
      "Iteration 28637, Loss: 0.05568055436015129\n",
      "Iteration 28638, Loss: 0.05567300319671631\n",
      "Iteration 28639, Loss: 0.05568035691976547\n",
      "Iteration 28640, Loss: 0.055673085153102875\n",
      "Iteration 28641, Loss: 0.055679481476545334\n",
      "Iteration 28642, Loss: 0.055671852082014084\n",
      "Iteration 28643, Loss: 0.0556815080344677\n",
      "Iteration 28644, Loss: 0.05567411705851555\n",
      "Iteration 28645, Loss: 0.055678289383649826\n",
      "Iteration 28646, Loss: 0.055671099573373795\n",
      "Iteration 28647, Loss: 0.05568234249949455\n",
      "Iteration 28648, Loss: 0.055674754083156586\n",
      "Iteration 28649, Loss: 0.05567781254649162\n",
      "Iteration 28650, Loss: 0.05567093938589096\n",
      "Iteration 28651, Loss: 0.05568234249949455\n",
      "Iteration 28652, Loss: 0.05567411705851555\n",
      "Iteration 28653, Loss: 0.05567856878042221\n",
      "Iteration 28654, Loss: 0.05567189306020737\n",
      "Iteration 28655, Loss: 0.055681150406599045\n",
      "Iteration 28656, Loss: 0.055672887712717056\n",
      "Iteration 28657, Loss: 0.055679917335510254\n",
      "Iteration 28658, Loss: 0.05567316338419914\n",
      "Iteration 28659, Loss: 0.05567992106080055\n",
      "Iteration 28660, Loss: 0.0556718111038208\n",
      "Iteration 28661, Loss: 0.055681031197309494\n",
      "Iteration 28662, Loss: 0.05567415803670883\n",
      "Iteration 28663, Loss: 0.05567892640829086\n",
      "Iteration 28664, Loss: 0.05567113682627678\n",
      "Iteration 28665, Loss: 0.05568166822195053\n",
      "Iteration 28666, Loss: 0.055674515664577484\n",
      "Iteration 28667, Loss: 0.05567852780222893\n",
      "Iteration 28668, Loss: 0.05567105859518051\n",
      "Iteration 28669, Loss: 0.0556817464530468\n",
      "Iteration 28670, Loss: 0.055674318224191666\n",
      "Iteration 28671, Loss: 0.05567880719900131\n",
      "Iteration 28672, Loss: 0.0556715726852417\n",
      "Iteration 28673, Loss: 0.05568119138479233\n",
      "Iteration 28674, Loss: 0.055673640221357346\n",
      "Iteration 28675, Loss: 0.055679600685834885\n",
      "Iteration 28676, Loss: 0.05567225068807602\n",
      "Iteration 28677, Loss: 0.055680397897958755\n",
      "Iteration 28678, Loss: 0.05567284673452377\n",
      "Iteration 28679, Loss: 0.05568035691976547\n",
      "Iteration 28680, Loss: 0.05567304417490959\n",
      "Iteration 28681, Loss: 0.055679600685834885\n",
      "Iteration 28682, Loss: 0.05567216873168945\n",
      "Iteration 28683, Loss: 0.05568099021911621\n",
      "Iteration 28684, Loss: 0.05567360296845436\n",
      "Iteration 28685, Loss: 0.0556793250143528\n",
      "Iteration 28686, Loss: 0.055672287940979004\n",
      "Iteration 28687, Loss: 0.05568091198801994\n",
      "Iteration 28688, Loss: 0.05567300319671631\n",
      "Iteration 28689, Loss: 0.05567992106080055\n",
      "Iteration 28690, Loss: 0.05567304417490959\n",
      "Iteration 28691, Loss: 0.0556800402700901\n",
      "Iteration 28692, Loss: 0.05567201226949692\n",
      "Iteration 28693, Loss: 0.055680833756923676\n",
      "Iteration 28694, Loss: 0.05567391961812973\n",
      "Iteration 28695, Loss: 0.05567900463938713\n",
      "Iteration 28696, Loss: 0.05567137524485588\n",
      "Iteration 28697, Loss: 0.05568147078156471\n",
      "Iteration 28698, Loss: 0.0556742362678051\n",
      "Iteration 28699, Loss: 0.05567892640829086\n",
      "Iteration 28700, Loss: 0.055671535432338715\n",
      "Iteration 28701, Loss: 0.05568131059408188\n",
      "Iteration 28702, Loss: 0.05567380040884018\n",
      "Iteration 28703, Loss: 0.055679284036159515\n",
      "Iteration 28704, Loss: 0.055671971291303635\n",
      "Iteration 28705, Loss: 0.055680833756923676\n",
      "Iteration 28706, Loss: 0.055673323571681976\n",
      "Iteration 28707, Loss: 0.05567976087331772\n",
      "Iteration 28708, Loss: 0.05567232891917229\n",
      "Iteration 28709, Loss: 0.05568035691976547\n",
      "Iteration 28710, Loss: 0.055673085153102875\n",
      "Iteration 28711, Loss: 0.055680159479379654\n",
      "Iteration 28712, Loss: 0.055672645568847656\n",
      "Iteration 28713, Loss: 0.05568011850118637\n",
      "Iteration 28714, Loss: 0.055672645568847656\n",
      "Iteration 28715, Loss: 0.055680517107248306\n",
      "Iteration 28716, Loss: 0.055672965943813324\n",
      "Iteration 28717, Loss: 0.055679801851511\n",
      "Iteration 28718, Loss: 0.055672530084848404\n",
      "Iteration 28719, Loss: 0.05568091198801994\n",
      "Iteration 28720, Loss: 0.05567344278097153\n",
      "Iteration 28721, Loss: 0.0556793250143528\n",
      "Iteration 28722, Loss: 0.05567201226949692\n",
      "Iteration 28723, Loss: 0.055681269615888596\n",
      "Iteration 28724, Loss: 0.055673759430646896\n",
      "Iteration 28725, Loss: 0.05567892640829086\n",
      "Iteration 28726, Loss: 0.05567173287272453\n",
      "Iteration 28727, Loss: 0.05568166822195053\n",
      "Iteration 28728, Loss: 0.05567368119955063\n",
      "Iteration 28729, Loss: 0.05567896366119385\n",
      "Iteration 28730, Loss: 0.05567213147878647\n",
      "Iteration 28731, Loss: 0.055681031197309494\n",
      "Iteration 28732, Loss: 0.05567304417490959\n",
      "Iteration 28733, Loss: 0.0556797981262207\n",
      "Iteration 28734, Loss: 0.05567288398742676\n",
      "Iteration 28735, Loss: 0.05568043515086174\n",
      "Iteration 28736, Loss: 0.055672526359558105\n",
      "Iteration 28737, Loss: 0.055680274963378906\n",
      "Iteration 28738, Loss: 0.055673323571681976\n",
      "Iteration 28739, Loss: 0.05567995831370354\n",
      "Iteration 28740, Loss: 0.05567213147878647\n",
      "Iteration 28741, Loss: 0.05568075552582741\n",
      "Iteration 28742, Loss: 0.05567368119955063\n",
      "Iteration 28743, Loss: 0.05567952245473862\n",
      "Iteration 28744, Loss: 0.0556720495223999\n",
      "Iteration 28745, Loss: 0.055680714547634125\n",
      "Iteration 28746, Loss: 0.05567348003387451\n",
      "Iteration 28747, Loss: 0.05567976087331772\n",
      "Iteration 28748, Loss: 0.05567236989736557\n",
      "Iteration 28749, Loss: 0.055680274963378906\n",
      "Iteration 28750, Loss: 0.05567280575633049\n",
      "Iteration 28751, Loss: 0.055680397897958755\n",
      "Iteration 28752, Loss: 0.05567292496562004\n",
      "Iteration 28753, Loss: 0.05567992106080055\n",
      "Iteration 28754, Loss: 0.055672649294137955\n",
      "Iteration 28755, Loss: 0.05568047612905502\n",
      "Iteration 28756, Loss: 0.05567268654704094\n",
      "Iteration 28757, Loss: 0.055680155754089355\n",
      "Iteration 28758, Loss: 0.05567316338419914\n",
      "Iteration 28759, Loss: 0.05567976087331772\n",
      "Iteration 28760, Loss: 0.05567189306020737\n",
      "Iteration 28761, Loss: 0.055681150406599045\n",
      "Iteration 28762, Loss: 0.05567415803670883\n",
      "Iteration 28763, Loss: 0.05567876622080803\n",
      "Iteration 28764, Loss: 0.055671095848083496\n",
      "Iteration 28765, Loss: 0.05568202584981918\n",
      "Iteration 28766, Loss: 0.05567467212677002\n",
      "Iteration 28767, Loss: 0.05567840859293938\n",
      "Iteration 28768, Loss: 0.05567105859518051\n",
      "Iteration 28769, Loss: 0.0556817464530468\n",
      "Iteration 28770, Loss: 0.05567431449890137\n",
      "Iteration 28771, Loss: 0.055679164826869965\n",
      "Iteration 28772, Loss: 0.055672209709882736\n",
      "Iteration 28773, Loss: 0.05568047612905502\n",
      "Iteration 28774, Loss: 0.055672526359558105\n",
      "Iteration 28775, Loss: 0.055680952966213226\n",
      "Iteration 28776, Loss: 0.055673956871032715\n",
      "Iteration 28777, Loss: 0.055678170174360275\n",
      "Iteration 28778, Loss: 0.055670659989118576\n",
      "Iteration 28779, Loss: 0.055683016777038574\n",
      "Iteration 28780, Loss: 0.05567578598856926\n",
      "Iteration 28781, Loss: 0.05567657947540283\n",
      "Iteration 28782, Loss: 0.05566966533660889\n",
      "Iteration 28783, Loss: 0.05568397045135498\n",
      "Iteration 28784, Loss: 0.05567610636353493\n",
      "Iteration 28785, Loss: 0.0556766614317894\n",
      "Iteration 28786, Loss: 0.05567038059234619\n",
      "Iteration 28787, Loss: 0.055682938545942307\n",
      "Iteration 28788, Loss: 0.05567435547709465\n",
      "Iteration 28789, Loss: 0.05567864701151848\n",
      "Iteration 28790, Loss: 0.05567248910665512\n",
      "Iteration 28791, Loss: 0.055680595338344574\n",
      "Iteration 28792, Loss: 0.05567193031311035\n",
      "Iteration 28793, Loss: 0.055681150406599045\n",
      "Iteration 28794, Loss: 0.05567467212677002\n",
      "Iteration 28795, Loss: 0.055678170174360275\n",
      "Iteration 28796, Loss: 0.055669866502285004\n",
      "Iteration 28797, Loss: 0.05568321794271469\n",
      "Iteration 28798, Loss: 0.05567602440714836\n",
      "Iteration 28799, Loss: 0.05567682161927223\n",
      "Iteration 28800, Loss: 0.0556693896651268\n",
      "Iteration 28801, Loss: 0.05568341538310051\n",
      "Iteration 28802, Loss: 0.05567570775747299\n",
      "Iteration 28803, Loss: 0.05567745491862297\n",
      "Iteration 28804, Loss: 0.055670659989118576\n",
      "Iteration 28805, Loss: 0.05568206310272217\n",
      "Iteration 28806, Loss: 0.055673759430646896\n",
      "Iteration 28807, Loss: 0.05567964166402817\n",
      "Iteration 28808, Loss: 0.05567304417490959\n",
      "Iteration 28809, Loss: 0.055679403245449066\n",
      "Iteration 28810, Loss: 0.05567117780447006\n",
      "Iteration 28811, Loss: 0.055682580918073654\n",
      "Iteration 28812, Loss: 0.055675625801086426\n",
      "Iteration 28813, Loss: 0.05567678064107895\n",
      "Iteration 28814, Loss: 0.055669188499450684\n",
      "Iteration 28815, Loss: 0.05568433180451393\n",
      "Iteration 28816, Loss: 0.0556768998503685\n",
      "Iteration 28817, Loss: 0.05567570775747299\n",
      "Iteration 28818, Loss: 0.055669110268354416\n",
      "Iteration 28819, Loss: 0.055684249848127365\n",
      "Iteration 28820, Loss: 0.05567590519785881\n",
      "Iteration 28821, Loss: 0.055676817893981934\n",
      "Iteration 28822, Loss: 0.05567089840769768\n",
      "Iteration 28823, Loss: 0.05568210408091545\n",
      "Iteration 28824, Loss: 0.055673401802778244\n",
      "Iteration 28825, Loss: 0.055679403245449066\n",
      "Iteration 28826, Loss: 0.055673323571681976\n",
      "Iteration 28827, Loss: 0.055679481476545334\n",
      "Iteration 28828, Loss: 0.05567077919840813\n",
      "Iteration 28829, Loss: 0.055682383477687836\n",
      "Iteration 28830, Loss: 0.05567570775747299\n",
      "Iteration 28831, Loss: 0.05567729473114014\n",
      "Iteration 28832, Loss: 0.0556691512465477\n",
      "Iteration 28833, Loss: 0.05568405240774155\n",
      "Iteration 28834, Loss: 0.055676739662885666\n",
      "Iteration 28835, Loss: 0.05567634478211403\n",
      "Iteration 28836, Loss: 0.05566906929016113\n",
      "Iteration 28837, Loss: 0.05568408966064453\n",
      "Iteration 28838, Loss: 0.05567634105682373\n",
      "Iteration 28839, Loss: 0.0556768998503685\n",
      "Iteration 28840, Loss: 0.055669985711574554\n",
      "Iteration 28841, Loss: 0.055682700127363205\n",
      "Iteration 28842, Loss: 0.05567443370819092\n",
      "Iteration 28843, Loss: 0.05567876622080803\n",
      "Iteration 28844, Loss: 0.055672287940979004\n",
      "Iteration 28845, Loss: 0.05568023771047592\n",
      "Iteration 28846, Loss: 0.05567189306020737\n",
      "Iteration 28847, Loss: 0.05568154901266098\n",
      "Iteration 28848, Loss: 0.05567479133605957\n",
      "Iteration 28849, Loss: 0.055677615106105804\n",
      "Iteration 28850, Loss: 0.05566990375518799\n",
      "Iteration 28851, Loss: 0.05568377301096916\n",
      "Iteration 28852, Loss: 0.05567646026611328\n",
      "Iteration 28853, Loss: 0.05567622184753418\n",
      "Iteration 28854, Loss: 0.055669426918029785\n",
      "Iteration 28855, Loss: 0.05568389222025871\n",
      "Iteration 28856, Loss: 0.055675946176052094\n",
      "Iteration 28857, Loss: 0.0556768998503685\n",
      "Iteration 28858, Loss: 0.05567077919840813\n",
      "Iteration 28859, Loss: 0.055682264268398285\n",
      "Iteration 28860, Loss: 0.055673401802778244\n",
      "Iteration 28861, Loss: 0.055679719895124435\n",
      "Iteration 28862, Loss: 0.055673640221357346\n",
      "Iteration 28863, Loss: 0.05567936226725578\n",
      "Iteration 28864, Loss: 0.05567061901092529\n",
      "Iteration 28865, Loss: 0.055682502686977386\n",
      "Iteration 28866, Loss: 0.05567582696676254\n",
      "Iteration 28867, Loss: 0.05567721650004387\n",
      "Iteration 28868, Loss: 0.055669307708740234\n",
      "Iteration 28869, Loss: 0.055683813989162445\n",
      "Iteration 28870, Loss: 0.0556766614317894\n",
      "Iteration 28871, Loss: 0.055676303803920746\n",
      "Iteration 28872, Loss: 0.05566919222474098\n",
      "Iteration 28873, Loss: 0.05568397417664528\n",
      "Iteration 28874, Loss: 0.055675946176052094\n",
      "Iteration 28875, Loss: 0.05567733570933342\n",
      "Iteration 28876, Loss: 0.05567070096731186\n",
      "Iteration 28877, Loss: 0.05568218231201172\n",
      "Iteration 28878, Loss: 0.05567391961812973\n",
      "Iteration 28879, Loss: 0.0556795634329319\n",
      "Iteration 28880, Loss: 0.055673085153102875\n",
      "Iteration 28881, Loss: 0.055679403245449066\n",
      "Iteration 28882, Loss: 0.05567113682627678\n",
      "Iteration 28883, Loss: 0.05568265914916992\n",
      "Iteration 28884, Loss: 0.05567570775747299\n",
      "Iteration 28885, Loss: 0.05567658320069313\n",
      "Iteration 28886, Loss: 0.05566946789622307\n",
      "Iteration 28887, Loss: 0.05568552017211914\n",
      "Iteration 28888, Loss: 0.05567812919616699\n",
      "Iteration 28889, Loss: 0.055675189942121506\n",
      "Iteration 28890, Loss: 0.05566859245300293\n",
      "Iteration 28891, Loss: 0.05568607896566391\n",
      "Iteration 28892, Loss: 0.05567789077758789\n",
      "Iteration 28893, Loss: 0.05567582696676254\n",
      "Iteration 28894, Loss: 0.05567014217376709\n",
      "Iteration 28895, Loss: 0.05568389222025871\n",
      "Iteration 28896, Loss: 0.05567499250173569\n",
      "Iteration 28897, Loss: 0.055679045617580414\n",
      "Iteration 28898, Loss: 0.05567316338419914\n",
      "Iteration 28899, Loss: 0.05568107217550278\n",
      "Iteration 28900, Loss: 0.05567213147878647\n",
      "Iteration 28901, Loss: 0.0556817464530468\n",
      "Iteration 28902, Loss: 0.05567526817321777\n",
      "Iteration 28903, Loss: 0.05567900463938713\n",
      "Iteration 28904, Loss: 0.05567082017660141\n",
      "Iteration 28905, Loss: 0.05568305775523186\n",
      "Iteration 28906, Loss: 0.055675946176052094\n",
      "Iteration 28907, Loss: 0.05567833036184311\n",
      "Iteration 28908, Loss: 0.055670659989118576\n",
      "Iteration 28909, Loss: 0.05568321794271469\n",
      "Iteration 28910, Loss: 0.05567590519785881\n",
      "Iteration 28911, Loss: 0.055678486824035645\n",
      "Iteration 28912, Loss: 0.05567137524485588\n",
      "Iteration 28913, Loss: 0.05568266287446022\n",
      "Iteration 28914, Loss: 0.05567483231425285\n",
      "Iteration 28915, Loss: 0.05567944422364235\n",
      "Iteration 28916, Loss: 0.05567248910665512\n",
      "Iteration 28917, Loss: 0.055681269615888596\n",
      "Iteration 28918, Loss: 0.05567371845245361\n",
      "Iteration 28919, Loss: 0.05568099021911621\n",
      "Iteration 28920, Loss: 0.055673956871032715\n",
      "Iteration 28921, Loss: 0.055679600685834885\n",
      "Iteration 28922, Loss: 0.055671971291303635\n",
      "Iteration 28923, Loss: 0.05568262189626694\n",
      "Iteration 28924, Loss: 0.055675189942121506\n",
      "Iteration 28925, Loss: 0.055678367614746094\n",
      "Iteration 28926, Loss: 0.05567129701375961\n",
      "Iteration 28927, Loss: 0.05568313971161842\n",
      "Iteration 28928, Loss: 0.05567523092031479\n",
      "Iteration 28929, Loss: 0.05567844957113266\n",
      "Iteration 28930, Loss: 0.055671773850917816\n",
      "Iteration 28931, Loss: 0.05568253993988037\n",
      "Iteration 28932, Loss: 0.05567459389567375\n",
      "Iteration 28933, Loss: 0.05567944422364235\n",
      "Iteration 28934, Loss: 0.05567280575633049\n",
      "Iteration 28935, Loss: 0.05568123236298561\n",
      "Iteration 28936, Loss: 0.05567300692200661\n",
      "Iteration 28937, Loss: 0.055680952966213226\n",
      "Iteration 28938, Loss: 0.05567403882741928\n",
      "Iteration 28939, Loss: 0.05568007752299309\n",
      "Iteration 28940, Loss: 0.0556720495223999\n",
      "Iteration 28941, Loss: 0.055681828409433365\n",
      "Iteration 28942, Loss: 0.05567479133605957\n",
      "Iteration 28943, Loss: 0.05567952245473862\n",
      "Iteration 28944, Loss: 0.05567193031311035\n",
      "Iteration 28945, Loss: 0.05568202584981918\n",
      "Iteration 28946, Loss: 0.0556747131049633\n",
      "Iteration 28947, Loss: 0.055679839104413986\n",
      "Iteration 28948, Loss: 0.05567236989736557\n",
      "Iteration 28949, Loss: 0.05568158999085426\n",
      "Iteration 28950, Loss: 0.05567439645528793\n",
      "Iteration 28951, Loss: 0.05567988008260727\n",
      "Iteration 28952, Loss: 0.055672209709882736\n",
      "Iteration 28953, Loss: 0.05568190664052963\n",
      "Iteration 28954, Loss: 0.05567455664277077\n",
      "Iteration 28955, Loss: 0.05567936226725578\n",
      "Iteration 28956, Loss: 0.0556718148291111\n",
      "Iteration 28957, Loss: 0.055682223290205\n",
      "Iteration 28958, Loss: 0.05567499250173569\n",
      "Iteration 28959, Loss: 0.05567912384867668\n",
      "Iteration 28960, Loss: 0.0556718111038208\n",
      "Iteration 28961, Loss: 0.055682383477687836\n",
      "Iteration 28962, Loss: 0.055674754083156586\n",
      "Iteration 28963, Loss: 0.0556793212890625\n",
      "Iteration 28964, Loss: 0.05567232891917229\n",
      "Iteration 28965, Loss: 0.05568154901266098\n",
      "Iteration 28966, Loss: 0.05567384138703346\n",
      "Iteration 28967, Loss: 0.055681031197309494\n",
      "Iteration 28968, Loss: 0.055674076080322266\n",
      "Iteration 28969, Loss: 0.05568075552582741\n",
      "Iteration 28970, Loss: 0.05567304417490959\n",
      "Iteration 28971, Loss: 0.05568253993988037\n",
      "Iteration 28972, Loss: 0.05567546933889389\n",
      "Iteration 28973, Loss: 0.0556793250143528\n",
      "Iteration 28974, Loss: 0.0556718111038208\n",
      "Iteration 28975, Loss: 0.05568361282348633\n",
      "Iteration 28976, Loss: 0.05567622184753418\n",
      "Iteration 28977, Loss: 0.055678170174360275\n",
      "Iteration 28978, Loss: 0.05567117780447006\n",
      "Iteration 28979, Loss: 0.055684369057416916\n",
      "Iteration 28980, Loss: 0.05567658320069313\n",
      "Iteration 28981, Loss: 0.05567781254649162\n",
      "Iteration 28982, Loss: 0.05567121505737305\n",
      "Iteration 28983, Loss: 0.05568432807922363\n",
      "Iteration 28984, Loss: 0.05567610636353493\n",
      "Iteration 28985, Loss: 0.05567844957113266\n",
      "Iteration 28986, Loss: 0.05567213147878647\n",
      "Iteration 28987, Loss: 0.05568317696452141\n",
      "Iteration 28988, Loss: 0.0556744746863842\n",
      "Iteration 28989, Loss: 0.055680155754089355\n",
      "Iteration 28990, Loss: 0.055673997849226\n",
      "Iteration 28991, Loss: 0.055681150406599045\n",
      "Iteration 28992, Loss: 0.055672526359558105\n",
      "Iteration 28993, Loss: 0.055682502686977386\n",
      "Iteration 28994, Loss: 0.05567582696676254\n",
      "Iteration 28995, Loss: 0.0556793250143528\n",
      "Iteration 28996, Loss: 0.05567121505737305\n",
      "Iteration 28997, Loss: 0.055684011429548264\n",
      "Iteration 28998, Loss: 0.05567682161927223\n",
      "Iteration 28999, Loss: 0.05567821115255356\n",
      "Iteration 29000, Loss: 0.055670659989118576\n",
      "Iteration 29001, Loss: 0.0556841716170311\n",
      "Iteration 29002, Loss: 0.05567654222249985\n",
      "Iteration 29003, Loss: 0.055678606033325195\n",
      "Iteration 29004, Loss: 0.0556715726852417\n",
      "Iteration 29005, Loss: 0.05568321794271469\n",
      "Iteration 29006, Loss: 0.05567523092031479\n",
      "Iteration 29007, Loss: 0.05568011850118637\n",
      "Iteration 29008, Loss: 0.055673085153102875\n",
      "Iteration 29009, Loss: 0.05568154901266098\n",
      "Iteration 29010, Loss: 0.05567356199026108\n",
      "Iteration 29011, Loss: 0.05568210408091545\n",
      "Iteration 29012, Loss: 0.05567535012960434\n",
      "Iteration 29013, Loss: 0.05567912384867668\n",
      "Iteration 29014, Loss: 0.0556713342666626\n",
      "Iteration 29015, Loss: 0.055684447288513184\n",
      "Iteration 29016, Loss: 0.05567725747823715\n",
      "Iteration 29017, Loss: 0.05567697808146477\n",
      "Iteration 29018, Loss: 0.05566982552409172\n",
      "Iteration 29019, Loss: 0.055685918778181076\n",
      "Iteration 29020, Loss: 0.05567813292145729\n",
      "Iteration 29021, Loss: 0.05567634105682373\n",
      "Iteration 29022, Loss: 0.055670104920864105\n",
      "Iteration 29023, Loss: 0.055685482919216156\n",
      "Iteration 29024, Loss: 0.05567685887217522\n",
      "Iteration 29025, Loss: 0.05567797273397446\n",
      "Iteration 29026, Loss: 0.05567201226949692\n",
      "Iteration 29027, Loss: 0.05568309873342514\n",
      "Iteration 29028, Loss: 0.055674437433481216\n",
      "Iteration 29029, Loss: 0.05568055436015129\n",
      "Iteration 29030, Loss: 0.05567431449890137\n",
      "Iteration 29031, Loss: 0.055680952966213226\n",
      "Iteration 29032, Loss: 0.055672407150268555\n",
      "Iteration 29033, Loss: 0.05568254366517067\n",
      "Iteration 29034, Loss: 0.055675625801086426\n",
      "Iteration 29035, Loss: 0.0556793212890625\n",
      "Iteration 29036, Loss: 0.05567125603556633\n",
      "Iteration 29037, Loss: 0.05568361654877663\n",
      "Iteration 29038, Loss: 0.055676303803920746\n",
      "Iteration 29039, Loss: 0.05567880719900131\n",
      "Iteration 29040, Loss: 0.05567149445414543\n",
      "Iteration 29041, Loss: 0.05568329617381096\n",
      "Iteration 29042, Loss: 0.05567570775747299\n",
      "Iteration 29043, Loss: 0.055679600685834885\n",
      "Iteration 29044, Loss: 0.055672645568847656\n",
      "Iteration 29045, Loss: 0.05568202584981918\n",
      "Iteration 29046, Loss: 0.05567427724599838\n",
      "Iteration 29047, Loss: 0.05568122863769531\n",
      "Iteration 29048, Loss: 0.05567403882741928\n",
      "Iteration 29049, Loss: 0.05568047612905502\n",
      "Iteration 29050, Loss: 0.05567280575633049\n",
      "Iteration 29051, Loss: 0.05568274110555649\n",
      "Iteration 29052, Loss: 0.055675189942121506\n",
      "Iteration 29053, Loss: 0.055679284036159515\n",
      "Iteration 29054, Loss: 0.05567201226949692\n",
      "Iteration 29055, Loss: 0.05568329617381096\n",
      "Iteration 29056, Loss: 0.05567558854818344\n",
      "Iteration 29057, Loss: 0.055679164826869965\n",
      "Iteration 29058, Loss: 0.05567232891917229\n",
      "Iteration 29059, Loss: 0.05568305775523186\n",
      "Iteration 29060, Loss: 0.05567499250173569\n",
      "Iteration 29061, Loss: 0.05567999929189682\n",
      "Iteration 29062, Loss: 0.055673401802778244\n",
      "Iteration 29063, Loss: 0.05568186566233635\n",
      "Iteration 29064, Loss: 0.05567359924316406\n",
      "Iteration 29065, Loss: 0.055681269615888596\n",
      "Iteration 29066, Loss: 0.05567439645528793\n",
      "Iteration 29067, Loss: 0.055680833756923676\n",
      "Iteration 29068, Loss: 0.055672965943813324\n",
      "Iteration 29069, Loss: 0.05568190664052963\n",
      "Iteration 29070, Loss: 0.05567483231425285\n",
      "Iteration 29071, Loss: 0.05568051338195801\n",
      "Iteration 29072, Loss: 0.05567280575633049\n",
      "Iteration 29073, Loss: 0.05568202584981918\n",
      "Iteration 29074, Loss: 0.05567479133605957\n",
      "Iteration 29075, Loss: 0.05568063259124756\n",
      "Iteration 29076, Loss: 0.05567312240600586\n",
      "Iteration 29077, Loss: 0.05568162724375725\n",
      "Iteration 29078, Loss: 0.055673997849226\n",
      "Iteration 29079, Loss: 0.05568154901266098\n",
      "Iteration 29080, Loss: 0.0556742362678051\n",
      "Iteration 29081, Loss: 0.05568011850118637\n",
      "Iteration 29082, Loss: 0.05567268654704094\n",
      "Iteration 29083, Loss: 0.05568297952413559\n",
      "Iteration 29084, Loss: 0.05567554756999016\n",
      "Iteration 29085, Loss: 0.05567876622080803\n",
      "Iteration 29086, Loss: 0.055671654641628265\n",
      "Iteration 29087, Loss: 0.05568397417664528\n",
      "Iteration 29088, Loss: 0.05567622184753418\n",
      "Iteration 29089, Loss: 0.05567833036184311\n",
      "Iteration 29090, Loss: 0.05567161366343498\n",
      "Iteration 29091, Loss: 0.05568377301096916\n",
      "Iteration 29092, Loss: 0.05567590519785881\n",
      "Iteration 29093, Loss: 0.05567892640829086\n",
      "Iteration 29094, Loss: 0.05567241087555885\n",
      "Iteration 29095, Loss: 0.05568305775523186\n",
      "Iteration 29096, Loss: 0.05567483231425285\n",
      "Iteration 29097, Loss: 0.05568011850118637\n",
      "Iteration 29098, Loss: 0.05567348375916481\n",
      "Iteration 29099, Loss: 0.055681824684143066\n",
      "Iteration 29100, Loss: 0.05567348003387451\n",
      "Iteration 29101, Loss: 0.05568134784698486\n",
      "Iteration 29102, Loss: 0.05567467212677002\n",
      "Iteration 29103, Loss: 0.05568055436015129\n",
      "Iteration 29104, Loss: 0.05567248910665512\n",
      "Iteration 29105, Loss: 0.055682383477687836\n",
      "Iteration 29106, Loss: 0.05567542836070061\n",
      "Iteration 29107, Loss: 0.0556795634329319\n",
      "Iteration 29108, Loss: 0.05567161366343498\n",
      "Iteration 29109, Loss: 0.05568317696452141\n",
      "Iteration 29110, Loss: 0.05567602440714836\n",
      "Iteration 29111, Loss: 0.0556790865957737\n",
      "Iteration 29112, Loss: 0.055671535432338715\n",
      "Iteration 29113, Loss: 0.055683575570583344\n",
      "Iteration 29114, Loss: 0.05567614361643791\n",
      "Iteration 29115, Loss: 0.05567896366119385\n",
      "Iteration 29116, Loss: 0.055671773850917816\n",
      "Iteration 29117, Loss: 0.05568313971161842\n",
      "Iteration 29118, Loss: 0.05567514896392822\n",
      "Iteration 29119, Loss: 0.055680155754089355\n",
      "Iteration 29120, Loss: 0.055673323571681976\n",
      "Iteration 29121, Loss: 0.05568134784698486\n",
      "Iteration 29122, Loss: 0.05567336454987526\n",
      "Iteration 29123, Loss: 0.0556819848716259\n",
      "Iteration 29124, Loss: 0.055675189942121506\n",
      "Iteration 29125, Loss: 0.055679403245449066\n",
      "Iteration 29126, Loss: 0.05567173287272453\n",
      "Iteration 29127, Loss: 0.05568377301096916\n",
      "Iteration 29128, Loss: 0.05567654222249985\n",
      "Iteration 29129, Loss: 0.05567797273397446\n",
      "Iteration 29130, Loss: 0.05567101761698723\n",
      "Iteration 29131, Loss: 0.05568448826670647\n",
      "Iteration 29132, Loss: 0.05567670240998268\n",
      "Iteration 29133, Loss: 0.05567800998687744\n",
      "Iteration 29134, Loss: 0.05567145347595215\n",
      "Iteration 29135, Loss: 0.055683813989162445\n",
      "Iteration 29136, Loss: 0.05567558854818344\n",
      "Iteration 29137, Loss: 0.05567920207977295\n",
      "Iteration 29138, Loss: 0.05567256733775139\n",
      "Iteration 29139, Loss: 0.055682700127363205\n",
      "Iteration 29140, Loss: 0.05567443370819092\n",
      "Iteration 29141, Loss: 0.05568051338195801\n",
      "Iteration 29142, Loss: 0.055673640221357346\n",
      "Iteration 29143, Loss: 0.055681586265563965\n",
      "Iteration 29144, Loss: 0.05567368119955063\n",
      "Iteration 29145, Loss: 0.05568110942840576\n",
      "Iteration 29146, Loss: 0.05567427724599838\n",
      "Iteration 29147, Loss: 0.05568107217550278\n",
      "Iteration 29148, Loss: 0.055673085153102875\n",
      "Iteration 29149, Loss: 0.05568178743124008\n",
      "Iteration 29150, Loss: 0.05567459389567375\n",
      "Iteration 29151, Loss: 0.05568055436015129\n",
      "Iteration 29152, Loss: 0.05567276477813721\n",
      "Iteration 29153, Loss: 0.05568210408091545\n",
      "Iteration 29154, Loss: 0.05567503347992897\n",
      "Iteration 29155, Loss: 0.055680155754089355\n",
      "Iteration 29156, Loss: 0.05567260831594467\n",
      "Iteration 29157, Loss: 0.05568230152130127\n",
      "Iteration 29158, Loss: 0.05567491427063942\n",
      "Iteration 29159, Loss: 0.05568039417266846\n",
      "Iteration 29160, Loss: 0.05567288398742676\n",
      "Iteration 29161, Loss: 0.05568202584981918\n",
      "Iteration 29162, Loss: 0.05567455291748047\n",
      "Iteration 29163, Loss: 0.05568055436015129\n",
      "Iteration 29164, Loss: 0.05567336082458496\n",
      "Iteration 29165, Loss: 0.055681467056274414\n",
      "Iteration 29166, Loss: 0.05567396059632301\n",
      "Iteration 29167, Loss: 0.055681150406599045\n",
      "Iteration 29168, Loss: 0.05567391961812973\n",
      "Iteration 29169, Loss: 0.055680833756923676\n",
      "Iteration 29170, Loss: 0.05567348003387451\n",
      "Iteration 29171, Loss: 0.05568186566233635\n",
      "Iteration 29172, Loss: 0.05567459389567375\n",
      "Iteration 29173, Loss: 0.05568019673228264\n",
      "Iteration 29174, Loss: 0.05567292496562004\n",
      "Iteration 29175, Loss: 0.0556824617087841\n",
      "Iteration 29176, Loss: 0.055674873292446136\n",
      "Iteration 29177, Loss: 0.05567976087331772\n",
      "Iteration 29178, Loss: 0.05567260831594467\n",
      "Iteration 29179, Loss: 0.055682700127363205\n",
      "Iteration 29180, Loss: 0.055675070732831955\n",
      "Iteration 29181, Loss: 0.05567964166402817\n",
      "Iteration 29182, Loss: 0.055672649294137955\n",
      "Iteration 29183, Loss: 0.05568265914916992\n",
      "Iteration 29184, Loss: 0.0556747131049633\n",
      "Iteration 29185, Loss: 0.05568007752299309\n",
      "Iteration 29186, Loss: 0.05567312613129616\n",
      "Iteration 29187, Loss: 0.0556819848716259\n",
      "Iteration 29188, Loss: 0.055673837661743164\n",
      "Iteration 29189, Loss: 0.05568079277873039\n",
      "Iteration 29190, Loss: 0.05567387863993645\n",
      "Iteration 29191, Loss: 0.05568147078156471\n",
      "Iteration 29192, Loss: 0.05567356199026108\n",
      "Iteration 29193, Loss: 0.05568131059408188\n",
      "Iteration 29194, Loss: 0.05567427724599838\n",
      "Iteration 29195, Loss: 0.05568099021911621\n",
      "Iteration 29196, Loss: 0.05567336082458496\n",
      "Iteration 29197, Loss: 0.05568131059408188\n",
      "Iteration 29198, Loss: 0.0556742362678051\n",
      "Iteration 29199, Loss: 0.05568119138479233\n",
      "Iteration 29200, Loss: 0.05567356199026108\n",
      "Iteration 29201, Loss: 0.05568119138479233\n",
      "Iteration 29202, Loss: 0.05567387863993645\n",
      "Iteration 29203, Loss: 0.0556815080344677\n",
      "Iteration 29204, Loss: 0.05567403882741928\n",
      "Iteration 29205, Loss: 0.05568079277873039\n",
      "Iteration 29206, Loss: 0.05567328259348869\n",
      "Iteration 29207, Loss: 0.0556819848716259\n",
      "Iteration 29208, Loss: 0.055674318224191666\n",
      "Iteration 29209, Loss: 0.05568023771047592\n",
      "Iteration 29210, Loss: 0.05567304417490959\n",
      "Iteration 29211, Loss: 0.05568234249949455\n",
      "Iteration 29212, Loss: 0.05567479133605957\n",
      "Iteration 29213, Loss: 0.05568019673228264\n",
      "Iteration 29214, Loss: 0.05567312613129616\n",
      "Iteration 29215, Loss: 0.055682066828012466\n",
      "Iteration 29216, Loss: 0.05567427724599838\n",
      "Iteration 29217, Loss: 0.055680714547634125\n",
      "Iteration 29218, Loss: 0.05567371845245361\n",
      "Iteration 29219, Loss: 0.05568122863769531\n",
      "Iteration 29220, Loss: 0.05567336082458496\n",
      "Iteration 29221, Loss: 0.05568166822195053\n",
      "Iteration 29222, Loss: 0.055674634873867035\n",
      "Iteration 29223, Loss: 0.055680517107248306\n",
      "Iteration 29224, Loss: 0.05567280575633049\n",
      "Iteration 29225, Loss: 0.055682264268398285\n",
      "Iteration 29226, Loss: 0.05567546933889389\n",
      "Iteration 29227, Loss: 0.05567964166402817\n",
      "Iteration 29228, Loss: 0.055672287940979004\n",
      "Iteration 29229, Loss: 0.055682264268398285\n",
      "Iteration 29230, Loss: 0.05567455664277077\n",
      "Iteration 29231, Loss: 0.055680952966213226\n",
      "Iteration 29232, Loss: 0.055673997849226\n",
      "Iteration 29233, Loss: 0.05568023771047592\n",
      "Iteration 29234, Loss: 0.05567248910665512\n",
      "Iteration 29235, Loss: 0.05568305775523186\n",
      "Iteration 29236, Loss: 0.05567578598856926\n",
      "Iteration 29237, Loss: 0.055678486824035645\n",
      "Iteration 29238, Loss: 0.05567105859518051\n",
      "Iteration 29239, Loss: 0.05568429082632065\n",
      "Iteration 29240, Loss: 0.05567685887217522\n",
      "Iteration 29241, Loss: 0.05567769333720207\n",
      "Iteration 29242, Loss: 0.05567105859518051\n",
      "Iteration 29243, Loss: 0.05568432807922363\n",
      "Iteration 29244, Loss: 0.055676065385341644\n",
      "Iteration 29245, Loss: 0.05567856878042221\n",
      "Iteration 29246, Loss: 0.05567225068807602\n",
      "Iteration 29247, Loss: 0.055682819336652756\n",
      "Iteration 29248, Loss: 0.05567439645528793\n",
      "Iteration 29249, Loss: 0.05568047612905502\n",
      "Iteration 29250, Loss: 0.05567396059632301\n",
      "Iteration 29251, Loss: 0.05568091198801994\n",
      "Iteration 29252, Loss: 0.05567248910665512\n",
      "Iteration 29253, Loss: 0.05568234249949455\n",
      "Iteration 29254, Loss: 0.05567551031708717\n",
      "Iteration 29255, Loss: 0.05567944049835205\n",
      "Iteration 29256, Loss: 0.05567161366343498\n",
      "Iteration 29257, Loss: 0.055683255195617676\n",
      "Iteration 29258, Loss: 0.055675946176052094\n",
      "Iteration 29259, Loss: 0.0556790865957737\n",
      "Iteration 29260, Loss: 0.05567173287272453\n",
      "Iteration 29261, Loss: 0.05568289756774902\n",
      "Iteration 29262, Loss: 0.055675309151411057\n",
      "Iteration 29263, Loss: 0.055679719895124435\n",
      "Iteration 29264, Loss: 0.05567272752523422\n",
      "Iteration 29265, Loss: 0.05568194389343262\n",
      "Iteration 29266, Loss: 0.055674437433481216\n",
      "Iteration 29267, Loss: 0.05568051338195801\n",
      "Iteration 29268, Loss: 0.055673401802778244\n",
      "Iteration 29269, Loss: 0.055681467056274414\n",
      "Iteration 29270, Loss: 0.05567387863993645\n",
      "Iteration 29271, Loss: 0.055681269615888596\n",
      "Iteration 29272, Loss: 0.055673956871032715\n",
      "Iteration 29273, Loss: 0.05568119138479233\n",
      "Iteration 29274, Loss: 0.05567384138703346\n",
      "Iteration 29275, Loss: 0.055681269615888596\n",
      "Iteration 29276, Loss: 0.055673759430646896\n",
      "Iteration 29277, Loss: 0.055681150406599045\n",
      "Iteration 29278, Loss: 0.055673837661743164\n",
      "Iteration 29279, Loss: 0.055681150406599045\n",
      "Iteration 29280, Loss: 0.055673640221357346\n",
      "Iteration 29281, Loss: 0.055681031197309494\n",
      "Iteration 29282, Loss: 0.05567380040884018\n",
      "Iteration 29283, Loss: 0.05568135157227516\n",
      "Iteration 29284, Loss: 0.055673640221357346\n",
      "Iteration 29285, Loss: 0.05568107217550278\n",
      "Iteration 29286, Loss: 0.05567387863993645\n",
      "Iteration 29287, Loss: 0.05568119138479233\n",
      "Iteration 29288, Loss: 0.05567359924316406\n",
      "Iteration 29289, Loss: 0.055680952966213226\n",
      "Iteration 29290, Loss: 0.05567359924316406\n",
      "Iteration 29291, Loss: 0.05568162724375725\n",
      "Iteration 29292, Loss: 0.055674195289611816\n",
      "Iteration 29293, Loss: 0.05568043515086174\n",
      "Iteration 29294, Loss: 0.055673204362392426\n",
      "Iteration 29295, Loss: 0.05568202584981918\n",
      "Iteration 29296, Loss: 0.055674515664577484\n",
      "Iteration 29297, Loss: 0.05568007752299309\n",
      "Iteration 29298, Loss: 0.05567280575633049\n",
      "Iteration 29299, Loss: 0.05568253993988037\n",
      "Iteration 29300, Loss: 0.055674951523542404\n",
      "Iteration 29301, Loss: 0.05567964166402817\n",
      "Iteration 29302, Loss: 0.05567248910665512\n",
      "Iteration 29303, Loss: 0.05568274110555649\n",
      "Iteration 29304, Loss: 0.05567491054534912\n",
      "Iteration 29305, Loss: 0.05567976087331772\n",
      "Iteration 29306, Loss: 0.05567300319671631\n",
      "Iteration 29307, Loss: 0.055682145059108734\n",
      "Iteration 29308, Loss: 0.05567411705851555\n",
      "Iteration 29309, Loss: 0.055680714547634125\n",
      "Iteration 29310, Loss: 0.05567380040884018\n",
      "Iteration 29311, Loss: 0.05568135157227516\n",
      "Iteration 29312, Loss: 0.05567328259348869\n",
      "Iteration 29313, Loss: 0.055681388825178146\n",
      "Iteration 29314, Loss: 0.05567443370819092\n",
      "Iteration 29315, Loss: 0.05568067356944084\n",
      "Iteration 29316, Loss: 0.05567276477813721\n",
      "Iteration 29317, Loss: 0.05568210408091545\n",
      "Iteration 29318, Loss: 0.05567499250173569\n",
      "Iteration 29319, Loss: 0.05568011850118637\n",
      "Iteration 29320, Loss: 0.05567260831594467\n",
      "Iteration 29321, Loss: 0.05568206310272217\n",
      "Iteration 29322, Loss: 0.0556747131049633\n",
      "Iteration 29323, Loss: 0.05568063631653786\n",
      "Iteration 29324, Loss: 0.055673323571681976\n",
      "Iteration 29325, Loss: 0.05568131059408188\n",
      "Iteration 29326, Loss: 0.05567391961812973\n",
      "Iteration 29327, Loss: 0.05568131059408188\n",
      "Iteration 29328, Loss: 0.05567387863993645\n",
      "Iteration 29329, Loss: 0.055680833756923676\n",
      "Iteration 29330, Loss: 0.05567344278097153\n",
      "Iteration 29331, Loss: 0.05568162724375725\n",
      "Iteration 29332, Loss: 0.055673956871032715\n",
      "Iteration 29333, Loss: 0.05568075552582741\n",
      "Iteration 29334, Loss: 0.05567368119955063\n",
      "Iteration 29335, Loss: 0.05568131059408188\n",
      "Iteration 29336, Loss: 0.05567356199026108\n",
      "Iteration 29337, Loss: 0.055681388825178146\n",
      "Iteration 29338, Loss: 0.05567439645528793\n",
      "Iteration 29339, Loss: 0.05568063259124756\n",
      "Iteration 29340, Loss: 0.05567272752523422\n",
      "Iteration 29341, Loss: 0.055682145059108734\n",
      "Iteration 29342, Loss: 0.05567499250173569\n",
      "Iteration 29343, Loss: 0.05568007752299309\n",
      "Iteration 29344, Loss: 0.055672287940979004\n",
      "Iteration 29345, Loss: 0.05568242073059082\n",
      "Iteration 29346, Loss: 0.05567523092031479\n",
      "Iteration 29347, Loss: 0.0556800402700901\n",
      "Iteration 29348, Loss: 0.055672645568847656\n",
      "Iteration 29349, Loss: 0.05568202584981918\n",
      "Iteration 29350, Loss: 0.05567455291748047\n",
      "Iteration 29351, Loss: 0.05568067356944084\n",
      "Iteration 29352, Loss: 0.05567360296845436\n",
      "Iteration 29353, Loss: 0.055680952966213226\n",
      "Iteration 29354, Loss: 0.05567336082458496\n",
      "Iteration 29355, Loss: 0.055682066828012466\n",
      "Iteration 29356, Loss: 0.0556747131049633\n",
      "Iteration 29357, Loss: 0.05567964166402817\n",
      "Iteration 29358, Loss: 0.055672090500593185\n",
      "Iteration 29359, Loss: 0.05568321794271469\n",
      "Iteration 29360, Loss: 0.05567554756999016\n",
      "Iteration 29361, Loss: 0.05567896366119385\n",
      "Iteration 29362, Loss: 0.0556720495223999\n",
      "Iteration 29363, Loss: 0.05568333715200424\n",
      "Iteration 29364, Loss: 0.05567535012960434\n",
      "Iteration 29365, Loss: 0.055679403245449066\n",
      "Iteration 29366, Loss: 0.055672887712717056\n",
      "Iteration 29367, Loss: 0.05568202584981918\n",
      "Iteration 29368, Loss: 0.055673759430646896\n",
      "Iteration 29369, Loss: 0.05568119138479233\n",
      "Iteration 29370, Loss: 0.05567467212677002\n",
      "Iteration 29371, Loss: 0.055680274963378906\n",
      "Iteration 29372, Loss: 0.05567201226949692\n",
      "Iteration 29373, Loss: 0.05568289756774902\n",
      "Iteration 29374, Loss: 0.05567590519785881\n",
      "Iteration 29375, Loss: 0.05567912384867668\n",
      "Iteration 29376, Loss: 0.055671416223049164\n",
      "Iteration 29377, Loss: 0.055683255195617676\n",
      "Iteration 29378, Loss: 0.05567602440714836\n",
      "Iteration 29379, Loss: 0.055679045617580414\n",
      "Iteration 29380, Loss: 0.055671971291303635\n",
      "Iteration 29381, Loss: 0.05568265914916992\n",
      "Iteration 29382, Loss: 0.055675070732831955\n",
      "Iteration 29383, Loss: 0.05568007752299309\n",
      "Iteration 29384, Loss: 0.05567288398742676\n",
      "Iteration 29385, Loss: 0.05568162724375725\n",
      "Iteration 29386, Loss: 0.055673997849226\n",
      "Iteration 29387, Loss: 0.05568119138479233\n",
      "Iteration 29388, Loss: 0.055673997849226\n",
      "Iteration 29389, Loss: 0.05568043515086174\n",
      "Iteration 29390, Loss: 0.055672965943813324\n",
      "Iteration 29391, Loss: 0.055682264268398285\n",
      "Iteration 29392, Loss: 0.055674754083156586\n",
      "Iteration 29393, Loss: 0.05567999929189682\n",
      "Iteration 29394, Loss: 0.05567268654704094\n",
      "Iteration 29395, Loss: 0.055682383477687836\n",
      "Iteration 29396, Loss: 0.05567455291748047\n",
      "Iteration 29397, Loss: 0.05568011850118637\n",
      "Iteration 29398, Loss: 0.055673204362392426\n",
      "Iteration 29399, Loss: 0.05568162724375725\n",
      "Iteration 29400, Loss: 0.055673759430646896\n",
      "Iteration 29401, Loss: 0.05568099394440651\n",
      "Iteration 29402, Loss: 0.05567391961812973\n",
      "Iteration 29403, Loss: 0.05568122863769531\n",
      "Iteration 29404, Loss: 0.05567344278097153\n",
      "Iteration 29405, Loss: 0.05568122863769531\n",
      "Iteration 29406, Loss: 0.05567403882741928\n",
      "Iteration 29407, Loss: 0.05568107217550278\n",
      "Iteration 29408, Loss: 0.05567344278097153\n",
      "Iteration 29409, Loss: 0.05568099021911621\n",
      "Iteration 29410, Loss: 0.05567368119955063\n",
      "Iteration 29411, Loss: 0.05568154901266098\n",
      "Iteration 29412, Loss: 0.05567403882741928\n",
      "Iteration 29413, Loss: 0.05568031594157219\n",
      "Iteration 29414, Loss: 0.05567312240600586\n",
      "Iteration 29415, Loss: 0.055682223290205\n",
      "Iteration 29416, Loss: 0.05567467212677002\n",
      "Iteration 29417, Loss: 0.05567964166402817\n",
      "Iteration 29418, Loss: 0.05567244812846184\n",
      "Iteration 29419, Loss: 0.055682819336652756\n",
      "Iteration 29420, Loss: 0.055675309151411057\n",
      "Iteration 29421, Loss: 0.055679164826869965\n",
      "Iteration 29422, Loss: 0.05567216873168945\n",
      "Iteration 29423, Loss: 0.05568309873342514\n",
      "Iteration 29424, Loss: 0.05567514896392822\n",
      "Iteration 29425, Loss: 0.05567936226725578\n",
      "Iteration 29426, Loss: 0.055672645568847656\n",
      "Iteration 29427, Loss: 0.055682580918073654\n",
      "Iteration 29428, Loss: 0.055674634873867035\n",
      "Iteration 29429, Loss: 0.05568011850118637\n",
      "Iteration 29430, Loss: 0.055673204362392426\n",
      "Iteration 29431, Loss: 0.05568166822195053\n",
      "Iteration 29432, Loss: 0.05567368119955063\n",
      "Iteration 29433, Loss: 0.055681150406599045\n",
      "Iteration 29434, Loss: 0.05567431449890137\n",
      "Iteration 29435, Loss: 0.05568063631653786\n",
      "Iteration 29436, Loss: 0.05567241087555885\n",
      "Iteration 29437, Loss: 0.05568234249949455\n",
      "Iteration 29438, Loss: 0.05567558854818344\n",
      "Iteration 29439, Loss: 0.05567912384867668\n",
      "Iteration 29440, Loss: 0.05567129701375961\n",
      "Iteration 29441, Loss: 0.05568365380167961\n",
      "Iteration 29442, Loss: 0.055676501244306564\n",
      "Iteration 29443, Loss: 0.055678367614746094\n",
      "Iteration 29444, Loss: 0.055670857429504395\n",
      "Iteration 29445, Loss: 0.055683933198451996\n",
      "Iteration 29446, Loss: 0.055676303803920746\n",
      "Iteration 29447, Loss: 0.05567876622080803\n",
      "Iteration 29448, Loss: 0.055671773850917816\n",
      "Iteration 29449, Loss: 0.055682819336652756\n",
      "Iteration 29450, Loss: 0.05567479133605957\n",
      "Iteration 29451, Loss: 0.05568035691976547\n",
      "Iteration 29452, Loss: 0.05567344278097153\n",
      "Iteration 29453, Loss: 0.05568119138479233\n",
      "Iteration 29454, Loss: 0.055673401802778244\n",
      "Iteration 29455, Loss: 0.05568178743124008\n",
      "Iteration 29456, Loss: 0.05567459389567375\n",
      "Iteration 29457, Loss: 0.05567988008260727\n",
      "Iteration 29458, Loss: 0.055672530084848404\n",
      "Iteration 29459, Loss: 0.055682700127363205\n",
      "Iteration 29460, Loss: 0.05567514896392822\n",
      "Iteration 29461, Loss: 0.055679403245449066\n",
      "Iteration 29462, Loss: 0.05567248910665512\n",
      "Iteration 29463, Loss: 0.055682383477687836\n",
      "Iteration 29464, Loss: 0.05567459389567375\n",
      "Iteration 29465, Loss: 0.05568023771047592\n",
      "Iteration 29466, Loss: 0.055673401802778244\n",
      "Iteration 29467, Loss: 0.05568142980337143\n",
      "Iteration 29468, Loss: 0.05567336454987526\n",
      "Iteration 29469, Loss: 0.05568142980337143\n",
      "Iteration 29470, Loss: 0.05567455664277077\n",
      "Iteration 29471, Loss: 0.05568023771047592\n",
      "Iteration 29472, Loss: 0.0556722916662693\n",
      "Iteration 29473, Loss: 0.055682502686977386\n",
      "Iteration 29474, Loss: 0.05567554756999016\n",
      "Iteration 29475, Loss: 0.05567936226725578\n",
      "Iteration 29476, Loss: 0.05567173287272453\n",
      "Iteration 29477, Loss: 0.055683016777038574\n",
      "Iteration 29478, Loss: 0.05567566677927971\n",
      "Iteration 29479, Loss: 0.05567936226725578\n",
      "Iteration 29480, Loss: 0.05567213147878647\n",
      "Iteration 29481, Loss: 0.05568262189626694\n",
      "Iteration 29482, Loss: 0.05567483231425285\n",
      "Iteration 29483, Loss: 0.05568031594157219\n",
      "Iteration 29484, Loss: 0.05567316338419914\n",
      "Iteration 29485, Loss: 0.055681269615888596\n",
      "Iteration 29486, Loss: 0.05567348375916481\n",
      "Iteration 29487, Loss: 0.05568158999085426\n",
      "Iteration 29488, Loss: 0.05567443370819092\n",
      "Iteration 29489, Loss: 0.05567999929189682\n",
      "Iteration 29490, Loss: 0.05567248910665512\n",
      "Iteration 29491, Loss: 0.05568242445588112\n",
      "Iteration 29492, Loss: 0.05567503347992897\n",
      "Iteration 29493, Loss: 0.05567952245473862\n",
      "Iteration 29494, Loss: 0.05567236989736557\n",
      "Iteration 29495, Loss: 0.05568274110555649\n",
      "Iteration 29496, Loss: 0.055675070732831955\n",
      "Iteration 29497, Loss: 0.05567967891693115\n",
      "Iteration 29498, Loss: 0.05567292496562004\n",
      "Iteration 29499, Loss: 0.055682223290205\n",
      "Iteration 29500, Loss: 0.05567411705851555\n",
      "Iteration 29501, Loss: 0.05568051338195801\n",
      "Iteration 29502, Loss: 0.055673759430646896\n",
      "Iteration 29503, Loss: 0.05568142980337143\n",
      "Iteration 29504, Loss: 0.05567344278097153\n",
      "Iteration 29505, Loss: 0.05568110942840576\n",
      "Iteration 29506, Loss: 0.055673997849226\n",
      "Iteration 29507, Loss: 0.055680952966213226\n",
      "Iteration 29508, Loss: 0.055673323571681976\n",
      "Iteration 29509, Loss: 0.05568110942840576\n",
      "Iteration 29510, Loss: 0.05567356199026108\n",
      "Iteration 29511, Loss: 0.05568131059408188\n",
      "Iteration 29512, Loss: 0.055673997849226\n",
      "Iteration 29513, Loss: 0.05568035691976547\n",
      "Iteration 29514, Loss: 0.05567316338419914\n",
      "Iteration 29515, Loss: 0.05568178743124008\n",
      "Iteration 29516, Loss: 0.055674076080322266\n",
      "Iteration 29517, Loss: 0.05568051338195801\n",
      "Iteration 29518, Loss: 0.05567360296845436\n",
      "Iteration 29519, Loss: 0.055681269615888596\n",
      "Iteration 29520, Loss: 0.05567336454987526\n",
      "Iteration 29521, Loss: 0.05568142980337143\n",
      "Iteration 29522, Loss: 0.0556744746863842\n",
      "Iteration 29523, Loss: 0.05568007752299309\n",
      "Iteration 29524, Loss: 0.05567217245697975\n",
      "Iteration 29525, Loss: 0.05568266287446022\n",
      "Iteration 29526, Loss: 0.05567574501037598\n",
      "Iteration 29527, Loss: 0.0556788444519043\n",
      "Iteration 29528, Loss: 0.05567121505737305\n",
      "Iteration 29529, Loss: 0.05568385124206543\n",
      "Iteration 29530, Loss: 0.055676501244306564\n",
      "Iteration 29531, Loss: 0.05567812919616699\n",
      "Iteration 29532, Loss: 0.05567077919840813\n",
      "Iteration 29533, Loss: 0.05568408966064453\n",
      "Iteration 29534, Loss: 0.055676303803920746\n",
      "Iteration 29535, Loss: 0.05567856878042221\n",
      "Iteration 29536, Loss: 0.05567189306020737\n",
      "Iteration 29537, Loss: 0.05568265914916992\n",
      "Iteration 29538, Loss: 0.05567439645528793\n",
      "Iteration 29539, Loss: 0.055680833756923676\n",
      "Iteration 29540, Loss: 0.05567435547709465\n",
      "Iteration 29541, Loss: 0.05567967891693115\n",
      "Iteration 29542, Loss: 0.05567149445414543\n",
      "Iteration 29543, Loss: 0.0556841716170311\n",
      "Iteration 29544, Loss: 0.05567725747823715\n",
      "Iteration 29545, Loss: 0.0556766614317894\n",
      "Iteration 29546, Loss: 0.05566906929016113\n",
      "Iteration 29547, Loss: 0.05568643659353256\n",
      "Iteration 29548, Loss: 0.0556788444519043\n",
      "Iteration 29549, Loss: 0.05567542836070061\n",
      "Iteration 29550, Loss: 0.0556691512465477\n",
      "Iteration 29551, Loss: 0.05568595975637436\n",
      "Iteration 29552, Loss: 0.05567741394042969\n",
      "Iteration 29553, Loss: 0.05567721650004387\n",
      "Iteration 29554, Loss: 0.05567161366343498\n",
      "Iteration 29555, Loss: 0.05568317696452141\n",
      "Iteration 29556, Loss: 0.05567411705851555\n",
      "Iteration 29557, Loss: 0.05568051338195801\n",
      "Iteration 29558, Loss: 0.0556742362678051\n",
      "Iteration 29559, Loss: 0.05568067356944084\n",
      "Iteration 29560, Loss: 0.05567213147878647\n",
      "Iteration 29561, Loss: 0.05568242073059082\n",
      "Iteration 29562, Loss: 0.055675629526376724\n",
      "Iteration 29563, Loss: 0.055679403245449066\n",
      "Iteration 29564, Loss: 0.05567145347595215\n",
      "Iteration 29565, Loss: 0.055683135986328125\n",
      "Iteration 29566, Loss: 0.05567598715424538\n",
      "Iteration 29567, Loss: 0.0556790828704834\n",
      "Iteration 29568, Loss: 0.05567149445414543\n",
      "Iteration 29569, Loss: 0.05568317696452141\n",
      "Iteration 29570, Loss: 0.05567570775747299\n",
      "Iteration 29571, Loss: 0.05567924305796623\n",
      "Iteration 29572, Loss: 0.055671971291303635\n",
      "Iteration 29573, Loss: 0.055682502686977386\n",
      "Iteration 29574, Loss: 0.055674873292446136\n",
      "Iteration 29575, Loss: 0.05568011850118637\n",
      "Iteration 29576, Loss: 0.05567316338419914\n",
      "Iteration 29577, Loss: 0.055681467056274414\n",
      "Iteration 29578, Loss: 0.05567371845245361\n",
      "Iteration 29579, Loss: 0.05568131059408188\n",
      "Iteration 29580, Loss: 0.055674199014902115\n",
      "Iteration 29581, Loss: 0.05568031594157219\n",
      "Iteration 29582, Loss: 0.055672965943813324\n",
      "Iteration 29583, Loss: 0.05568218603730202\n",
      "Iteration 29584, Loss: 0.05567467212677002\n",
      "Iteration 29585, Loss: 0.05567995831370354\n",
      "Iteration 29586, Loss: 0.05567288398742676\n",
      "Iteration 29587, Loss: 0.05568206310272217\n",
      "Iteration 29588, Loss: 0.05567439645528793\n",
      "Iteration 29589, Loss: 0.05568007752299309\n",
      "Iteration 29590, Loss: 0.05567304417490959\n",
      "Iteration 29591, Loss: 0.05568186566233635\n",
      "Iteration 29592, Loss: 0.055674195289611816\n",
      "Iteration 29593, Loss: 0.05568039417266846\n",
      "Iteration 29594, Loss: 0.05567312240600586\n",
      "Iteration 29595, Loss: 0.05568186566233635\n",
      "Iteration 29596, Loss: 0.055674079805612564\n",
      "Iteration 29597, Loss: 0.055680274963378906\n",
      "Iteration 29598, Loss: 0.05567316338419914\n",
      "Iteration 29599, Loss: 0.055681824684143066\n",
      "Iteration 29600, Loss: 0.055673997849226\n",
      "Iteration 29601, Loss: 0.05568047612905502\n",
      "Iteration 29602, Loss: 0.05567368119955063\n",
      "Iteration 29603, Loss: 0.05568131059408188\n",
      "Iteration 29604, Loss: 0.055673323571681976\n",
      "Iteration 29605, Loss: 0.055681388825178146\n",
      "Iteration 29606, Loss: 0.05567459389567375\n",
      "Iteration 29607, Loss: 0.05568019673228264\n",
      "Iteration 29608, Loss: 0.05567213147878647\n",
      "Iteration 29609, Loss: 0.055682580918073654\n",
      "Iteration 29610, Loss: 0.05567574501037598\n",
      "Iteration 29611, Loss: 0.05567896366119385\n",
      "Iteration 29612, Loss: 0.05567125603556633\n",
      "Iteration 29613, Loss: 0.05568353459239006\n",
      "Iteration 29614, Loss: 0.05567622557282448\n",
      "Iteration 29615, Loss: 0.05567844957113266\n",
      "Iteration 29616, Loss: 0.05567117780447006\n",
      "Iteration 29617, Loss: 0.055683575570583344\n",
      "Iteration 29618, Loss: 0.05567586421966553\n",
      "Iteration 29619, Loss: 0.055679164826869965\n",
      "Iteration 29620, Loss: 0.05567225068807602\n",
      "Iteration 29621, Loss: 0.055682145059108734\n",
      "Iteration 29622, Loss: 0.05567427724599838\n",
      "Iteration 29623, Loss: 0.05568099021911621\n",
      "Iteration 29624, Loss: 0.05567403882741928\n",
      "Iteration 29625, Loss: 0.055680274963378906\n",
      "Iteration 29626, Loss: 0.05567236989736557\n",
      "Iteration 29627, Loss: 0.055683016777038574\n",
      "Iteration 29628, Loss: 0.055675946176052094\n",
      "Iteration 29629, Loss: 0.05567833036184311\n",
      "Iteration 29630, Loss: 0.055670980364084244\n",
      "Iteration 29631, Loss: 0.05568432807922363\n",
      "Iteration 29632, Loss: 0.05567678064107895\n",
      "Iteration 29633, Loss: 0.05567777156829834\n",
      "Iteration 29634, Loss: 0.05567113682627678\n",
      "Iteration 29635, Loss: 0.05568409338593483\n",
      "Iteration 29636, Loss: 0.05567578598856926\n",
      "Iteration 29637, Loss: 0.05567876622080803\n",
      "Iteration 29638, Loss: 0.05567248910665512\n",
      "Iteration 29639, Loss: 0.05568234249949455\n",
      "Iteration 29640, Loss: 0.055673956871032715\n",
      "Iteration 29641, Loss: 0.05568067356944084\n",
      "Iteration 29642, Loss: 0.05567403882741928\n",
      "Iteration 29643, Loss: 0.05568087100982666\n",
      "Iteration 29644, Loss: 0.05567260831594467\n",
      "Iteration 29645, Loss: 0.055682145059108734\n",
      "Iteration 29646, Loss: 0.05567527189850807\n",
      "Iteration 29647, Loss: 0.0556797981262207\n",
      "Iteration 29648, Loss: 0.055671971291303635\n",
      "Iteration 29649, Loss: 0.05568265914916992\n",
      "Iteration 29650, Loss: 0.05567542836070061\n",
      "Iteration 29651, Loss: 0.055679600685834885\n",
      "Iteration 29652, Loss: 0.0556720495223999\n",
      "Iteration 29653, Loss: 0.055682580918073654\n",
      "Iteration 29654, Loss: 0.05567503347992897\n",
      "Iteration 29655, Loss: 0.05567999929189682\n",
      "Iteration 29656, Loss: 0.05567276477813721\n",
      "Iteration 29657, Loss: 0.05568178743124008\n",
      "Iteration 29658, Loss: 0.055674076080322266\n",
      "Iteration 29659, Loss: 0.05568119138479233\n",
      "Iteration 29660, Loss: 0.055673997849226\n",
      "Iteration 29661, Loss: 0.05568023771047592\n",
      "Iteration 29662, Loss: 0.05567244812846184\n",
      "Iteration 29663, Loss: 0.05568286031484604\n",
      "Iteration 29664, Loss: 0.055675629526376724\n",
      "Iteration 29665, Loss: 0.05567852780222893\n",
      "Iteration 29666, Loss: 0.05567137524485588\n",
      "Iteration 29667, Loss: 0.05568405240774155\n",
      "Iteration 29668, Loss: 0.05567646026611328\n",
      "Iteration 29669, Loss: 0.055677734315395355\n",
      "Iteration 29670, Loss: 0.05567101761698723\n",
      "Iteration 29671, Loss: 0.055684369057416916\n",
      "Iteration 29672, Loss: 0.05567626282572746\n",
      "Iteration 29673, Loss: 0.05567809194326401\n",
      "Iteration 29674, Loss: 0.055671773850917816\n",
      "Iteration 29675, Loss: 0.05568329617381096\n",
      "Iteration 29676, Loss: 0.055674754083156586\n",
      "Iteration 29677, Loss: 0.05567976087331772\n",
      "Iteration 29678, Loss: 0.05567359924316406\n",
      "Iteration 29679, Loss: 0.05568110942840576\n",
      "Iteration 29680, Loss: 0.05567256733775139\n",
      "Iteration 29681, Loss: 0.055682264268398285\n",
      "Iteration 29682, Loss: 0.05567582696676254\n",
      "Iteration 29683, Loss: 0.0556788444519043\n",
      "Iteration 29684, Loss: 0.05567058175802231\n",
      "Iteration 29685, Loss: 0.055684369057416916\n",
      "Iteration 29686, Loss: 0.055677495896816254\n",
      "Iteration 29687, Loss: 0.0556771382689476\n",
      "Iteration 29688, Loss: 0.055669546127319336\n",
      "Iteration 29689, Loss: 0.05568516254425049\n",
      "Iteration 29690, Loss: 0.055677615106105804\n",
      "Iteration 29691, Loss: 0.0556773766875267\n",
      "Iteration 29692, Loss: 0.05567061901092529\n",
      "Iteration 29693, Loss: 0.055683933198451996\n",
      "Iteration 29694, Loss: 0.055675625801086426\n",
      "Iteration 29695, Loss: 0.05567952245473862\n",
      "Iteration 29696, Loss: 0.055673085153102875\n",
      "Iteration 29697, Loss: 0.055681150406599045\n",
      "Iteration 29698, Loss: 0.05567288398742676\n",
      "Iteration 29699, Loss: 0.05568242445588112\n",
      "Iteration 29700, Loss: 0.05567586421966553\n",
      "Iteration 29701, Loss: 0.05567825213074684\n",
      "Iteration 29702, Loss: 0.055670540779829025\n",
      "Iteration 29703, Loss: 0.05568492412567139\n",
      "Iteration 29704, Loss: 0.05567765608429909\n",
      "Iteration 29705, Loss: 0.05567669868469238\n",
      "Iteration 29706, Loss: 0.05566966533660889\n",
      "Iteration 29707, Loss: 0.055685561150312424\n",
      "Iteration 29708, Loss: 0.055677495896816254\n",
      "Iteration 29709, Loss: 0.055677175521850586\n",
      "Iteration 29710, Loss: 0.05567101761698723\n",
      "Iteration 29711, Loss: 0.05568373203277588\n",
      "Iteration 29712, Loss: 0.055674873292446136\n",
      "Iteration 29713, Loss: 0.05567995831370354\n",
      "Iteration 29714, Loss: 0.055673837661743164\n",
      "Iteration 29715, Loss: 0.05568063259124756\n",
      "Iteration 29716, Loss: 0.05567213147878647\n",
      "Iteration 29717, Loss: 0.05568266287446022\n",
      "Iteration 29718, Loss: 0.055676184594631195\n",
      "Iteration 29719, Loss: 0.05567876622080803\n",
      "Iteration 29720, Loss: 0.05567105859518051\n",
      "Iteration 29721, Loss: 0.055683694779872894\n",
      "Iteration 29722, Loss: 0.0556764230132103\n",
      "Iteration 29723, Loss: 0.05567840859293938\n",
      "Iteration 29724, Loss: 0.05567117780447006\n",
      "Iteration 29725, Loss: 0.05568349361419678\n",
      "Iteration 29726, Loss: 0.05567566677927971\n",
      "Iteration 29727, Loss: 0.055679403245449066\n",
      "Iteration 29728, Loss: 0.05567248910665512\n",
      "Iteration 29729, Loss: 0.05568190664052963\n",
      "Iteration 29730, Loss: 0.05567384138703346\n",
      "Iteration 29731, Loss: 0.055681467056274414\n",
      "Iteration 29732, Loss: 0.05567455664277077\n",
      "Iteration 29733, Loss: 0.055679801851511\n",
      "Iteration 29734, Loss: 0.055672209709882736\n",
      "Iteration 29735, Loss: 0.05568321794271469\n",
      "Iteration 29736, Loss: 0.05567590519785881\n",
      "Iteration 29737, Loss: 0.05567837134003639\n",
      "Iteration 29738, Loss: 0.05567105859518051\n",
      "Iteration 29739, Loss: 0.055684249848127365\n",
      "Iteration 29740, Loss: 0.05567646399140358\n",
      "Iteration 29741, Loss: 0.05567809194326401\n",
      "Iteration 29742, Loss: 0.055671337991952896\n",
      "Iteration 29743, Loss: 0.05568377301096916\n",
      "Iteration 29744, Loss: 0.055675387382507324\n",
      "Iteration 29745, Loss: 0.0556793250143528\n",
      "Iteration 29746, Loss: 0.05567300319671631\n",
      "Iteration 29747, Loss: 0.05568178743124008\n",
      "Iteration 29748, Loss: 0.05567328259348869\n",
      "Iteration 29749, Loss: 0.05568162724375725\n",
      "Iteration 29750, Loss: 0.05567499250173569\n",
      "Iteration 29751, Loss: 0.055679719895124435\n",
      "Iteration 29752, Loss: 0.055671654641628265\n",
      "Iteration 29753, Loss: 0.05568309873342514\n",
      "Iteration 29754, Loss: 0.05567622184753418\n",
      "Iteration 29755, Loss: 0.05567876622080803\n",
      "Iteration 29756, Loss: 0.05567117780447006\n",
      "Iteration 29757, Loss: 0.055683694779872894\n",
      "Iteration 29758, Loss: 0.055675946176052094\n",
      "Iteration 29759, Loss: 0.0556790865957737\n",
      "Iteration 29760, Loss: 0.0556720495223999\n",
      "Iteration 29761, Loss: 0.05568242445588112\n",
      "Iteration 29762, Loss: 0.05567455291748047\n",
      "Iteration 29763, Loss: 0.05568051338195801\n",
      "Iteration 29764, Loss: 0.055673521012067795\n",
      "Iteration 29765, Loss: 0.05568075552582741\n",
      "Iteration 29766, Loss: 0.05567304417490959\n",
      "Iteration 29767, Loss: 0.05568218231201172\n",
      "Iteration 29768, Loss: 0.05567511171102524\n",
      "Iteration 29769, Loss: 0.055679164826869965\n",
      "Iteration 29770, Loss: 0.055671852082014084\n",
      "Iteration 29771, Loss: 0.05568321794271469\n",
      "Iteration 29772, Loss: 0.05567570775747299\n",
      "Iteration 29773, Loss: 0.055678725242614746\n",
      "Iteration 29774, Loss: 0.05567189306020737\n",
      "Iteration 29775, Loss: 0.05568309873342514\n",
      "Iteration 29776, Loss: 0.055675309151411057\n",
      "Iteration 29777, Loss: 0.05567964166402817\n",
      "Iteration 29778, Loss: 0.05567292496562004\n",
      "Iteration 29779, Loss: 0.055681947618722916\n",
      "Iteration 29780, Loss: 0.055673759430646896\n",
      "Iteration 29781, Loss: 0.055680952966213226\n",
      "Iteration 29782, Loss: 0.05567411705851555\n",
      "Iteration 29783, Loss: 0.055680714547634125\n",
      "Iteration 29784, Loss: 0.05567280575633049\n",
      "Iteration 29785, Loss: 0.05568202584981918\n",
      "Iteration 29786, Loss: 0.05567483231425285\n",
      "Iteration 29787, Loss: 0.05568011850118637\n",
      "Iteration 29788, Loss: 0.05567244812846184\n",
      "Iteration 29789, Loss: 0.055682066828012466\n",
      "Iteration 29790, Loss: 0.055674754083156586\n",
      "Iteration 29791, Loss: 0.05568047612905502\n",
      "Iteration 29792, Loss: 0.055673204362392426\n",
      "Iteration 29793, Loss: 0.055681388825178146\n",
      "Iteration 29794, Loss: 0.055673956871032715\n",
      "Iteration 29795, Loss: 0.05568119138479233\n",
      "Iteration 29796, Loss: 0.055673640221357346\n",
      "Iteration 29797, Loss: 0.055680952966213226\n",
      "Iteration 29798, Loss: 0.05567344278097153\n",
      "Iteration 29799, Loss: 0.055681705474853516\n",
      "Iteration 29800, Loss: 0.055674195289611816\n",
      "Iteration 29801, Loss: 0.05568039417266846\n",
      "Iteration 29802, Loss: 0.055673204362392426\n",
      "Iteration 29803, Loss: 0.05568206310272217\n",
      "Iteration 29804, Loss: 0.055674318224191666\n",
      "Iteration 29805, Loss: 0.055680155754089355\n",
      "Iteration 29806, Loss: 0.055672965943813324\n",
      "Iteration 29807, Loss: 0.055682383477687836\n",
      "Iteration 29808, Loss: 0.055674754083156586\n",
      "Iteration 29809, Loss: 0.05567999929189682\n",
      "Iteration 29810, Loss: 0.05567304417490959\n",
      "Iteration 29811, Loss: 0.055682264268398285\n",
      "Iteration 29812, Loss: 0.05567439645528793\n",
      "Iteration 29813, Loss: 0.05568035691976547\n",
      "Iteration 29814, Loss: 0.05567348003387451\n",
      "Iteration 29815, Loss: 0.055681586265563965\n",
      "Iteration 29816, Loss: 0.05567368119955063\n",
      "Iteration 29817, Loss: 0.05568087100982666\n",
      "Iteration 29818, Loss: 0.055673956871032715\n",
      "Iteration 29819, Loss: 0.055681150406599045\n",
      "Iteration 29820, Loss: 0.05567328259348869\n",
      "Iteration 29821, Loss: 0.055681269615888596\n",
      "Iteration 29822, Loss: 0.05567403882741928\n",
      "Iteration 29823, Loss: 0.055680952966213226\n",
      "Iteration 29824, Loss: 0.05567336454987526\n",
      "Iteration 29825, Loss: 0.05568142980337143\n",
      "Iteration 29826, Loss: 0.055674195289611816\n",
      "Iteration 29827, Loss: 0.055680952966213226\n",
      "Iteration 29828, Loss: 0.05567336454987526\n",
      "Iteration 29829, Loss: 0.05568131059408188\n",
      "Iteration 29830, Loss: 0.055673956871032715\n",
      "Iteration 29831, Loss: 0.055681150406599045\n",
      "Iteration 29832, Loss: 0.055673640221357346\n",
      "Iteration 29833, Loss: 0.05568099021911621\n",
      "Iteration 29834, Loss: 0.05567371845245361\n",
      "Iteration 29835, Loss: 0.05568154901266098\n",
      "Iteration 29836, Loss: 0.05567396059632301\n",
      "Iteration 29837, Loss: 0.055680833756923676\n",
      "Iteration 29838, Loss: 0.05567356199026108\n",
      "Iteration 29839, Loss: 0.055681586265563965\n",
      "Iteration 29840, Loss: 0.05567403882741928\n",
      "Iteration 29841, Loss: 0.05568063259124756\n",
      "Iteration 29842, Loss: 0.05567336082458496\n",
      "Iteration 29843, Loss: 0.05568190664052963\n",
      "Iteration 29844, Loss: 0.05567435547709465\n",
      "Iteration 29845, Loss: 0.05568019673228264\n",
      "Iteration 29846, Loss: 0.055672965943813324\n",
      "Iteration 29847, Loss: 0.0556824617087841\n",
      "Iteration 29848, Loss: 0.055674951523542404\n",
      "Iteration 29849, Loss: 0.05567964166402817\n",
      "Iteration 29850, Loss: 0.05567268654704094\n",
      "Iteration 29851, Loss: 0.05568265914916992\n",
      "Iteration 29852, Loss: 0.0556747131049633\n",
      "Iteration 29853, Loss: 0.055679917335510254\n",
      "Iteration 29854, Loss: 0.05567292496562004\n",
      "Iteration 29855, Loss: 0.055682383477687836\n",
      "Iteration 29856, Loss: 0.0556744746863842\n",
      "Iteration 29857, Loss: 0.05568043515086174\n",
      "Iteration 29858, Loss: 0.05567371845245361\n",
      "Iteration 29859, Loss: 0.05568154901266098\n",
      "Iteration 29860, Loss: 0.055673401802778244\n",
      "Iteration 29861, Loss: 0.05568135157227516\n",
      "Iteration 29862, Loss: 0.055674515664577484\n",
      "Iteration 29863, Loss: 0.05568055436015129\n",
      "Iteration 29864, Loss: 0.05567272752523422\n",
      "Iteration 29865, Loss: 0.055682066828012466\n",
      "Iteration 29866, Loss: 0.055674951523542404\n",
      "Iteration 29867, Loss: 0.05568019673228264\n",
      "Iteration 29868, Loss: 0.055672530084848404\n",
      "Iteration 29869, Loss: 0.05568218603730202\n",
      "Iteration 29870, Loss: 0.055674873292446136\n",
      "Iteration 29871, Loss: 0.05568031594157219\n",
      "Iteration 29872, Loss: 0.05567292496562004\n",
      "Iteration 29873, Loss: 0.05568194389343262\n",
      "Iteration 29874, Loss: 0.05567455291748047\n",
      "Iteration 29875, Loss: 0.05568055436015129\n",
      "Iteration 29876, Loss: 0.055672965943813324\n",
      "Iteration 29877, Loss: 0.0556817464530468\n",
      "Iteration 29878, Loss: 0.05567439645528793\n",
      "Iteration 29879, Loss: 0.055680833756923676\n",
      "Iteration 29880, Loss: 0.05567344278097153\n",
      "Iteration 29881, Loss: 0.05568122863769531\n",
      "Iteration 29882, Loss: 0.05567384138703346\n",
      "Iteration 29883, Loss: 0.05568154901266098\n",
      "Iteration 29884, Loss: 0.05567415803670883\n",
      "Iteration 29885, Loss: 0.05568023771047592\n",
      "Iteration 29886, Loss: 0.05567300692200661\n",
      "Iteration 29887, Loss: 0.055682580918073654\n",
      "Iteration 29888, Loss: 0.055675070732831955\n",
      "Iteration 29889, Loss: 0.05567952245473862\n",
      "Iteration 29890, Loss: 0.05567232891917229\n",
      "Iteration 29891, Loss: 0.05568309873342514\n",
      "Iteration 29892, Loss: 0.05567535012960434\n",
      "Iteration 29893, Loss: 0.0556790828704834\n",
      "Iteration 29894, Loss: 0.055672287940979004\n",
      "Iteration 29895, Loss: 0.05568302050232887\n",
      "Iteration 29896, Loss: 0.055674873292446136\n",
      "Iteration 29897, Loss: 0.05567976087331772\n",
      "Iteration 29898, Loss: 0.05567312613129616\n",
      "Iteration 29899, Loss: 0.0556819848716259\n",
      "Iteration 29900, Loss: 0.05567372217774391\n",
      "Iteration 29901, Loss: 0.055681150406599045\n",
      "Iteration 29902, Loss: 0.05567443370819092\n",
      "Iteration 29903, Loss: 0.05568055436015129\n",
      "Iteration 29904, Loss: 0.05567232891917229\n",
      "Iteration 29905, Loss: 0.055682580918073654\n",
      "Iteration 29906, Loss: 0.05567566677927971\n",
      "Iteration 29907, Loss: 0.055679481476545334\n",
      "Iteration 29908, Loss: 0.055671654641628265\n",
      "Iteration 29909, Loss: 0.05568329617381096\n",
      "Iteration 29910, Loss: 0.055676184594631195\n",
      "Iteration 29911, Loss: 0.055678967386484146\n",
      "Iteration 29912, Loss: 0.05567137524485588\n",
      "Iteration 29913, Loss: 0.05568353459239006\n",
      "Iteration 29914, Loss: 0.05567598715424538\n",
      "Iteration 29915, Loss: 0.055679045617580414\n",
      "Iteration 29916, Loss: 0.05567201226949692\n",
      "Iteration 29917, Loss: 0.05568277835845947\n",
      "Iteration 29918, Loss: 0.05567514896392822\n",
      "Iteration 29919, Loss: 0.05567999929189682\n",
      "Iteration 29920, Loss: 0.05567300692200661\n",
      "Iteration 29921, Loss: 0.05568162724375725\n",
      "Iteration 29922, Loss: 0.05567380040884018\n",
      "Iteration 29923, Loss: 0.05568154901266098\n",
      "Iteration 29924, Loss: 0.05567459389567375\n",
      "Iteration 29925, Loss: 0.0556797981262207\n",
      "Iteration 29926, Loss: 0.055672287940979004\n",
      "Iteration 29927, Loss: 0.055683255195617676\n",
      "Iteration 29928, Loss: 0.05567610263824463\n",
      "Iteration 29929, Loss: 0.05567824840545654\n",
      "Iteration 29930, Loss: 0.05567101761698723\n",
      "Iteration 29931, Loss: 0.05568460747599602\n",
      "Iteration 29932, Loss: 0.05567678064107895\n",
      "Iteration 29933, Loss: 0.05567800998687744\n",
      "Iteration 29934, Loss: 0.055671416223049164\n",
      "Iteration 29935, Loss: 0.055683813989162445\n",
      "Iteration 29936, Loss: 0.05567542836070061\n",
      "Iteration 29937, Loss: 0.055679600685834885\n",
      "Iteration 29938, Loss: 0.055673204362392426\n",
      "Iteration 29939, Loss: 0.05568162724375725\n",
      "Iteration 29940, Loss: 0.05567300319671631\n",
      "Iteration 29941, Loss: 0.05568202584981918\n",
      "Iteration 29942, Loss: 0.055675387382507324\n",
      "Iteration 29943, Loss: 0.0556795597076416\n",
      "Iteration 29944, Loss: 0.05567137524485588\n",
      "Iteration 29945, Loss: 0.05568349361419678\n",
      "Iteration 29946, Loss: 0.055676382035017014\n",
      "Iteration 29947, Loss: 0.055678486824035645\n",
      "Iteration 29948, Loss: 0.055670857429504395\n",
      "Iteration 29949, Loss: 0.0556841716170311\n",
      "Iteration 29950, Loss: 0.055676817893981934\n",
      "Iteration 29951, Loss: 0.05567821115255356\n",
      "Iteration 29952, Loss: 0.055670980364084244\n",
      "Iteration 29953, Loss: 0.055683933198451996\n",
      "Iteration 29954, Loss: 0.05567614361643791\n",
      "Iteration 29955, Loss: 0.05567900463938713\n",
      "Iteration 29956, Loss: 0.05567213147878647\n",
      "Iteration 29957, Loss: 0.05568277835845947\n",
      "Iteration 29958, Loss: 0.0556747131049633\n",
      "Iteration 29959, Loss: 0.05568039417266846\n",
      "Iteration 29960, Loss: 0.05567360296845436\n",
      "Iteration 29961, Loss: 0.05568091198801994\n",
      "Iteration 29962, Loss: 0.055673085153102875\n",
      "Iteration 29963, Loss: 0.05568234249949455\n",
      "Iteration 29964, Loss: 0.055675309151411057\n",
      "Iteration 29965, Loss: 0.05567924305796623\n",
      "Iteration 29966, Loss: 0.05567161366343498\n",
      "Iteration 29967, Loss: 0.055683813989162445\n",
      "Iteration 29968, Loss: 0.05567614361643791\n",
      "Iteration 29969, Loss: 0.055678289383649826\n",
      "Iteration 29970, Loss: 0.05567129701375961\n",
      "Iteration 29971, Loss: 0.055684130638837814\n",
      "Iteration 29972, Loss: 0.055676184594631195\n",
      "Iteration 29973, Loss: 0.05567840859293938\n",
      "Iteration 29974, Loss: 0.055671773850917816\n",
      "Iteration 29975, Loss: 0.05568329617381096\n",
      "Iteration 29976, Loss: 0.05567502975463867\n",
      "Iteration 29977, Loss: 0.055679600685834885\n",
      "Iteration 29978, Loss: 0.05567324534058571\n",
      "Iteration 29979, Loss: 0.05568190664052963\n",
      "Iteration 29980, Loss: 0.05567344278097153\n",
      "Iteration 29981, Loss: 0.05568131059408188\n",
      "Iteration 29982, Loss: 0.055674754083156586\n",
      "Iteration 29983, Loss: 0.05568023771047592\n",
      "Iteration 29984, Loss: 0.05567213147878647\n",
      "Iteration 29985, Loss: 0.05568277835845947\n",
      "Iteration 29986, Loss: 0.055675748735666275\n",
      "Iteration 29987, Loss: 0.05567944049835205\n",
      "Iteration 29988, Loss: 0.055671773850917816\n",
      "Iteration 29989, Loss: 0.05568321794271469\n",
      "Iteration 29990, Loss: 0.05567570775747299\n",
      "Iteration 29991, Loss: 0.05567944049835205\n",
      "Iteration 29992, Loss: 0.0556722916662693\n",
      "Iteration 29993, Loss: 0.05568242073059082\n",
      "Iteration 29994, Loss: 0.055674754083156586\n",
      "Iteration 29995, Loss: 0.055680595338344574\n",
      "Iteration 29996, Loss: 0.05567359924316406\n",
      "Iteration 29997, Loss: 0.055680714547634125\n",
      "Iteration 29998, Loss: 0.05567304417490959\n",
      "Iteration 29999, Loss: 0.05568262189626694\n",
      "Computed Homography from GDRectifier:\n",
      "[[ 9.9999249e-01 -6.9046287e-06  0.0000000e+00]\n",
      " [ 4.2759834e-06  1.0000292e+00  0.0000000e+00]\n",
      " [ 8.6135871e-05 -5.7591293e-03  1.0000000e+00]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from HomoTopiContinuation.Rectifier.GDRectifier import GDRectifier\n",
    "\n",
    "H_computed, _, _ = GDRectifier.rectify(   \n",
    "    img.C_img_noise,\n",
    "    alpha=0.00000001,\n",
    "    iterations=30000,\n",
    ")\n",
    "\n",
    "print(\"Computed Homography from GDRectifier:\")\n",
    "print(H_computed.H)\n",
    "\n",
    "warpedConicsGD = ConicWarper().warpConics(img.C_img_noise, H_computed)\n",
    "\n",
    "plotter = Plotter.Plotter(2, 2, title=\"Warped Scene with Circles (GDRectifier)\")\n",
    "plotter.plotExperiment(\n",
    "    sceneDescription=sceneDescription,\n",
    "    img=img,\n",
    "    warpedConics=warpedConicsGD,\n",
    ")\n",
    "plotter.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
